{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Features selection'\n",
        "author: \"Cheryl KOUADIO\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Activity 1 : Mutual Information and HSIC\n",
        "\n",
        "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features. \n",
        "\n",
        "The usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\n",
        "\n",
        "In this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n",
        "\n",
        "$$I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)$$\n",
        "\n",
        "\n",
        "where $p(x,y)$ is the joint probability distribution function of X and Y, and $p(x)$ and $p(y)$ are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\n",
        "\n",
        "\n",
        "\n",
        "In order to have confidence in this measure, we will consider a bivariate gaussian variable $Z=(X,Y)$ with mean $\\mu = (0,0)$ and covariance matrix $\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}$. We will compute the mutual information between X and Y for a grid a $\\rho$ between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to $-\\frac{1}{2} \\log(1-\\rho^2)$.\n",
        "\n",
        "## 1. Accuracy of mutual information estimation\n"
      ],
      "id": "38e2a364"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Function to generate bivariate Gaussian data\n",
        "def generate_bivariate_gaussian(n, rho):\n",
        "    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n",
        "    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n",
        "    mean = [mu_X, mu_Y]\n",
        "    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n",
        "           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n",
        "    return np.random.multivariate_normal(mean, cov, size=n)\n",
        "\n",
        "# Function to compute true mutual information\n",
        "def true_mutual_information(rho):\n",
        "    return -0.5 * np.log(1 - rho**2)"
      ],
      "id": "5c3b23ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize parameters\n",
        "n_samples = 1000\n",
        "n_repeats = 100\n",
        "rho_values = np.linspace(0.01, 0.99, 10)\n",
        "\n",
        "# Store mutual information estimates\n",
        "estimated_mi = []\n",
        "true_mi_values = []\n",
        "\n",
        "for rho in rho_values:\n",
        "    mi_estimates = []\n",
        "    for _ in range(n_repeats):\n",
        "        # Generate data\n",
        "        data_test = generate_bivariate_gaussian(n_samples, rho)\n",
        "        X = data_test[:, 0].reshape(-1, 1)  # Feature\n",
        "        Y = data_test[:, 1]  # Target\n",
        "        \n",
        "        # Estimate mutual information\n",
        "        mi = mutual_info_regression(X, Y, discrete_features=False)\n",
        "        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n",
        "        \n",
        "    # Store results\n",
        "    estimated_mi.append(mi_estimates)\n",
        "    true_mi_values.append(true_mutual_information(rho))\n",
        "\n",
        "# Convert estimated MI to array for easy plotting\n",
        "estimated_mi = np.array(estimated_mi)\n",
        "\n",
        "# Plot boxplots of the estimated MI for each value of rho\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=estimated_mi.T)\n",
        "plt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\n",
        "plt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\n",
        "plt.xlabel(r'$\\rho$')\n",
        "plt.ylabel('Mutual Information')\n",
        "plt.title('Estimated vs True Mutual Information')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "0250d1c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter $\\rho$ increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function `mutual_info_regression` is well implemented and can be used to select the relevant features in a dataset.\n",
        "\n",
        "## 2. Comparison between mutual information and correlation tests\n",
        "\n",
        "\n",
        "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal $X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)$ and the 5 last variables are uniform $X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)$. We will consider the target variable $Y$ as a linear combination of some variables :\n",
        "\n",
        "$$ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})$$\n"
      ],
      "id": "f229dc23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to generate data\n",
        "def generate_data(n):\n",
        "    data = pd.DataFrame()\n",
        "    for i in range(15):\n",
        "        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n",
        "        \n",
        "    for i in range(5):\n",
        "        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n",
        "    pi = math.pi\n",
        "    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n",
        "                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n",
        "                \n",
        "    return data"
      ],
      "id": "6dd43f06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# compute the correlation between each feature and the target\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "concat_spearman = []\n",
        "concat_pearson = []\n",
        "\n",
        "for sim in range(50):\n",
        "    data = generate_data(500)\n",
        "    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n",
        "    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n",
        "    concat_spearman.append(np.abs(spearman_corr))\n",
        "    concat_pearson.append(np.abs(pearson_corr))"
      ],
      "id": "44f91bd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "feature_names = data.columns[:-1]  # Get feature names from the dataset\n",
        "\n",
        "spearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\n",
        "pearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n",
        "\n",
        "# Spearman correlations\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=spearman_df)\n",
        "plt.title('Spearman Correlations for Each Feature')\n",
        "plt.ylabel('Spearman Correlation')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=90) \n",
        "plt.show()"
      ],
      "id": "fc7ee4a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pearson correlations\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=pearson_df)\n",
        "plt.title('Pearson Correlations for Each Feature')\n",
        "plt.ylabel('Pearson Correlation')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=90) \n",
        "plt.show()"
      ],
      "id": "f448a292",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables $X_1$, $X_4$ and $X_{19}$ related to the target variable $Y$. If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as $X_1$, $X_4$, $X_6$, $X_7$, and $X_{19}$, however some irrelevant features are also selected such as $X_2$ and $X_{11}$.\n"
      ],
      "id": "60f6bf41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mutual_info = []\n",
        "\n",
        "for sim in range(50):\n",
        "    data = generate_data(500)\n",
        "    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n",
        "    mutual_info.append(mi_corr)\n",
        "\n",
        "mutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n",
        "\n",
        "# Plotting boxplots for Spearman correlations\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=mutual_info)\n",
        "plt.title('Mutual information for each feature')\n",
        "plt.ylabel('Mutual info')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=90)  # Rotate feature names if there are many features\n",
        "plt.show()"
      ],
      "id": "b6194b14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n",
        "\n",
        "### What about feature selection with lasso regression?\n",
        "\n",
        "We might be interested in the behavior of the lasso regression in the same dataset. We will use the `Lasso` class from the `sklearn.linear_model` module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the `LassoCV` class to select the best value of the regularization parameter $\\alpha$ using cross-validation.\n"
      ],
      "id": "617ae08a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "n_simulations = 50\n",
        "n_samples = 500\n",
        "n_features = data.shape[1] - 1  \n",
        "\n",
        "selected_features = np.zeros((n_simulations, n_features))\n",
        "scaler = StandardScaler()\n",
        "for sim in range(n_simulations):\n",
        "    # Generate data\n",
        "    data = generate_data(n_samples)\n",
        "    X = data.drop(columns=[\"Y\"]) \n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    y = data[\"Y\"] \n",
        "    \n",
        "    lasso = LassoCV().fit(X_scaled, y)\n",
        "    \n",
        "    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)"
      ],
      "id": "3341d765",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Frequency of selection for each feature\n",
        "selection_frequency = np.mean(selected_features, axis=0)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "selection_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Selection Frequency': selection_frequency\n",
        "})\n",
        "\n",
        "\n",
        "# You can also visualize this with a bar plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\n",
        "plt.title('Feature Selection Frequency by Lasso')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "id": "2b7d986e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n",
        "\n",
        "## 3. Hilbert-Schmidt Independence Criterion\n",
        "\n",
        "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n",
        "\n",
        "$$MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]$$\n",
        "\n",
        "where $k$ is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the `MMD` class from the `sklearn.metrics.pairwise` module to compute the MMD between the variables. For a continuous variables, this writes :\n",
        "\n",
        "$$MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'$$\n",
        "\n",
        "where $p$ and $q$ are the probability distribution functions of X and Y.\n",
        "\n",
        "This is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n",
        "\n",
        "$$HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)$$\n",
        "\n",
        "where $K$ is the kernel matrix of X and $L$ is the kernel matrix of Y and H is the centering matrix $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T$.\n",
        "Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n",
        "\n",
        "$$k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)$$\n"
      ],
      "id": "2341fb63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sobolev_kernel(x,y):\n",
        "    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n",
        "\n",
        "def compute_gram_matrix(z):\n",
        "    z = np.asarray(z)\n",
        "    M = sobolev_kernel(z[:,None],z[None,:])\n",
        "    return M\n",
        "\n",
        "def hsic(X, Y):\n",
        "    n = X.shape[0]\n",
        "    H = np.eye(n) - (1 / n) * np.ones((n, n))\n",
        "\n",
        "    # Compute Gram matrices with Sobolev kernel\n",
        "    K = compute_gram_matrix(X)\n",
        "    L = compute_gram_matrix(Y)\n",
        "\n",
        "    # Calculate HSIC\n",
        "    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n",
        "    return hsic_value"
      ],
      "id": "2d4db7ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "hsic_values = []\n",
        "\n",
        "for sim in range(50):\n",
        "    data = generate_data(500)\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n",
        "    hsic_values.append(hsic_var)\n",
        "\n",
        "hsic_df = pd.DataFrame(hsic_values, columns=feature_names)"
      ],
      "id": "4a589fdb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plotting boxplots for Spearman correlations\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data=hsic_df)\n",
        "plt.title('HSIC for Each Feature')\n",
        "plt.ylabel('HSIC')\n",
        "plt.xlabel('Feature')\n",
        "plt.xticks(rotation=90)  # Rotate feature names if there are many features\n",
        "plt.show()"
      ],
      "id": "c0122fe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold selection for mutual information and HSIC\n",
        "\n",
        "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset.\n",
        "As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n"
      ],
      "id": "0302dcd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.utils import resample\n",
        "data = generate_data(500)\n",
        "\n",
        "Y = data[\"Y\"]\n",
        "X = data.drop(columns=[\"Y\"])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "X_scaled = data_scaled.drop(columns=[\"Y\"])\n",
        "Y_scaled = data_scaled[\"Y\"]\n",
        "\n",
        "mi_train = mutual_info_regression(X, Y)\n",
        "\n",
        "n_rep = 500\n",
        "all_miXYindep = np.zeros((n_rep, n_features))\n",
        "all_HSICindep = np.zeros((n_rep, n_features))\n",
        "\n",
        "for rep in range(n_rep):\n",
        "    # Permutation\n",
        "    yb = resample(Y, replace = False)\n",
        "    # Compute mutual information between all features and Y\n",
        "    mi_temp = mutual_info_regression(X, np.ravel(yb))\n",
        "    # Store the MI from this repetition\n",
        "    all_miXYindep[rep, :] = mi_temp\n",
        "    # Permutation\n",
        "    yb_train = resample(Y_scaled, replace = False)\n",
        "\n",
        "\n",
        "p_values_mi = np.mean(mi_train < all_miXYindep, axis=0)"
      ],
      "id": "a41e6455",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(p_values_mi, 'o')\n",
        "plt.title('Mutual Information p-value')\n",
        "plt.axhline(y=0.05, color='r', linestyle='-')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('p-value')\n",
        "plt.show()"
      ],
      "id": "6b9e960a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n"
      ],
      "id": "a7644763"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Selected variables with MI p-values %s \" % np.where(p_values_mi < 0.05))"
      ],
      "id": "c4baf3b9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/cherylkouadio/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}