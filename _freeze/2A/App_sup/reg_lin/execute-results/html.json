{
  "hash": "6ddb000f83c03e8216f8a94948eb0672",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: La régression linéaire\nauthor:\n  - Cheryl Kouadio\n---\n\n\n# Introduction\nLa régression linéaire est une méthode d'apprentissage supervisé qui vise à évaluer, lorsqu'il existe, la relation linéaire entre une variable d'intérêt et des variables explicatives.\n\nPour un ensemble $(y_i,x_i)$ de données constitué de n échantillons iid (indépendant et identiquement distribué), le modèle de regression linéaire s'écrit comme suit :\n\n\n```{=tex}\n\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\n```\n\noù $y_i$ est la variable cible, $x_{i1}, \\dots, x_{ip}$ sont les variables explicatives et $\\xi_i$ est l'erreur, l'information que les autres variables explicatives ne donnent pas.\n\nL'hypothèse fondamentale de la régression linéaire est l'existence d'une relation linéaire entre la variable cible et les variables explicatives. Pour s'assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire (à l'aide de visualisation ou de tests- spearman, pearson, etc.)\n\nL'hypothèse de rang plein est la seconde plus grande hypothèse, elle stipule que les variables explicatives ne soient pas corrélées entre elles. Cette condition est nécessaire pour garantir l'unicité des estimations des paramètres du modèle et ainsi l'identifiabilité du modèle étudié\n\nPar ailleurs pour que les estimations des paramètres du modèle linéaire soient fiables, les erreurs du modèle, représentées par $\\xi_i$, doivent répondre à plusieurs critères :\n\n-   **Erreurs centrées** : La moyenne attendue des erreurs doit être nulle, soit $E[\\xi_i] = 0$. Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\n-   **Homoscédasticité** : La variance des erreurs doit être constante pour toutes les observations, exprimée par $V[\\xi_i] = \\sigma^2$. Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\n-   **Décorrélation des erreurs** : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d'erreurs est nulle, $Cov(\\xi_i, \\xi_j) = 0$ pour $1\\leq i \\neq j \\leq n$. Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d'observer une hypothèse supplémentaire sur la loi des erreurs. En effet, les erreurs sont souvent supposées suivre une loi normale, c'est à dire que $\\xi_i \\sim N(0, \\sigma^2)$. Celà nous permet de faire des inférences sur les paramètres du modèle et de construire des intervalles de confiance.\n\n## Estimation des paramètres\n\nToutes les hypothèses étant respectées, et sous reserve qu'il n'y a pas de multicolinéarité entre les variables explicatives du modèles i.e. $X^T X$ est inversible(l'hypothèse de rang plein est respectée), l'estimateur $\\hat \\beta$ de $\\beta$ obtenus par moindre carré ordinaire est donné par la formule suivante :\n\n$$\n\\hat \\beta = (X^T X)^{-1} X^T y\n$$\n\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \n$$\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n$$\n\nD'après le théorème de Gauss-Markov, l'estimateur $\\hat \\beta$ est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l'estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\n\nLorsque les erreurs sont supposées suivre une loi normale, l'estimateur $\\hat \\beta$ est également l'estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale $\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})$.\n\nL'estimateur de la variance des erreurs $\\sigma^2$ est donné par :\n\n$$\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n$$ \nSCR = somme des carrés des résidus.\n\n## Evaluation du modèle\n\nDans l'optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l'erreur quadratique moyenne (MSE), des critères d'informations (AIC, BIC) etc.\n\n### 1. Coefficient de détermination R²\nLe coefficient de détermination $R^2$ est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\n$$\nR^2 = 1 - \\frac{SCR}{SCT}\n$$\navec SCT qui est la somme des carrés totaux ($SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2$) et SCR qui est la somme des carrés des résidus.\n\nNéanmoins, le $R^2$ n'est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n'ont pas de lien avec la variable cible. Pour pallier à ce problème, le $R^2$ ajusté est utilisé. Il est défini comme suit :\n\n$$\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n$$\n\n### 2. Critères d'information (AIC, BIC)\n\nLes critères AIC et BIC sont des critères d'information qui servent à mesurer l'attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l'échantillon étudié. De fait, plus l'AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu'il a le modèle choisie a une probabilité plus élevée d'être correct et une complexité plus faible. \n\nMathématiquement, \n\n$$\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n$$\n\n$$\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n$$\n\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque $\\log(n) \\geq 2$, i.e il y a environ 8 observations dans l'échantillon sélectionné.\n\n# Application\n\n## 1. Simulation des données\nPour évaluer l'intérêt de la regréssion linéaire, nous allons simuler un échantillon de taille n=200, où la variable cible Y est une fonction linéaire de la variable explicative X. La vraie relation est donnée par $Y = 2 + 3X + \\epsilon$, où $\\epsilon \\sim N(0, 1.6)$. De fait le modèle linéaire est adéquat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(314)\nn<-200\nX<-runif(n,0,10)\n\nsigma2<-1.6\nepsilon<-rnorm(n,0,sigma2)\nY<- 2 + 3*X + epsilon\n\nsim1<-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n```\n\n::: {.cell-output-display}\n![](reg_lin_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nEn ajustant un modèle linéaire simple à nos données, nous obtenons une estimation des paramètres $\\hat \\beta_0 = 1.92$ et $\\hat \\beta_1 = 2.98$. Les erreurs du modèle suivent une loi normale avec une variance $\\hat \\sigma^2 = 1.45$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim1_lm<-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.92097    0.20774   9.247   <2e-16 ***\nX            2.97748    0.03582  83.117   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,\tAdjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## 2. Evaluation du modèle\n\n### 2.1. Hypothèses sur les erreurs et l'existence d'une relation linéaire\n\nPour évaluer la qualité du modèle, nous allons tracer les résidus studentisés en fonction des valeurs ajustées. Les résidus studentisés sont les résidus divisés par l'écart-type des erreurs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajustées\", ylab=\"Résidus studentisés\")\nabline(h=0,lty=2)\n```\n\n::: {.cell-output-display}\n![](reg_lin_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nLe plot ci dessus nous montre que lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression, est bien adaptée aux données, et donc que l’hypothèse de linéarité est acceptable.\n\nSi l'on observait une forme de trompette, celà reviendrait à soulever une question sur l’hétéroscédascité des résidus, tandis qu'une forme de banane revèle plutôt une relation de non-linéarité.\n\nLorsque le nuage de point n’a pas de structure particulière, a priori l’hypothèse d’homoscédascticité n’est pas remise en question, comme cela semble être le cas ici. Attention : ces principes peuvent parfois être mis en défaut et il vaut toujours mieux réaliser plusieurs contrôles différents.\n\nPour vérifier l'hypothèse d'homoscédasticité, nouspouvons également utiliser le test de Breusch-Pagan. Ce test est basé sur la régression des carrés des résidus sur les variables explicatives. Si le test est significatif, l'hypothèse d'homoscédasticité est rejetée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(leaps)\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLe chargement a nécessité le package : carData\n```\n\n\n:::\n\n```{.r .cell-code}\nncvTest(sim1_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n```\n\n\n:::\n:::\n\n\nPour tester l'hypothèse de non corrélation des résidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est basé sur l'autocorrélation des résidus. Si le test est significatif, l'hypothèse de non corrélation des résidus est rejetée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndurbinWatsonTest(sim1_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n```\n\n\n:::\n:::\n\n\nEn ce qui concerne l'hypothèse de normalité des résidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est basé sur la comparaison des résidus avec une loi normale. Si le test est significatif, l'hypothèse de normalité des résidus est rejetée.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(sim1_lm$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n```\n\n\n:::\n:::\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l'hypothèse de normalité des résidus n'est pas rejetée.\n\n\n### 2.2. Qualité du modèle\n\nPour évaluer la qualité du modèle, nous allons calculer le coefficient de détermination $R^2$ et le $R^2$ ajusté.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(R2<-summary(sim1_lm)$r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9721382\n```\n\n\n:::\n\n```{.r .cell-code}\n(R2_adj<-summary(sim1_lm)$adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9719975\n```\n\n\n:::\n\n```{.r .cell-code}\n(AIC(sim1_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 719.3729\n```\n\n\n:::\n\n```{.r .cell-code}\n(BIC(sim1_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 729.2678\n```\n\n\n:::\n:::\n\nNous obtenons un $R^2$ et un $R^2$ ajusté de 0.97. Cela signifie que 97% de la variance de la variable cible est expliquée par le modèle. Notre modèle de régression linéaire est bien ajusté à nos données.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}