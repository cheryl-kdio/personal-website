{
  "hash": "d5b9d01e99fea6d4c771ea8caa710adf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle : Ridge regression vs. Lasso regression\nauthor : \"Cheryl KOUADIO\"\njupyter: python3\ndate: \"2024-09-25\"\n---\n\n\n\n\n\n\n\n\n# Activity 1 : Variants of OLS\n## 1. Ridge regression\nThe ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix $X^TX$ is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It's the reason wwhy the ridge regression is also called a L2 regularization.\n\n$$\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n$$\n\nThe penalty term is controlled by a hyperparameter $\\lambda$. It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter $\\lambda$.\n\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. \nIn the ridge regression, the bias of the ridge estimator is given by the formula:\n\n$$\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*$$ \n\nFor the variance, it is given by:\n\n$$\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X $$\n\nwhere $\\sigma^2$ is the variance of the noise in the data.\n\n<u>Hint to prove the bias formula</u> : factorize by $\\lambda ( X^TX + \\lambda I_d)^{-1}$\n\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean.\nLet's start by create a function to generate the data with a noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n```\n:::\n\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of $\\lambda$ when it varies between 10² and 10⁴ (in the log-domain).\nWe will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of $\\lambda$. The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n```\n:::\n\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each $\\lambda$, store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp1_files/figure-pdf/cell-5-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#Plot squared theorical bias : −λ(X^T X + λId) (−1) θ\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp1_files/figure-pdf/cell-6-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of $\\lambda$. This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of $\\lambda$ but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\n\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix $\\frac{X^TX}{n}$. \nIn face, using the large law of numbers, we can see that the bias is proportional to $\\lambda$ and the variance is inversely proportional to $\\lambda$.\n\n<u><b>Proof</b></u> :\n\n$\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)$, when $n \\rightarrow \\infty$, and $V(X)$ is the covariance matrix of the features. When X is centered and normalized, $V(X) = I_d$.\nUsing this in the formula of the bias and the variance, we get:\n\n$$\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*$$\n\n$$\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d$$\n\n## 2. Lasso regression\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don't have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\n$$\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n$$\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp1_files/figure-pdf/cell-8-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n# Activity 2 : Contamination dataset\n\nWe now work with a dataset related to diabetes. The initial data consists of $n$ = 442 patients and $d$ = 10 covariates and the output  variable $Y$ is a score reflecting the disease progressing. During a cyberattack of the hospital housing the data, a bad robot has contaminated the dataset by adding 200 inappropriate exploratory variables. Since simply noising the data was not sufficient for the robot, he also arbitrarily permuted the variables. To complete the picture, the robot has erased any trace of his villainous act and thus we do not know which variables are relevant. The new data set contains $n$ = 442 patients and $d$ = 210 covariates. \n\nWe are trying to resolve the enigma created by the playful machine and retrieve the relevant variables. We will hence use a regularized regression model to select the relevant variables.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport warnings \nwarnings.filterwarnings('ignore')\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#read data from csv file\nimport pandas as pd\ndata = pd.read_csv(\"data_dm3.csv\", sep=\",\", header=None)\n\n# renommer les 210 premiers colonnes en X1, X2, ..., X210 et la dernière colonne en Y\ndata.columns = [f'X{i}' for i in range(1, 211)] + ['Y']\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X202</th>\n      <th>X203</th>\n      <th>X204</th>\n      <th>X205</th>\n      <th>X206</th>\n      <th>X207</th>\n      <th>X208</th>\n      <th>X209</th>\n      <th>X210</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.298173</td>\n      <td>-0.162249</td>\n      <td>1.223379</td>\n      <td>1.355554</td>\n      <td>1.080171</td>\n      <td>0.634979</td>\n      <td>0.298741</td>\n      <td>0.548270</td>\n      <td>0.731773</td>\n      <td>1.018645</td>\n      <td>...</td>\n      <td>0.588278</td>\n      <td>0.210106</td>\n      <td>1.861458</td>\n      <td>-0.436399</td>\n      <td>0.279299</td>\n      <td>-1.416020</td>\n      <td>-2.332363</td>\n      <td>0.215096</td>\n      <td>-0.693319</td>\n      <td>151.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166951</td>\n      <td>-0.338060</td>\n      <td>-0.618867</td>\n      <td>0.759366</td>\n      <td>1.134281</td>\n      <td>-0.536844</td>\n      <td>-0.075120</td>\n      <td>0.970251</td>\n      <td>-0.327487</td>\n      <td>0.717310</td>\n      <td>...</td>\n      <td>-0.251054</td>\n      <td>-0.825716</td>\n      <td>0.339139</td>\n      <td>1.119430</td>\n      <td>0.225958</td>\n      <td>-0.822288</td>\n      <td>0.382838</td>\n      <td>-0.718829</td>\n      <td>-0.188993</td>\n      <td>75.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.416177</td>\n      <td>-0.205659</td>\n      <td>-1.282226</td>\n      <td>1.675500</td>\n      <td>1.523746</td>\n      <td>0.192029</td>\n      <td>-0.235840</td>\n      <td>-1.954626</td>\n      <td>-0.853309</td>\n      <td>0.892791</td>\n      <td>...</td>\n      <td>1.283837</td>\n      <td>0.372516</td>\n      <td>-0.652557</td>\n      <td>-2.579347</td>\n      <td>0.139267</td>\n      <td>-1.901196</td>\n      <td>0.048210</td>\n      <td>0.220205</td>\n      <td>0.471588</td>\n      <td>141.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.867184</td>\n      <td>-0.398667</td>\n      <td>0.093501</td>\n      <td>0.025971</td>\n      <td>1.852099</td>\n      <td>0.789774</td>\n      <td>0.801775</td>\n      <td>0.376711</td>\n      <td>0.853689</td>\n      <td>0.247953</td>\n      <td>...</td>\n      <td>0.446582</td>\n      <td>0.334733</td>\n      <td>0.399074</td>\n      <td>-0.884172</td>\n      <td>0.723819</td>\n      <td>1.316367</td>\n      <td>0.088218</td>\n      <td>0.619496</td>\n      <td>1.061662</td>\n      <td>206.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.193282</td>\n      <td>-0.936980</td>\n      <td>-0.725039</td>\n      <td>0.766078</td>\n      <td>0.223489</td>\n      <td>-1.584622</td>\n      <td>1.146866</td>\n      <td>0.086136</td>\n      <td>-0.088780</td>\n      <td>-0.945066</td>\n      <td>...</td>\n      <td>0.786157</td>\n      <td>-1.058179</td>\n      <td>-0.155788</td>\n      <td>-0.642504</td>\n      <td>2.040010</td>\n      <td>-1.703110</td>\n      <td>-1.901502</td>\n      <td>1.778811</td>\n      <td>-0.489853</td>\n      <td>135.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 211 columns</p>\n</div>\n```\n:::\n:::\n\n\nFirst, we will split the dataset into training and test sets. We will use the first 80% of the dataset for training and the remaining 20% for testing.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# split dataset to train and test\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(columns='Y')\ny = data['Y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#test if the splitting is effective\nn=X.shape[0]\nprint(f'Train size (proportion) : {X_train.shape[0]/n : .2f} \\nTest size (proportion) : {X_test.shape[0]/n : .2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain size (proportion) :  0.80 \nTest size (proportion) :  0.20\n```\n:::\n:::\n\n\n## 1. Ridge regression\n\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter $\\lambda$ by cross-validation.\nWe will use the function `RidgeCV` from the `sklearn` library to perform the cross-validation. Since we didn't center the covariables, we will set the parameter `fit_intercept` to `True` in order to include an intercept in the model.\n\n    By default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n```\n:::\n\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter λ. This is called regularization path. We can do this by fitting the model for different values of λ and store the coefficients.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'λ = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp1_files/figure-pdf/cell-14-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\n\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter $\\lambda$ wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients. \n\n<i> In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. </i>\n\n#### a. Check on the intercept value of the model using lambda found by cross validation\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nprint(f'Intercept value : {ridge_cv.intercept_}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept value : 152.28937186306018\n```\n:::\n:::\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features.\n\n## 2. Lasso regression\n\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation.\nIt is important to perform a cross-validation in order to choose the best value of the regularization parameter $\\lambda$ as we have demonstrated in the first activity.\nFor the lasso regression, we will use the function `LassoCV` from the `sklearn` library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'λ = {lasso_cv.alpha_:.2f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp1_files/figure-pdf/cell-17-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter $\\lambda$ given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of $\\lambda$, we can select more or less variables.\n\n``` python\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n```\n\n#### a. Check on the intercept value of the model using lambda found by cross validation\n\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept value : 151.95282341561403\n```\n:::\n:::\n\n\n## 3. Quality of the models (ridge regression vs lasso regression)\n\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\n$$\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2$$\n\nwhere $\\hat g(x)$ is the predicted value of the output variable y given the input variable x, $\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j$.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n```\n:::\n:::\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\n\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model.\n\n",
    "supporting": [
      "Tp1_files/figure-pdf"
    ],
    "filters": []
  }
}