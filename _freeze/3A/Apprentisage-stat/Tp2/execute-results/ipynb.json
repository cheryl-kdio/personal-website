{
  "hash": "4e1faae975a628fbaf7a35e8b1dd546f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Kernel Trick and SVM'\nauthor: \"Cheryl KOUADIO\"\njupyter: python3\ndate: \"2024-10-04\"\n---\n\n\n\n\n\n\n# Activity 1 : Kernel Ridge Regression (KRR)\nIn regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear. \n\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable.\nIn particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. \nThe optimization problem of KRR is given by:\n$$\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n$$\nwhere $x_i$ is the $i$-th row of the matrix $X$ and $y_i$ is the $i$-th element of the vector $y$. The parameter $\\lambda$ is the regularization parameter. The solution of the optimization problem is given by:\n\n$$\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n$$\n\nwhere $I_d$ and $I_n$ are the identity matrix.\n\nIn prediction, the target variable is given by:\n$$\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n$$\nwhere $\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}$.\nWe easily see that the prediction is a linear combination of the scalar product between the test point $x^*$ and the training points $x_i$, we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\n$$\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n$$\n\nwhere $K(x_i, x^*)$ is the kernel function.\n\n##  I. Fit the Kernel Ridge Regression (KRR)\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the `KernelRidge` class from the `sklearn` library to fit the KRR model.\nWe will start by using the Gaussian/RBF kernel which is defined by:\n$$\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n$$\nwhere $\\gamma$ is an hyperparameter.\n\n::: {#39285b4a .cell execution_count=1}\n``` {.python .cell-code}\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n::: {#105ebe5d .cell execution_count=2}\n``` {.python .cell-code}\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>times</th>\n      <th>accel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>133.000000</td>\n      <td>133.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.419649</td>\n      <td>-25.545865</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.218868</td>\n      <td>48.322050</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.040000</td>\n      <td>-134.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.260000</td>\n      <td>-54.900000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.390000</td>\n      <td>-13.300000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.580000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.960000</td>\n      <td>75.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#59bddc10 .cell execution_count=3}\n``` {.python .cell-code}\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\n### 1. Training and testing RBF kernel ridge regression\n\nSince we have two hyperparameter ($\\gamma$ and $\\lambda$) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\n::: {#d4e36c82 .cell execution_count=4}\n``` {.python .cell-code}\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters by CV : {'alpha': np.float64(0.07196856730011521), 'gamma': np.float64(35.564803062231285)}\n```\n:::\n:::\n\n\n::: {#707223fd .cell execution_count=5}\n``` {.python .cell-code}\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot mean square error:  22.98\n```\n:::\n:::\n\n\n::: {#100bdec3 .cell execution_count=6}\n``` {.python .cell-code}\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-7-output-1.png){}\n:::\n:::\n\n\n## II. Ridge regression\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter $\\lambda$.\n\n::: {#74e37526 .cell execution_count=7}\n``` {.python .cell-code}\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Ridge regression :  46.17\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nText(0.5, 1.0, 'Ridge Regression')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-8-output-3.png){}\n:::\n:::\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable $x$ by $\\tilde{x}=\\frac{\\sqrt(2)}{\\pi}\\cos(\\pi x)$ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n::: {#ecf1624d .cell execution_count=8}\n``` {.python .cell-code}\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n```\n:::\n\n\n::: {#aa55d5f6 .cell execution_count=9}\n``` {.python .cell-code}\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Ridge regression with x tilde: 45.87\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-10-output-2.png){}\n:::\n:::\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let's try to use many sinusoidal transformation to see if we can improve the performance of the model.\nWe will fit y using $\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}$\n\n::: {#55589ba1 .cell execution_count=10}\n``` {.python .cell-code}\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE Ridge regression :  22.96\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-11-output-2.png){}\n:::\n:::\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n## III. Insigths :\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \n$$\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n$$\n\nwhere $B_1 = x - 1/2$ and $B_2 = x^2 - x - 1/6$.\n\nUsing the fourier series expansion for $x \\in [0,1]$, we have :\n$$\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n$$\n\n### Kernel de sobolev\n\n::: {#ed830159 .cell execution_count=11}\n``` {.python .cell-code}\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n```\n:::\n\n\n::: {#14a1e41f .cell execution_count=12}\n``` {.python .cell-code}\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n{'alpha': np.float64(0.07196856730011521)}\n```\n:::\n:::\n\n\n::: {#8ee4cca9 .cell execution_count=13}\n``` {.python .cell-code}\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot mean square error :  61.98\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-14-output-3.png){}\n:::\n:::\n\n\n# Activity 2 : Support Vector Machine\n\nThe support vector machines (SVM) is a supervised learning algorithm that can be used for classification. The SVM is based on the concept of finding the hyperplane that best separates the data into two classes. In fact, if the problem is **linearly separable**, the SVM finds the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the closest point of each class (support vectors).\n\nWhen the problem is non-linearly separable, the SVM uses the same functionnement but allows some points to be misclassified. When the problem is too complex, the SVM uses the kernel trick to transform the data into a higher-dimensional space where the relationship is linear.\n\n\n## I. Toy dataset\n\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\n::: {#51647cb5 .cell execution_count=14}\n``` {.python .cell-code}\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nText(0.5, 1.0, 'Two moons dataset')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-15-output-2.png){}\n:::\n:::\n\n\n::: {#5fc38b77 .cell execution_count=15}\n``` {.python .cell-code}\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n```\n:::\n\n\n### 1. K-nearest neighbors  \n\n::: {#3f44dbb0 .cell execution_count=16}\n``` {.python .cell-code}\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters by CV : {'n_neighbors': np.int64(1)}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n```\n:::\n:::\n\n\n::: {#83e48626 .cell execution_count=17}\n``` {.python .cell-code}\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-18-output-1.png){}\n:::\n:::\n\n\n### 2. Logistic regression\n\n::: {#3e41d598 .cell execution_count=18}\n``` {.python .cell-code}\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n```\n:::\n:::\n\n\n::: {#21654790 .cell execution_count=19}\n``` {.python .cell-code}\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-20-output-1.png){}\n:::\n:::\n\n\n### 3. SVM\n\n::: {#4f3c919f .cell execution_count=20}\n``` {.python .cell-code}\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters by CV : {'C': np.float64(0.030888435964774818), 'gamma': np.float64(3.727593720314938)}\n```\n:::\n:::\n\n\n::: {#187dba61 .cell execution_count=21}\n``` {.python .cell-code}\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n```\n:::\n:::\n\n\n::: {#6076f882 .cell execution_count=22}\n``` {.python .cell-code}\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-23-output-1.png){}\n:::\n:::\n\n\n### 4. Conclusion\n\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\n\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\n\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\n\n**Whenever we have a classification problem, it is hence always useful to try the SVM and the KNN.**\n\n## II. Image dataset\n\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255).\nWe are going to use the SVM with the gaussian kernel to classify the images.\n\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\n::: {#79c3a9ee .cell execution_count=23}\n``` {.python .cell-code}\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDescription of train dataset : \n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#dc377655 .cell execution_count=24}\n``` {.python .cell-code}\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n```\n:::\n\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n::: {#bd4dfc0e .cell execution_count=25}\n``` {.python .cell-code}\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n```\n:::\n\n\n::: {#46848968 .cell execution_count=26}\n``` {.python .cell-code}\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-27-output-2.png){}\n:::\n:::\n\n\n::: {#6b59d532 .cell execution_count=27}\n``` {.python .cell-code}\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n```\n:::\n:::\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\n\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n::: {#42ae7239 .cell execution_count=28}\n``` {.python .cell-code}\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n```\n:::\n:::\n\n\n::: {#c3e58a2b .cell execution_count=29}\n``` {.python .cell-code}\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp2_files/figure-ipynb/cell-30-output-1.png){}\n:::\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/cherylkouadio/Documents/Repositories/personal-website/.venv/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.9.6\n---\n",
    "supporting": [
      "Tp2_files/figure-ipynb"
    ],
    "filters": []
  }
}