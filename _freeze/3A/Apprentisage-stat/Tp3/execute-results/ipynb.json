{
  "hash": "c7a8db9a0ad86102f5f14cb833ba573d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Gradient boosting'\nauthor: \"Cheryl KOUADIO\"\njupyter: python3\ndate: \"2024-10-18\"\n\n---\n\n\n\n\n\n\n\n# Activity 1 : SAMME Algorithm\n\n\nAdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\n\nHence, for M weak learners, the final prediction is given by $g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)$ where $g_m(x)$ is the m-th weak learner and $\\alpha_m$ is the weight associated with the m-th weak learner. \nThe optimisation problem of AdaBoost is given by:\n\n$$ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) $$\n\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function $l(y,f(x))=\\exp(-yf(x))$, $y \\in \\{-1,+1\\}$. It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n- 1. Initialize the observation weights $w_i^{(1)} = 1/n$ for $i=1,\\dots,n$\n- 2. For m=1 to M:\n    - a. Fit a weak learner $g_m(x)$ to the training data using weights $w_i^{(m)}$\n    - b. Compute the error rate $err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))$ where $I$ is the indicator function\n    - c. Compute the weight $\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)$\n    - d. Update the weights $w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)$\n \nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems.\nIt is a generalization of the AdaBoost algorithm to multi-class classification problems. \n\nTo inspect how the errors and the weights vary with the number of iterations, we will the function `make_gaussian_quantiles` from `sklearn`. This function generates a multi-dimensional standard normale distribution with a given number of samples $n$ per class $K$. We will generate a dataset of size $n=2000$ with $K=3$ classes and $d=10$ features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n::: {#fcacea00 .cell execution_count=1}\n``` {.python .cell-code}\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n```\n:::\n\n\n## I. Decision Tree\n\n::: {#900320cc .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  0.5075\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp3_files/figure-ipynb/cell-3-output-2.png){}\n:::\n:::\n\n\n## II. SAMME Algorithm (Multi-class AdaBoost with Decision Trees)s\n\n::: {#29eb5003 .cell execution_count=3}\n``` {.python .cell-code}\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  0.7\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp3_files/figure-ipynb/cell-4-output-3.png){}\n:::\n:::\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree.\nIf we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n::: {#80dd401c .cell execution_count=4}\n``` {.python .cell-code}\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp3_files/figure-ipynb/cell-5-output-1.png){}\n:::\n:::\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases. \n\n::: {#55c09a17 .cell execution_count=5}\n``` {.python .cell-code}\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Tp3_files/figure-ipynb/cell-6-output-1.png){}\n:::\n:::\n\n\n# Activity 2 : Boosting with not so weak learners\n\nAs mentioned earlier, the boosting algorithm uses multiple weak learners. This is a crucial aspect of the algorithm, as it allows combining multiple weak learners to create a strong learner. However, we might be interested in using stronger learners instead of weak ones, and we could observe that the algorithm performs poorly.\n\nFor example, let's consider boosting with a linear regression as the base learner. In boosting, we can use the errors in two ways : either grade and reweight in the training, or either train the model on the residuals.\n\nAt the first iteration, we fit a linear regression to the target variable in order to obtain the parameters $\\beta$:\n\n$$\n\\beta_{1}^* = \\underset{\\beta_1}{\\arg \\min} || Y - X\\beta_1 ||^2\n$$\n\nThis gives us the first learner $g_1(x) = X\\beta_1$, where $\\beta_1 = (X^TX)^{-1}X^TY$.\n\nIn the following iterations, we fit a linear regression on the residuals to obtain the parameters $\\beta_m$:\n\n$$\n\\beta_m^* = \\underset{\\beta_m}{\\arg \\min} || Y - X\\hat{\\beta}_{m-1} - X\\beta_m ||^2 = \\underset{\\beta}{\\arg \\min} || R_{m-1} - X\\beta_m ||^2\n$$\n\nIt's easy to see that the optimal $\\beta_m = 0$, since the residuals are orthogonal to the previous residuals. Therefore, the optimal $\\beta_m = 0$ and the optimal $g_m(x) = 0$.\n\nFor example, when $m=2$, we have the following optimization problem:\n\n$$\n\\beta_2^* = \\underset{\\beta_2}{\\arg \\min} || Y - X\\hat{\\beta}_{1} - X\\beta_2 ||^2 = \\underset{\\beta_2}{\\arg \\min} || R_1 - X\\beta_2 ||^2\n$$\n\nThus, $\\beta_2 = (X^TX)^{-1}X^TR_1 = (X^TX)^{-1}X^T(Y - X\\hat{\\beta}_1) = \\hat{\\beta}_1 - I_d \\hat{\\beta}_1 = 0$.\n\nThis shows that, at the second iteration, the algorithm fails to improve because the residuals are already orthogonal, meaning the gradient direction leads to no further improvement. Essentially, this explains why using a strong learner such as linear regression in boosting can result in poor performance. In this case, the linear model fits perfectly in the first iteration, leaving no room for further improvement, as the residuals contain no additional information.\n\nBoosting algorithms like AdaBoost or Gradient Boosting rely on the fact that weak learners make small, incremental improvements, allowing the algorithm to progressively focus on harder-to-classify examples or areas where the model still has room to learn. By using a strong learner that perfectly fits the data in the first round, we lose the incremental learning aspect, which is key to the success of boosting.\n\n# Activity 3 : Xgboost\n\nXGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. When the dataset is higly correlated, we can still use XGBoost, as it can handle multicollinearity.\n\n::: {#a0c05652 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\n# normalize the data\nX_train  = data_train.iloc[:, 1:]/255\ny_train = data_train[\"label\"]\nX_test  = data_test.iloc[:, 1:]/255\ny_test = data_test[\"label\"]\n```\n:::\n\n\nAs we can see, the model performs rather well with an accuracy of 0.87 (quite similar to the one obtained with SVM). However, we can see that the model is not perfect as it makes some mistakes.\n\n::: {#efc94cb2 .cell execution_count=7}\n``` {.python .cell-code}\n# Train Xgboost on the data\nfrom xgboost import XGBClassifier\nmodel_xgboost = XGBClassifier(colsample_bylevel=0.1,gamma=0.1).fit(X_train, y_train)\ny_pred_xgboost = model_xgboost.predict(X_test)\naccuracy_xgboost = accuracy_score(y_test, y_pred_xgboost)\nprint(\"Accuracy: \", accuracy_xgboost)\n\ncm_xgboost = confusion_matrix(y_test, y_pred_xgboost)\nsns.heatmap(cm_xgboost, annot=True)\nplt.title('XGBoost confusion matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy:  0.8893\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Tp3_files/figure-ipynb/cell-8-output-2.png){}\n:::\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/cherylkouadio/Library/Python/3.9/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.9.6\n---\n",
    "supporting": [
      "Tp3_files/figure-ipynb"
    ],
    "filters": []
  }
}