{
  "hash": "db29709685c349023f488739ca6c4ace",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Projet de gestion de risques multiples\njupyter: python3\nauthor: \n    - Cheryl Kouadio\n    - Mariyam Ouyassin\ndate: 2025-03-13\n---\n\n\nLes banques jouent un rôle central dans l’économie nationale et internationale. En effet, elles assurent l’intermédiation entre les agents disposant d’un excédent de financement et ceux ayant un besoin de financement, facilitant ainsi les transactions et soutenant l’investissement, et donc la croissance économique. Toutefois, cette activité les expose à divers risques majeurs, notamment le risque de crédit, le risque de liquidité et le risque de marché. La quantification de ces risques est essentielle pour permettre aux institutions bancaires de s’en prémunir et de les surveiller efficacement. Dans ce cadre, la Value-at-Risk (VaR) s’impose comme une mesure de référence. Elle permet d’évaluer la perte potentielle maximale qu’une institution pourrait subir, avec un certain niveau de confiance, sur un horizon temporel donné et pour un portefeuille spécifique.\n\nDans l’analyse de la VaR, un portefeuille est souvent composé d’au moins deux créances. Il est donc indispensable de prendre en compte les dépendances entre les facteurs de risque. Une approche classique consiste à supposer que le vecteur des risques individuels suit une distribution normale multivariée et à utiliser le coefficient de corrélation linéaire de Pearson comme mesure de dépendance. Cependant, cette hypothèse est souvent trop restrictive en finance : les distributions des facteurs de risque ne sont pas nécessairement gaussiennes et le coefficient de Pearson ne permet pas toujours de capturer les structures de dépendance non linéaires. De plus, cette mesure de corrélation est pertinente uniquement dans un cadre gaussien, qui représente rarement les dynamiques financières réelles.\n\nDans ce contexte, la théorie des copules constitue un outil statistique puissant permettant de modéliser la dépendance entre les risques sans se limiter à l’hypothèse de normalité. Une copule est une fonction qui caractérise la structure de dépendance entre plusieurs variables aléatoires indépendamment de leurs distributions marginales. En séparant la modélisation des distributions marginales et celle de la dépendance conjointe, les copules offrent une flexibilité accrue pour l’analyse du risque et permettent de mieux représenter les interactions entre les actifs financiers.\n\nL’objectif de ce projet est d’évaluer le risque de crédit en calculant une CreditVaR avec un niveau de confiance de 99\\% sur un portefeuille composé de deux obligations bancaires. Ces obligations, bien que différentes en termes de subordination et de risque de recouvrement, appartiennent au même secteur, ce qui accroît le risque global du portefeuille. Il est donc essentiel de modéliser adéquatement la dépendance entre ces actifs pour obtenir une estimation réaliste du risque de crédit.\n\n::: {.callout-important}\nLe rapport de ce projet est disponible sur [ce lien](../rapports/Rapport_Kouadio_Ouyassin_copules.pdf).\n:::\n\n# I. Données \n\nNous disposons des prix des actions des composantes d'un portefeuille constitué de deux créances issues du secteur bancaire, ayant un même notionnel de $N = 1000$ euros et une maturité de $T = 4$ ans. En l'absence de lignes de crédit, l'exposition en cas de défaut de ces banques est égale au notionnel, soit $EAD = N = 1000$ euros.\n\nLa première obligation, de type senior, est émise par BNP et présente un taux de recouvrement moyen de 60% ainsi qu’une volatilité de 15%. La seconde, de type junior (ou subordonnée), est émise par la Société Générale et se caractérise par un taux de recouvrement moyen de 30% et une volatilité de 25%. On suppose qu’il n’existe aucune dépendance entre les taux de recouvrement.\n\nPar ailleurs, nous disposons d'informations sur les spreads des Credit Default Swaps (CDS) des deux banques, qui permettent d’évaluer le risque de crédit en fonction de la maturité des actifs. En effet, les CDS sont des contrats d’assurance contre le défaut d'une entreprise. Le spread d’un CDS représente le coût de cette assurance : plus ce spread est élevé, plus le marché perçoit l’entreprise comme risquée.\n\nOn considère que les spreads de CDS de BNP et de Société Générale pour une maturité de $T = 4$ ans sont respectivement de 100 bps et 120 bps, tandis que pour une maturité de $T = 5$ ans, ils s’élèvent à 120 bps et 150 bps. Ces CDS ont été évalués avec un taux de recouvrement $R$ fixé à 40%.\n\nPuisque les spreads de Société Générale sont systématiquement plus élevés, quelle que soit la maturité, on en déduit que l'obligation émise par SG est plus risquée que celle émise par BNP. Cette observation est ainsi cohérente avec la hiérarchie des obligations considérées.\n\n\nPour extraire les probabilités de défauts, nous allons utilisé les CDS étant plus liquides que les obligations. Les CDS sont des contrats d'assurance contre le défaut d'une entreprise. Le spread de CDS est le prix de cette assurance. Plus le spread est élevé, plus le marché estime que l'entreprise est risquée. Le fair spread est donnée par :\n\n$$\ns* = (1-\\delta) \\times \\lambda\n$$\n\nAvec $\\delta$ le taux de recouvrement et $\\lambda$ le hazard rate. Le hazard rate est la probabilité de défaut de l'entreprise. On peut donc isoler le hazard rate :\n\n$$\n\\lambda = \\frac{s*}{1-\\delta}\n$$\n\nDe ce fait, on peut calculer la probabilité de défaut donnée par :\n$$\nPD = P(\\tau < T) = 1 - e^{-\\lambda \\times T}\n$$\n\ncar $\\tau \\sim Exp(\\lambda)$\n\n::: {#df362e01 .cell execution_count=2}\n``` {.python .cell-code}\n# Noms des entreprises\nfrom pprint import pprint\nimport numpy as np\nentreprises = [\"BNP\", \"SG\"]\n\ndef PD(T,spread,Recouvrement):\n    lambda_ = spread / (1 - Recouvrement)\n    PD = 1 - np.exp(-lambda_*T)\n    return PD,lambda_\n\nspread_4Y = np.array([100, 120]) / 10000 # 100 et 120 points de base\nspread_5Y = np.array([120, 150]) / 10000 # 120 et 150 points de base\ntx_recouvrement = 0.4\n\nproba_defaut_quatre = np.zeros(len(spread_4Y))\nproba_defaut_cinq = np.zeros(len(spread_5Y))\n\nfor i in range(len(spread_4Y)) :\n    proba_defaut_quatre[i],_ = PD(spread=spread_4Y[i],T = 4,Recouvrement=0.4)\n    print(_)\n    proba_defaut_cinq[i],_ = PD(spread=spread_5Y[i],T = 5,Recouvrement=0.4)\n    print(_)\n\n# Création d'un dictionnaire pour stocker les probabilités de défaut\nprobabilites_defaut = {}\n\n# Remplir le dictionnaire\nfor i, entreprise in enumerate(entreprises):\n    probabilites_defaut[entreprise] = {\n        \"4 ans\": round(proba_defaut_quatre[i]*100,4),\n        \"5 ans\": round(proba_defaut_cinq[i]*100,4)\n    }\n\n# Afficher le dictionnaire\npprint(probabilites_defaut)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.016666666666666666\n0.02\n0.02\n0.025\n{'BNP': {'4 ans': np.float64(6.4493), '5 ans': np.float64(9.5163)},\n 'SG': {'4 ans': np.float64(7.6884), '5 ans': np.float64(11.7503)}}\n```\n:::\n:::\n\n\nNous constatons que les probabilités de défaut pour les deux entreprises pour une même maturité sont généralement plus élevés pour la Société Générale que pour BNP Paribas. Cela est cohérent avec le type d'obligation émise par les deux entreprises. En effet, une obligation junior est plus risquée qu'une obligation senior.\n\n# II. Caractérisation les distributions des taux de recouvrement de chacune des deux créances\n\nPour caractériser les distributions des taux de recouvrement, on va utiliser la loi Beta(a,b). La loi Beta est une loi de probabilité continue définie sur l'intervalle [0, 1]. Elle est souvent utilisée pour modéliser des variables aléatoires qui prennent des valeurs dans un intervalle borné en forme de U ce qui est le cas des taux de recouvrement. L'expression de sa densité est donnée par :\n\n$$\nf(x) = \\frac{x^{a-1} \\times (1-x)^{b-1}}{B(a,b)} 1_{[0,1]}(x)\n$$\n\nPuisque nous possédons des taux de recouvrement de moyenne 60% et de volatilité 15% pour la première créance et de moyenne 30% et de volatilité 25% pour la seconde créance, nous allons utiliser la méthode des moments pour déterminer les paramètres a et b de la loi Beta. De ce fait, les paramètres sont calibrés de la manière suivante :\n\n$$\na = \\frac{\\mu^2 \\times (1-\\mu)}{\\sigma^2} - \\mu; \\quad b = \\frac{\\mu^2 \\times (1-\\mu)}{ \\mu \\sigma^2} - (1-\\mu )\n$$\n\nLa première est une obligation BNP senior de taux de recouvrement de moyenne 60% et de volatilité\n15%, et la seconde est une obligation Société Générale junior (ou subordonnée) de taux de recouvrement\nde moyenne 30% et de volatilité 25%.\n\n### II.1. Caractérisation de la distribution des tx de recouvrement BNP\n\n::: {#d2f3ad72 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\ntx_recouvrement_BNP = 0.6\nvol_recouvrement_BNP = 0.15\n\ndef calibration_beta(mu,sigma):\n    a = ((mu**2) * (1 - mu) / (sigma**2)) - mu\n    b = ((mu**2) * (1 - mu)**2 / (mu*(sigma**2))) - (1-mu)\n    return a,b\n\n# def moment_beta(moyenne, variance):\n#     alpha = (moyenne * (1 - moyenne) / variance**2 - 1) * moyenne\n#     beta = (moyenne * (1 - moyenne) / variance**2 - 1) * (1 - moyenne)\n#     return alpha, beta\n\na_BNP,b_BNP = calibration_beta(tx_recouvrement_BNP,vol_recouvrement_BNP)\n\nprint(\"a_BNP = \", a_BNP)\nprint(\"b_BNP = \", b_BNP)\n\n# densité beta\n\nfrom scipy.stats import beta\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0,1,100)\ny_BNP = beta.pdf(x,a_BNP,b_BNP)\n\nplt.plot(x,y_BNP)\nplt.title(\"Densité de probabilité du taux de recouvrement de BNP\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na_BNP =  5.8\nb_BNP =  3.8666666666666676\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.0, 'Densité de probabilité du taux de recouvrement de BNP')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-3-output-3.png){width=571 height=432}\n:::\n:::\n\n\nNous constatons que la distribution des taux de recouvrement de BNP est légèrement asymétrique à droite. En effet, le paramètre a, contrôle la concentration des valeurs proches de 1, est supérieur au paramètre b, contrôle la concentration des valeurs proches de 0. De ce fait, la distribution est asymétrique à gauche.\nCependant que la majorité des valeurs sont concentrées autour de la moyenne et il y a une **faible probabilité de recouvrement très bas ou très élevé**.\n\n### II.2. Caractérisation de la distribution des tx de recouvrement SG\n\n::: {#e9d23d7b .cell execution_count=4}\n``` {.python .cell-code}\ntx_recouvrement_SG = 0.3\nvol_recouvrement_SG = 0.25\n\n\na_SG,b_SG = calibration_beta(tx_recouvrement_SG,vol_recouvrement_SG)\n\nprint(\"a_SG = \", a_SG)\nprint(\"b_SG = \", b_SG)\n\n# densité beta\ny_SG = beta.pdf(x,a_SG,b_SG)\n\nplt.plot(x,y_SG)\nplt.title(\"Densité de probabilité du taux de recouvrement de BNP\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na_SG =  0.708\nb_SG =  1.652\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Densité de probabilité du taux de recouvrement de BNP')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-4-output-3.png){width=571 height=432}\n:::\n:::\n\n\nEn ce qui concerne la distribution des taux de recouvrement de la Société Générale, nous constatons une asymétrie à gauche également.\n\nLa densité est **très asymétrique**, décroissant fortement à partir de 0. On constate une forte **concentration des valeurs proches de 0** indique un **risque élevé de faible recouvrement**. La longue traîne vers la droite signifie que, bien que des recouvrements plus élevés soient possibles, ils sont peu probables.\n\n## II.3. Comparaison des distributions\n\n::: {#e257510a .cell execution_count=5}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\n\nplt.plot(x, y_BNP, label='BNP')\nplt.plot(x, y_SG, label='SG')\nplt.title(\"Densités de probabilité des taux de recouvrement\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-5-output-1.png){width=794 height=432}\n:::\n:::\n\n\n- **BNP senior** : L'obligation senior bénéficie d’un **taux de recouvrement plus élevé et plus stable**, donc **moins risqué**.\n\n- **SG junior** : L'obligation junior a une **probabilité élevée d'un recouvrement très faible**, ce qui reflète un **risque plus important**.\n\nCela est cohérent avec la catégorisation des obligations.  Dans la hiérarchie des dettes, une obligation peut être classée comme senior ou junior (subordonnée) en fonction de la priorité de remboursement en cas de faillite de l’émetteur. Cette distinction est essentielle pour évaluer le risque de crédit et le taux de recouvrement attendu.\nUne obligation classée senior est moins risquée qu'une obligation junior, car elle est remboursée en premier en cas de défaut de l'émetteur, cependant le rendement attendu est moins élevé. Par conséquent, les obligations senior ont un taux de recouvrement plus élevé et plus stable que les obligations junior.\n\n# III. Caractérisation des lois de défauts\n\nPour caractériser les lois de défaut, nous avons besoin de prendre en compte la structure de dépendance en plus des lois marginales présentées dans la section I. Pour ce faire, nous devons utiliser un produit dnt le prix dépendant de la dépendance entre les défauts des deux entreprises. Dans notre cas, nous utiliserons le prix des actions des deux entreprises pour déterminer la dépendance entre les défauts.\n\n## III.1. Analyse exploratoire univariée des données actions de ces deux entreprises.\n\nEn observant le prix des actions BNP et SG, nous constatons que les actions BNP ont un prix plus élevés que les actions SG. Cela est cohérent avec la capitalisation boursière des deux entreprises. De plus, nous constatons que les rendements de BNP sont semblables à ceux de SG. Cela est cohérent avec le fait que les deux entreprises sont des banques françaises et sont donc exposées aux mêmes risques macroéconomiques. Néanmoins, les actions de BNP présentent une volatilité légèrement plus élevé que celles de SG.\n\n::: {#51cc307b .cell execution_count=6}\n``` {.python .cell-code}\n# read data.txt\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data_copules.txt', sep=\"\\t\")\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BNP</th>\n      <th>SG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>42.36</td>\n      <td>55.24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42.72</td>\n      <td>55.59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43.20</td>\n      <td>56.45</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42.67</td>\n      <td>55.55</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>41.81</td>\n      <td>54.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#54e6dd7c .cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.plot(data['BNP'], label='BNP')\nplt.plot(data['SG'], label='SG')\nplt.title(\"Prix des actions\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-7-output-1.png){width=789 height=431}\n:::\n:::\n\n\n::: {#539ab36f .cell execution_count=8}\n``` {.python .cell-code}\nreturns = pd.DataFrame()\nreturns[\"BNP\"] = data[\"BNP\"].pct_change().dropna()\nreturns[\"SG\"] = data[\"SG\"].pct_change().dropna()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Premier sous-graphique : Rendements BNP\naxes[0].plot(returns[\"BNP\"], color='tab:blue')\naxes[0].set_title(\"Rendements BNP\")\naxes[0].grid(True)\n\n# Deuxième sous-graphique : Rendements SG\naxes[1].plot(returns[\"SG\"], color='tab:orange')\naxes[1].set_title(\"Rendements SG\")\naxes[1].grid(True)\n\n# Ajustement automatique pour éviter les chevauchements\nplt.tight_layout()\n\n# Affichage\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-8-output-1.png){width=1142 height=470}\n:::\n:::\n\n\n::: {#911c930a .cell execution_count=9}\n``` {.python .cell-code}\nreturns.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BNP</th>\n      <th>SG</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>999.000000</td>\n      <td>999.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.000590</td>\n      <td>-0.000473</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.024340</td>\n      <td>0.020690</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.116199</td>\n      <td>-0.093616</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.013771</td>\n      <td>-0.011760</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.000358</td>\n      <td>-0.000547</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.012605</td>\n      <td>0.011286</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.086786</td>\n      <td>0.079479</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## III.2. Modélisation des distributions univariées des facteurs de risques\n\nDans le cadre des prix des actions, le seul facteur de risque est le rendement. Nous allons donc modéliser les rendements des actions de BNP et SG par des lois normales, lois student, skew student, et Normal Inverse Gaussian afin de déterminer la loi qui s'ajuste le mieux aux données.\n\n### III.2.1. Modélisation des rendements de BNP\n\nEn ce qui concerne les rendements de BNP, nous constatons que les lois de student et normal inverse gaussian sont les plus adaptées pour modéliser les rendements de BNP. En effet, lorsqu'on compare les QQ-plot des rendements de BNP avec les lois normales, student, skew student et normal inverse gaussian, on constate que les quantiles empiriques des rendements de BNP sont plus proches des quantiles théoriques des lois student et normal inverse gaussian. De plus, les densités ajustées semblent également mieux coller aux données.\n\nSi l'on devait choisir une loi pour modéliser les rendements de BNP, nous choisirions la loi normal inverse gaussian. En effet, bien qu'elle soit plus complexe à modéliser, elle permet d'avoir un p-value, au test de Kolmogorov-Smirnov, plus élevé que la loi student. Cela signifie que la loi normal inverse gaussian est plus adaptée pour modéliser les rendements de BNP.\n\n::: {#180c273e .cell execution_count=10}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de données pour l'exemple (remplace par tes données réelles)\ndata = returns[\"BNP\"]\n\n# Création de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densité théorique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densité ajustée - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des paramètres de la distribution de Student pour vos données.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densité théorique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densité ajustée - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densité ajustée - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU = pd.DataFrame(index=returns.index, columns=returns.columns)\n\nU['BNP'] = stats.norminvgauss.cdf(data,*params_nig)\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-10-output-1.png){width=1333 height=950}\n:::\n:::\n\n\n::: {#40c913f4 .cell execution_count=11}\n``` {.python .cell-code}\nprint(params_norm)\nprint(params_std)\nprint(params_nig)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(np.float64(-0.0005897932128196069), np.float64(0.024327449407557697))\n(np.float64(4.605552436800593), np.float64(-0.00046153919290846087), np.float64(0.018734588738427177))\n(np.float64(1.114374971099747), np.float64(-0.01637147555477723), np.float64(-0.00021011775496697913), np.float64(0.025828797181227353))\n```\n:::\n:::\n\n\n::: {#af5ed951 .cell execution_count=12}\n``` {.python .cell-code}\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.051878  0.008908\nStudent                   0.027225  0.441752\nNormal Inverse Gaussian   0.026466  0.477953\n==================================================\n```\n:::\n:::\n\n\n### III.2.1. Modélisation des rendements de SG\n\nComme les rendements de BNP, les rendements de SG sont mieux modélisés par les lois student et normal inverse gaussian.\n\n::: {#0b73886d .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de données pour l'exemple (remplace par tes données réelles)\ndata = returns[\"SG\"]\n\n# Création de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densité théorique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densité ajustée - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des paramètres de la distribution de Student pour vos données.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densité théorique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densité ajustée - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densité ajustée - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU['SG'] = stats.norminvgauss.cdf(data,*params_nig)\n\nU = U.to_numpy()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-13-output-1.png){width=1334 height=950}\n:::\n:::\n\n\n::: {#1b3bc359 .cell execution_count=14}\n``` {.python .cell-code}\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.047345  0.021967\nStudent                   0.025635  0.519206\nNormal Inverse Gaussian   0.022462  0.685866\n==================================================\n```\n:::\n:::\n\n\n## III.3. Etude de la structure de dépendance \n\nL'évaluation de la dépendance entre les facteurs de risque sera réalisée en utilisant des outils graphiques basés sur des critères non paramétriques, tels que les nuages de points, les ajustements linéaires et le dépendogramme. \n\nCes méthodes, choisies pour leur capacité à traiter des données sans présupposer une distribution spécifique, offrent une approche flexible et visuelle pour identifier et analyser les relations entre les variables de risque. Par exemple, les nuages de points permettent de visualiser la dispersion et la relation potentielle entre deux variables, tandis que les ajustements linéaires cherchent à modéliser la relation par une ligne droite, facilitant ainsi la compréhension des tendances générales.\n\nLe dépendogramme, quant à lui, représente la structure de dépendance sous la forme du nuage de points des marges uniformes extraites de l'échantillon n couples de données $\\left(\\left(x_{1,1} ; x_{2,1}\\right), \\cdots,\\left(x_{1, n} ; x_{2, n}\\right)\\right)$, i.e. :\n\n$$\nu_{i, j}=\\frac{1}{n} \\sum_{k=1}^n 1_{\\left\\{x_{j, k} \\leq x_{j,i}\\right\\}}, \\quad i \\in[1, n], \\quad \\forall j \\in[1,2]\n$$\n\nLe dépendogramme de l'échantillon est donc la représentation de n couples  $\\left(\\left(u_{1,1} ; u_{2,1}\\right), \\cdots,\\left(u_{1, n} ; u_{2, n}\\right)\\right)$. Il permet d'observer le caractère plus ou moins simultané des réalisations issues de l'échantillon. \n\n::: {#66ac3a71 .cell execution_count=15}\n``` {.python .cell-code}\n# Tableax avec tx de pearson, spearman et kendall\n\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\npearson = pearsonr(returns[\"BNP\"], returns[\"SG\"])\nspearman = spearmanr(returns[\"BNP\"], returns[\"SG\"])\nkendall = kendalltau(returns[\"BNP\"], returns[\"SG\"])\n\ntableau_correlation = pd.DataFrame({\n    \"Pearson\": pearson,\n    \"Spearman\": spearman,\n    \"Kendall\": kendall\n}, index=[\"Coefficient\", \"p-value\"])\n\ntableau_correlation\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pearson</th>\n      <th>Spearman</th>\n      <th>Kendall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coefficient</th>\n      <td>8.646588e-01</td>\n      <td>8.409565e-01</td>\n      <td>6.690959e-01</td>\n    </tr>\n    <tr>\n      <th>p-value</th>\n      <td>2.383438e-300</td>\n      <td>3.609139e-268</td>\n      <td>9.880880e-220</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn analysant la corrélation entre les rendements de BNP et SG, nous constatons qu'il y a une corrélation positive significative peu importe le test de corrélation effectué. La corrélation de spearman et de pearson indique qu'il y a une liaison monotone et linéaire forte d'au moins 84% entre les rendements des deux entreprises. \nEn ce qui concerne le taux de kendall, nous constatons une corrélation positive significative de 67% environ. Cela signifie que les rendements des actions de BNP et SG sont positivement corrélés.\n\n::: {.callout-warning}\nAttention la corrélation de pearson n'est pas une mesure de concordance contrairement au coefficient de spearman. Dans notre cas, il indique une corrélation linéaire forte.\n:::\n\n::: {#a0bcadc8 .cell execution_count=16}\n``` {.python .cell-code}\n# Dependogramme\n# Posons x1, u1 = BNP et x2, u2 = SG\nimport warnings\nfrom scipy.stats import rankdata\n\nwarnings.filterwarnings(\"ignore\")\n\n# 1. Transformation en pseudo-observations\ndef pseudo_observations(X):\n    \"\"\"Transforme les données en pseudo-observations U dans [0,1].\"\"\"\n    n, d = X.shape\n    U = np.zeros((n, d))\n    for j in range(d):\n        U[:, j] = rankdata(X[:, j]) / (n + 1)  # Pour éviter les 1 stricts\n    return U\n\nX = returns.to_numpy()\nu_obs = pseudo_observations(X)\n\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1,2,1) # 1 ligne, 2 colonnes, premier graphique\n\n# nuages de points des données BNP et SG\nplt.scatter(returns[\"BNP\"], returns[\"SG\"], cmap=\"viridis\")\n# Ajout la droite qui s'ajuste aux données\nplt.plot(np.unique(returns[\"BNP\"]), np.poly1d(np.polyfit(returns[\"BNP\"], returns[\"SG\"], 1))(np.unique(returns[\"BNP\"])), color=\"red\", label=\"Droite d'ajustement\")\nplt.title(\"Nuages de points des rendements\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.grid(False)\n\nplt.subplot(1,2,2)\nplt.scatter(u_obs[:,0], u_obs[:,1])\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-16-output-1.png){width=840 height=450}\n:::\n:::\n\n\nEn observant le dépendogramme, nous constatons la même dépendance positive entre les deux entreprises. Cela signifie que si l'une des entreprises fait défaut, l'autre a plus de chance de faire défaut également. Cela est cohérent avec la corrélation positive observée entre les rendements des actions de BNP et SG.\n\nDe plus, en observant le dépendogramme, il semble avoir des dépendances à gauche et à droite entre les deux entreprises. Nous allons tout de même tester un éventail de copules afin de vérifier laquelle des copules est la plus adaptée à notre cas : \n- Copules elliptiques : gaussienne, Student.\n- Copules archimédiennes : Clayton, Gumbel, Frank.\n\nIl faudrait au préalable estimer les paramètres des copules archimédiennes et elliptiques pour déterminer laquelle des copules est la plus adaptée à notre cas. Précedemment, nous avons estimé les paramètres des lois marginales des rendements de BNP et SG. Nous allons utiliser ces paramètres pour estimer les paramètres des copules. Nous allons utiliser ces paramètres pour la modélisation des différentes copules.\n\n::: {#e6a26374 .cell execution_count=17}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\nfrom statsmodels.distributions.copula.api import (\n    GaussianCopula, StudentTCopula, ClaytonCopula, GumbelCopula, FrankCopula\n)\n\ndef get_copula(copula_type, params):\n    if copula_type == \"gaussian\":\n        rho = params[0]\n        return GaussianCopula(corr=np.array([[1, rho], [rho, 1]]))\n    elif copula_type == \"student\":\n        rho, nu = params\n        return StudentTCopula(corr=np.array([[1, rho], [rho, 1]]), df=nu)\n    elif copula_type == \"clayton\":\n        theta = params[0]\n        return ClaytonCopula(theta=theta)\n    elif copula_type == \"gumbel\":\n        theta = params[0]\n        return GumbelCopula(theta=theta)\n    elif copula_type == \"frank\":\n        theta = params[0]\n        return FrankCopula(theta=theta)\n    else:\n        raise ValueError(f\"Copula type {copula_type} not supported\")\n\n\n# Log-vraisemblance négative pour estimation MLE\ndef negative_log_likelihood(params, U, copula_type):\n    copula = get_copula(copula_type, params)\n    log_likelihood = copula.logpdf(U)\n    return -np.sum(log_likelihood)\n\n# Ajustement de la copule (MLE)\ndef fit_copula(U, copula_type):\n    if copula_type in [\"gaussian\", \"gumbel\", \"clayton\", \"frank\"]:\n        if copula_type==\"gaussian\":\n            x0 = [0.6]\n        else:\n            x0 = [3]\n        bounds = [(1e-5, 10)] if copula_type != \"gaussian\" else [(-0.99, 0.99)]\n    elif copula_type == \"student\":\n        x0 = [0.6, 3]  # rho et df\n        bounds = [(-0.99, 0.99), (2, 30)]\n\n    result = minimize(negative_log_likelihood, x0, args=(U, copula_type), bounds=bounds, method='Nelder-Mead')\n\n    if not result.success:\n        raise RuntimeError(f\"MLE failed for {copula_type} copula: {result.message}\")\n\n    return result.x\n```\n:::\n\n\n## III.4. Modélisation la structure de dépendance au moyen des copules paramétriques\n\n\nPour modéliser la structure de dépendance entre les défauts, de manière précise, nous utiliserons les copules. Une copule est une fonction de répartition multivariée de marginales uniformes sur $[0,1]$. Dans le cas bivarié, on a:\n\n$$\n\\mathrm{C}\\left(\\mathrm{u}_1, \\mathrm{u}_{\\mathrm{2}}\\right)=\\mathrm{P}\\left[\\mathrm{U}_1 \\leq \\mathrm{u}_1,\\mathrm{U}_{\\mathrm{2}} \\leq \\mathrm{u}_{\\mathrm{2}}\\right]\n$$\n\nDans le cadre de ce projet, nous allons étudier deux principales familles de copules présentés dans le tableau :\n\n- Copules elliptiques : gaussienne, Student\n- Copules archimédiennes : Clayton, Gumbel, Frank\n\n$$\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{l|c|cc}\n\\hline\nFamille & Nom    & Copule $C(u,v)$   & Paramètres \\\\ \\hline\n\\multirow{2}{*}{Elliptique} & Gaussien & $\\Phi_{\\Sigma}\\left(\\Phi^{-1}(u),\\Phi^{-1}(v)\\right)$ & $\\rho$ \\\\\n& Student &  $T_{\\Sigma}\\left(T_{\\nu}^{-1}(u),T_{\\nu}^{-1}(v)\\right)$ & $\\rho, \\nu$ \\\\\n\\midrule\n\\multirow{3}{*}{Archimédienne} & Frank  & $\\frac{1}{\\theta} \\left(-\\ln(1+ \\frac{(e^{-\\theta u}  - 1)(e^{-\\theta v} - 1)}{e^{-\\theta v} - 1})\\right)$ & $\\theta \\ne 0$ \\\\\n& Gumbel & $\\exp\\left(-\\left((- \\ln(u))^{\\theta} + (- \\ln(v))^{\\theta}\\right)^{\\frac{1}{\\theta}}\\right)$ & $\\theta \\geq 1$ \\\\\n& Clayton  & $(u^{-\\theta} + v^{-\\theta} -1)^{-\\frac{1}{\\theta}}$ & $\\theta > 0$\n \\\\ \\hline\n\\end{tabular}\\\\\n{\\footnotesize *$\\Sigma$ est la matrice de variance covariance. }\n\\caption{Copules archimédiennes bivariées les plus courantes.}\n\\label{tab:copule_family}\n\\end{table}\n$$\n\nIl s'agira après estimation des paramètres et des tests d'ajustement, la copule la plus adéquate pour modéliser au mieux la dépendance entre les variables étudiées.\n\n\nPour l'estimation des paramètres des copules sélectionnées, plusieurs approches méthodologiques s'offrent à nous: la méthode des moments, la méthode du maximum de vraisemblance et l'approche IFM. Nous privilégierons l'approche IFM (Inference Functions for Margins) présenté ci dessous (algo \\ref{IFM}). Cet algorithme a l'avantage d'être plus rapide que la méthode du maximum de vraisemblance.\n\nPour l'évaluation de l'ajustement des copules à la structure de dépendance d'un échantillon, nous utiliserons des outils graphiques tels que le dépendogramme, présenté précedemment, et le Kendall plot. \n\nLe Kendall plot permet une comparaison directe entre la copule empirique et la copule théorique. Plus le Kendall plot se rapproche d'une droite, plus l'ajustement entre la structure de dépendance de l'échantillon et la copule estimée sur ce même échantillon est bon.\n\n\n### III.4.1 Copule gaussienne\n\n#### III.4.1.a. Estimation des paramètres de la copule gaussienne\n\nDans le cadre de la copule gaussienne, il nous faut la matrice de variance qui est donnée par :\n\n$$\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1 \n\\end{pmatrix}\n$$\n\nDans notre cas, le seul paramètre à estimer est le coefficient de corrélation de pearson. Nous allons donc estimer le coefficient de corrélation de pearson entre les rendements des actions de BNP et SG.\n\n::: {#8ff9377e .cell execution_count=18}\n``` {.python .cell-code}\nrho = fit_copula(u_obs, \"gaussian\")[0]\n\nmat= np.array([[1, rho], [rho, 1]])\nmat\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([[1.        , 0.86097656],\n       [0.86097656, 1.        ]])\n```\n:::\n:::\n\n\n#### III.4.1.b. Simulation de la copule gaussienne\n\nPour simuler la copule gaussienne, nous allons utiliser la méthode de distribution puisque la loi est facile à implémenter.\n\n::: {#02dc15c4 .cell execution_count=19}\n``` {.python .cell-code}\n# # Simuler réalisation W suivant une loi normale centrée multivariée\n# import scipy.stats as stats\n\n# np.random.seed(0)\n# n = 1000\n# W = np.random.multivariate_normal([0, 0], mat, n)\n\n# # Calculer U1 et U2\n# U = np.zeros((n, 2))\n\n# for i in range(n):\n#     U[i,0] = stats.norm.cdf(W[i,0])\n#     U[i,1] = stats.norm.cdf(W[i,1])\n\n\nn = u_obs.shape[0]\n_ = GaussianCopula(corr = rho).rvs(n)\nu_est = pseudo_observations(_)\n```\n:::\n\n\n#### III.4.1.c. Test d'ajustement\n\n::: {#0ab55eef .cell execution_count=20}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Données empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule gaussienne\", color = \"red\")\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-20-output-1.png){width=812 height=450}\n:::\n:::\n\n\n::: {#2f17aecc .cell execution_count=21}\n``` {.python .cell-code}\n# Kendall plot\nimport numpy as np\n\ndef calculer_Hi_vect(U):\n    n=len(U)\n    H = np.zeros(n, dtype=int)\n    # Comparaison de chaque paire une seule fois\n    for i in range(n):\n        # Créer des masques booléens pour les conditions\n        cond1 = U[:, 0] <= U[i, 0]  # u_{1,j} <= u_{1,i}\n        cond2 = U[:, 1] <= U[i, 1]  # u_{2,j} <= u_{2,i}\n\n        # Appliquer les conditions et exclure le cas où i == j\n        H[i] = (np.sum(np.logical_and(cond1, cond2)) - 1)\n\n    return H/(len(U)-1)\n\n\ndef kendall_plot(U,S=1000,copula=\"gaussian\",rho=None,nu=None,theta=None):\n\n    H_i = calculer_Hi_vect(U)\n    n = len(U)\n\n    H_means = np.zeros((S, n)) # S lignes et n colonnes\n    for s in range(S):\n        if copula == 'gaussian':\n            X_ = GaussianCopula(corr=rho).rvs(n)\n        elif copula == 'student':\n            X_ = StudentTCopula(df = nu, corr = rho).rvs(n)\n        elif copula == \"gumbel\":\n            X_= GumbelCopula(theta = theta).rvs(n)\n        elif copula == \"clayton\":\n            X_= ClaytonCopula(theta = theta).rvs(n)\n        elif copula == \"frank\":\n            X_= FrankCopula(theta = theta).rvs(n)\n        U_=pseudo_observations(X_)\n        H_means[s] = np.sort(calculer_Hi_vect(U_))\n\n    H_mean = np.mean(H_means, axis=0) # axis=0 pour moyenne par colonne\n\n    x,y = np.sort(H_i), np.sort(H_mean)\n    print(x.shape, y.shape)\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(x, y)\n    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n    plt.title(f\"Kendall plot for {copula} copula\")\n    plt.xlabel(\"i\")\n    plt.ylabel(\"Kendall\")\n    plt.grid(True)\n    plt.show()\n\nS=1000\nkendall_plot(u_obs,S,copula=\"gaussian\",rho=rho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999,) (999,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-21-output-2.png){width=663 height=449}\n:::\n:::\n\n\n#### III.4.1.d. Test d'adéquation\n\n::: {#513f5082 .cell execution_count=22}\n``` {.python .cell-code}\n# Copule empirique Cn(u)\ndef empirical_copula_cdf(U, u):\n    \"\"\"Calcule la copule empirique Cn(u).\"\"\"\n    return np.mean(np.all(U <= u, axis=1))\n\n# Statistique de Cramér-von Mises\ndef cramer_von_mises_stat(U, copula):\n    \"\"\"Calcule la statistique de test Tn.\"\"\"\n    n = len(U)\n    Tn = 0.0\n    for i in range(n):\n        u_i = U[i]\n        Cn = empirical_copula_cdf(U, u_i)  # Copule empirique Cn(u_i)\n        C_theta = copula.cdf([u_i])  # Copule théorique estimée Cθ(u_i)\n        Tn += (Cn - C_theta) ** 2\n    return Tn\n\n# Test d’adéquation complet avec bootstrap paramétrique\ndef adequation_test(X, copula_type=\"gaussian\", M=500):\n    \"\"\"\n    Test d'adéquation de Genest & Rémillard (2008) pour une copule avec\n    statistique de Cramér-von Mises et bootstrap paramétrique.\n    \"\"\"\n    # Pseudo-observations\n    U = pseudo_observations(X)\n    n = len(U)\n\n    # Estimation MLE de la copule sur les données\n    params = fit_copula(U, copula_type)\n    copula = get_copula(copula_type, params)\n\n    # Calcul de la statistique observée Tn\n    T_obs = cramer_von_mises_stat(U, copula)\n\n    # Bootstrap paramétrique\n    T_boot = []\n    for _ in range(M):\n        # 1. Simulation d’un échantillon sous la copule ajustée\n        U_boot = copula.rvs(n)\n\n        # 2. Ré-estimation de la copule sur U_boot\n        params_boot = fit_copula(U_boot, copula_type)\n        copula_boot = get_copula(copula_type, params_boot)\n\n        # 3. Calcul de Tn pour cet échantillon bootstrap\n        T_boot.append(cramer_von_mises_stat(U_boot, copula_boot))\n\n    # Calcul de la p-value (proportion des T_boot supérieurs à T_obs)\n    p_value = np.mean(np.array(T_boot) >= T_obs)\n\n    return {\n        \"copula_type\": copula_type,\n        \"params\": params,\n        \"T_obs\": T_obs,\n        \"p_value\": p_value\n    }\n\nfrom pprint import pprint\nresult_gaussian = adequation_test(X, copula_type=\"gaussian\")\npprint(result_gaussian)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'T_obs': np.float64(0.054564311624880506),\n 'copula_type': 'gaussian',\n 'p_value': np.float64(0.886),\n 'params': array([0.86097656])}\n```\n:::\n:::\n\n\n### III.4.2 Copule de student\n\n#### III.4.2.a. Estimation des paramètres de la copule student\n\n::: {#e5451056 .cell execution_count=23}\n``` {.python .cell-code}\nrho,nu = fit_copula(u_obs, \"student\")\n\nprint(\"rho = \", rho)\nprint(\"nu = \", nu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrho =  0.8496143388748167\nnu =  2.0\n```\n:::\n:::\n\n\n#### III.4.2.b. Simulation de la copule de student\n\n::: {#45539844 .cell execution_count=24}\n``` {.python .cell-code}\nn = u_obs.shape[0]\n_ = StudentTCopula(corr = rho, df=nu).rvs(n)\nu_est = pseudo_observations(_)\n```\n:::\n\n\n#### III.4.2.c. Test d'ajustement\n\n::: {#fa9bc028 .cell execution_count=25}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Données empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de student\", color = \"red\")\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-25-output-1.png){width=812 height=450}\n:::\n:::\n\n\n::: {#4d30afde .cell execution_count=26}\n``` {.python .cell-code}\nkendall_plot(u_obs,S,copula=\"student\",rho=rho,nu=nu)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999,) (999,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-26-output-2.png){width=663 height=449}\n:::\n:::\n\n\n:::{.callout-warning}\nAttention, nous avons utilisé le package copula.api de statsmodels pour implémenter les copules. Cependant, la classe correspondant à la copule de Student ne permet pas d'obtenir une fonction de répartition (voir [lien](https://www.statsmodels.org/dev/_modules/statsmodels/distributions/copula/elliptical.html#StudentTCopula.cdf)).\n\nEn utilisant un environnement virtuel, il a été possible de modifier le fichier statsmodels/distributions/copula/elliptical.py du package afin d'implémenter la méthode pour la fonction de répartition. Vous trouverez ce fichier [ci-joint](utiles/elliptical.py) afin de garantir le bon fonctionnement du code si vous devez le relancer.\n:::\n\n#### III.4.2.d. Test d'adéquation\n\n::: {#816bc9d9 .cell execution_count=27}\n``` {.python .cell-code}\nfrom pprint import pprint\nresult_std = adequation_test(X, copula_type=\"student\") # ATTENTION DIFFICULTE POUR ESSTIMER COPULE STUDENT\npprint(result_std)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'T_obs': np.float64(0.05938600115289895),\n 'copula_type': 'student',\n 'p_value': np.float64(0.842),\n 'params': array([0.84961434, 2.        ])}\n```\n:::\n:::\n\n\n### III.4.3 Copule de clayton\n\n#### III.4.3.a. Estimation des paramètres de la copule clayton\n\n::: {#eea34a95 .cell execution_count=28}\n``` {.python .cell-code}\ntheta = fit_copula(u_obs, \"clayton\")[0]\n\nprint(\"theta = \", theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntheta =  2.4768310546874988\n```\n:::\n:::\n\n\n#### III.4.3.b. Simulation de la copule de clayton\n\n::: {#732442ab .cell execution_count=29}\n``` {.python .cell-code}\nn = u_obs.shape[0]\n_ = ClaytonCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n```\n:::\n\n\n#### III.4.3.c. Test d'ajustement\n\n::: {#34ae8db2 .cell execution_count=30}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Données empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de clayton\", color = \"red\")\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-30-output-1.png){width=812 height=450}\n:::\n:::\n\n\n::: {#c5464cc5 .cell execution_count=31}\n``` {.python .cell-code}\nS=1000\nkendall_plot(u_obs,S,copula=\"clayton\",theta=theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999,) (999,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-31-output-2.png){width=663 height=449}\n:::\n:::\n\n\n#### III.4.3.d. Test d'adéquation\n\n::: {#3191f732 .cell execution_count=32}\n``` {.python .cell-code}\nfrom pprint import pprint\nresult_clayton= adequation_test(X, copula_type=\"clayton\") \npprint(result_clayton)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'T_obs': array([0.86477661]),\n 'copula_type': 'clayton',\n 'p_value': np.float64(0.012),\n 'params': array([2.47683105])}\n```\n:::\n:::\n\n\n### III.4.4 Copule de gumbel\n\n#### III.4.4.a. Estimation des paramètres de la copule gumbel\n\n::: {#137df49c .cell execution_count=33}\n``` {.python .cell-code}\ntheta = fit_copula(u_obs, \"gumbel\")[0]\n\nprint(\"theta = \", theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntheta =  2.9939208984374996\n```\n:::\n:::\n\n\n#### III.4.4.b. Simulation de la copule de clayton\n\n::: {#00ff74f6 .cell execution_count=34}\n``` {.python .cell-code}\nn = u_obs.shape[0]\n_ = GumbelCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n```\n:::\n\n\n#### III.4.4.c. Test d'ajustement\n\n::: {#e451fe16 .cell execution_count=35}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Données empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de gumbel\", color = \"red\")\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-35-output-1.png){width=812 height=450}\n:::\n:::\n\n\n::: {#abf19c0e .cell execution_count=36}\n``` {.python .cell-code}\nkendall_plot(u_obs,S,copula=\"gumbel\",theta=theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999,) (999,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-36-output-2.png){width=663 height=449}\n:::\n:::\n\n\n#### III.4.4.d. Test d'adéquation\n\n::: {#f3029130 .cell execution_count=37}\n``` {.python .cell-code}\nfrom pprint import pprint\nresult_gumbel = adequation_test(X, copula_type=\"gumbel\") \npprint(result_gumbel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'T_obs': array([0.03485682]),\n 'copula_type': 'gumbel',\n 'p_value': np.float64(0.976),\n 'params': array([2.9939209])}\n```\n:::\n:::\n\n\n### III.4.4 Copule de frank\n\n#### III.4.4.a. Estimation des paramètres de la copule frank\n\n::: {#d268f9e1 .cell execution_count=38}\n``` {.python .cell-code}\ntheta = fit_copula(u_obs, \"frank\")[0]\n\nprint(\"theta = \", theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntheta =  10.0\n```\n:::\n:::\n\n\n#### III.4.4.b. Simulation de la copule de clayton\n\n::: {#76d3055f .cell execution_count=39}\n``` {.python .cell-code}\nn = u_obs.shape[0]\n_ = FrankCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n```\n:::\n\n\n#### III.4.4.c. Test d'ajustement\n\n::: {#0b85cdfe .cell execution_count=40}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Données empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de frank\", color = \"red\")\nplt.title(\"Dépendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-40-output-1.png){width=812 height=450}\n:::\n:::\n\n\n::: {#e73e6f31 .cell execution_count=41}\n``` {.python .cell-code}\nkendall_plot(u_obs,S,copula=\"frank\",theta=theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999,) (999,)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-41-output-2.png){width=663 height=449}\n:::\n:::\n\n\n#### III.4.4.d. Test d'adéquation\n\n::: {#54d4e9cd .cell execution_count=42}\n``` {.python .cell-code}\nfrom pprint import pprint\nresult_frank = adequation_test(X, copula_type=\"frank\") \npprint(result_frank)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'T_obs': array([0.12454173]),\n 'copula_type': 'frank',\n 'p_value': np.float64(0.432),\n 'params': array([10.])}\n```\n:::\n:::\n\n\n### III.5. Résultats\n\nNos résultats mettent en évidence l’importance de la structure de dépendance dans l’évaluation du risque de crédit. Après avoir testé plusieurs copules paramétriques, nous avons retenu la copule de Gumbel comme la plus appropriée, en raison de son bon ajustement aux données et de sa capacité à capturer les asymétries et les queues de distribution lourdes, essentielles dans un contexte de crise financière. \n\n::: {#9b795f5c .cell execution_count=43}\n``` {.python .cell-code}\ndict_list = [result_gaussian, result_std, result_clayton, result_gumbel, result_frank]\n\n# Convert to DataFrame\ndf = pd.DataFrame(dict_list)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>copula_type</th>\n      <th>params</th>\n      <th>T_obs</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gaussian</td>\n      <td>[0.8609765625000002]</td>\n      <td>0.054564</td>\n      <td>0.886</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>student</td>\n      <td>[0.8496143388748167, 2.0]</td>\n      <td>0.059386</td>\n      <td>0.842</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>clayton</td>\n      <td>[2.4768310546874988]</td>\n      <td>[0.8647766118220173]</td>\n      <td>0.012</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gumbel</td>\n      <td>[2.9939208984374996]</td>\n      <td>[0.034856823632549765]</td>\n      <td>0.976</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>frank</td>\n      <td>[10.0]</td>\n      <td>[0.12454172887238589]</td>\n      <td>0.432</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# IV. Estimation de la Credit VaR\n\n\nLa simulation Monte Carlo nous a permis d’estimer la distribution des pertes du portefeuille et de déterminer une CreditVaR à 99\\% de 1286,61 euros. L’analyse de la convergence des résultats confirme la robustesse de cette estimation, qui reste stable malgré les variations de taille d’échantillon.\n\nAinsi, cette étude illustre la pertinence des copules pour modéliser de manière plus réaliste la dépendance entre les actifs financiers, là où les approches classiques fondées sur la corrélation linéaire sont insuffisantes. Une perspective d’amélioration pourrait consister à explorer des copules dynamiques ou à affiner l’estimation des paramètres via des méthodes bayésiennes.\n\n::: {#456a0c5a .cell execution_count=44}\n``` {.python .cell-code}\n# Calcul de la credit var par monte carlo\n\ndef credit_var(params_R1,params_R2,lambda_1,lambda_2,EAD,params,S,tau,copula_type=\"gumbel\",alpha=0.99):\n    \"\"\"\n        Cette fonction est Var de niveau alpha qui estime la perte maximale sur l'horizon tau\n        et retourne le quantile de niveau 99% pour dim=2.\n    \"\"\"\n\n    copula = get_copula(copula_type, params)\n    U = copula.rvs(S)\n    a1,b1 = params_R1\n    a2,b2 = params_R2\n\n    # Générer des échantillons pour lgd_bnp et lgd_sg avec la loi Beta, vectorisée\n    LGD1 = 1 - np.random.beta(a1, b1, S)\n    LGD2 = 1 - np.random.beta(a2, b2, S)\n\n\n    # Simuler tau_bnp et tau_sg en utilisant la fonction de répartition inverse de la loi exponentielle\n    tau_bnp = stats.expon.ppf(U[:, 0], scale=1/lambda_1)\n    tau_sg = stats.expon.ppf(U[:, 1], scale=1/lambda_2)\n\n    # Calculer les pertes, vectorisée\n    L1 = np.where(tau_bnp < tau, LGD1 * EAD, 0)\n    L2 = np.where(tau_sg < tau, LGD2 * EAD, 0)\n\n    # Somme des pertes\n    L = L1 + L2\n\n    # Calculer et retourner le quantile de niveau 99%\n    return np.percentile(L, alpha * 100)\n```\n:::\n\n\n::: {#8b510b90 .cell execution_count=45}\n``` {.python .cell-code}\n# Tracer la courbe de la VaR en fonction de S\n# set seed\nnp.random.seed(123)\n\nS_values = np.arange(100, 100000, 100)\nparams = fit_copula(U, \"gumbel\")\nEAD = 1000\ntau = 4\ndef PD(T,spread,Recouvrement):\n    lambda_ = spread / (1 - Recouvrement)\n    PD = 1 - np.exp(-lambda_*T)\n    return PD,lambda_\n\n_,lambda_1 = PD(4,spread_4Y[0],tx_recouvrement)\n_,lambda_2 = PD(4,spread_5Y[0],tx_recouvrement)\n\nparams_R1 = a_BNP,b_BNP\nparams_R2 = a_SG,b_SG\nVaR_values = [credit_var(params_R1=params_R1,params_R2=params_R2, lambda_1=lambda_1, \\\n                         lambda_2=lambda_2, EAD=EAD, params=params, S=S, tau=tau) for S in S_values]\n\n# Je veux tracer une droite horizontale pour la mediane de la VaR et mettre ça valeur sur l'axe des ordonnées\nplt.figure(figsize=(10, 5))\nplt.plot(S_values, VaR_values)\nplt.axhline(np.mean(VaR_values), color=\"red\", linestyle=\"--\")\nplt.xlabel(\"S\")\nplt.ylabel(\"VaR\")\nplt.grid(False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_copules_files/figure-html/cell-45-output-1.png){width=825 height=429}\n:::\n:::\n\n\n::: {#52203d8c .cell execution_count=46}\n``` {.python .cell-code}\nnp.mean(VaR_values)\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\nnp.float64(1286.2298311022887)\n```\n:::\n:::\n\n\n::: {#061542cd .cell execution_count=47}\n``` {.python .cell-code}\nS = 40000\ncredit_var(params_R1=params_R1,params_R2=params_R2, lambda_1=lambda_1, \\\n                         lambda_2=lambda_2, EAD=EAD, params=params, S=S, tau=tau)\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\nnp.float64(1290.3822356040268)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "var_copules_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}