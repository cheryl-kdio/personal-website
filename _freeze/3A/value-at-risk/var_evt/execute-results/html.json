{
  "hash": "3f27747bcf93b485a4083d681ff52ab2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle:  TP2:Méthodes basées sur la théorie des valeurs extrêmes]\njupyter: python3\ndate: \"2025-02-28\"\n---\n\n\n\n\nCe TP est une continuité du TP-1 dans lequel on souhaitait implémenter la VaR (Value at Risk) et l'ES (Expected Shortfall) en utilisant les méthodes classiques proposées dans la réglementation bâloise, i.e. la méthode historique, paramétrique et bootstrap. Cependant, une limite de ces méthodes est qu'elles ne prennent pas en compte la queue de distribution de la perte. Pour remédier à cela, on peut utiliser des méthodes avec la théorie des valeurs extrêmes, i.e. l'approche Block Maxima et l'approche Peaks Over Threshold.\n\n::: {#060e40a0 .cell execution_count=2}\n``` {.python .cell-code}\n# Définition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n\n```\n:::\n:::\n\n\n::: {#e671fbd2 .cell execution_count=3}\n``` {.python .cell-code}\n# Import des données du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la première ligne\ndata = data.dropna()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYF.download() has changed argument auto_adjust default to True\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[*********************100%***********************]  1 of 1 completed\n```\n:::\n:::\n\n\n::: {#5fd986cc .cell execution_count=4}\n``` {.python .cell-code}\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test\n```\n:::\n\n\n## I. Implémentation de la VaR avec la théorie des valeurs extrêmes\n\n### I.1. VaR TVE : Approche Maxima par bloc\n\nL'approche des Block Maxima (BM) est une méthode modélise les maxima des rendements sur des blocs de taille fixe $s$ en utilisant la distribution GEV. Le seuil de confiance $\\alpha_{\\text{GEV}}$ est ajusté pour correspondre à l'horizon temporel de la VaR via la relation :\n\n$$\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n$$\n\nNous allons dans ce projet une taille de blocs $s = 21$ jours ouvrés comme ce qui souvent utilisé en pratique. De ce fait, nous parvenons à construire 239 blocs de taille 21 et un bloc de taille  De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ \\alpha_{\\text{VaR}}$ est :\n\n$$\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n$$\n\noù G est la fonction de répartition de la GEV ($\\hat \\mu, \\hat \\sigma, \\hat \\xi$) estimée.\n\n### I.1.1. Construction de l'échantillon de maxima sur data_train\n\n::: {#73ade8e1 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extrêmes d'une série de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associées aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # Sélectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # Récupérer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des données suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) >= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extrêmes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-5-output-1.png){width=832 height=450}\n:::\n:::\n\n\nPour avoir une idée de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour $s=21$, nous utilisons un Gumbel plot qui est un outil graphique pour juger de l’hypothèse $\\xi=0$, i.e. la distribution GEV se réduit à la distribution de Gumbel.\n\nPour le construire, nous devons suivre les étapes suivantes :\n\n1. calculer l'abscisse avec la série des maximas ordonées $R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}$.\n2. calculer l'ordonnée de la manière suivante :\n\n$$\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n$$\n\nLorsque la distribution adaptée est celle de Gumbel alors le Gumbel plot est linéaire. Dans notre cas, nous constatons une courbure ce qui nous pousse à conclure qu'une distribution Gumbel n'est pas adaptée dans la modélisation des maxima des pertes de rendements du CAC 40. Une distribution Fréchet ou de Weibull serait plus adaptée.\n\n::: {#13dce6ee .cell execution_count=6}\n``` {.python .cell-code}\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-6-output-1.png){width=579 height=431}\n:::\n:::\n\n\n### I.1.2. Estimation des paramètres de la loi de GEV\n\nEn estimant les paramètres de la loi GEV, nous utilisons la méthode du maximum de vraisemblance. Les paramètres estimés par maximisation de la fonction de vraisemblance sont les suivants $\\xi = -0.15, \\mu=0.02, \\sigma=0.01$. Nous constatons par ailleurs que le paramètre de forme $\\xi$ est négatif ce qui est cohérent avec notre observation précédente. \n\n::: {#747c4dd1 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les paramètres estimés\nprint(\"=\"*50)\nprint(\"Paramètres estimés de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nParamètres estimés de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n```\n:::\n:::\n\n\nPour accorder plus de poids à cette observation, nous avons calculé un intervalle de confiance profilé à 95% pour le paramètre de forme $\\xi$. Pour ce faire, nous avons suivi les étapes suivantes :\n1. Estimation des paramètres par maximum de vraisemblance : Nous avons estimé $\\hat{\\xi}$, $\\hat{\\mu}$ et $\\hat{\\sigma}$ en maximisant la log-vraisemblance de la loi GEV.\n\n2. Construction du profil de vraisemblance : Nous avons fixé $\\xi$ à différentes valeurs autour de $\\hat{\\xi}$ et, pour chacune, réestimé $\\mu$ et $\\sigma$ afin d'obtenir une log-vraisemblance profilée.\n\n3. Seuil basé sur le test du rapport de vraisemblance : Le seuil critique est déterminé par la statistique $ \\chi^2(1) $ :  \n    $$\n    \\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n    $$\n\n4. Détermination des bornes de l’IC : L’intervalle est formé par les valeurs de $\\xi$ pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l'incertitude en évitant les approximations asymptotiques classiques.\nLa modélisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adaptée.\n\nNous obtenons ainsi un intervalle de confiance à 95% pour le paramètre de forme $\\xi$ de $[-0.284, -0.039]$. Comme 0 n'appartient pas à cet intervalle, nous pouvons rejeter l'hypothèse $\\xi=0$. De ce fait, la distribution de Weibull est plus adaptée pour modéliser les maxima des pertes de rendements du CAC 40 car $\\xi$ est négatif.\n\n::: {#879b1544 .cell execution_count=8}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance négative de la distribution GEV\n    en fixant le paramètre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale <= 0:  # Contrainte pour éviter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profilés pour le paramètre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estimée\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # Réoptimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' à la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# Déterminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood >= chi2_threshold)[0]\nif len(valid_points) > 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profilé pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de déterminer des IC profilés avec les paramètres actuels.\")\n\n# Tracé du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIC profilé pour shape: [-0.287, -0.042]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-8-output-2.png){width=601 height=449}\n:::\n:::\n\n\n#### a. Validation ex-ante\n\nOn remarque la loi GEV estimée par une weibull semble coller à la distribution des rendements extrêmes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles théoriques de la GEV-Weibull et empiriques sembelnt alignés sauf pour les quantiles élévés où l'on constate un décrochage.\n\n::: {#b0e1d274 .cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observées')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajustée')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-9-output-1.png){width=790 height=431}\n:::\n:::\n\n\n::: {#5d14c658 .cell execution_count=10}\n``` {.python .cell-code}\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi GEV\")\nplt.xlabel('Quantiles théoriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-10-output-1.png){width=827 height=451}\n:::\n:::\n\n\n#### b. Calcul de la VaR TVE par MB\n\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\n$$\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n$$\n\noù G est la fonction de répartition de la GEV$(\\hat \\mu, \\hat \\sigma, \\hat \\xi)$ estimée, et $\\alpha_{\\text{GEV}}$ est ajusté pour correspondre à l'horizon temporel de la VaR via la relation :\n\n$$\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n$$\n\nPour convertir la VaR à horizon 1jour en VaR à horizon T jours, la méthode de scaling soulève quelques questions, car elle repose essentiellement sur la normalité et l'indépendance des rendements ce qui n'est pas le cas en pratique. De ce fait, nous utiliserons la méthode alternative reposant sur la théorie des valeurs extrêmes.\n\ny revenir\n\n::: {#9e7ee060 .cell execution_count=11}\n``` {.python .cell-code}\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n```\n:::\n:::\n\n\n### I.1.3. Estimation des paramètres de la loi de EV\n\nBien que l'intervalle de confiance à 95% pour le paramètre de forme $\\xi$ de $[-0.284, -0.039]$ ne contienne pas 0, nous avons tout de même estimé les paramètres de la loi EV pour comparer les résultats avec ceux de la loi GEV.\nEn estimant tout de même les paramètres de la loi EV, nous obtenons les paramètres suivants : $\\mu=0.02, \\sigma=0.01, \\xi=0$.\n\nNous constatons que la loi EV ne semble pas mal s'adapter à la distribution des rendements extrêmes du CAC 40.\n\n::: {#994f54cc .cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les paramètres estimés\nprint(\"=\"*50)\nprint(\"Paramètres estimés de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nParamètres estimés de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n```\n:::\n:::\n\n\n::: {#68a5f0c4 .cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observées')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densité GEV ajustée\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajustée')\n\n# Densité Gumbel ajustée\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajustée')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-13-output-1.png){width=790 height=431}\n:::\n:::\n\n\n::: {#7f26bb62 .cell execution_count=14}\n``` {.python .cell-code}\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi GEV Weibull\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi Gumbel\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-14-output-1.png){width=1142 height=471}\n:::\n:::\n\n\n::: {#f2fff384 .cell execution_count=15}\n``` {.python .cell-code}\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n```\n:::\n:::\n\n\nDe plus, les résultats en terme de VaR sont très proches entre les deux modèles.\n\n## I.2.\tVaR TVE : Approche Peak over threshold\n\n### I.2.1. Choix du seuil u\n\nCette méthode est basée sur la modélisation de la distribution des excès au-dessus d'un seuil élevé de log-rendement négatif ($u$), seuil déterminé de manière subjective à partir de l'analyse du mean residual life plot, en ajustant une distribution de Pareto généralisée (GPD). Dans le mean residual life plot, si les excès au-delà de 𝒖 suivent une loi GPD, alors le mean-excess plot a un comportement linéaire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est linéaire. Nous ne privilégions pas les seuils $u$ élevés puisque la moyenne est faite sur peu d'observations.\n\nNous allons choisir un seuil $u = 0.03$ pour lequel le mean residual life plot est linéaire. Nous allons ensuite ajuster une distribution GPD pour les excès au-dessus de ce seuil.\n\n::: {#13ade2cd .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Paramètres :\n    - data : array-like, données d'entrée.\n    - tlim : tuple (min, max), limites des seuils (si None, calculé automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils à considérer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les données\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn <= 5:\n        raise ValueError(\"Les données contiennent trop peu de valeurs valides.\")\n\n    # Définition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # Évite les 4 plus grandes valeurs\n\n    if np.all(data <= tlim[1]):\n        raise ValueError(\"La borne supérieure du seuil est trop élevée.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data <= tlim[0]), np.mean(data <= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les résultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data > u] - u  # Excès au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Tracé du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile à 5%\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-16-output-1.png){width=683 height=449}\n:::\n:::\n\n\n### I.2.2. Estimation des paramètres de la loi GPD\n\nEn estimant les paramètres de la loi GPD, nous utilisons la méthode du maximum de vraisemblance. Les paramètres estimés par maximisation de la fonction de vraisemblance sont les suivants $\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01$. De ce fait, la distribution de Pareto généralisée est adaptée pour modéliser les excès au-dessus du seuil $u = 0.03$.\n\n::: {#d2b3132b .cell execution_count=17}\n``` {.python .cell-code}\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value >= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les paramètres estimés\nprint(\"Paramètres estimés de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParamètres estimés de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n```\n:::\n:::\n\n\n### I.2.3. Validation ex-ante\n\nEn comparant la distribution GPD estimée et la distribution empirique des excès, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estimé indique que les quantiles théoriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observés dans notre distribution des excès. Nous concluons que la distribution GPD n'est pas adaptée pour modéliser les excès au-dessus du seuil $u = 0.03$. Cela peut être dû à un mauvais choix du seuil $u$, une analyse plus aprofondie aurait été nécessaire pour choisir un seuil plus adapté.\n\n::: {#6bd407aa .cell execution_count=18}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Données observées des excès')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densité GPD ajustée\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajustée')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-18-output-1.png){width=798 height=431}\n:::\n:::\n\n\n::: {#ac4f4383 .cell execution_count=19}\n``` {.python .cell-code}\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une modélisation par loi GPD\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-19-output-1.png){width=820 height=451}\n:::\n:::\n\n\n### I.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance ${\\alpha}$ est alors obtenue par la formule :\n\n$$\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n$$\n\noù $\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})$ est la fonction de répartition de la GPD($\\hat{\\sigma},\\hat{\\xi}$) estimée, $\\alpha_{\\text{POT}}$ est le quantile ajusté, nécessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\n\nComme on ne se concentre que sur l’échantillon des excès dans cette modélisation, l’estimation de la VaR à partir de la GPD ne doit pas se faire au niveau $\\alpha$, mais à un niveau ajusté $\\alpha_{\\text{POT}}$ défini par la relation suivante :\n\n$$\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n$$\n\noù $n$ représente le nombre total d'observations, $N_u$ correspond au nombre d'excès au-delà du seuil $u$,\n\n::: {#0fa7abbd .cell execution_count=20}\n``` {.python .cell-code}\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value >= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%\n```\n:::\n:::\n\n\n",
    "supporting": [
      "var_evt_files"
    ],
    "filters": [],
    "includes": {}
  }
}