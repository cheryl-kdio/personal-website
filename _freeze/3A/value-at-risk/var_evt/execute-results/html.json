{
  "hash": "3f27747bcf93b485a4083d681ff52ab2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle:  TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes]\njupyter: python3\ndate: \"2025-02-28\"\n---\n\n\n\n\nCe TP est une continuit√© du TP-1 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l'ES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e. la m√©thode historique, param√©trique et bootstrap. Cependant, une limite de ces m√©thodes est qu'elles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, on peut utiliser des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e. l'approche Block Maxima et l'approche Peaks Over Threshold.\n\n::: {#060e40a0 .cell execution_count=2}\n``` {.python .cell-code}\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n\n```\n:::\n:::\n\n\n::: {#e671fbd2 .cell execution_count=3}\n``` {.python .cell-code}\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYF.download() has changed argument auto_adjust default to True\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[*********************100%***********************]  1 of 1 completed\n```\n:::\n:::\n\n\n::: {#5fd986cc .cell execution_count=4}\n``` {.python .cell-code}\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test\n```\n:::\n\n\n## I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes\n\n### I.1. VaR TVE : Approche Maxima par bloc\n\nL'approche des Block Maxima (BM) est une m√©thode mod√©lise les maxima des rendements sur des blocs de taille fixe $s$ en utilisant la distribution GEV. Le seuil de confiance $\\alpha_{\\text{GEV}}$ est ajust√© pour correspondre √† l'horizon temporel de la VaR via la relation :\n\n$$\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n$$\n\nNous allons dans ce projet une taille de blocs $s = 21$ jours ouvr√©s comme ce qui souvent utilis√© en pratique. De ce fait, nous parvenons √† construire 239 blocs de taille 21 et un bloc de taille  De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ \\alpha_{\\text{VaR}}$ est :\n\n$$\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n$$\n\no√π G est la fonction de r√©partition de la GEV ($\\hat \\mu, \\hat \\sigma, \\hat \\xi$) estim√©e.\n\n### I.1.1. Construction de l'√©chantillon de maxima sur data_train\n\n::: {#73ade8e1 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) >= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extr√™mes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-5-output-1.png){width=832 height=450}\n:::\n:::\n\n\nPour avoir une id√©e de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour $s=21$, nous utilisons un Gumbel plot qui est un outil graphique pour juger de l‚Äôhypoth√®se $\\xi=0$, i.e. la distribution GEV se r√©duit √† la distribution de Gumbel.\n\nPour le construire, nous devons suivre les √©tapes suivantes :\n\n1. calculer l'abscisse avec la s√©rie des maximas ordon√©es $R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}$.\n2. calculer l'ordonn√©e de la mani√®re suivante :\n\n$$\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n$$\n\nLorsque la distribution adapt√©e est celle de Gumbel alors le Gumbel plot est lin√©aire. Dans notre cas, nous constatons une courbure ce qui nous pousse √† conclure qu'une distribution Gumbel n'est pas adapt√©e dans la mod√©lisation des maxima des pertes de rendements du CAC 40. Une distribution Fr√©chet ou de Weibull serait plus adapt√©e.\n\n::: {#13dce6ee .cell execution_count=6}\n``` {.python .cell-code}\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-6-output-1.png){width=579 height=431}\n:::\n:::\n\n\n### I.1.2. Estimation des param√®tres de la loi de GEV\n\nEn estimant les param√®tres de la loi GEV, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants $\\xi = -0.15, \\mu=0.02, \\sigma=0.01$. Nous constatons par ailleurs que le param√®tre de forme $\\xi$ est n√©gatif ce qui est coh√©rent avec notre observation pr√©c√©dente. \n\n::: {#747c4dd1 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nParam√®tres estim√©s de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n```\n:::\n:::\n\n\nPour accorder plus de poids √† cette observation, nous avons calcul√© un intervalle de confiance profil√© √† 95% pour le param√®tre de forme $\\xi$. Pour ce faire, nous avons suivi les √©tapes suivantes :\n1. Estimation des param√®tres par maximum de vraisemblance : Nous avons estim√© $\\hat{\\xi}$, $\\hat{\\mu}$ et $\\hat{\\sigma}$ en maximisant la log-vraisemblance de la loi GEV.\n\n2. Construction du profil de vraisemblance : Nous avons fix√© $\\xi$ √† diff√©rentes valeurs autour de $\\hat{\\xi}$ et, pour chacune, r√©estim√© $\\mu$ et $\\sigma$ afin d'obtenir une log-vraisemblance profil√©e.\n\n3. Seuil bas√© sur le test du rapport de vraisemblance : Le seuil critique est d√©termin√© par la statistique $ \\chi^2(1) $ :  \n    $$\n    \\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n    $$\n\n4. D√©termination des bornes de l‚ÄôIC : L‚Äôintervalle est form√© par les valeurs de $\\xi$ pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l'incertitude en √©vitant les approximations asymptotiques classiques.\nLa mod√©lisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adapt√©e.\n\nNous obtenons ainsi un intervalle de confiance √† 95% pour le param√®tre de forme $\\xi$ de $[-0.284, -0.039]$. Comme 0 n'appartient pas √† cet intervalle, nous pouvons rejeter l'hypoth√®se $\\xi=0$. De ce fait, la distribution de Weibull est plus adapt√©e pour mod√©liser les maxima des pertes de rendements du CAC 40 car $\\xi$ est n√©gatif.\n\n::: {#879b1544 .cell execution_count=8}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance n√©gative de la distribution GEV\n    en fixant le param√®tre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale <= 0:  # Contrainte pour √©viter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profil√©s pour le param√®tre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estim√©e\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # R√©optimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' √† la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# D√©terminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood >= chi2_threshold)[0]\nif len(valid_points) > 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profil√© pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de d√©terminer des IC profil√©s avec les param√®tres actuels.\")\n\n# Trac√© du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIC profil√© pour shape: [-0.287, -0.042]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-8-output-2.png){width=601 height=449}\n:::\n:::\n\n\n#### a. Validation ex-ante\n\nOn remarque la loi GEV estim√©e par une weibull semble coller √† la distribution des rendements extr√™mes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles th√©oriques de la GEV-Weibull et empiriques sembelnt align√©s sauf pour les quantiles √©l√©v√©s o√π l'on constate un d√©crochage.\n\n::: {#b0e1d274 .cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-9-output-1.png){width=790 height=431}\n:::\n:::\n\n\n::: {#5d14c658 .cell execution_count=10}\n``` {.python .cell-code}\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV\")\nplt.xlabel('Quantiles th√©oriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-10-output-1.png){width=827 height=451}\n:::\n:::\n\n\n#### b. Calcul de la VaR TVE par MB\n\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\n$$\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n$$\n\no√π G est la fonction de r√©partition de la GEV$(\\hat \\mu, \\hat \\sigma, \\hat \\xi)$ estim√©e, et $\\alpha_{\\text{GEV}}$ est ajust√© pour correspondre √† l'horizon temporel de la VaR via la relation :\n\n$$\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n$$\n\nPour convertir la VaR √† horizon 1jour en VaR √† horizon T jours, la m√©thode de scaling soul√®ve quelques questions, car elle repose essentiellement sur la normalit√© et l'ind√©pendance des rendements ce qui n'est pas le cas en pratique. De ce fait, nous utiliserons la m√©thode alternative reposant sur la th√©orie des valeurs extr√™mes.\n\ny revenir\n\n::: {#9e7ee060 .cell execution_count=11}\n``` {.python .cell-code}\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n```\n:::\n:::\n\n\n### I.1.3. Estimation des param√®tres de la loi de EV\n\nBien que l'intervalle de confiance √† 95% pour le param√®tre de forme $\\xi$ de $[-0.284, -0.039]$ ne contienne pas 0, nous avons tout de m√™me estim√© les param√®tres de la loi EV pour comparer les r√©sultats avec ceux de la loi GEV.\nEn estimant tout de m√™me les param√®tres de la loi EV, nous obtenons les param√®tres suivants : $\\mu=0.02, \\sigma=0.01, \\xi=0$.\n\nNous constatons que la loi EV ne semble pas mal s'adapter √† la distribution des rendements extr√™mes du CAC 40.\n\n::: {#994f54cc .cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nParam√®tres estim√©s de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n```\n:::\n:::\n\n\n::: {#68a5f0c4 .cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GEV ajust√©e\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\n\n# Densit√© Gumbel ajust√©e\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajust√©e')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-13-output-1.png){width=790 height=431}\n:::\n:::\n\n\n::: {#7f26bb62 .cell execution_count=14}\n``` {.python .cell-code}\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV Weibull\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi Gumbel\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-14-output-1.png){width=1142 height=471}\n:::\n:::\n\n\n::: {#f2fff384 .cell execution_count=15}\n``` {.python .cell-code}\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n```\n:::\n:::\n\n\nDe plus, les r√©sultats en terme de VaR sont tr√®s proches entre les deux mod√®les.\n\n## I.2.\tVaR TVE : Approche Peak over threshold\n\n### I.2.1. Choix du seuil u\n\nCette m√©thode est bas√©e sur la mod√©lisation de la distribution des exc√®s au-dessus d'un seuil √©lev√© de log-rendement n√©gatif ($u$), seuil d√©termin√© de mani√®re subjective √† partir de l'analyse du mean residual life plot, en ajustant une distribution de Pareto g√©n√©ralis√©e (GPD). Dans le mean residual life plot, si les exc√®s au-del√† de ùíñ suivent une loi GPD, alors le mean-excess plot a un comportement lin√©aire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est lin√©aire. Nous ne privil√©gions pas les seuils $u$ √©lev√©s puisque la moyenne est faite sur peu d'observations.\n\nNous allons choisir un seuil $u = 0.03$ pour lequel le mean residual life plot est lin√©aire. Nous allons ensuite ajuster une distribution GPD pour les exc√®s au-dessus de ce seuil.\n\n::: {#13ade2cd .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Param√®tres :\n    - data : array-like, donn√©es d'entr√©e.\n    - tlim : tuple (min, max), limites des seuils (si None, calcul√© automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils √† consid√©rer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les donn√©es\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn <= 5:\n        raise ValueError(\"Les donn√©es contiennent trop peu de valeurs valides.\")\n\n    # D√©finition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # √âvite les 4 plus grandes valeurs\n\n    if np.all(data <= tlim[1]):\n        raise ValueError(\"La borne sup√©rieure du seuil est trop √©lev√©e.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data <= tlim[0]), np.mean(data <= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les r√©sultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data > u] - u  # Exc√®s au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Trac√© du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile √† 5%\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-16-output-1.png){width=683 height=449}\n:::\n:::\n\n\n### I.2.2. Estimation des param√®tres de la loi GPD\n\nEn estimant les param√®tres de la loi GPD, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants $\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01$. De ce fait, la distribution de Pareto g√©n√©ralis√©e est adapt√©e pour mod√©liser les exc√®s au-dessus du seuil $u = 0.03$.\n\n::: {#d2b3132b .cell execution_count=17}\n``` {.python .cell-code}\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value >= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n```\n:::\n:::\n\n\n### I.2.3. Validation ex-ante\n\nEn comparant la distribution GPD estim√©e et la distribution empirique des exc√®s, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estim√© indique que les quantiles th√©oriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observ√©s dans notre distribution des exc√®s. Nous concluons que la distribution GPD n'est pas adapt√©e pour mod√©liser les exc√®s au-dessus du seuil $u = 0.03$. Cela peut √™tre d√ª √† un mauvais choix du seuil $u$, une analyse plus aprofondie aurait √©t√© n√©cessaire pour choisir un seuil plus adapt√©.\n\n::: {#6bd407aa .cell execution_count=18}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Donn√©es observ√©es des exc√®s')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GPD ajust√©e\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajust√©e')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-18-output-1.png){width=798 height=431}\n:::\n:::\n\n\n::: {#ac4f4383 .cell execution_count=19}\n``` {.python .cell-code}\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une mod√©lisation par loi GPD\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_evt_files/figure-html/cell-19-output-1.png){width=820 height=451}\n:::\n:::\n\n\n### I.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance ${\\alpha}$ est alors obtenue par la formule :\n\n$$\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n$$\n\no√π $\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})$ est la fonction de r√©partition de la GPD($\\hat{\\sigma},\\hat{\\xi}$) estim√©e, $\\alpha_{\\text{POT}}$ est le quantile ajust√©, n√©cessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\n\nComme on ne se concentre que sur l‚Äô√©chantillon des exc√®s dans cette mod√©lisation, l‚Äôestimation de la VaR √† partir de la GPD ne doit pas se faire au niveau $\\alpha$, mais √† un niveau ajust√© $\\alpha_{\\text{POT}}$ d√©fini par la relation suivante :\n\n$$\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n$$\n\no√π $n$ repr√©sente le nombre total d'observations, $N_u$ correspond au nombre d'exc√®s au-del√† du seuil $u$,\n\n::: {#0fa7abbd .cell execution_count=20}\n``` {.python .cell-code}\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value >= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%\n```\n:::\n:::\n\n\n",
    "supporting": [
      "var_evt_files"
    ],
    "filters": [],
    "includes": {}
  }
}