{
  "hash": "8381be30c6d27460c688a98c188b277d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: TP3:Méthodes d'une calcul d'une VaR dynamique (basé sur GARCH)\njupyter: python3\ndate: \"2025-02-28\"\n---\n\n\n\n\n\n\n\n\nCe TP est une continuité du TP-1 et du TP-2 dans lequel on souhaitait implémenter la VaR (Value at Risk) et l'ES (Expected Shortfall) en utilisant les méthodes classiques proposées dans la réglementation bâloise, i.e. la méthode historique, paramétrique et bootstrap (TP1). Cependant, une limite de ces méthodes est qu'elles ne prennent pas en compte la queue de distribution de la perte. Pour remédier à cela, nous avons utilisé des méthodes avec la théorie des valeurs extrêmes, i.e. l'approche Block Maxima et l'approche Peaks Over Threshold (TP2). Jusqu'à maintenant, on considérait que la série est iid. Cependant, dans la réalité, les séries financières sont souvent caractérisées par une dépendance temporelle et une volatilité conditionnelle.  \n\nDans le cadre du TP3, il s'agira de prendre en compte la dépendance temporelle et la volatilité conditionnelle dans les séries temporelles financières. Pour ce faire, nous utiliserons un modèle de VAR dynamique avec le modèle GARCH.\n\nLe modèle GARCH (Generalized Autoregressive Conditional Heteroskedasticity) est un modèle de volatilité conditionnelle qui permet de modéliser la volatilité des rendements financiers. Il a été introduit par Bollerslev en 1986. Le modèle GARCH est une extension du modèle ARCH (Autoregressive Conditional Heteroskedasticity) introduit par Engle en 1982. Le modèle GARCH est défini par les équations suivantes:\n\n$$\nr_t = \\mu_t + \\epsilon_t\n$$\n\n$$\n\\epsilon_t = \\sigma_t z_t\n$$\n\n$$\n\\sigma_t^2 = \\omega + \\sum \\alpha_i \\epsilon_{t-i}^2 + \\sum \\beta_i \\sigma_{t-i}^2\n$$\n\nDans ce modèle $\\mu_t$ est un paramètre de tendance moyenne à identifier, $\\epsilon_t$ est le résidu, $\\sigma_t^2$ est la variance conditionnelle, $z_t$ est un bruit blanc, $\\omega$ est un paramètre de constante, $\\alpha_i$ et $\\beta_i$ sont les paramètres du modèle GARCH à identifier.\n\n::: {#4bf50d33 .cell execution_count=1}\n``` {.python .cell-code}\n# Définition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n\n```\n:::\n:::\n\n\n::: {#b129b7b6 .cell execution_count=2}\n``` {.python .cell-code}\n# Import des données du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la première ligne\ndata = data.dropna()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYF.download() has changed argument auto_adjust default to True\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[*********************100%***********************]  1 of 1 completed\n```\n:::\n:::\n\n\n::: {#28b2a30d .cell execution_count=3}\n``` {.python .cell-code}\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\n```\n:::\n\n\n## I. Implémentation de la VaR dynamique\n### I.1. Pertinence du modèle AR(1)-GARCH(1,1)\nLe modèle AR(1)-GARCH(1,1) est le modèle qui, en pratique, est utilisé pour réaliser la VaR dynamique. Cependant, il n'est pas tout le temps adapté aux données financières. Dans ce TP, nous allons commencer par tester l'éligibilité de ce modèle dans le cadre des données que nous possédons. \n\n::: {#a1d7d8d0 .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, label='Train')\nplt.title('CAC 40 Log Returns')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-5-output-1.png){}\n:::\n:::\n\n\n::: {#b3c5988c .cell execution_count=5}\n``` {.python .cell-code}\n## ACF et PACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(data_train, ax=plt.gca(), lags=40)\nplt.subplot(222)\nplot_pacf(data_train, ax=plt.gca(), lags=40)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-6-output-1.png){}\n:::\n:::\n\n\nDans la série temporelle que nous possédons, nous constatons que la série peut être modéliser par un AR(1). Pour un test plus rigoureux de cette hypothèse, nous allons utiliser la méthode de Lljung Box afin de déterminer le meilleur modèle qui puisse modéliser la série. Ainsi, pour un ordre pmax = 2 et qmax=2, nous allons :\n1. Estimer les paramètres du modèle ARMA(p,q) pour chaque combinaison de p et q\n2. Calculer la statistique de Ljung Box pour chaque combinaison de p et q afin d'examiner si les résidus d'un modèle sont du bruit blanc \n3. Filtrer les modèles pour lesquels les résidus sont du bruit blanc \n4. Choisir le meilleur modèle en utilisant le critère d'Akaike\n\n::: {#63bd3448 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Paramètres du modèle\np_max = 2\nq_max = 2\nbest_aic = np.inf\nbest_order = (0, 0, 0)\n\n# Chargement de la série temporelle (remplacer par la vraie série data_unindex)\n# Exemple fictif avec des données aléatoires\nnp.random.seed(42)\ndata_unindex = data_train.copy()\ndata_unindex.reset_index(drop=True, inplace=True)\n\n# Création de la matrice pour stocker les AIC\naic_matrix = pd.DataFrame(np.nan, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\nbb_test = pd.DataFrame(0, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\n# Boucle pour estimer les modèles et stocker les AIC\nfor p in range(p_max + 1):\n    for q in range(q_max + 1):\n        try:\n            model = ARIMA(data_unindex, order=(p, 0, q))\n            out = model.fit()\n            aic_matrix.loc[f\"p={p}\", f\"q={q}\"] = out.aic  # Stockage de l'AIC\n            \n            # Test de la blancheur des résidus\n            ljung_box_result = acorr_ljungbox(out.resid, lags=[1], return_df=True)\n            p_value = ljung_box_result['lb_pvalue'].iloc[0]\n\n            if p_value > 0.05:\n                bb_test.loc[f\"p={p}\", f\"q={q}\"] = 1\n            \n            # Mise à jour du meilleur modèle\n            if out.aic < best_aic :\n                best_aic = out.aic\n                best_order = (p, 0, q)\n                \n        except Exception as e:\n            print(f\"Erreur avec (p={p}, q={q}): {e}\")\n\nprint(f\"Meilleur modèle ARIMA: {best_order} avec AIC={best_aic}\")\n\nprint(\"=\"*30)\nprint(\"Matrice des AIC:\")\nprint(aic_matrix)\nprint(\"=\"*30)\nprint(\"Matrice des test de Lljung box (1 lorsque résidus non autocorrélés):\")\nprint(bb_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeilleur modèle ARIMA: (0, 0, 0) avec AIC=-20100.176479566246\n==============================\nMatrice des AIC:\n              q=0           q=1           q=2\np=0 -20100.176480 -20098.205891 -20097.679059\np=1 -20098.227385 -20099.862840 -20097.046957\np=2 -20097.887027 -20098.545030 -20094.033191\n==============================\nMatrice des test de Lljung box (1 lorsque résidus non autocorrélés):\n     q=0  q=1  q=2\np=0    1    1    1\np=1    1    1    1\np=2    1    1    1\n```\n:::\n:::\n\n\n::: {#4a227937 .cell execution_count=7}\n``` {.python .cell-code}\np = 1\nq = 0\n\nAR1 = ARIMA(data_unindex, order=(p, 0, q))\nprint(AR1.fit().summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Mon, 10 Mar 2025   AIC                         -20098.227\nTime:                        22:57:52   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n```\n:::\n:::\n\n\nEn utilisant la méthode énoncée plus haut, nous constatons que le modèle ARMA(0,0) est le meilleur modèle. En effet, c'est le modèle avec le critère d'Akaike le plus faible. Cela porte à croire que la tendance moyenne de la série est constante. Nous allons tout de même utiliser un modèle AR(1) pour la modéliser. En effet, c'est le deuxième modèle avec un AIC faible.\n\nDans la série des résidus, nous constatons des clusters de volatilité ce qui est signe d'une volatilité conditionnelle, et donc de la présence d'un GARCH. De plus, dans la série des résidus du log-rendement, nous constatons une faible autocorrélation, ce qui les fait ressembler à du bruit blanc. Toutefois, lorsque l'on examine ces résidus au carré, la série temporelle présente généralement une forte autocorrélation, mise en évidence par la présence de grappes de volatilité. Cela suggère que les rendements représentent un processus hétéroscédastique, ce qui rend le modèle GARCH particulièrement pertinent dans le cadre de notre étude.\n\n::: {#0cf7be4f .cell execution_count=8}\n``` {.python .cell-code}\nAR1_resid = AR1.fit().resid\nplt.figure(figsize=(10, 5))\nplt.plot(AR1_resid)\nplt.title(\"Résidus du modèle AR(1)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-9-output-1.png){}\n:::\n:::\n\n\n::: {#fae6e656 .cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(AR1_resid, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus AR(1)\")\nplt.subplot(222)\nplot_acf(AR1_resid**2, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus AR(1) au carré\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-10-output-1.png){}\n:::\n:::\n\n\nMotivés par les commentaires de (Franke, Härdle et Hafner 2004) suggérant que, dans les applications pratiques, les modèles GARCH avec des ordres plus petits décrivent souvent suffisamment les données et que dans la plupart des cas GARCH(1,1) est adéquat, nous avons considéré quatre combinaisons différentes de p=0, 1 et q=1, 2 pour chaque période afin d'entraîner le modèle GARCH, en supposant que les résidus standardisés suivent une distribution normale.\n\n::: {#8fec1ed6 .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\n\ndef find_garch(p_min, p_max, q_min, q_max, data, dist=\"normal\"):\n    \"\"\"\n    Trouve le meilleur modèle GARCH(p, q) en minimisant l'AIC.\n\n    Paramètres :\n    - p_min, p_max : Bornes pour p (ordre de l'AR dans la variance)\n    - q_min, q_max : Bornes pour q (ordre de MA dans la variance)\n    - data : Série temporelle utilisée pour l'estimation\n    - dist : Distribution des erreurs (\"normal\", \"t\", \"ged\", etc.)\n\n    Retour :\n    - DataFrame contenant les valeurs de AIC pour chaque combinaison (p, q)\n    - Meilleur modèle GARCH trouvé en fonction du critère AIC\n    \"\"\"\n    \n    best_aic = np.inf\n    best_order = (0, 0, 0)\n    \n    results = []\n\n    for p in range(p_min, p_max + 1):\n        for q in range(q_min, q_max + 1):\n            try:\n                # Spécification du modèle GARCH(p, q)\n                garch_spec = arch_model(data, vol='Garch', p=p, q=q, mean='Zero', dist=dist)\n                out = garch_spec.fit(disp=\"off\")\n                \n                # Calcul de l'AIC\n                current_aic = out.aic * len(data)\n\n                # Mettre à jour le meilleur modèle si un plus petit AIC est trouvé\n                if current_aic < best_aic:\n                    best_aic = current_aic\n                    best_order = (p, 0, q)\n                \n                # Ajouter les résultats dans la liste\n                results.append({'p': p, 'q': q, 'aic': current_aic, 'relative_gap': np.nan})\n            \n            except Exception as e:\n                print(f\"Erreur pour (p={p}, q={q}): {e}\")\n                continue\n    \n    # Convertir en DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Calculer l'écart relatif par rapport au meilleur AIC\n    results_df['relative_gap'] = (results_df['aic'] - best_aic) * 100 / best_aic\n    \n    return results_df, best_order\n\nresults_df, best_garch_order = find_garch(p_min=1, p_max=2, q_min=0, q_max=2, data=data_unindex, dist=\"normal\")\n\nprint(f\"Meilleur modèle GARCH: {best_garch_order} avec AIC={best_aic}\")\nprint(\"=\"*30)\nprint(\"Résultats pour les modèles testés:\")\nresults_df.sort_values(by='relative_gap', ascending=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeilleur modèle GARCH: (1, 0, 1) avec AIC=-20100.176479566246\n==============================\nRésultats pour les modèles testés:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>q</th>\n      <th>aic</th>\n      <th>relative_gap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-7.493455e+07</td>\n      <td>-0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>1</td>\n      <td>-7.491663e+07</td>\n      <td>-0.023918</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>-7.486519e+07</td>\n      <td>-0.092567</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>2</td>\n      <td>-7.483014e+07</td>\n      <td>-0.139333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>0</td>\n      <td>-7.189514e+07</td>\n      <td>-4.056091</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>-7.073946e+07</td>\n      <td>-5.598342</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn utilisant le critère AIC pour sélectionner le meilleur modèle, nous avons conclu que GARCH(1,1) est effectivement le meilleur modèle.\n\n::: {#6863f976 .cell execution_count=11}\n``` {.python .cell-code}\ngarch11 = arch_model(data_unindex, vol='Garch', p=1, q=1, mean='Zero', dist='normal')\nprint(\"=\"*78)\nprint(\"Résumé du modèle GARCH(1,1)\")\nprint(\"=\"*78)\nprint(garch11.fit(disp=\"off\").summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==============================================================================\nRésumé du modèle GARCH(1,1)\n==============================================================================\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Mon, Mar 10 2025   Df Residuals:                     3523\nTime:                        22:57:53   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P>|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\n```\n:::\n:::\n\n\n::: {#ef26d511 .cell execution_count=12}\n``` {.python .cell-code}\ncond_resid =garch11.fit(disp=\"off\").conditional_volatility # Volatilité conditionnelle => sigma_t\nresid = garch11.fit(disp=\"off\").resid # résidus du modèle => eps_t\nresid_std = garch11.fit(disp=\"off\").std_resid  # résidus studentisés => eta_t\n\n# jarque bera test\n\nfrom scipy.stats import jarque_bera\n\njb_test = jarque_bera(resid_std)\nprint(\"H0: Les résidus studentisés suivent une loi normale\")\nprint(f\"Test de Jarque-Bera sur les résidus studentisés: JB={jb_test[0]}, p-value={jb_test[1]}\")\n# reject the null hypothesis of normality for the distribution of the residuals, \n# as a rule of thumb, which implies that the data to be fitted is not\n# normally distributed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH0: Les résidus studentisés suivent une loi normale\nTest de Jarque-Bera sur les résidus studentisés: JB=848.8557767883675, p-value=4.71313744144075e-185\n```\n:::\n:::\n\n\n::: {#a64ff9ad .cell execution_count=13}\n``` {.python .cell-code}\n### y revenir\n\n### coeff <1\n```\n:::\n\n\n::: {#770fcfcf .cell execution_count=14}\n``` {.python .cell-code}\n# Test d'homoscédasticité\n# Ljung-Box test sur résidus\nlb_test_resid = acorr_ljungbox(resid_std, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur résidus:\\n\", lb_test_resid)\n\n# Ljung-Box test sur carrés des résidus\nlb_test_resid_sq = acorr_ljungbox(resid_std**2, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur carrés des résidus:\\n\", lb_test_resid_sq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLjung-Box Test sur résidus:\n      lb_stat  lb_pvalue\n1   0.028087   0.866904\n2   0.572026   0.751253\n3   0.690100   0.875530\n4   1.235029   0.872298\n5   2.199461   0.820914\n6   2.491801   0.869384\n7   2.828137   0.900433\n8   2.941010   0.938005\n9   3.793237   0.924486\n10  4.644433   0.913631\n11  4.727144   0.943657\n12  6.763448   0.872842\nLjung-Box Test sur carrés des résidus:\n       lb_stat  lb_pvalue\n1    0.280711   0.596235\n2    0.339634   0.843819\n3    6.670837   0.083163\n4    7.395445   0.116409\n5    8.091586   0.151260\n6    8.233789   0.221471\n7    8.724987   0.273009\n8    9.386238   0.310768\n9    9.938908   0.355454\n10  11.579309   0.314198\n11  13.394501   0.268325\n12  13.845698   0.310670\n```\n:::\n:::\n\n\n::: {#a33f5ea4 .cell execution_count=15}\n``` {.python .cell-code}\n# LM test pour les effets ARCH\nfrom statsmodels.stats.diagnostic import het_arch\n\nlm_test = het_arch(resid_std)\nprint('LM Test Statistique: %.3f, p-value: %.3f' % (lm_test[0], lm_test[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLM Test Statistique: 12.218, p-value: 0.271\n```\n:::\n:::\n\n\n::: {#89523292 .cell execution_count=16}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(resid_std, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus studentisés\")\nplt.title(\"Résidus studentisés du modèle GARCH(1,1)\")\nplt.subplot(222)\nplot_pacf(resid_std, lags=40, ax=plt.gca())\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-17-output-1.png){}\n:::\n:::\n\n\nLe modèle AR(1)-GARCH(1,1) estimé est le suivant :\n\n$$\nr_t = \\mu_t + \\epsilon_t\n$$\n\noù $\\mu_t = 0.0001 - 0.0037 r_{t-1}$\n\n$$\n\\epsilon_t = \\sigma_t \\eta_t\n$$\n\n$$\n\\sigma_t^2 = 3.89 \\times 10^{-6} + 0.10 \\times \\epsilon_{t-i}^2 + 0.88 \\times \\sigma_{t-i}^2\n$$\n\navec $\\eta_t$ un bruit blanc supposée gaussien.\n\nDans ce cas, nous rencontrons des problèmes au niveau de la significativité du coefficient AR(1). En effet, il aurait été plus judicieux de ne pas modéliser la tendance moyenne du rendement et la supposer constante. De plus, au niveau du GARCH(1,1), les résidus sont bien des bruits blancs homoscédastiques (test de lljung box et test LM). Cependant, nous avons supposé que $\\eta_t$ est un bruit blanc gaussien. Cela n'est pas vérifié. Il aurait été judicieux de tester d'autres distributions telles que Students’s t (’t’, ‘studentst’), Skewed Student’s t (‘skewstudent’, ‘skewt’) ou encore Generalized Error Distribution (GED).\n\n```\n**Test de Lagrange Multiplier (LM) pour l'effet ARCH**\n\nLe test de Lagrange Multiplier (LM) pour l'effet ARCH est un outil statistique qui vérifie la présence d'effets ARCH (AutoRegressive Conditional Heteroskedasticity) dans une série temporelle.\n\nL'effet ARCH se manifeste lorsque la variance d'une erreur est une fonction de ses erreurs passées. Cette propriété est courante dans les séries temporelles financières, où de grandes variations des rendements sont souvent suivies par de grandes variations et vice versa.\n\nLe test de LM vérifie l'hypothèse nulle que les erreurs sont homoscédastiques (variance constante). Si la p-value du test est inférieure à un seuil prédéfini (généralement 0,05), l'hypothèse nulle est rejetée, indiquant la présence d'effets ARCH.\n```\n\n::: {#a9c24238 .cell execution_count=17}\n``` {.python .cell-code}\n# Création de la figure avec des sous-graphiques alignés verticalement\nplt.figure(figsize=(10, 12))\n\n# Premier graphique : CAC 40\nplt.subplot(311)\nplt.plot(resid) \nplt.title(\"Résidus du modèle AR(1)\")\n\n# Deuxième graphique : Résidus du modèle AR(1)\nplt.subplot(312)\nplt.plot(cond_resid)\nplt.title(\"Volatile conditionnelle du modèle GARCH(1,1)\")\n\n# Troisième graphique : Résidus studentisés du modèle GARCH(1,1)\nplt.subplot(313)\nplt.plot(resid_std, label='Résidus studentisés du modèle GARCH(1,1)')\nplt.title(\"Résidus studentisés du modèle GARCH(1,1)\")\n\n# Affichage des graphiques\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-18-output-1.png){}\n:::\n:::\n\n\n### I.2. Dynamique historique de $\\mu_t$ et $\\sigma_t$\n\n$$\nr_t = \\mu_t + \\sigma_t \\times \\epsilon_t\n\\quad\n\\text{avec} \\quad\n\\begin{cases}\n    \\mu_t = \\mu + \\varphi r_{t-1} \\\\\n    \\sigma_t^2 = \\omega + a (r_{t-1} - \\mu_{t-1})^2 + b \\sigma_{t-1}^2\n\\end{cases}\n$$\n\nPour avoir la dynamique historique de $\\mu_t$ et $\\sigma_t$, nous allons utiliser les données historiques de la série temporelle ainsi que les estimations des paramètres $\\Theta = (\\mu, \\varphi, \\omega, a, b)$ du modèle AR(1)-GARCH(1,1) que nous avons estimé précédemment par maximum de vraisemblance.\n\nPour $t=1$, nous allons initialiser $\\mu_1$ par la moyenne $\\hat{\\mu}$ et $\\sigma_1$ par la variance à long terme $\\frac{\\omega}{1 - a - b}$.\n\n::: {#2437c7ca .cell execution_count=18}\n``` {.python .cell-code}\nprint(AR1.fit().summary())\n\n# tester arima avec arch_model ou arch\nmu = AR1.fit().params[0]\nprint(f\"Paramètre mu: {mu}\")\nphi = AR1.fit().params[1]\nprint(f\"Paramètre phi: {phi}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Mon, 10 Mar 2025   AIC                         -20098.227\nTime:                        22:57:53   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nParamètre mu: 0.00014959052741773347\nParamètre phi: -0.003743634042716415\n```\n:::\n:::\n\n\n::: {#87f99591 .cell execution_count=19}\n``` {.python .cell-code}\nprint(garch11.fit(disp=\"off\").summary())\nomega = garch11.fit(disp=\"off\").params[0]\nprint(f\"Paramètre omega: {omega}\")\na = garch11.fit(disp=\"off\").params[1]\nprint(f\"Paramètre alpha: {a}\")\nb = garch11.fit(disp=\"off\").params[2]\nprint(f\"Paramètre beta: {b}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Mon, Mar 10 2025   Df Residuals:                     3523\nTime:                        22:57:53   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P>|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\nParamètre omega: 3.892997741815931e-06\nParamètre alpha: 0.1\nParamètre beta: 0.88\n```\n:::\n:::\n\n\n::: {#d52bc5a6 .cell execution_count=20}\n``` {.python .cell-code}\nT_train = len(data_train)\nT_test = len(data_test)\n\nT = T_train + T_test\n\n# Initialisation des séries\nr = pd.concat([data_train, data_test], axis=0)\nmu_t = np.zeros(T)    # Composante moyenne\nsigma2 = np.zeros(T)  # Variance conditionnelle\n\n# Conditions initiales\nmu_t[0] = mu\nsigma2[0] = omega / (1 - a - b)  # Variance de long terme\n\n# Simulation du modèle\nfor t in range(1, T):\n    mu_t[t] = mu + phi * r[t-1]  # Partie moyenne\n    sigma2[t] = omega + a * (r[t-1] - mu_t[t-1])**2 + b * sigma2[t-1]  # Variance conditionnelle\n\n# Affichage des résultats\nfig, ax = plt.subplots(3, 1, figsize=(10, 12))\n\nax[0].plot(r, color=\"blue\")\nax[0].set_title(\"Rendements $r_t$\")\nax[0].legend()\n\nax[1].plot(mu_t, color=\"green\")\nax[1].set_title(\"Composante moyenne $\\mu_t$\")\nax[1].legend()\n\nax[2].plot(np.sqrt(sigma2), color=\"red\")\nax[2].set_title(\"Volatilité conditionnelle $\\sigma_t$\")\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-21-output-1.png){}\n:::\n:::\n\n\nEn analysant la dynamique de $\\mu_t$, nous constatons que la tendance moyenne est très semblable à la série des log-rendements. Cela est dû au fait que le modèle AR(1) n'est pas pertinent pour modéliser la série. En effet, la série des log-rendements ressemble déjà à un bruit blanc. Par ailleurs, nous observons de fortes périodes de volatilité dans la série des log-rendements pendant les périodes de crises, i.e. 2008-2009 qui correspond à la crise des subprimes et 2020 qui correspond à la crise du Covid-19. Le modèle GARCH semble bien capturer ces périodes de volatilité dans la volatilité conditionnelle calibrée.\n\n### I.3. Estimation de la VaR\n\n::: {#da858fed .cell execution_count=21}\n``` {.python .cell-code}\n# VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1 - alpha))\n\n# VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n# Loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n    \n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les paramètres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n    \n    return - loglik\n\n# Optimisation des paramètres avec contraintes de positivité sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des paramètres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(resid_std)\nprint(\"=\"*80)\nprint(\"Les paramètres estimés de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0], \n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha <0 or alpha >1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n    \n# Objectif : écrire une fonction qui calcule la VaR skew-student\n\ndef sstd_var(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\n#### A FAIRE VAR POT et BM\n\nfrom scipy.stats import genextreme as gev\n\nimport numpy as np\nimport pandas as pd\nneg_resid = -resid_std\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extrêmes d'une série de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associées aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # Sélectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # Récupérer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des données suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) >= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nextremes = get_extremes(neg_resid, block_size=21, min_last_block=0.6)\nparams_gev = gev.fit(extremes)\nprint(\"=\"*80)\nprint(\"Les paramètres estimés de la loi de GEV sont : \")\nprint(\"-\"*15)\nprint(f\"Shape (xi) = {params_gev[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gev[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gev[2]:.2f}\")\nprint(\"=\"*80)\n\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value >= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\n\nu = 0.03\nexcess_values = [value - u for value in neg_resid if value >= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les paramètres estimés\nprint(\"=\"*80)\nprint(\"Paramètres estimés de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\nprint(\"=\"*80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n================================================================================\nLes paramètres estimés de la loi de Skew Student sont : \n---------------\nMu :  0.42506987856855155\nSigma :  0.8686238872541445\nGamma :  -0.6074089740677895\nNu :  5.607559653340765\n================================================================================\n================================================================================\nLes paramètres estimés de la loi de GEV sont : \n---------------\nShape (xi) = -0.01\nLocalisation (mu) = 1.64\nEchelle (sigma) = 0.72\n================================================================================\n================================================================================\nParamètres estimés de la distribution GPD:\nShape (xi) = -0.04\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.80\n================================================================================\n```\n:::\n:::\n\n\n::: {#ac436cec .cell execution_count=22}\n``` {.python .cell-code}\nalpha = 0.99\n\nvar_hist_train = historical_var(resid_std, alpha=alpha)\nvar_gauss_train = gaussian_var(resid_std, alpha=alpha)\nvar_sstd_train = sstd_var(alpha, params_sstd)\nvar_BM_train,_ = BM_var(0.99, 21, *params_gev)\nvar_POT_train,_ = POT_var(neg_resid, alpha, u,*params_gpd)\n\n# in a df\nvar = pd.DataFrame({\n    'Historique': [var_hist_train],\n    'Gaussienne': [var_gauss_train],\n    'Skew Student': [var_sstd_train],\n    'Block Maxima': [var_BM_train],\n    'Peak Over Threshold': [var_POT_train]\n})\n\nprint(\"=\"*80)\nprint(\"Value at Risk sur les résidus studentisés (en %) pour h=1j\")\nprint(round(100*var,2))\nprint(\"=\"*80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n================================================================================\nValue at Risk sur les résidus studentisés (en %) pour h=1j\n   Historique  Gaussienne  Skew Student  Block Maxima  Peak Over Threshold\n0      264.12      229.76        280.29        268.68               284.98\n================================================================================\n```\n:::\n:::\n\n\n#### a. VaR historique dynamique\n\n::: {#616f026d .cell execution_count=23}\n``` {.python .cell-code}\nvar_t = np.zeros(T_test)    # Composante moyenne\nnb_exp = 0\nfor t in range(T_test):\n    var_t[t] = - (mu_t[t+T_train] + np.sqrt(sigma2[t+T_train])*var_hist_train)\n    nb_exp += (r[t+T_train] < var_t[t]).astype(int)\n    \nvar_t = pd.Series(var_t, index=data_test.index)\nprint(f\"Nombre d'exceptions = {nb_exp} sur {T_test} jours\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNombre d'exceptions = 4 sur 586 jours\n```\n:::\n:::\n\n\n::: {#9ad6f82d .cell execution_count=24}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"blue\", label='Train')\nplt.plot(data_test, color=\"orange\", label='Test')\nplt.plot(var_t, color=\"red\",label='VaR dynamique')\nplt.axvline(x=data_test.index[0], color='black', linestyle='--')\nplt.legend()\nplt.title('Série des log-rendements et VaR dynamique')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-25-output-1.png){}\n:::\n:::\n\n\n::: {#482a679e .cell execution_count=25}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 5))\nplt.plot(data_test, color=\"orange\")\nplt.plot(var_t, color=\"red\")\nplt.title('Zoom sur la VaR dynamique')\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nText(0.5, 1.0, 'Zoom sur la VaR dynamique')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](var_garch_files/figure-ipynb/cell-26-output-2.png){}\n:::\n:::\n\n\n::: {#c8f00bb3 .cell execution_count=26}\n``` {.python .cell-code}\n# backtest à faire (optionnel)\n```\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/cherylkouadio/Library/Python/3.9/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.9.6\n---\n",
    "supporting": [
      "var_garch_files/figure-ipynb"
    ],
    "filters": []
  }
}