[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Ce site web a été conçu et généré à l’aide de Quarto, dans le but principal d’offrir un soutien aux étudiants de l’ENSAI qui pourraient se trouver face à des défis similaires à ceux que j’ai rencontrés au cours de mon parcours académique, notamment durant mes 2ème et 3ème années d’étude. L’intention derrière la création de ce site n’est pas de remplacer les enseignements prodigués par nos estimés professeurs. Au contraire, il vise à compléter leur travail remarquable en partageant mes expériences personnelles et les projets que j’ai réalisés. L’objectif est de fournir une ressource supplémentaire qui peut aider les étudiants à naviguer dans leurs propres projets et défis académiques."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A propos de moi",
    "section": "",
    "text": "Cheryl KOUADIO\n\n\n\n\nEtudiante gestion des risques financiers\n\n\nÉcole Nationale de la Statistique et de l’analyse de l’information\n\n\n\nEmail : cheryl.s.kouadio@gmail.com\n\n\nCV (Français/English) : cv_cheryl_kouadio_fr.pdf/cv_cheryl_kouadio_en.pdf\n\n\n\n\nIntérêts\n\nJe m’intéresse particulièrement à la modélisation des risques financiers (risques de marché et de crédit) ainsi qu’aux normes et réglementations qui régissent le système bancaire (Bâle III, etc.), à l’audit des modèles quantitatifs et à la validation des méthodes statistiques appliquées à la gestion des risques. Je suis également passionné par l’évolution des approches statistiques, ce qui m’amène à effectuer une veille constante des nouvelles méthodes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#régression",
    "href": "index.html#régression",
    "title": "Cheryl KOUADIO",
    "section": "Régression",
    "text": "Régression\n\n\nRegression linéaire (regression_lin.qmd)"
  },
  {
    "objectID": "App_sup/reg_lin.html",
    "href": "App_sup/reg_lin.html",
    "title": "La régression linéaire",
    "section": "",
    "text": "La régression linéaire est une méthode d’apprentissage supervisé qui vise à évaluer, lorsqu’il existe, la relation linéaire entre une variable d’intérêt et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de données constitué de n échantillons iid (indépendant et identiquement distribué), le modèle de regression linéaire s’écrit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\noù \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l’erreur, l’information que les autres variables explicatives ne donnent pas.\nL’hypothèse fondamentale de la régression linéaire est l’existence d’une relation linéaire entre la variable cible et les variables explicatives. Pour s’assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire (à l’aide de visualisation ou de tests- spearman, pearson, etc.)\nL’hypothèse de rang plein est la seconde plus grande hypothèse, elle stipule que les variables explicatives ne soient pas corrélées entre elles. Cette condition est nécessaire pour garantir l’unicité des estimations des paramètres du modèle et ainsi l’identifiabilité du modèle étudié\nPar ailleurs pour que les estimations des paramètres du modèle linéaire soient fiables, les erreurs du modèle, représentées par \\(\\xi_i\\), doivent répondre à plusieurs critères :\n\nErreurs centrées : La moyenne attendue des erreurs doit être nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\nHomoscédasticité : La variance des erreurs doit être constante pour toutes les observations, exprimée par \\(V[\\xi_i] = \\sigma^2\\). Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\nDécorrélation des erreurs : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d’erreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d’observer une hypothèse supplémentaire sur la loi des erreurs. En effet, les erreurs sont souvent supposées suivre une loi normale, c’est à dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Celà nous permet de faire des inférences sur les paramètres du modèle et de construire des intervalles de confiance.\n\n\nToutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus.\n\n\n\nDans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "App_sup/reg_lin.html#estimation-des-paramètres",
    "href": "App_sup/reg_lin.html#estimation-des-paramètres",
    "title": "La régression linéaire",
    "section": "",
    "text": "Toutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus."
  },
  {
    "objectID": "App_sup/reg_lin.html#evaluation-du-modèle",
    "href": "App_sup/reg_lin.html#evaluation-du-modèle",
    "title": "La régression linéaire",
    "section": "",
    "text": "Dans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "App_sup/reg_lin.html#evaluation-du-modèle-1",
    "href": "App_sup/reg_lin.html#evaluation-du-modèle-1",
    "title": "La régression linéaire",
    "section": "2. Evaluation du modèle",
    "text": "2. Evaluation du modèle\n\n2.1. Hypothèses sur les erreurs et l’existence d’une relation linéaire\nPour évaluer la qualité du modèle, nous allons tracer les résidus studentisés en fonction des valeurs ajustées. Les résidus studentisés sont les résidus divisés par l’écart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajustées\", ylab=\"Résidus studentisés\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression, est bien adaptée aux données, et donc que l’hypothèse de linéarité est acceptable.\nSi l’on observait une forme de trompette, celà reviendrait à soulever une question sur l’hétéroscédascité des résidus, tandis qu’une forme de banane revèle plutôt une relation de non-linéarité.\nLorsque le nuage de point n’a pas de structure particulière, a priori l’hypothèse d’homoscédascticité n’est pas remise en question, comme cela semble être le cas ici. Attention : ces principes peuvent parfois être mis en défaut et il vaut toujours mieux réaliser plusieurs contrôles différents.\nPour vérifier l’hypothèse d’homoscédasticité, nouspouvons également utiliser le test de Breusch-Pagan. Ce test est basé sur la régression des carrés des résidus sur les variables explicatives. Si le test est significatif, l’hypothèse d’homoscédasticité est rejetée.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a nécessité le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l’hypothèse de non corrélation des résidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est basé sur l’autocorrélation des résidus. Si le test est significatif, l’hypothèse de non corrélation des résidus est rejetée.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l’hypothèse de normalité des résidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est basé sur la comparaison des résidus avec une loi normale. Si le test est significatif, l’hypothèse de normalité des résidus est rejetée.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l’hypothèse de normalité des résidus n’est pas rejetée.\n\n\n2.2. Qualité du modèle\nPour évaluer la qualité du modèle, nous allons calculer le coefficient de détermination \\(R^2\\) et le \\(R^2\\) ajusté.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajusté de 0.97. Cela signifie que 97% de la variance de la variable cible est expliquée par le modèle. Notre modèle de régression linéaire est bien ajusté à nos données."
  },
  {
    "objectID": "App_sup/reg_lin.html#simulation-des-données",
    "href": "App_sup/reg_lin.html#simulation-des-données",
    "title": "La régression linéaire",
    "section": "1. Simulation des données",
    "text": "1. Simulation des données\nPour évaluer l’intérêt de la regréssion linéaire, nous allons simuler un échantillon de taille n=200, où la variable cible Y est une fonction linéaire de la variable explicative X. La vraie relation est donnée par \\(Y = 2 + 3X + \\epsilon\\), où \\(\\epsilon \\sim N(0, 1.6)\\). De fait le modèle linéaire est adéquat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un modèle linéaire simple à nos données, nous obtenons une estimation des paramètres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du modèle suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "index.html#classification",
    "href": "index.html#classification",
    "title": "Cheryl KOUADIO",
    "section": "Classification",
    "text": "Classification\n\n\nApplications des arbres & forêts aléatoires sur un jeu de données (arbres_&_forets_aleatoires.html)"
  },
  {
    "objectID": "index.html#régressions",
    "href": "index.html#régressions",
    "title": "Cheryl KOUADIO",
    "section": "Régressions",
    "text": "Régressions\n\nRegression linéaire (regression_lin.qmd)"
  },
  {
    "objectID": "index.html#classifications",
    "href": "index.html#classifications",
    "title": "Cheryl KOUADIO",
    "section": "Classifications",
    "text": "Classifications\n\nApplications des arbres & forêts aléatoires sur un jeu de données (arbres_&_forets_aleatoires.html)"
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Préambule\n\nCompte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire.\n\n\n\nAnalyse financière\n\nConstruction du bilan d’entreprise (bilan_entreprise.qmd)\n\n\n\nReglementation prudentielle\n\nReglementation prudentielle (reglementation_prudentielle.qmd)\n\n\n\nRisques financiers\n\nDéfinition du risque financier (risque_def.qmd)\nValue-at-risk (VaR) :\n\nDéfinition de la VaR (var_def.qmd)\nImplémentation de la VaR sur python (var_application.qmd)\n\n\n\n\nAutres sujets\n\nInvestissement socialement responsable (ISR.qmd)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_gdr.html#régressions",
    "href": "index_gdr.html#régressions",
    "title": "Cheryl KOUADIO",
    "section": "Régressions",
    "text": "Régressions\n\nRegression linéaire (regression_lin.qmd)"
  },
  {
    "objectID": "index_gdr.html#classifications",
    "href": "index_gdr.html#classifications",
    "title": "Cheryl KOUADIO",
    "section": "Classifications",
    "text": "Classifications\n\nApplications des arbres & forêts aléatoires sur un jeu de données (arbres_&_forets_aleatoires.html)"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html",
    "href": "2A/App_sup/reg_lin.html",
    "title": "La régression linéaire",
    "section": "",
    "text": "La régression linéaire est une méthode d’apprentissage supervisé qui vise à évaluer, lorsqu’il existe, la relation linéaire entre une variable d’intérêt et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de données constitué de n échantillons iid (indépendant et identiquement distribué), le modèle de regression linéaire s’écrit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\noù \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l’erreur, l’information que les autres variables explicatives ne donnent pas.\nL’hypothèse fondamentale de la régression linéaire est l’existence d’une relation linéaire entre la variable cible et les variables explicatives. Pour s’assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire (à l’aide de visualisation ou de tests- spearman, pearson, etc.)\nL’hypothèse de rang plein est la seconde plus grande hypothèse, elle stipule que les variables explicatives ne soient pas corrélées entre elles. Cette condition est nécessaire pour garantir l’unicité des estimations des paramètres du modèle et ainsi l’identifiabilité du modèle étudié\nPar ailleurs pour que les estimations des paramètres du modèle linéaire soient fiables, les erreurs du modèle, représentées par \\(\\xi_i\\), doivent répondre à plusieurs critères :\n\nErreurs centrées : La moyenne attendue des erreurs doit être nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\nHomoscédasticité : La variance des erreurs doit être constante pour toutes les observations, exprimée par \\(V[\\xi_i] = \\sigma^2\\). Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\nDécorrélation des erreurs : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d’erreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d’observer une hypothèse supplémentaire sur la loi des erreurs. En effet, les erreurs sont souvent supposées suivre une loi normale, c’est à dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Celà nous permet de faire des inférences sur les paramètres du modèle et de construire des intervalles de confiance.\n\n\nToutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus.\n\n\n\nDans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "href": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "title": "La régression linéaire",
    "section": "",
    "text": "Toutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "title": "La régression linéaire",
    "section": "",
    "text": "Dans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#simulation-des-données",
    "href": "2A/App_sup/reg_lin.html#simulation-des-données",
    "title": "La régression linéaire",
    "section": "1. Simulation des données",
    "text": "1. Simulation des données\nPour évaluer l’intérêt de la regréssion linéaire, nous allons simuler un échantillon de taille n=200, où la variable cible Y est une fonction linéaire de la variable explicative X. La vraie relation est donnée par \\(Y = 2 + 3X + \\epsilon\\), où \\(\\epsilon \\sim N(0, 1.6)\\). De fait le modèle linéaire est adéquat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un modèle linéaire simple à nos données, nous obtenons une estimation des paramètres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du modèle suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "title": "La régression linéaire",
    "section": "2. Evaluation du modèle",
    "text": "2. Evaluation du modèle\n\n2.1. Hypothèses sur les erreurs et l’existence d’une relation linéaire\nPour évaluer la qualité du modèle, nous allons tracer les résidus studentisés en fonction des valeurs ajustées. Les résidus studentisés sont les résidus divisés par l’écart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajustées\", ylab=\"Résidus studentisés\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression, est bien adaptée aux données, et donc que l’hypothèse de linéarité est acceptable.\nSi l’on observait une forme de trompette, celà reviendrait à soulever une question sur l’hétéroscédascité des résidus, tandis qu’une forme de banane revèle plutôt une relation de non-linéarité.\nLorsque le nuage de point n’a pas de structure particulière, a priori l’hypothèse d’homoscédascticité n’est pas remise en question, comme cela semble être le cas ici. Attention : ces principes peuvent parfois être mis en défaut et il vaut toujours mieux réaliser plusieurs contrôles différents.\nPour vérifier l’hypothèse d’homoscédasticité, nouspouvons également utiliser le test de Breusch-Pagan. Ce test est basé sur la régression des carrés des résidus sur les variables explicatives. Si le test est significatif, l’hypothèse d’homoscédasticité est rejetée.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a nécessité le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l’hypothèse de non corrélation des résidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est basé sur l’autocorrélation des résidus. Si le test est significatif, l’hypothèse de non corrélation des résidus est rejetée.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l’hypothèse de normalité des résidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est basé sur la comparaison des résidus avec une loi normale. Si le test est significatif, l’hypothèse de normalité des résidus est rejetée.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l’hypothèse de normalité des résidus n’est pas rejetée.\n\n\n2.2. Qualité du modèle\nPour évaluer la qualité du modèle, nous allons calculer le coefficient de détermination \\(R^2\\) et le \\(R^2\\) ajusté.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajusté de 0.97. Cela signifie que 97% de la variance de la variable cible est expliquée par le modèle. Notre modèle de régression linéaire est bien ajusté à nos données."
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "",
    "text": "L’analyse financière constitue l’ensemble des outils permettant de donner un avis objectif d’une organisation (entreprises, fondations, etc.) sur la santé finanière et les risques financiers auxquels elle sera confrontée. Il s’agit de determiner quels sont les critères d’une santé financière, qu’est le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le gère-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financière d’une entreprise. Il s’agit du bilan et du compte de résultat. Ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise. Comprendre comment ils fonctionnent permet de mieux appréhender la situation financière d’une banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un état des lieux de la situation patrimoniale de l’entreprise à un moment donné. Il est composé de deux parties : l’actif et le passif. L’actif ou l’emploi regroupe l’ensemble des biens et des droits de l’entreprise tandis que le passif regroupe l’ensemble des ressources de l’entreprise (d’où vient l’argent et où peut-on s’en procurer). Le bilan est équilibré en valeur nette, c’est-à-dire que l’actif est égal au passif.\nLe compte de résultats, quant à lui, est un document qui permet de faire un état des lieux des performances de l’entreprise sur une période donnée (il résume les bénéfices ou pertes générées). Il est composé du détail des produits et des charges de l’entreprise. Les produits sont les éléments qui génèrent des revenus pour l’entreprise tandis que les charges sont les éléments qui génèrent des dépenses pour l’entreprise. Le compte de résultat alimente par ailleurs la partie “résultat de l’exercice” du bilan comptable.\nLe coeur de l’entreprise à analyser comme ressources supplémentaires dans le compte de résultat est l’ensembles des charges financières & exceptionnelles ainsi que l’ensemble des produits d’exploitation et financiers. Ces éléments clés permettent de déterminer la rentabilité de l’entreprise. En effet, si les charges sont supérieures aux produits, l’entreprise est en perte. Si les produits sont supérieurs aux charges, l’entreprise est en bénéfice.\nIl est important de noter que ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d’analyser la situation financière d’une entreprise, il faut donc le remodeler en un bilan “fonctionnel” pour pouvoir l’analyser. Le bilan fonctionnel est un document qui permet de faire un état des lieux de la situation financière de l’entreprise en fonction de son activité, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d’investissement à long terme\nEmplois stables\n\nactifs immobilisés en valeur brute\n\nCycle de financement à long terme\nRessources stables\n\nCapitaux propres,\nEmprunts à long terme,\nAmortissements et dépréciation,\nProvisions pour risques\n\n\n\nCycle d’exploitation\nEmplois d’exploitation\n\nStocks et encours\nCréances\n\nCycle d’exploitation\nRessources d’exploitation\n\nDettes circulantes\n\n\n\nTrésorerie active\n\nDisponibilités\n\nTrésorerie passive\n\nDécouverts bancaires\n\n\n\n\nLes ressources stables font référence aux ressources saines du bilan etfont face aux emplois stables. La trésorerie passive fait référence aux découverts bancaires. Il est important de souligner qu’une trésorerie passive est perçue négativement dans le bilan fonctionnel. En effet, une trésorerie passive signifie que l’entreprise a des dettes à court terme qui ne sont pas couvertes par des actifs à court terme d’où la nécessité d’avoir des découverts bancaires.\nNb : La provision pour le risque peuve être considérée comme une ressource stable ou une ressource d’exploitation en fonction de l’entreprise. Tout dépend de la longevité des provisions.\n\nEquilibre financier\nNous dirons qu’il y a équilibre financier lorsque :\n\nLes emplois stables soient entièrement financés par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a necéssité d’un fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) dépend du cycle d’exploitation (entre autre, la rapidité de rotation des stocks et des créances). Il doit couvrir les besoins de financement du cycle d’exploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, créances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la trésorerie (\\(\\text{Trésorerie}=FDR-BFR\\)). Si la trésorerie est positive, il y a équilibre financier. Celà signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d’exploitation. Lorsqu’il est négatif, il faut trouver des ressources pour financer le cycle d’exploitation. Si la trésorerie est nulle, il y a équilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-même le cycle d’exploitation de l’entreprise. C’est ce qu’on appelle le crédit fournisseur. Il est important de noter que le crédit fournisseur est une source de financement gratuite pour l’entreprise. C’est le cas des E-commerce où les acteurs encaissent leurs clients avant même d’acheter les stocks auprès des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transformé en ressources en fonds de roulement, celà est une situation très favorable pour l’entreprise et est appelée “crédit inter-entreprises”."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Analyse du compte de résultat",
    "text": "Analyse du compte de résultat\nNous pouvons faire les mêmes critiques faites au bilan comptable sur le compte de résultat. En effet, le compte de résultat est conçu de sorte à fournir des informations au seul détenteur du capital, à savoir les actionnaires. Il fait apparaitre uniquement le bénéfice ou la perte. C’est un document d’intérêt pour l’Etat pour déterminer si un pays est en croissance ou en récession. Pour en faire un vrai diagnostic financier, il faut le découper en sous-soldes appelés “soldes intermédiaires de gestion” (SIG). Les SIG permettent de déterminer la rentabilité de l’entreprise, sa capacité d’autofinancement, sa capacité de remboursement, sa capacité de financement, etc.\nIl existe 9 soldes intermédiaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajoutée\nL’excédent brut d’exploitation (EBE)\nLe résultat d’exploitation\nLe résultat courant avant impôt\nLe résultat exceptionnel\nLe résultat net\nLa plus ou moins value de cession\n\nSelon la théorie de prise de décisions, il y a deux grands types de décisions : des décisions qui permettent de créer de la riches (Marge co., production et valeur ajoutée) et des décisions qui permettent de distribuer/dépenser de la richesse (EBE, résultat d’exploitation, résultat courant avant impôt, résultat exceptionnel, résultat net et plus ou moins value de cession). Lorsqu’on dépense la riches, il faudrait qu’elle soit bien dépensée.\n\nSoldes de création de richesse\nLes soldes qui contribuent à la création de richesse sont la marge commerciale, la production et la valeur ajoutée :\n\nLa marge commerciale est la différence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C’est un solde des entreprises commerciales (par exemple, les supermarchés). Pour une entreprise qui n’ont pas de marchandises, le marge commerciale est nulle.\nLa production de l’exercice est la somme des produits vendus(\\(\\pm\\) les produits stockées) et des produits immobilisées par l’entreprise (certaines entreprises peuvent se vendre des produits à elles-mêmes). C’est un solde des entreprises industrielles.\nLa valeur ajoutée est la richesse créée par l’entreprise. C’est la somme des marges commerciales, de la production de l’exercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services extérieurs).\n\nLa valeur ajouté est un indicateur très suivi par l’Etat pour déterminer le produit intérieur brut (PIB) afin de déterminer si un pays est en croissance ou en récession. Par ailleurs, la valeur ajoutée divisée par le nombre de salariés permet de déterminer le niveau de technicité de l’entreprise. Plus la valeur ajoutée par salarié est élevée, plus l’entreprise est techniquement avancée.\n\n\nLa richesse dédiée à l’activité économique\nIl existe 5 tiers à qui l’entreprise redistribue la VA (rangée par ordre de priorité) :\n\nLe personnel (à travers les salaires),\nL’Etat (à travers les impôts),\nLe capital technique (via les amortissements),\nLes banques (via les intérêts),\nLes actionnaires ou les associés (via le bénéfice comptable)\n\nLes soldes qui permettent de financer l’activité économique (Etat, personnel, capital technique) sont l’excédent brut d’exploitation et le résultat d’exploitation :\n\nLe solde EBE rémunère le personnel et l’Etat. Il représente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d’entreprise pour l’Etat. Un EBE positif signifie que l’entreprise est capable de rémunérer le personnel et l’Etat, et donc de financer l’emploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajoutée} - \\text{Impôts, tâxes et versements assimilés} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de résultat d’exploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de rémunérer le capital technique (machines etc.) et appartient à tout ceux qui dépendent du capital financier et mesure les performances industrielles et commerciales de l’entreprise.\n\n\\[\\begin{align*}\n\\text{Résultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse dédiée à l’activité financière\nLes soldes qui permettent de financer l’activité financière (banques, actionnaires) sont le résultat courant avant impôt, le résultat exceptionnel, le résultat net et la plus ou moins value de cession :\n\nLe résultat courant avant impôt est le solde qui permet de rémunérer les banques. Il est un indicateur de la capacité de l’entreprise à rembourser ses dettes et est un témoin de l’incidence de la politique financière de l’entreprise sur son résultat. Il faut distinguer les intérêts à long terme et ceux de court terme. Plus ceux ci sont liés à des dettes de court terme (ex. : découverts), on peut dire que l’entreprise est en difficulté financière tandis que l’endettement à long terme est un signe de bonne santé financière, car il est voulu plutôt que subi. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat courant avant impôt} &= \\text{Résultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financières}\n\\end{align*}\\]\n\nLe résultat exceptionnel est le solde qui est le moins analysé car il est souvent lié à des évènements exceptionnels (ex. : vente d’un bien immobilier). Il est calculé comme étant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe résultat net est le solde qui permet de rémunérer les actionnaires. C’est le solde en bas du compte de résultat. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat net} &= \\text{Résultat courant avant impôt} + \\text{Résultat exceptionnel} \\\\\n&- \\text{participations des salariés} - \\text{Impôts sur les bénéfices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu’une entreprise vend une immobilisation. Ce ratio permet de déterminer si l’entreprise a vendu une immobilisation à un prix supérieur ou inférieur à sa valeur comptable. Celà constitue un temoin d’alerte sur la santé de l’entreprise et permet de déterminer si l’entreprise est en difficulté financière (car rien ne l’oblige à vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "La capacité d’autofinancement",
    "text": "La capacité d’autofinancement\nLa capacité d’autofinancement (CAF) est un indicateur qui permet de déterminer si l’entreprise est capable de financer ses investissements sans recourir à des financements extérieurs. Elle regroupe la capacité à dégager de la liquidité. Il n’y a pas de correspondance entre la trésorerie et le bénéfice. En effet, une entreprise peut être en bénéfice mais en difficulté financière. Pour la calculer, il faut éliminer les sommes non encaissanles et non décaissables (ex. : Dotations, provision, reprise sur amortissements, les écritures exceptionnelles).\nPour passer du bénéfice à la CAF, on ne conserve que les éléments qui sont encaissables et décaissables et est calculée comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges décaissables (intérêt bancaire, impôt sur bénéfice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu versé par l’entreprise à ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas à elle toute seule de déterminer l’autofinancement de l’entreprise. Dans le cadre légal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu’à 95% du bénéfice comptable imposé par l’Etat et garder 5% à l’entreprise. C’est ce qu’on appelle le “dividende légal”. Au delà de 10%, les actionnaires peuvent retirer jusqu’à 100% du bénéfice comptable. C’est ce qu’on appelle le “dividende statutaire”.\nAinsi, l’autofinancement est la somme qui reste de la CAF après le dividende légal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaissés par les actionnaires détermine la politique d’autofinancement de l’entreprise.\nL’autofinancement est essentiel pour l’entreprise car il permet de:\n\nrembourser les emprunts,\naméliorer la trésorerie,\ncouvrir les risque de l’entreprise (provisions pour risque),\nfinancer l’exploitation (stocks & créances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La réglementation prudentielle",
    "section": "",
    "text": "La réglementation prudentielle a été initiée par le développement des marchés financiers et des chocs alimentés par diverses crises financières. Face à ce constat, les autorités de contrôle bancaire ainsi que les autorités de marché ont pris des décisions pour réguler les marchés. C’est notamment le rôle qu’occupe le Comité de Bâle ou la Commission bancaire, qui ont pour objectif de renforcer la stabilité des marchés financiers. En France, l’ACPR (Autorité de Contrôle Prudentiel et de Résolution) et la Banque de France sont membres du Comité de Bâle et participent à ses travaux et décisions.\nIl existe par ailleurs plusieurs textes réglementaires ou documents relatifs au risque de marché. Parmi ces textes, on peut citer le document de référence pour calculer le ratio de solvabilité de la Commission bancaire, intitulé “Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL”, actualisé tous les ans par l’ACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "title": "La réglementation prudentielle",
    "section": "Approche standard de mesure du risque de marché",
    "text": "Approche standard de mesure du risque de marché\nL’approche standard de mesure du risque de marché consiste à calculer les exigences en fonds propres pour chaque catégorie de risque, à savoir :\n\nle risque de taux (général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque lié aux titres de propriété(général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque de change calculé sur l’ensemble des opérations appartenant aussi bien au portefeuille de négociation ou non;\nle risque sur matières premières calculé sur l’ensemble des opérations du portefeuille de négociation ou non;\nles risques opérationnels calculés sur les options associées à chachune des catégories de risque citées ci-dessus.\n\nPar la suite, il s’agit de les additionner de manière arithmétique. Par exemple, pour les titres de propriété, l’exigence de fonds propres est la somme de l’exigence de fonds propres pour le risque général et l’exigence de fonds propres pour le risque spécifique.\nPour le calcul des exigences de fonds propres au titre des risques de marché, il faut tout d’abord déterminer les positions nettes. Les positions de titrisation logées dans le portefeuille de négociation sont traitées comme tout instrument de dette au titre du risque de taux.\nPour le risque spécifique, l’exigence en fonds propres sera la somme des positions nettes multipliées par un coefficient de pondération (2%, 4%, 8% ou 12%) choisi en fonction de la liquidité et la diversification de la position. Pour le risque général, l’exigence en fonds propres est la somme des positions nettes globales (pour chaque marché national) multipliées par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "href": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "title": "La réglementation prudentielle",
    "section": "Approche modèle interne",
    "text": "Approche modèle interne\nL’approche modèle interne est une méthode de calcul des exigences en fonds propres pour le risque de marché qui permet aux établissements de calculer leurs propres exigences. L’exigence en fonds propres est généralement un calcul de la VaR. Cette approche est soumise à des conditions strictes et à une validation par l’ACPR.\nConcernant l’utilisation conjointe des modèles internes et de l’approche standard, la position de la commission prête une attention particulière à la permanence des méthodes ainsi qu’à leur évolution. L’objectif est de s’orienter vers un modèle global qui tient compte de l’ensemble des risques de marché.\n\nAinsi, un établissement commençant à utiliser des modèles pour une ou plusieurs catégories de facteurs de risque doit en principe étendre progressivement ce système à tous ses risques à la méthodologie standardisée (à moins que la Commission Bancaire ne lui ait retiré son agrément pour ses modèles).\n\nPour une banque, la construction d’un modèle interne doit permettre de fournir une mesure plus économique du risque de marché. Au titre de l’article 363 du CRR (Règlement sur les exigences de fonds propres), l’autorité compétente autorise les établissements assujettis à utiliser leurs modèles internes pour calculer les exigences de fonds propres pour risques de marché, après avoir vérifié qu’ils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3ème partie du CRR [@journal]. L’autorisation d’utiliser des modèles internes accordée par les autorités compétentes est requise pour chaque catégorie de risques (risque général et spécifique liés aux actions et titres de créance, risque de change et risque sur matières premières), et elle n’est accordée que si le modèle interne couvre une part importante des positions d’une certaine catégorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives à la mesure du risque (articles 367) mais aussi d’ordre général (article 365).\n\nExigences générales\nLe calcul de la valeur en risque visée à l’article 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprimé en centiles et unilatéral, de 99 %;\nune période de détention de dix jours;\nune période effective d’observation historique d’au moins un an, à moins qu’une période d’observation plus courte ne soit justifiée par une augmentation significative de la volatilité des prix;\ndes mises à jour au moins mensuelles des séries de données.\n\nL’établissement peut utiliser des mesures de la valeur en risque calculées sur la base de périodes de détention inférieures à dix jours, qu’il porte à dix jours selon une méthode appropriée qu’il revoit régulièrement.\nChaque établissement doit également calculer, au moins hebdomadairement, une “valeur en risque en situation de tensions” (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit être calculée conformément aux mêmes exigences que la VaR standard énoncées plus haut (intervalle de confiance de 99% etc.). Cependant, les données d’entrée du modèle de Stressed VaR doivent être calibrées par rapport à une période historique de tensions financières significatives d’au moins 12 mois, pertinente pour le portefeuille de l’établissement. Le choix de cette période de tensions historiques fait l’objet d’un examen au moins annuel par l’établissement, qui en communique les résultats aux autorités compétentes. L’objectif est de s’assurer que la Stressed VaR reflète de manière adéquate les risques auxquels l’établissement serait exposé en période de crise financière.\nPour résumer, les établissements doivent calculer la perte potentielle quotidiennement pour une période de détention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent également calculer une Stressed VaR au moins une fois par semaine, en utilisant des données historiques de périodes de tensions financières significatives.\nNotons \\(VaR(t)\\) la valeur en risque à la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions à la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) à la date t pour le risque de marché sont calculées comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\noù \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu’on vera plus tard.\nDans des périodes normales, l’exigence en fonds propres sera donc la somme d’un multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n’est que dans les périodes de crises financières que l’exigence en fonds propres correspond à la VaR ou à la sVaR du jour précédent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est égal à la somme du chiffre 3, au minimum, et d’un cumulateur compris entre 0 et 1 conformément au tableau 1. Ce cumulateur dépend du nombre de dépassements, sur les 250 derniers jours ouvrés, mis en évidence par les contrôles a posteriori de la mesure de la valeur en risque, au sens de l’article 365, paragraphe 1, effectués par l’établissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.dépassements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque spécifique, tout modèle interne utilisé pour calculer les exigences de fonds propres et tout modèle interne utilisé pour la négociation en corrélation satisfont aux exigences supplémentaires suivantes:\n\nle modèle interne explique la variation historique des prix à l’inté rieur du portefeuille;\nil reflète la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement défavorable;\nil est validé par des contrôles a posteriori(backtesting) visant à établir si le risque spécifique a été correctement pris en compte. Si l’établissement effectue ces contrôles a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de manière cohérente;\nil tient compte du risque de base lié à la signature et, en particulier, il est sensible aux différences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d’événement.\n\nLe risque spécifique vise à tenir compte du risque de contrepartie lié à l’emetteur de l’instrument.\nPour en savoir plus, reportez au règlement (UE) No 575/2013 du parlement européen du journal officiel de l’Union Européenne, appelé aussi règlement CRR. (voir aussi la notice 2020 relative aux « Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV»)."
  }
]