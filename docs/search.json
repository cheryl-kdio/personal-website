[
  {
    "objectID": "index_stat.html",
    "href": "index_stat.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Théorie des valeurs extrêmes\n\n\nRésumé de la théorie des valeurs extrêmes\n\n\n\n\nMicroéconométrie appliquée\n\n\nRésumé des méthodes de microéconométrie appliquée enseignées à la promo 2024-2025 ENSAI\n\n\n\n\nApprentissage supervisé\n\n\nRégression linéaire\nKernel Trick and Support Vector Machines (SVM)\nGradient boosting\nFeature selection\nRégression ridge et lasso\nCART & Random Forest\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Modèles de courbe de taux\n\n\n41 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\n\n\n37 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP2:Méthodes basées sur la théorie des valeurs extrêmes\n\n\n17 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP3:Méthodes d’une calcul d’une VaR dynamique (basé sur GARCH)\n\n\n20 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le modèle d’Heston\n\n\n9 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGestion de risques de portefeuille\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement/ de liquidation de portefeuille\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du modèle à volatilité stochastique de Taylor : Filtre particulaire\n\n\n15 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du modèle à volatilité stochastique de Taylor : Filtre de Kalman\n\n\n8 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du modèle Black-Scholes\n\n\n8 min\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFeatures selection\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\n\n7 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Trick and SVM\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRidge regression vs. Lasso regression\n\n\n12 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nSep 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa réglementation prudentielle\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu’est ce que c’est ?\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa régression linéaire\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de résultat d’une entreprise\n\n\n27 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "autres/spearman-pearson.html",
    "href": "autres/spearman-pearson.html",
    "title": "Corrélation de Spearman vs corrélation de Pearson",
    "section": "",
    "text": "Corrélation de Spearman vs corrélation de Pearson\nLa corrélation de Spearman est une mesure de corrélation non paramétrique qui permet de mesurer la relation monotone entre deux variables. Elle est souvent utilisée pour mesurer la dépendance entre les variables aléatoires. La corrélation de Spearman est basée sur les rangs des observations et est moins sensible aux valeurs extrêmes que la corrélation de Pearson. Elle est donc plus robuste et plus adaptée pour mesurer la dépendance entre les variables aléatoires.\nLa corrélation de Pearson, quant à elle, ne permet de comparer les dépendances linéaires des variables. De plus, elle ne permet de comparer les corrélation que lorsque les variables aléatoires sont normales. En effet, soit (\\(X_1,X_2,X_3\\)), si \\(\\rho(X_1,X_2) &gt; \\rho(X_1,X_3)\\), cela ne veut dire que la corrélation entre \\(X_1\\) et \\(X_2\\) est plus forte que celle entre \\(X_1\\) et \\(X_3\\) que si ces variables sont gaussiennes.\nNous verrons dans la suite de ce document que la corrélation de Spearman est plus adaptée pour mesurer la dépendance entre les variables aléatoires, car elle est une mesure de concordance. De ce fait, elle est définit par une copule. Il n’en est pas ainsi pour la corrélation de Pearson.\n\nQu’est ce qu’une mesure de concordance ?\nUne mesure de concordance est une mesure qui permet de quantifier la relation entre deux variables aléatoires. Cinq propriétés sont généralement attribuées à une mesure de concordance :\n\nSymétrie : la mesure de concordance entre X et Y est la même que celle entre Y et X.\nNormalisation : la mesure de concordance est comprise entre -1 et 1.\n\\(\\delta(X_1, X_2) = 1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(U) \\right)\\)\n\\(\\delta(X_1, X_2) = -1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(1-U) \\right)\\)\n\\(\\delta(f(X_1), X_2) = \\delta(X_1, X_2)\\) si \\(f\\) est croissante; \\(\\delta(f(X_1), X_2) = -\\delta(X_1, X_2)\\) si \\(f\\) est décroissante.\n\nEn raison des propriétés 3 et 4, la corrélation de pearson n’est pas une mesure de concordance lorsque les variables aléatoires ne sont pas gaussiennes. En effet, la corrélation de pearson ne vérifie pas la propriété 3. C’est pourquoi, la corrélation de spearman est plus adaptée pour mesurer la dépendance entre les variables aléatoires. Une mesure encore plus appropriée est la copule. La corrélation de spearman, quant à elle, est une mesure de concordance.\n\n\nExemple :\nSoit un vecteur gaussien X = (\\(X_1, X_2, X_3\\)) suivant une loi N(0,\\(\\Sigma\\)) avec :\n\\[\n\\Sigma = \\begin{pmatrix}\n1 & 0.4 & 0.2 \\\\\n0.4 & 1 & -0.8 \\\\\n0.2 & -0.8 & 1\n\\end{pmatrix}\n\\]\n\nm &lt;- 3\nn &lt;- 2000\nsigma &lt;- matrix(c(1,0.4,0.2,0.4,1,-0.8,0.2,-0.8,1), nrow=3, ncol=3)\nX &lt;- mvrnorm(n,mu=rep(0,m),Sigma=sigma)\npairs.panels(X,method=\"spearman\")\n\n\n\n\n\n\n\n\nOn crée ensuite un vecteur Z constitué des fonctions de répartition des éléments de X. On constate que la corrélation de Spearman entre les éléments de Z ne change pas. Celà s’explique par le fait que nous appliquons une fonction croissante à chaque élément de X. Cependant, la distribution de Z est différente de celle de X. En effet, les composantes de Z suivent une loi uniforme \\(\\mathcal{U}(0,1)\\).\n\nZ &lt;- pnorm(X, 0,1)\npar(mfrow=c(1,2))\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"pearson\")\n\n\n\n\n\n\n\n\nA partir de Z, nous construisons un vecteur W tel que les composantes de W suivent des lois marginales différentes, resp \\(\\beta(0,1), \\gamma(2,1), \\beta(2,1)\\). La corrélation de Spearman entre les éléments de W ne change pas bien que la distribution des marginales ait changé, puisque nous appliquons une fonction croissante à chaque élément de Z.\n\nw1 &lt;- qbeta(Z[,1], 2, 1)\nw2 &lt;- qgamma(Z[,2], 2, 1)\nw3 &lt;- qbeta(Z[,3], 2,1)\nW &lt;- cbind(w1,w2,w3)\n\npar(mfrow=c(1,2))\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"pearson\")\n\n\n\n\n\n\n\n\nOn en conclut que la structure de dépendance d’un vecteur de variables aléatoire peut être isolée, caractériséee, et modélisée indépendamment des lois marginales/distributions univariées des composantes du vecteur aléatoire. Le concept de copule permet de modéliser cette structure de dépendance.\nSi l’on utilise la correlation de Pearson, on constate que la corrélation entre les éléments de W et Z change. En effet, la corrélation de Pearson est une mesure de corrélation linéaire et ne permet pas de comparer, xdans tous les cas, les dépendances entre les variables aléatoires.\n\npairs.panels(X,method=\"pearson\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "autres/ISR.html",
    "href": "autres/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL’ISR présente de multiples avantages tant pour les investisseurs que pour les entreprises engagées dans cette démarche. Ces avantages reflètent l’évolution des attentes sociétales et la reconnaissance croissante de l’importance de la durabilité et de l’éthique dans le monde des affaires. J’en ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement améliorer la réputation d’une entreprise. Elle témoigne de son engagement envers des pratiques durables et éthiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financière : De nombreuses études démontrent que les entreprises avec une notation ESG élevée tendent de meilleures performances financièrement sur le long terme. Cela s’explique par une meilleure anticipation des risques, une gestion plus efficace et une capacité à saisir les opportunités de marché liées à la durabilité.\nMeilleure gestion des risques : L’adoption de pratiques ESG solides permet aux entreprises de mieux identifier et gérer les risques, qu’ils soient climatiques, sociaux ou de marché.\nMeilleure attractivité pour les investisseurs : En démontrant un engagement clair envers la durabilité et l’éthique, les entreprises attirent davantage d’investisseurs conscients de l’importance des critères ESG. Cette attractivité accrue peut se traduire par un accès facilité au capital et à de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution à la réduction de certains risques financiers : En investissant dans des entreprise intégrant les critères ESG dans leur processus de décision, les investisseurs contribuent indirectement à une meilleure identification et anticipation les risques liés au changement climatique, aux problématiques sociales, et aux défis de gouvernance, ce qui contribue à une meilleure protection de leur capital sur le long terme.\nImpact positif sur la société : L’ISR permet aux investisseurs de contribuer activement à une économie plus durable et équitable. En choisissant d’investir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des modèles économiques respectueux de l’environnement et du bien-être social.\n\nEn somme, l’ISR offre une perspective d’investissement qui va au-delà des retours financiers immédiats pour embrasser des bénéfices à long terme, tant sur le plan économique que social et environnemental."
  },
  {
    "objectID": "autres/ISR.html#comment-investir",
    "href": "autres/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire à un fonds ISR ?\nSouscrire à un fonds1 ISR (Investissement Socialement Responsable) est une démarche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la société et l’environnement[@comment]. En consultant son conseiller financier ou son établissement bancaire, il est possible de placer son argent dans une variété de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d’Épargne en Actions (PEA) : (sous réserver de s’assurer que le fonds ISR choisi est bien éligible au PEA) Offre la possibilité de placer son épargne en actions de sociétés européennes.\nLes compte-titres ordinaires(CTO) : permet d’investir en bourse sur les marchés financiers français et/ou étrangers et dans tout type de valeurs mobilières (OPC2, actions, obligations, monétaire, warrants, trackers…).\nL’épargne salariale ou les plans d’épargne d’entreprise (PEE) : un produit d’épargne collectif qui permet aux salariés d’une entreprise de se constituer un portefeuille de valeurs mobilières qui peuvent proposer des fonds ISR.\nEnfin, certains produits d’épargne retraite individuelle, comme le Plan d’Epargne Retraite (PER).\n\nCes véhicules d’investissement permettent aux particuliers de contribuer à une économie plus durable tout en recherchant une performance financière. Il est recommandé de se rapprocher d’un conseiller pour déterminer le produit le mieux adapté à ses objectifs financiers et à ses valeurs éthiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) nécessite une approche combinant analyses personnelle, financière et extra-financière, cette dernière se concentrant sur les critères ESG (Environnementaux, Sociaux et de Gouvernance)[@comment2021].\nPour choisir efficacement une entreprise ISR, il est crucial de réaliser une analyse à triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR qui vous convient le mieux. Elles vous aideront à identifier le type de placement à privilégier et définir par exemple des fonds thématiques, d’exclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financière : Elle permet d’évaluer la performance économique de l’entreprise, sa santé financière, sa capacité à générer des profits et à maintenir une croissance durable. Cette analyse est indispensable pour s’assurer que l’entreprise est non seulement responsable, mais aussi viable et performante à long terme.\nAnalyse extra-financière (ESG) : Cette analyse complète l’évaluation financière en examinant comment l’entreprise aborde les défis et saisit les opportunités liées aux critères environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds intégrant les critères ESG comme les fonds labellisés ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du ministère de l’économie et des finances français qui garantit que le fonds d’investissement respecte des critères ESG stricts4 dans ses choix d’investissement. Il assure également que le fonds investit dans des entreprises qui adhèrent à ces principes, offrant ainsi une couche supplémentaire de confiance pour les investisseurs soucieux de l’impact de leurs placements.\nCe label est attribué aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,classées en 6 catégories, qui constituent les 6 piliers du référentiel [@critèresa]."
  },
  {
    "objectID": "autres/ISR.html#footnotes",
    "href": "autres/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appelé fonds de placement. Il s’agit d’une société d’ordre public ou privé qui investit du capital pour soutenir des projets souvent innovants.↩︎\norganismes de placement collectif↩︎\nLes fonds d’investissement d’exclusion normatifs font référence aux fonds faisant l’objet de plusieurs controverses.↩︎\ncf liste des fonds labellisés [@listede].↩︎"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html",
    "href": "3A/value-at-risk/var_garch.html",
    "title": "TP3:Méthodes d’une calcul d’une VaR dynamique (basé sur GARCH)",
    "section": "",
    "text": "Ce TP est une continuité du TP-1 et du TP-2 dans lequel on souhaitait implémenter la VaR (Value at Risk) et l’ES (Expected Shortfall) en utilisant les méthodes classiques proposées dans la réglementation bâloise, i.e. la méthode historique, paramétrique et bootstrap (TP1). Cependant, une limite de ces méthodes est qu’elles ne prennent pas en compte la queue de distribution de la perte. Pour remédier à cela, nous avons utilisé des méthodes avec la théorie des valeurs extrêmes, i.e. l’approche Block Maxima et l’approche Peaks Over Threshold (TP2). Jusqu’à maintenant, on considérait que la série est iid. Cependant, dans la réalité, les séries financières sont souvent caractérisées par une dépendance temporelle et une volatilité conditionnelle.\nDans le cadre du TP3, il s’agira de prendre en compte la dépendance temporelle et la volatilité conditionnelle dans les séries temporelles financières. Pour ce faire, nous utiliserons un modèle de VAR dynamique avec le modèle GARCH.\nLe modèle GARCH (Generalized Autoregressive Conditional Heteroskedasticity) est un modèle de volatilité conditionnelle qui permet de modéliser la volatilité des rendements financiers. Il a été introduit par Bollerslev en 1986. Le modèle GARCH est une extension du modèle ARCH (Autoregressive Conditional Heteroskedasticity) introduit par Engle en 1982. Le modèle GARCH est défini par les équations suivantes:\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\sum \\alpha_i \\epsilon_{t-i}^2 + \\sum \\beta_i \\sigma_{t-i}^2\n\\]\nDans ce modèle \\(\\mu_t\\) est un paramètre de tendance moyenne à identifier, \\(\\epsilon_t\\) est le résidu, \\(\\sigma_t^2\\) est la variance conditionnelle, \\(z_t\\) est un bruit blanc, \\(\\omega\\) est un paramètre de constante, \\(\\alpha_i\\) et \\(\\beta_i\\) sont les paramètres du modèle GARCH à identifier.\n# Définition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des données du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la première ligne\ndata = data.dropna()\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html#i.-implémentation-de-la-var-dynamique",
    "href": "3A/value-at-risk/var_garch.html#i.-implémentation-de-la-var-dynamique",
    "title": "TP3:Méthodes d’une calcul d’une VaR dynamique (basé sur GARCH)",
    "section": "I. Implémentation de la VaR dynamique",
    "text": "I. Implémentation de la VaR dynamique\n\nI.1. Pertinence du modèle AR(1)-GARCH(1,1)\nLe modèle AR(1)-GARCH(1,1) est le modèle qui, en pratique, est utilisé pour réaliser la VaR dynamique. Cependant, il n’est pas tout le temps adapté aux données financières. Dans ce TP, nous allons commencer par tester l’éligibilité de ce modèle dans le cadre des données que nous possédons.\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, label='Train')\nplt.title('CAC 40 Log Returns')\nplt.show()\n\n\n\n\n\n\n\n\n\n## ACF et PACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(data_train, ax=plt.gca(), lags=40)\nplt.subplot(222)\nplot_pacf(data_train, ax=plt.gca(), lags=40)\nplt.show()\n\n\n\n\n\n\n\n\nDans la série temporelle que nous possédons, nous constatons que la série peut être modéliser par un AR(1). Pour un test plus rigoureux de cette hypothèse, nous allons utiliser la méthode de Lljung Box afin de déterminer le meilleur modèle qui puisse modéliser la série. Ainsi, pour un ordre pmax = 2 et qmax=2, nous allons : 1. Estimer les paramètres du modèle ARMA(p,q) pour chaque combinaison de p et q 2. Calculer la statistique de Ljung Box pour chaque combinaison de p et q afin d’examiner si les résidus d’un modèle sont du bruit blanc 3. Filtrer les modèles pour lesquels les résidus sont du bruit blanc 4. Choisir le meilleur modèle en utilisant le critère d’Akaike\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Paramètres du modèle\np_max = 2\nq_max = 2\nbest_aic = np.inf\nbest_order = (0, 0, 0)\n\n# Chargement de la série temporelle (remplacer par la vraie série data_unindex)\n# Exemple fictif avec des données aléatoires\nnp.random.seed(42)\ndata_unindex = data_train.copy()\ndata_unindex.reset_index(drop=True, inplace=True)\n\n# Création de la matrice pour stocker les AIC\naic_matrix = pd.DataFrame(np.nan, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\nbb_test = pd.DataFrame(0, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\n# Boucle pour estimer les modèles et stocker les AIC\nfor p in range(p_max + 1):\n    for q in range(q_max + 1):\n        try:\n            model = ARIMA(data_unindex, order=(p, 0, q))\n            out = model.fit()\n            aic_matrix.loc[f\"p={p}\", f\"q={q}\"] = out.aic  # Stockage de l'AIC\n            \n            # Test de la blancheur des résidus\n            ljung_box_result = acorr_ljungbox(out.resid, lags=[1], return_df=True)\n            p_value = ljung_box_result['lb_pvalue'].iloc[0]\n\n            if p_value &gt; 0.05:\n                bb_test.loc[f\"p={p}\", f\"q={q}\"] = 1\n            \n            # Mise à jour du meilleur modèle\n            if out.aic &lt; best_aic :\n                best_aic = out.aic\n                best_order = (p, 0, q)\n                \n        except Exception as e:\n            print(f\"Erreur avec (p={p}, q={q}): {e}\")\n\nprint(f\"Meilleur modèle ARIMA: {best_order} avec AIC={best_aic}\")\n\nprint(\"=\"*30)\nprint(\"Matrice des AIC:\")\nprint(aic_matrix)\nprint(\"=\"*30)\nprint(\"Matrice des test de Lljung box (1 lorsque résidus non autocorrélés):\")\nprint(bb_test)\n\nMeilleur modèle ARIMA: (0, 0, 0) avec AIC=-20100.176479566246\n==============================\nMatrice des AIC:\n              q=0           q=1           q=2\np=0 -20100.176480 -20098.205891 -20097.679059\np=1 -20098.227385 -20099.862840 -20097.046957\np=2 -20097.887027 -20098.545030 -20094.033191\n==============================\nMatrice des test de Lljung box (1 lorsque résidus non autocorrélés):\n     q=0  q=1  q=2\np=0    1    1    1\np=1    1    1    1\np=2    1    1    1\n\n\n\np = 1\nq = 0\n\nAR1 = ARIMA(data_unindex, order=(p, 0, q))\nprint(AR1.fit().summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nEn utilisant la méthode énoncée plus haut, nous constatons que le modèle ARMA(0,0) est le meilleur modèle. En effet, c’est le modèle avec le critère d’Akaike le plus faible. Cela porte à croire que la tendance moyenne de la série est constante. Nous allons tout de même utiliser un modèle AR(1) pour la modéliser. En effet, c’est le deuxième modèle avec un AIC faible.\nDans la série des résidus, nous constatons des clusters de volatilité ce qui est signe d’une volatilité conditionnelle, et donc de la présence d’un GARCH. De plus, dans la série des résidus du log-rendement, nous constatons une faible autocorrélation, ce qui les fait ressembler à du bruit blanc. Toutefois, lorsque l’on examine ces résidus au carré, la série temporelle présente généralement une forte autocorrélation, mise en évidence par la présence de grappes de volatilité. Cela suggère que les rendements représentent un processus hétéroscédastique, ce qui rend le modèle GARCH particulièrement pertinent dans le cadre de notre étude.\n\nAR1_resid = AR1.fit().resid\nplt.figure(figsize=(10, 5))\nplt.plot(AR1_resid)\nplt.title(\"Résidus du modèle AR(1)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(AR1_resid, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus AR(1)\")\nplt.subplot(222)\nplot_acf(AR1_resid**2, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus AR(1) au carré\")\nplt.show()\n\n\n\n\n\n\n\n\nMotivés par les commentaires de (Franke, Härdle et Hafner 2004) suggérant que, dans les applications pratiques, les modèles GARCH avec des ordres plus petits décrivent souvent suffisamment les données et que dans la plupart des cas GARCH(1,1) est adéquat, nous avons considéré quatre combinaisons différentes de p=0, 1 et q=1, 2 pour chaque période afin d’entraîner le modèle GARCH, en supposant que les résidus standardisés suivent une distribution normale.\n\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\n\ndef find_garch(p_min, p_max, q_min, q_max, data, dist=\"normal\"):\n    \"\"\"\n    Trouve le meilleur modèle GARCH(p, q) en minimisant l'AIC.\n\n    Paramètres :\n    - p_min, p_max : Bornes pour p (ordre de l'AR dans la variance)\n    - q_min, q_max : Bornes pour q (ordre de MA dans la variance)\n    - data : Série temporelle utilisée pour l'estimation\n    - dist : Distribution des erreurs (\"normal\", \"t\", \"ged\", etc.)\n\n    Retour :\n    - DataFrame contenant les valeurs de AIC pour chaque combinaison (p, q)\n    - Meilleur modèle GARCH trouvé en fonction du critère AIC\n    \"\"\"\n    \n    best_aic = np.inf\n    best_order = (0, 0, 0)\n    \n    results = []\n\n    for p in range(p_min, p_max + 1):\n        for q in range(q_min, q_max + 1):\n            try:\n                # Spécification du modèle GARCH(p, q)\n                garch_spec = arch_model(data, vol='Garch', p=p, q=q, mean='Zero', dist=dist)\n                out = garch_spec.fit(disp=\"off\")\n                \n                # Calcul de l'AIC\n                current_aic = out.aic * len(data)\n\n                # Mettre à jour le meilleur modèle si un plus petit AIC est trouvé\n                if current_aic &lt; best_aic:\n                    best_aic = current_aic\n                    best_order = (p, 0, q)\n                \n                # Ajouter les résultats dans la liste\n                results.append({'p': p, 'q': q, 'aic': current_aic, 'relative_gap': np.nan})\n            \n            except Exception as e:\n                print(f\"Erreur pour (p={p}, q={q}): {e}\")\n                continue\n    \n    # Convertir en DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Calculer l'écart relatif par rapport au meilleur AIC\n    results_df['relative_gap'] = (results_df['aic'] - best_aic) * 100 / best_aic\n    \n    return results_df, best_order\n\nresults_df, best_garch_order = find_garch(p_min=1, p_max=2, q_min=0, q_max=2, data=data_unindex, dist=\"normal\")\n\nprint(f\"Meilleur modèle GARCH: {best_garch_order} avec AIC={best_aic}\")\nprint(\"=\"*30)\nprint(\"Résultats pour les modèles testés:\")\nresults_df.sort_values(by='relative_gap', ascending=False)\n\nMeilleur modèle GARCH: (1, 0, 1) avec AIC=-20100.176479566246\n==============================\nRésultats pour les modèles testés:\n\n\n\n\n\n\n\n\n\np\nq\naic\nrelative_gap\n\n\n\n\n1\n1\n1\n-7.493455e+07\n-0.000000\n\n\n4\n2\n1\n-7.491663e+07\n-0.023918\n\n\n2\n1\n2\n-7.486519e+07\n-0.092567\n\n\n5\n2\n2\n-7.483014e+07\n-0.139333\n\n\n3\n2\n0\n-7.189514e+07\n-4.056091\n\n\n0\n1\n0\n-7.073946e+07\n-5.598342\n\n\n\n\n\n\n\nEn utilisant le critère AIC pour sélectionner le meilleur modèle, nous avons conclu que GARCH(1,1) est effectivement le meilleur modèle.\n\ngarch11 = arch_model(data_unindex, vol='Garch', p=1, q=1, mean='Zero', dist='normal')\nprint(\"=\"*78)\nprint(\"Résumé du modèle GARCH(1,1)\")\nprint(\"=\"*78)\nprint(garch11.fit(disp=\"off\").summary())\n\n==============================================================================\nRésumé du modèle GARCH(1,1)\n==============================================================================\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\n\n\n\ncond_resid =garch11.fit(disp=\"off\").conditional_volatility # Volatilité conditionnelle =&gt; sigma_t\nresid = garch11.fit(disp=\"off\").resid # résidus du modèle =&gt; eps_t\nresid_std = garch11.fit(disp=\"off\").std_resid  # résidus studentisés =&gt; eta_t\n\n# jarque bera test\n\nfrom scipy.stats import jarque_bera\n\njb_test = jarque_bera(resid_std)\nprint(\"H0: Les résidus studentisés suivent une loi normale\")\nprint(f\"Test de Jarque-Bera sur les résidus studentisés: JB={jb_test[0]}, p-value={jb_test[1]}\")\n# reject the null hypothesis of normality for the distribution of the residuals, \n# as a rule of thumb, which implies that the data to be fitted is not\n# normally distributed\n\nH0: Les résidus studentisés suivent une loi normale\nTest de Jarque-Bera sur les résidus studentisés: JB=848.8557767883675, p-value=4.71313744144075e-185\n\n\n\n### y revenir\n\n### coeff &lt;1\n\n\n# Test d'homoscédasticité\n# Ljung-Box test sur résidus\nlb_test_resid = acorr_ljungbox(resid_std, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur résidus:\\n\", lb_test_resid)\n\n# Ljung-Box test sur carrés des résidus\nlb_test_resid_sq = acorr_ljungbox(resid_std**2, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur carrés des résidus:\\n\", lb_test_resid_sq)\n\nLjung-Box Test sur résidus:\n      lb_stat  lb_pvalue\n1   0.028087   0.866904\n2   0.572026   0.751253\n3   0.690100   0.875530\n4   1.235029   0.872298\n5   2.199461   0.820914\n6   2.491801   0.869384\n7   2.828137   0.900433\n8   2.941010   0.938005\n9   3.793237   0.924486\n10  4.644433   0.913631\n11  4.727144   0.943657\n12  6.763448   0.872842\nLjung-Box Test sur carrés des résidus:\n       lb_stat  lb_pvalue\n1    0.280711   0.596235\n2    0.339634   0.843819\n3    6.670837   0.083163\n4    7.395445   0.116409\n5    8.091586   0.151260\n6    8.233789   0.221471\n7    8.724987   0.273009\n8    9.386238   0.310768\n9    9.938908   0.355454\n10  11.579309   0.314198\n11  13.394501   0.268325\n12  13.845698   0.310670\n\n\n\n# LM test pour les effets ARCH\nfrom statsmodels.stats.diagnostic import het_arch\n\nlm_test = het_arch(resid_std)\nprint('LM Test Statistique: %.3f, p-value: %.3f' % (lm_test[0], lm_test[1]))\n\nLM Test Statistique: 12.218, p-value: 0.271\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(resid_std, lags=40, ax=plt.gca())\nplt.title(\"ACF des résidus studentisés\")\nplt.title(\"Résidus studentisés du modèle GARCH(1,1)\")\nplt.subplot(222)\nplot_pacf(resid_std, lags=40, ax=plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\nLe modèle AR(1)-GARCH(1,1) estimé est le suivant :\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\noù \\(\\mu_t = 0.0001 - 0.0037 r_{t-1}\\)\n\\[\n\\epsilon_t = \\sigma_t \\eta_t\n\\]\n\\[\n\\sigma_t^2 = 3.89 \\times 10^{-6} + 0.10 \\times \\epsilon_{t-i}^2 + 0.88 \\times \\sigma_{t-i}^2\n\\]\navec \\(\\eta_t\\) un bruit blanc supposée gaussien.\nDans ce cas, nous rencontrons des problèmes au niveau de la significativité du coefficient AR(1). En effet, il aurait été plus judicieux de ne pas modéliser la tendance moyenne du rendement et la supposer constante. De plus, au niveau du GARCH(1,1), les résidus sont bien des bruits blancs homoscédastiques (test de lljung box et test LM). Cependant, nous avons supposé que \\(\\eta_t\\) est un bruit blanc gaussien. Cela n’est pas vérifié. Il aurait été judicieux de tester d’autres distributions telles que Students’s t (’t’, ‘studentst’), Skewed Student’s t (‘skewstudent’, ‘skewt’) ou encore Generalized Error Distribution (GED).\n**Test de Lagrange Multiplier (LM) pour l'effet ARCH**\n\nLe test de Lagrange Multiplier (LM) pour l'effet ARCH est un outil statistique qui vérifie la présence d'effets ARCH (AutoRegressive Conditional Heteroskedasticity) dans une série temporelle.\n\nL'effet ARCH se manifeste lorsque la variance d'une erreur est une fonction de ses erreurs passées. Cette propriété est courante dans les séries temporelles financières, où de grandes variations des rendements sont souvent suivies par de grandes variations et vice versa.\n\nLe test de LM vérifie l'hypothèse nulle que les erreurs sont homoscédastiques (variance constante). Si la p-value du test est inférieure à un seuil prédéfini (généralement 0,05), l'hypothèse nulle est rejetée, indiquant la présence d'effets ARCH.\n\n# Création de la figure avec des sous-graphiques alignés verticalement\nplt.figure(figsize=(10, 12))\n\n# Premier graphique : CAC 40\nplt.subplot(311)\nplt.plot(resid) \nplt.title(\"Résidus du modèle AR(1)\")\n\n# Deuxième graphique : Résidus du modèle AR(1)\nplt.subplot(312)\nplt.plot(cond_resid)\nplt.title(\"Volatile conditionnelle du modèle GARCH(1,1)\")\n\n# Troisième graphique : Résidus studentisés du modèle GARCH(1,1)\nplt.subplot(313)\nplt.plot(resid_std, label='Résidus studentisés du modèle GARCH(1,1)')\nplt.title(\"Résidus studentisés du modèle GARCH(1,1)\")\n\n# Affichage des graphiques\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2. Dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\)\n\\[\nr_t = \\mu_t + \\sigma_t \\times \\epsilon_t\n\\quad\n\\text{avec} \\quad\n\\begin{cases}\n    \\mu_t = \\mu + \\varphi r_{t-1} \\\\\n    \\sigma_t^2 = \\omega + a (r_{t-1} - \\mu_{t-1})^2 + b \\sigma_{t-1}^2\n\\end{cases}\n\\]\nPour avoir la dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\), nous allons utiliser les données historiques de la série temporelle ainsi que les estimations des paramètres \\(\\Theta = (\\mu, \\varphi, \\omega, a, b)\\) du modèle AR(1)-GARCH(1,1) que nous avons estimé précédemment par maximum de vraisemblance.\nPour \\(t=1\\), nous allons initialiser \\(\\mu_1\\) par la moyenne \\(\\hat{\\mu}\\) et \\(\\sigma_1\\) par la variance à long terme \\(\\frac{\\omega}{1 - a - b}\\).\n\nprint(AR1.fit().summary())\n\n# tester arima avec arch_model ou arch\nmu = AR1.fit().params[0]\nprint(f\"Paramètre mu: {mu}\")\nphi = AR1.fit().params[1]\nprint(f\"Paramètre phi: {phi}\")\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nParamètre mu: 0.00014959052741773347\nParamètre phi: -0.003743634042716415\n\n\n\nprint(garch11.fit(disp=\"off\").summary())\nomega = garch11.fit(disp=\"off\").params[0]\nprint(f\"Paramètre omega: {omega}\")\na = garch11.fit(disp=\"off\").params[1]\nprint(f\"Paramètre alpha: {a}\")\nb = garch11.fit(disp=\"off\").params[2]\nprint(f\"Paramètre beta: {b}\")\n\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\nParamètre omega: 3.892997741815931e-06\nParamètre alpha: 0.1\nParamètre beta: 0.88\n\n\n\nT_train = len(data_train)\nT_test = len(data_test)\n\nT = T_train + T_test\n\n# Initialisation des séries\nr = pd.concat([data_train, data_test], axis=0)\nmu_t = np.zeros(T)    # Composante moyenne\nsigma2 = np.zeros(T)  # Variance conditionnelle\n\n# Conditions initiales\nmu_t[0] = mu\nsigma2[0] = omega / (1 - a - b)  # Variance de long terme\n\n# Simulation du modèle\nfor t in range(1, T):\n    mu_t[t] = mu + phi * r[t-1]  # Partie moyenne\n    sigma2[t] = omega + a * (r[t-1] - mu_t[t-1])**2 + b * sigma2[t-1]  # Variance conditionnelle\n\n# Affichage des résultats\nfig, ax = plt.subplots(3, 1, figsize=(10, 12))\n\nax[0].plot(r, color=\"blue\")\nax[0].set_title(\"Rendements $r_t$\")\nax[0].legend()\n\nax[1].plot(mu_t, color=\"green\")\nax[1].set_title(\"Composante moyenne $\\mu_t$\")\nax[1].legend()\n\nax[2].plot(np.sqrt(sigma2), color=\"red\")\nax[2].set_title(\"Volatilité conditionnelle $\\sigma_t$\")\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEn analysant la dynamique de \\(\\mu_t\\), nous constatons que la tendance moyenne est très semblable à la série des log-rendements. Cela est dû au fait que le modèle AR(1) n’est pas pertinent pour modéliser la série. En effet, la série des log-rendements ressemble déjà à un bruit blanc. Par ailleurs, nous observons de fortes périodes de volatilité dans la série des log-rendements pendant les périodes de crises, i.e. 2008-2009 qui correspond à la crise des subprimes et 2020 qui correspond à la crise du Covid-19. Le modèle GARCH semble bien capturer ces périodes de volatilité dans la volatilité conditionnelle calibrée.\n\n\nI.3. Estimation de la VaR\n\n# VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1 - alpha))\n\n# VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n# Loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n    \n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les paramètres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n    \n    return - loglik\n\n# Optimisation des paramètres avec contraintes de positivité sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des paramètres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(resid_std)\nprint(\"=\"*80)\nprint(\"Les paramètres estimés de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0], \n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n    \n# Objectif : écrire une fonction qui calcule la VaR skew-student\n\ndef sstd_var(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\n#### A FAIRE VAR POT et BM\n\nfrom scipy.stats import genextreme as gev\n\nimport numpy as np\nimport pandas as pd\nneg_resid = -resid_std\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extrêmes d'une série de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associées aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # Sélectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # Récupérer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des données suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nextremes = get_extremes(neg_resid, block_size=21, min_last_block=0.6)\nparams_gev = gev.fit(extremes)\nprint(\"=\"*80)\nprint(\"Les paramètres estimés de la loi de GEV sont : \")\nprint(\"-\"*15)\nprint(f\"Shape (xi) = {params_gev[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gev[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gev[2]:.2f}\")\nprint(\"=\"*80)\n\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\n\nu = 0.03\nexcess_values = [value - u for value in neg_resid if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les paramètres estimés\nprint(\"=\"*80)\nprint(\"Paramètres estimés de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\nprint(\"=\"*80)\n\n================================================================================\nLes paramètres estimés de la loi de Skew Student sont : \n---------------\nMu :  0.42506987856855155\nSigma :  0.8686238872541445\nGamma :  -0.6074089740677895\nNu :  5.607559653340765\n================================================================================\n================================================================================\nLes paramètres estimés de la loi de GEV sont : \n---------------\nShape (xi) = -0.01\nLocalisation (mu) = 1.64\nEchelle (sigma) = 0.72\n================================================================================\n================================================================================\nParamètres estimés de la distribution GPD:\nShape (xi) = -0.04\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.80\n================================================================================\n\n\n\nalpha = 0.99\n\nvar_hist_train = historical_var(resid_std, alpha=alpha)\nvar_gauss_train = gaussian_var(resid_std, alpha=alpha)\nvar_sstd_train = sstd_var(alpha, params_sstd)\nvar_BM_train,_ = BM_var(0.99, 21, *params_gev)\nvar_POT_train,_ = POT_var(neg_resid, alpha, u,*params_gpd)\n\n# in a df\nvar = pd.DataFrame({\n    'Historique': [var_hist_train],\n    'Gaussienne': [var_gauss_train],\n    'Skew Student': [var_sstd_train],\n    'Block Maxima': [var_BM_train],\n    'Peak Over Threshold': [var_POT_train]\n})\n\nprint(\"=\"*80)\nprint(\"Value at Risk sur les résidus studentisés (en %) pour h=1j\")\nprint(round(100*var,2))\nprint(\"=\"*80)\n\n================================================================================\nValue at Risk sur les résidus studentisés (en %) pour h=1j\n   Historique  Gaussienne  Skew Student  Block Maxima  Peak Over Threshold\n0      264.12      229.76        280.29        268.68               284.98\n================================================================================\n\n\n\na. VaR historique dynamique\n\nvar_t = np.zeros(T_test)    # Composante moyenne\nnb_exp = 0\nfor t in range(T_test):\n    var_t[t] = - (mu_t[t+T_train] + np.sqrt(sigma2[t+T_train])*var_hist_train)\n    nb_exp += (r[t+T_train] &lt; var_t[t]).astype(int)\n    \nvar_t = pd.Series(var_t, index=data_test.index)\nprint(f\"Nombre d'exceptions = {nb_exp} sur {T_test} jours\")\n\nNombre d'exceptions = 4 sur 586 jours\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"blue\", label='Train')\nplt.plot(data_test, color=\"orange\", label='Test')\nplt.plot(var_t, color=\"red\",label='VaR dynamique')\nplt.axvline(x=data_test.index[0], color='black', linestyle='--')\nplt.legend()\nplt.title('Série des log-rendements et VaR dynamique')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_test, color=\"orange\")\nplt.plot(var_t, color=\"red\")\nplt.title('Zoom sur la VaR dynamique')\n\nText(0.5, 1.0, 'Zoom sur la VaR dynamique')\n\n\n\n\n\n\n\n\n\n\n# backtest à faire (optionnel)"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html",
    "href": "3A/value-at-risk/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque réglementaire correspond à la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s’agit dans cette section de développer la notion de VaR pour des portefeuilles linéaires et non linéaires."
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#le-backtesting",
    "href": "3A/value-at-risk/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contrôle de la qualité de la VaR pour un horizon de 1 jour. Il permet de vérifier si la VaR est bien calibrée. Pour cela, on compare la VaR calculée avec la perte réelle. Si la VaR est bien calibrée, la perte réelle ne doit pas dépasser la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d’exception pour valider le modèle. Notons PnL le profit and loss du portefeuille et VaR la valeur à risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsidérons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inférieur à l’opposé de la VaR avec probabilité \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une période ouvré comptant n jours, la probabilité d’avoir \\(i\\) exceptions est donnée par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilité d’avoir plus de \\(k\\) exceptions est donnée par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilité d’avoir au plus de \\(i\\) exceptions est donnée par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilité d’exception n’excède pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d’exceptions empirique est supérieur à celui attendu, le modèle est rejeté :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donnée par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d’exceptions observée lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donnée par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) où \\(t\\) est la valeur observée de la statistique de test et \\(\\phi\\) est la fonction de répartition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#cas-général",
    "href": "3A/value-at-risk/var_def.html#cas-général",
    "title": "La VaR",
    "section": "2.2 Cas général",
    "text": "2.2 Cas général\nDans cette approche, nous considérons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en déduisons donc que la VaR est calculé comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction décroissante de l’espérance de PnL et une fonction croissante de la volatilité du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de prévoir l’espérance du PnL futur.\n\n2.2.1 Exemple\nNous considérons une position courte de 1 million de dollars sur le contrat à terme S&P 500. Nous estimons que la volatilité annualisée \\(\\sigma_{\\text{SPX}}\\) est égale à 35%.\nLa perte du portefeuille est égale à \\(L(w) = N \\times R_{\\text{SPX}}\\) où \\(N\\) est le montant de l’exposition (−1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l’indice S&P 500. Nous déduisons que la volatilité de la perte annualisée est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur à risque pour une période de détention d’un an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l’investisseur sur un 1an s’élève à 815 500€ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre période de détention, nous utilisons la règle de la racine carré pour convertir la volatilité pour une fréquence donné \\(f_1\\) en une autre volatilité pour une autre fréquence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les résultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calculé sur 1 jour, pour l’avoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#modèles-linéaires-de-facteurs",
    "href": "3A/value-at-risk/var_def.html#modèles-linéaires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Modèles linéaires de facteurs",
    "text": "2.3 Modèles linéaires de facteurs\nNous considérons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est linéaire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La première idée est de choisir les facteurs comme étant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du marché et chaque actif possède son propre facteur de risque.\nLe problème est que les prix sont loin d’être stationnaires, ce qui nous amène à devoir affronter certains problèmes pour modéliser la distribution \\(F_t\\). Une autre idée est de récrire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] où \\(R_i(t;h)\\) est le retour de l’actif entre \\(t\\) et \\(t+h\\).\nNous déduisons que le PnL aléatoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\noù \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l’exposition nominale)dans l’actif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s’ensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous déduisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l’Équation (2.6), l’expression de la valeur à risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d’estimer la matrice de covariance des retours des actifs pour calculer la valeur à risque. Cela explique la popularité de ce modèle, surtout lorsque le P&L du portefeuille est une fonction linéaire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsidérons l’exemple des entreprises d’Apple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l’écart type estimé des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corrélation croisée est égale à 12,0787 %. Il s’ensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous déduisons que la valeur à risque quotidienne à 99% est de 41,21 $. Nous obtenons une figure inférieure à celle de la valeur à risque historique, qui était de 47,39 $. Nous expliquons ce résultat par le fait que la distribution gaussienne sous-estime la probabilité des événements extrêmes et n’est donc pas adaptée à des calculs précis de risque dans des situations de marché volatiles.\n\n\n2.3.2 Exemple de portefeuille linéaire d’actifs\nConsidérons un portefeuille linéaire composé de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne égaux à : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilités journalières sont égales à 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244€, 135€,315€. La matrice de corrélation est donnée par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journée est donc de 18.42€ avec un risque 1% de se tromper.\n\n2.3.2.1 Implémentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les écarts-types et les corrélations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#modèles-factoriels-de-risque",
    "href": "3A/value-at-risk/var_def.html#modèles-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Modèles factoriels de risque",
    "text": "2.4 Modèles factoriels de risque\nNous supposons que la valeur du portefeuille dépend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille à \\(t+h\\) dépend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\noù g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit linéaire par rapport aux facteurs de risque, ainsi donc le retour des actifs à l’horizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] où \\(B\\) est la matrice des sensibilités du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque à l’horizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables aléatoires gaussiennes indépendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable aléatoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calculé comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette méthode repose sur 3 hypothèses : l’indépendance temporelle des variations de la valeur du portefeuille, la normalité des facteurs et la relation linéaire entre les facteurs et la valeur du portefeuille. En général, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons à partir des données historiques des facteurs et \\(B\\) est le vecteur des sensibilités du portefeuille aux facteurs de risque. La seuil difficulté de cette méthode est l’estimation de la matrice de variance covariance.\n\n2.4.1 Exemple d’un portefeuille obligataire sans risque de crédit\nNous considérons une exposition sur une obligation américaine à $t=$31 décembre 2014. Le nominal de l’obligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont égaux à 5, \\(t_m&gt;t\\). La maturité résiduelle est de cinq ans et les dates de fixation sont à la fin de décembre (\\(n_C=5\\). Le nombre d’obligations détenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d’une obligation zéro coupon (montant qu’un investisseur serait prêt à payer aujourd’hui pour recevoir un paiement fixe à une date future : combien me rapport un euro à maturité \\(T\\) aujourd’hui?) au temps \\(t\\) pour l’échéance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) où \\(R_t(T)\\) est le taux de rendement zéro coupon.\nLa valeur de l’obligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en déduit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\noù \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l’obligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l’obligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en déduisons que le prix de l’obligation est de \\(P(t)=115,47 \\$\\) et l’exposition totale est de 1 154 706 $. En utilisant la période historique de l’année 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d’intérêt à coupon zéro sachant que l’écart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), où \\(h\\) correspond à un jour de bourse. Pour la matrice de corrélation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en déduisons que la valeur à risque à 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur à risque de 4970$ pour une période de détention d’un jour.\n\n2.4.1.1 Implémentation en R\n\n# Définition des écarts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "3A/value-at-risk/var_application.html",
    "href": "3A/value-at-risk/var_application.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous intéresser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d’estimer les pertes maximales potentielles d’un portefeuille d’actifs financiers sur un horizon de temps donné, à un certain niveau de confiance. Elle est largement utilisée par les institutions financières pour évaluer et gérer les risques de marché, de crédit et de liquidité (cf. Value at-Risk).\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-analytique",
    "href": "3A/value-at-risk/var_application.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est basée sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR à horizon 1 jour. La VaR à horizon 1 jour est définie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] où \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) à \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l’hypothèse nulle est que la population étudiée suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\nnp.float64(5.073604966554165e-41)\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l’hypothèse de la distribution normale de nos rendements. Celà est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(8, 6))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR à horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR à horizon 1 jour est de 0.0326\n\n\nLa VaR à horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l’échantillon test les rendements avec la VaR à horizon 1 jour. Si le rendement est inférieur à l’opposé de la VaR gaussienne, alors la VaR est violée et celà correspond à une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d’exceptions que nous comptabilisons sur nos données test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 30 et en déduisons que le taux d’exception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.13\n\n\nPour tester la pertinence de la VaR calculée, il faudrait idéalement que le taux d’exception soit inférieur à 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\nnp.float64(0.27668172410611824)\n\n\nLa pvaleur de ce test est 3.70%, celà est inférieur à 5% donc nous rejetons l’hypothèse nulle selon laquelle le taux d’exception est égale à 0.01 au risque 5% de se tromper. Celà nous indique que la VaR gaussienne n’est pas performante. Ceci n’est pas surprenant étant donné que nous faisons une hypothèse sur la distribution des rendements qui n’est pas vérifiée."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-historique",
    "href": "3A/value-at-risk/var_application.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est basée sur les rendements historiques. Elle est définie comme l’opposé du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsidérons les mouvements de prix quotidiens pour l’indice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 scénarios ou cas qui serviront de guide pour les performances futures de l’indice, c’est-à-dire que les 6513 derniers jours seront représentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour à 99% correspond au 1er percentile de la distribution de probabilité des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique à horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique à horizon 1 jour est de 0.0396\n\n\nNous en déduisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que la VaR gaussienne. Le taux d’exception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 14 et en déduisons que le taux d’exception est 0.64%. Ce taux d’exception est statistiquement supérieur à 1% (car la pvaleur est d’environ 0.95). Ainsi, la VaR historique est performante pour la période considérée.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\nnp.float64(0.9853349189614367)"
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "href": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est basée sur la simulation de trajectoires de rendements. Nous allons simuler jusqu’à 10000 scénarios de rendements et calculer la VaR à horizon 1 jour en posant une hypothèse de normalité sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise à partir de 3000 scénarios. Nous utiliserons donc 3000 simulations de rendements. Nous en déduisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\nnp.float64(0.046018349211036626)\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que les deux autres VaRs. En effet, le taux d’exception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inférieur à 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\nnp.float64(0.9994129059878106)"
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La réglementation prudentielle",
    "section": "",
    "text": "La réglementation prudentielle a été initiée par le développement des marchés financiers et des chocs alimentés par diverses crises financières. Face à ce constat, les autorités de contrôle bancaire ainsi que les autorités de marché ont pris des décisions pour réguler les marchés. C’est notamment le rôle qu’occupe le Comité de Bâle ou la Commission bancaire, qui ont pour objectif de renforcer la stabilité des marchés financiers. En France, l’ACPR (Autorité de Contrôle Prudentiel et de Résolution) et la Banque de France sont membres du Comité de Bâle et participent à ses travaux et décisions.\nIl existe par ailleurs plusieurs textes réglementaires ou documents relatifs au risque de marché. Parmi ces textes, on peut citer le document de référence pour calculer le ratio de solvabilité de la Commission bancaire, intitulé “Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL”, actualisé tous les ans par l’ACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "title": "La réglementation prudentielle",
    "section": "Approche standard de mesure du risque de marché",
    "text": "Approche standard de mesure du risque de marché\nL’approche standard de mesure du risque de marché consiste à calculer les exigences en fonds propres pour chaque catégorie de risque, à savoir :\n\nle risque de taux (général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque lié aux titres de propriété(général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque de change calculé sur l’ensemble des opérations appartenant aussi bien au portefeuille de négociation ou non;\nle risque sur matières premières calculé sur l’ensemble des opérations du portefeuille de négociation ou non;\nles risques opérationnels calculés sur les options associées à chachune des catégories de risque citées ci-dessus.\n\nPar la suite, il s’agit de les additionner de manière arithmétique. Par exemple, pour les titres de propriété, l’exigence de fonds propres est la somme de l’exigence de fonds propres pour le risque général et l’exigence de fonds propres pour le risque spécifique.\nPour le calcul des exigences de fonds propres au titre des risques de marché, il faut tout d’abord déterminer les positions nettes. Les positions de titrisation logées dans le portefeuille de négociation sont traitées comme tout instrument de dette au titre du risque de taux.\nPour le risque spécifique, l’exigence en fonds propres sera la somme des positions nettes multipliées par un coefficient de pondération (2%, 4%, 8% ou 12%) choisi en fonction de la liquidité et la diversification de la position. Pour le risque général, l’exigence en fonds propres est la somme des positions nettes globales (pour chaque marché national) multipliées par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "href": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "title": "La réglementation prudentielle",
    "section": "Approche modèle interne",
    "text": "Approche modèle interne\nL’approche modèle interne est une méthode de calcul des exigences en fonds propres pour le risque de marché qui permet aux établissements de calculer leurs propres exigences. L’exigence en fonds propres est généralement un calcul de la VaR. Cette approche est soumise à des conditions strictes et à une validation par l’ACPR.\nConcernant l’utilisation conjointe des modèles internes et de l’approche standard, la position de la commission prête une attention particulière à la permanence des méthodes ainsi qu’à leur évolution. L’objectif est de s’orienter vers un modèle global qui tient compte de l’ensemble des risques de marché.\n\nAinsi, un établissement commençant à utiliser des modèles pour une ou plusieurs catégories de facteurs de risque doit en principe étendre progressivement ce système à tous ses risques à la méthodologie standardisée (à moins que la Commission Bancaire ne lui ait retiré son agrément pour ses modèles).\n\nPour une banque, la construction d’un modèle interne doit permettre de fournir une mesure plus économique du risque de marché. Au titre de l’article 363 du CRR (Règlement sur les exigences de fonds propres), l’autorité compétente autorise les établissements assujettis à utiliser leurs modèles internes pour calculer les exigences de fonds propres pour risques de marché, après avoir vérifié qu’ils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3ème partie du CRR [@journal]. L’autorisation d’utiliser des modèles internes accordée par les autorités compétentes est requise pour chaque catégorie de risques (risque général et spécifique liés aux actions et titres de créance, risque de change et risque sur matières premières), et elle n’est accordée que si le modèle interne couvre une part importante des positions d’une certaine catégorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives à la mesure du risque (articles 367) mais aussi d’ordre général (article 365).\n\nExigences générales\nLe calcul de la valeur en risque visée à l’article 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprimé en centiles et unilatéral, de 99 %;\nune période de détention de dix jours;\nune période effective d’observation historique d’au moins un an, à moins qu’une période d’observation plus courte ne soit justifiée par une augmentation significative de la volatilité des prix;\ndes mises à jour au moins mensuelles des séries de données.\n\nL’établissement peut utiliser des mesures de la valeur en risque calculées sur la base de périodes de détention inférieures à dix jours, qu’il porte à dix jours selon une méthode appropriée qu’il revoit régulièrement.\nChaque établissement doit également calculer, au moins hebdomadairement, une “valeur en risque en situation de tensions” (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit être calculée conformément aux mêmes exigences que la VaR standard énoncées plus haut (intervalle de confiance de 99% etc.). Cependant, les données d’entrée du modèle de Stressed VaR doivent être calibrées par rapport à une période historique de tensions financières significatives d’au moins 12 mois, pertinente pour le portefeuille de l’établissement. Le choix de cette période de tensions historiques fait l’objet d’un examen au moins annuel par l’établissement, qui en communique les résultats aux autorités compétentes. L’objectif est de s’assurer que la Stressed VaR reflète de manière adéquate les risques auxquels l’établissement serait exposé en période de crise financière.\nPour résumer, les établissements doivent calculer la perte potentielle quotidiennement pour une période de détention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent également calculer une Stressed VaR au moins une fois par semaine, en utilisant des données historiques de périodes de tensions financières significatives.\nNotons \\(VaR(t)\\) la valeur en risque à la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions à la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) à la date t pour le risque de marché sont calculées comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\noù \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu’on vera plus tard.\nDans des périodes normales, l’exigence en fonds propres sera donc la somme d’un multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n’est que dans les périodes de crises financières que l’exigence en fonds propres correspond à la VaR ou à la sVaR du jour précédent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est égal à la somme du chiffre 3, au minimum, et d’un cumulateur compris entre 0 et 1 conformément au tableau 1. Ce cumulateur dépend du nombre de dépassements, sur les 250 derniers jours ouvrés, mis en évidence par les contrôles a posteriori de la mesure de la valeur en risque, au sens de l’article 365, paragraphe 1, effectués par l’établissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.dépassements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque spécifique, tout modèle interne utilisé pour calculer les exigences de fonds propres et tout modèle interne utilisé pour la négociation en corrélation satisfont aux exigences supplémentaires suivantes:\n\nle modèle interne explique la variation historique des prix à l’inté rieur du portefeuille;\nil reflète la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement défavorable;\nil est validé par des contrôles a posteriori(backtesting) visant à établir si le risque spécifique a été correctement pris en compte. Si l’établissement effectue ces contrôles a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de manière cohérente;\nil tient compte du risque de base lié à la signature et, en particulier, il est sensible aux différences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d’événement.\n\nLe risque spécifique vise à tenir compte du risque de contrepartie lié à l’emetteur de l’instrument.\nPour en savoir plus, reportez au règlement (UE) No 575/2013 du parlement européen du journal officiel de l’Union Européenne, appelé aussi règlement CRR. (voir aussi la notice 2020 relative aux « Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV»)."
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "Le modèle classique de volatilité stochastique est défini par les équations suivantes :\n\nProcessus des rendements :\n\\[ r_t = \\exp(x_t / 2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0,1) \\]\nProcessus de la volatilité logarithmique :\n\\[ x_t = \\mu + \\phi x_{t-1} + \\sigma_t \\eta_t, \\quad \\eta_t \\sim N(0,1) \\]\n\n\n( x_t ) suit un processus autorégressif de premier ordre (AR(1)) et suit une distribution normale conditionnelle : \\[ p(x_t) \\sim N(\\frac{\\mu}{1-\\phi} , \\frac{\\sigma_t^2}{1-\\phi^2}) \\]\n\\[ x_t | x_{t-1} \\sim N(\\mu + \\phi x_{t-1}, \\sigma_t^2) \\]\n( r_t ) suit une distribution normale conditionnelle :\n\\[ r_t | x_t \\sim N(0, \\exp(x_t)) \\]\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Simulation d'un modèle à vol stochastique de Taylor\nn &lt;- 252\nmu &lt;- -0.8\nphi &lt;- 0.9\nsigma_squared &lt;- 0.09\n\nx &lt;- numeric(n)  # Log-volatilité\nr &lt;- numeric(n)  # Rendements simulés\n\nfor (t in 1:n) {\n  if (t == 1) {\n    # Densité de transition stationnaire de x_t\n    x[t] &lt;- rnorm(1, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n  } else {\n    # Évolution de l'état\n    x[t] &lt;- mu + phi *x[t-1] + sqrt(sigma_squared) * rnorm(1, mean = 0, sd = 1)\n  }\n  # Simulation des rendements\n  r[t] &lt;- exp(x[t] / 2) * rnorm(1, mean = 0, sd = 1)\n}\n\n# extraction dans fichier csv\nwrite.csv(data.frame(r, x), \"true_sv_taylor.csv\", row.names = FALSE)\n\n\npar(mfrow=c(1,2))\nplot(x, lwd = 2, type = \"l\", col = \"blue\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Log-volatilité simulé\")\nplot(r, lwd = 2, type = \"l\", col = \"red\", ylab = \"Rendements\", xlab = \"Temps\", main = \"Rendements simulés\")\n\n\n\n\n\n\n\n\n\n\n\nparams &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# Définition des paramètres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalisés\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Modèle Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densité d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Modèle log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "params &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# Définition des paramètres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalisés\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Modèle Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-modèle-log-sv-de-taylor",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-modèle-log-sv-de-taylor",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "set.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densité d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Modèle log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap avec les rendements",
    "text": "Filtre bootstrap avec les rendements\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Definition des variables\n# Définition des paramètres\n\nparams &lt;- c(mu,phi,sigma_squared)\n\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalisés\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t,] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ] - mu), sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Modèle Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nUtilisation de library(pmhtutorial)\n\nlibrary(pmhtutorial)\n\n# particleFilterSVmodel takes sigma as parameters\nparams[3] &lt;- sqrt(params[3])\nx_hat_2&lt;- particleFilterSVmodel(r,params,M)\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"log-volatilité\")\nlines(x_hat_2$xHatFiltered, type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Log-volatilité estimé\")\n# legend\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat_2$xHatFiltered / 2) * rnorm(length(x_hat_2$xHatFiltered), mean = 0, sd = 1)\n\n# Superposition des trajectoires\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",\n  col=\"#1B9E77\", main=\"True returns\")\n\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", \n     main = \"Estimated returns\",bty=\"n\")\n\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-modèle-log-sv-de-taylor-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-modèle-log-sv-de-taylor-1",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap sur le modèle log-sv de taylor",
    "text": "Filtre bootstrap sur le modèle log-sv de taylor\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nparams &lt;- c(mu,phi,sigma_squared)\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densité d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules à t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (basés sur la distribution de l'état initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  Étape de prédiction (échantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ]-mu), sd = sqrt(sigma_squared))\n    \n    #  Mise à jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  Rééchantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # Réinitialisation des poids après rééchantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x à l'instant t (pondérée)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilité\", xlab = \"Temps\", main = \"Modèle log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilité\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Modèle log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html",
    "href": "3A/proc_stochastique/modele_heston.html",
    "title": "Calibration avec le modèle d’Heston",
    "section": "",
    "text": "Le but de ce TP est de calculer des prix d’options sous le modèle d’Heston puis de calibrer ce modèle par filtrage. On considère le modèle suivant :\n\\[\n\\begin{cases}\ndS_s = S_s \\left( rds + \\sqrt{v_s} dW_s^1 \\right) \\\\\ndv_s = \\kappa (\\beta - v_s) ds + \\sigma \\sqrt{v_s} dW_s^2 \\\\\ndW_s^1 dW_s^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\noù \\(W_s^1\\) et \\(W_s^2\\) sont deux mouvements browniens et \\(r\\) est le taux sans risque. Pour ce modèle, les rendements sont modélisés par un mouvement brownien géométrique avec une variance stochastique.\nLa volatilité non observée \\(v_t\\) est déterminée par un processus stochastique de retour à la moyenne (1) introduit en 1985 par Cox, Ingersoll et Ross pour la modélisation des taux d’intérêt à court terme.\nLe paramètre \\(\\kappa\\) est le paramètre de retour à la moyenne positive, \\(\\beta\\) est le paramètre positif à long terme et \\(\\eta\\) la volatilité positive du paramètre de variance. De plus, Heston a introduit une corrélation entre les deux mouvements browniens \\(W_s^1\\) et \\(W_s^2\\), représentée par le paramètre \\(\\rho\\) appartenant à \\([-1,1]\\)."
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "title": "Calibration avec le modèle d’Heston",
    "section": "Avec la forme close",
    "text": "Avec la forme close\nSoit un Call de strike K et à échéance \\(\\tau\\) sous le modèle (1) avec les paramètres suivants : \\(\\kappa\\) = 4,\\(\\beta\\) = 0.03,\\(\\sigma\\) = 0.4,r =0.05,\\(\\rho\\)=−0.5,\\(\\tau\\) = 1, \\(S_0\\) = K=100,\\(v_0\\) = \\(\\beta\\).\nPour calculer le prix d’un Call, on peut utiliser la formule close de Heston (Heston 1993) :\n\\[\nC(S_0, K, \\tau) = S_0 P_1 - K e^{-r \\tau} P_2\n\\]\navec :\n\\[\nP_j(x, \\nu, T, \\ln(K)) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\Re \\left( \\frac{e^{-i \\ln(K) u} f_j(x,\\nu,t,u)}{i u} \\right) du\n\\]\noù :\n\\[\nx = \\ln(S_t), \\quad f(x,\\nu,t,u) = \\exp(C(t,u) + D(t,u) \\nu + i \\phi x)\n\\]\net :\n\\[\nC(T-t = \\tau, \\phi) = r i \\phi t + \\frac{a}{\\sigma^2} \\left( (bj - \\rho \\sigma \\phi i + d)\\tau - 2 \\ln \\left( \\frac{1 - g e^{d \\tau}}{1 - g} \\right) \\right)\n\\]\n\\[\nD(T-t = \\tau, \\phi) = \\left( \\frac{bj - \\rho \\sigma \\phi i + d}{\\sigma^2} \\right) \\left( \\frac{1 - e^{d \\tau}}{1 - g e^{d \\tau}} \\right)\n\\]\n\\[\ng = \\frac{bj - \\rho \\sigma \\phi i + d}{bj - \\rho \\sigma \\phi i - d}\n\\]\n\\[\nd = \\sqrt{(\\rho \\sigma \\phi i - bj)^2 - \\sigma^2 (2 u_j \\phi i - \\phi^2)}\n\\]\n\\[\nu_1 = 1/2, \\quad u_2 = -1/2, a = \\lambda, b = \\kappa \\beta, \\quad t_1 = \\kappa - \\rho \\sigma, \\quad t_2 = \\kappa\n\\]\nPour ce faire, nous allons utiliser la fonction Heston_Call_Function.R qui permet de calculer le prix d’un Call sous le modèle d’Heston avec la formule close.\n\n# Paramètres\nkappa &lt;- 4\nbeta &lt;- 0.03\nsigma &lt;- 0.4\nr &lt;- 0.05\nrho &lt;- -0.5\ntau &lt;- 1\nS0&lt;- 100\nK &lt;- 100\nv0 &lt;- beta\n\n# Import Heston_Call_Function.R\nsource(\"data/Heston_Call_Function.R\")\n\n# Calcul du prix du Call\nCall_Heston &lt;- HestonCallClosedForm(lambda = kappa, vbar = beta, eta = sigma, rho = rho, v0 = v0, r = r, tau = tau, S0 = S0, K = K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.410405"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-méthode-de-monte-carlo-schéma-deuler",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-méthode-de-monte-carlo-schéma-deuler",
    "title": "Calibration avec le modèle d’Heston",
    "section": "Avec la méthode de Monte Carlo (Schéma d’Euler)",
    "text": "Avec la méthode de Monte Carlo (Schéma d’Euler)\nLorsqu’on a pas accès à la formule close, on peut utiliser la méthode de Monte Carlo pour calculer le prix d’un Call. Il s’agit de simuler le modèle (1) et de calculer le prix du Call à partir des simulations. Pour simuler le modèle (1), on peut utiliser la discrétisation d’Euler du modèle de Heston (Euler and Milstein Discretization, Fabrice Douglas Rouah) ou utiliser la formule de Ito pour le modèle de Heston.\nDans notre cas, nous allons utiliser la discrétisation d’Euler du modèle de Heston pour simuler le modèle (1) comme suit : \\[\n\\begin{cases}\nS_t = S_{t-1} \\left(1 + r \\Delta + \\sqrt{\\Delta v_t} W_t^1 \\right) \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\beta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}} \\Delta W_t^2 \\right| \\\\[10pt]\n\\text{Cov}(W_t^1, W_t^2) = \\rho\n\\end{cases}\n\\]\navec \\(W_t^1\\) et \\(W_t^2\\) des variables aléatoires gaussiennes centrées réduites et corrélées entre elles telles que \\(\\text{Cov}(W_t^1, W_t^2) = \\rho\\). De plus, \\(\\Delta = \\frac{\\tau}{n}\\) est le pas de discrétisation, avec \\(n\\) le nombre de pas de discrétisation.\nDans notre cas, on définit \\(n = 100\\) et on simule \\(M = 1000\\) modèle (1) pour calculer le prix d’un Call.\n\nHestonCallMC &lt;- function(M, N, lambda, vbar, eta, rho, v0, r, tau, S0, K){\n  # M: Number of Monte Carlo simulations\n  # N: Number of time steps\n  \n  set.seed(123)\n  dt &lt;- tau / N  # Time step\n\n  # Store final stock prices\n  ST &lt;- numeric(M)\n  \n  for (i in 1:M){\n    S &lt;- numeric(N+1)\n    v &lt;- numeric(N+1)\n    \n    S[1] &lt;- S0\n    v[1] &lt;- v0\n    \n    for (t in 1:N){\n      # Generate correlated Brownian motions\n      W1 &lt;- rnorm(1)\n      W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n      \n      # Euler discretization of variance process (ensure non-negativity)\n      v[t+1] &lt;- abs(v[t] + lambda * (vbar - v[t]) * dt + eta * sqrt(v[t] * dt) * W1)\n      \n      # Euler discretization of the stock price process (log-normal form)\n      S[t+1] &lt;- S[t] * exp((r - 0.5 * v[t]) * dt + sqrt(v[t] * dt) * W2)\n    }\n    \n    # Store final stock price\n    ST[i] &lt;- S[N+1]\n  }\n\n  # Compute Call option price using Monte Carlo method\n  Call &lt;- exp(-r * tau) * mean(pmax(ST - K, 0), na.rm=TRUE)\n  \n  return(Call)\n}\n\nM &lt;- 1000\nN &lt;- 100\nCall_Heston &lt;-HestonCallMC(M,N, kappa, beta, sigma, rho, v0, r, tau, S0, K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.797915"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html",
    "href": "3A/gestion_actifs/profil_liquid.html",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "",
    "text": "Nous souhaitons calculer le profil d’écoulement/liquidation dans les scénarios suivants :\nDans l’ordre des étapes, il s’agira dans ce TP de faire :\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\ndef get_data(start_date, end_date, index_ticker, tickers, column=\"Close\"):\n    \"\"\"\n    Extraction de données de cours d'actions\n    Args:\n        start_date (str): Date de début au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des volumes historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    # Extraction des volumes historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=3*31)\n\nselected_assets = {\n    \"AC.PA\": \"Accor\",\n    \"AI.PA\": \"Air Liquide\",\n    \"AIR.PA\": \"Airbus\",\n    \"MT.AS\": \"ArcelorMittal\",\n    \"CS.PA\": \"AXA\",\n    \"BNP.PA\": \"BNP Paribas\",\n    \"EN.PA\": \"Bouygues\",\n    \"BVI.PA\": \"Bureau Veritas\",\n    \"CAP.PA\": \"Capgemini\",\n    \"CA.PA\": \"Carrefour\",\n    \"ACA.PA\": \"Crédit Agricole\",\n    \"BN.PA\": \"Danone\",\n    \"DSY.PA\": \"Dassault Systèmes\",\n    \"EDEN.PA\": \"Edenred\",\n    \"ENGI.PA\": \"Engie\",\n    \"EL.PA\": \"EssilorLuxottica\",\n    \"ERF.PA\": \"Eurofins Scientific\",\n    \"RMS.PA\": \"Hermès\",\n    \"KER.PA\": \"Kering\",\n    \"LR.PA\": \"Legrand\",\n    \"OR.PA\": \"L'Oréal\",\n    \"MC.PA\": \"LVMH\",\n    \"ML.PA\": \"Michelin\",\n    \"ORA.PA\": \"Orange\",\n    \"RI.PA\": \"Pernod Ricard\",\n    \"PUB.PA\": \"Publicis\",\n    \"RNO.PA\": \"Renault\",\n    \"SAF.PA\": \"Safran\",\n    \"SGO.PA\": \"Saint-Gobain\",\n    \"SAN.PA\": \"Sanofi\",\n    \"SU.PA\": \"Schneider Electric\",\n    \"GLE.PA\": \"Société Générale\",\n    \"STLA\": \"Stellantis\",\n    \"STMPA.PA\": \"STMicroelectronics\",\n    \"TEP.PA\": \"Teleperformance\",\n    \"HO.PA\": \"Thales\",\n    \"TTE.PA\": \"TotalEnergies\",\n    \"UNBLF\": \"Unibail-Rodamco-Westfield\",\n    \"VIE.PA\": \"Veolia\",\n    \"DG.PA\": \"Vinci\",\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker, column=\"Volume\")\n\n[                       0%                       ][                       0%                       ][****                   8%                       ]  3 of 40 completed[*****                 10%                       ]  4 of 40 completed[*****                 10%                       ]  4 of 40 completed[*******               15%                       ]  6 of 40 completed[*********             18%                       ]  7 of 40 completed[**********            20%                       ]  8 of 40 completed[***********           22%                       ]  9 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[***************       32%                       ]  13 of 40 completed[*****************     35%                       ]  14 of 40 completed[******************    38%                       ]  15 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************52%                       ]  21 of 40 completed[**********************55%*                      ]  22 of 40 completed[**********************57%**                     ]  23 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************90%******************     ]  36 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nAI.PA\nAIR.PA\nBN.PA\nBNP.PA\nBVI.PA\nCA.PA\nCAP.PA\nCS.PA\n...\nSAF.PA\nSAN.PA\nSGO.PA\nSTLA\nSTMPA.PA\nSU.PA\nTEP.PA\nTTE.PA\nUNBLF\nVIE.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-11-07\n698430.0\n13751044.0\n813490.0\n1091627.0\n1059857.0\n5322315.0\n646260.0\n2794758.0\n326259.0\n4163091.0\n...\n576180.0\n1527095.0\n1430888.0\n6819700.0\n2082219.0\n892806.0\n404002.0\n3786884.0\n0.0\n2809097.0\n\n\n2024-11-08\n640605.0\n4781311.0\n712553.0\n1541520.0\n1045775.0\n4731196.0\n367776.0\n4129105.0\n340411.0\n2828675.0\n...\n672105.0\n1414195.0\n1037715.0\n8197900.0\n1881978.0\n737207.0\n288437.0\n3327420.0\n100.0\n2139007.0\n\n\n2024-11-11\n544390.0\n3965216.0\n615456.0\n1013673.0\n1139407.0\n3062744.0\n621229.0\n2577272.0\n332633.0\n2860191.0\n...\n678397.0\n1208511.0\n877175.0\n7181500.0\n2067857.0\n808043.0\n268883.0\n3669304.0\n100.0\n1474874.0\n\n\n2024-11-12\n476433.0\n6774868.0\n957769.0\n1451643.0\n1319645.0\n3871602.0\n503435.0\n2216855.0\n419941.0\n4554097.0\n...\n928357.0\n1941126.0\n1059643.0\n5832100.0\n3211099.0\n939216.0\n376479.0\n5104044.0\n0.0\n2051518.0\n\n\n2024-11-13\n503706.0\n5911519.0\n752386.0\n1381466.0\n1085131.0\n2782805.0\n674357.0\n1985037.0\n520292.0\n3538137.0\n...\n665527.0\n1394445.0\n1431965.0\n6821700.0\n2456917.0\n990623.0\n206989.0\n4171178.0\n100.0\n2188795.0\n\n\n\n\n5 rows × 40 columns\nportfolio_data.index\n\nDatetimeIndex(['2024-11-07', '2024-11-08', '2024-11-11', '2024-11-12',\n               '2024-11-13', '2024-11-14', '2024-11-15', '2024-11-18',\n               '2024-11-19', '2024-11-20', '2024-11-21', '2024-11-22',\n               '2024-11-25', '2024-11-26', '2024-11-27', '2024-11-28',\n               '2024-11-29', '2024-12-02', '2024-12-03', '2024-12-04',\n               '2024-12-05', '2024-12-06', '2024-12-09', '2024-12-10',\n               '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-16',\n               '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20',\n               '2024-12-23', '2024-12-24', '2024-12-26', '2024-12-27',\n               '2024-12-30', '2024-12-31', '2025-01-02', '2025-01-03',\n               '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09',\n               '2025-01-10', '2025-01-13', '2025-01-14', '2025-01-15',\n               '2025-01-16', '2025-01-17', '2025-01-20', '2025-01-21',\n               '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-27',\n               '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31',\n               '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06',\n               '2025-02-07'],\n              dtype='datetime64[ns]', name='Date', freq=None)\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\nadv_3m\n\nADV = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\n\n\n\n\nAC.PA\n2.166276e+06\n\n\nAI.PA\n6.087734e+05\n\n\nAIR.PA\n3.842644e+05\n\n\nMT.AS\n2.288744e+05\n\n\nCS.PA\n5.688459e+05\n# Génération des quantités\nnp.random.seed(123)\nADV[\"Quantity\"] =  round(1.5 * np.random.rand(len(ADV[\"ADV\"])) * ADV[\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n\n\nCS.PA\n5.688459e+05\n613900.0\nOn fait l’hypothèse que la profondeur de marché est de 20%. Celà signifie que l’on peut vendre 20% de la quantité sans impacter le prix de façon considérable. Au delà, le prix est impacté. Cette profondeur est ce qui est observé en pratique dans les carnets d’ordre à tel point que l’AMF le recommande.\nmarket_depth = 20/100\nADV[\"Quantity in 1day\"] = round(ADV[\"Quantity\"] * market_depth)\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n# Calcul du nombre de jours de liquidation\nADV[\"Days of liquidation\"] = ADV[\"Quantity\"]/ADV[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV[\"Days of liquidation\"] = ADV[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\nDays of liquidation\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n5\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n5\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n5\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n5\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n5\nprint(f\"Temps de liquidation du portefeuille : {ADV['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 5 jours"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#présence-de-déformation",
    "href": "3A/gestion_actifs/profil_liquid.html#présence-de-déformation",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "Présence de déformation",
    "text": "Présence de déformation\n\nSous conditions normales avec déformation (waterfall liquidation)\nOn peut être également interessé par la quantité de liquidation sur plusieurs jours. Pour celà, on fait l’hypothèse qu’on liquide les prochains jours aux prix observés aujourd’hui. Ce que je peux véritablement liquider en 1 jour est donc la quantité que je peux vendre sans impacter le prix, i.e. min(quantité liquidable en 1 jour, quantité restant dans le portefeuille).\nOn peut calculer la valeur du portefeuille initiale et sur les jours de liquidation désirée. On l’exprime généraleent en pourcentage des encours totaux. On peut également calculer le cumul du pourcentage liquidé sur les jours de liquidation désirée. Cela nous permet d’obtenir le profil d’écoulement.\n\n# Initialisation d'une colonne pour suivre les quantités liquidées\nADV[\"Quantity liquidated\"] = 0  # Initialement, rien n'est liquidé\n\n# Création d'une liste pour suivre la liquidation jour par jour\n# Au jour 0, on a liquidé 0. La colonne 0 sert de quantité initiale\nquantity_liquidated_per_day = [ADV[\"Quantity\"]]\n\nfor nb_day in range(1, 8):  # Pour chaque jour\n    # Calculer la quantité liquide au jour i\n    liquidated_today = np.minimum(ADV[\"Quantity in 1day\"], ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"])\n    \n    # Mettre à jour les quantités liquidées dans le DataFrame\n    ADV[\"Quantity liquidated\"] += liquidated_today\n    \n    # Stocker les quantités liquidées ce jour dans une liste\n    quantity_liquidated_per_day.append(liquidated_today)\n\n# Conversion des résultats jour par jour en DataFrame pour plus de clarté\nliquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\nliquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n\nliquidation_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n2263116.0\n452623.0\n452623.0\n452623.0\n452623.0\n452623.0\n1.0\n0.0\n\n\nAI.PA\n261291.0\n52258.0\n52258.0\n52258.0\n52258.0\n52258.0\n1.0\n0.0\n\n\nAIR.PA\n130756.0\n26151.0\n26151.0\n26151.0\n26151.0\n26151.0\n1.0\n0.0\n\n\nMT.AS\n189273.0\n37855.0\n37855.0\n37855.0\n37855.0\n37853.0\n0.0\n0.0\n\n\nCS.PA\n613900.0\n122780.0\n122780.0\n122780.0\n122780.0\n122780.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=1)\nprice_data = get_data(start_date, end_date, index, assets_ticker, column=\"Close\")\n\nprice_data[\"portfolio_data\"].head()\nprice_dict = price_data[\"portfolio_data\"].iloc[-1].to_dict()\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********            20%                       ]  8 of 40 completed[**********            20%                       ]  8 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************48%                       ]  19 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************75%***********            ]  30 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Valeur liquide des actions par jour de liquidation\nmarket_value =[\n    price_dict[ticker] * liquidation_df.loc[ticker]\n    for ticker in selected_assets\n]\n\nmarket_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\nmarket_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n1.129295e+08\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n49.900002\n0.0\n\n\nAI.PA\n4.418953e+07\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n169.119995\n0.0\n\n\nAIR.PA\n2.182579e+07\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n166.919998\n0.0\n\n\nMT.AS\n5.178509e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035658e+06\n0.000000\n0.0\n\n\nCS.PA\n2.305808e+07\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n0.000000\n0.0\n\n\n\n\n\n\n\n\n# Calcul de la valeur de marché initiale et totale\nmarket_value_0 = market_value.iloc[:, 0]\ntotal_market_value_0 = market_value_0.sum()\n\n# Calcul de la valeur de marché cumulée (à partir de la colonne 1)\ncumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\ncumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\ncumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\ncumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\nweights = {}\nfor ticker in assets_ticker :\n    weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\nweights = pd.DataFrame(weights).T\nweights.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n0.010344\n0.010344\n0.010344\n0.010344\n0.010344\n0.012815\nNaN\nNaN\n\n\nAI.PA\n0.004048\n0.004048\n0.004048\n0.004048\n0.004048\n0.043432\nNaN\nNaN\n\n\nAIR.PA\n0.001999\n0.001999\n0.001999\n0.001999\n0.001999\n0.042867\nNaN\nNaN\n\n\nMT.AS\n0.000474\n0.000474\n0.000474\n0.000474\n0.000474\n0.000000\nNaN\nNaN\n\n\nCS.PA\n0.002112\n0.002112\n0.002112\n0.002112\n0.002112\n0.000000\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Initialiser le graphique\nplt.figure(figsize=(12, 6))\n\n# Barplot empilé\nbottom = None\nfor asset in weights.index:\n    plt.bar(\n        pd.to_numeric(weights.columns),  # Les jours\n        weights.loc[asset],  # Poids de l'actif pour chaque jour\n        bottom=bottom,  # Position de départ pour empiler les barres\n        label=selected_assets[asset]  # Légende pour chaque actif\n    )\n    bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\nplt.xlabel(\"Days of Liquidation\")\nplt.ylabel(\"Portfolio Weights\")\nplt.title(\"Déformation du portefeuille\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nPour un fond de droit français reglementé, on a pas le droit d’investir plus de 5% du portefeuille dans une société. Exceptionnellement, pour certains titre, on a le droit d’investir jusqu’à 10% du portefeuille, à condition que les titres qui sont exposées à plus de 5% du portefeuille ne dépassent pas 40% du portefeuille. C’est la règle des 5/10/40. C’est un ratio réglementaire pour les OPC. Toutes les pertes réalisées en raison du défaut de ce ratio doivent être supportées par la société de gestion. Ces depassements doivent être déclarés à l’AMF. Dans notre cas, ce ratio n’est pas respecté, l’équilibre du portefeuille est chamboulé.\n\n# Valeur liquide du portefeuille\nmarket_value_df = pd.DataFrame()\n\nmarket_value_df[\"market_value\"] = market_value.sum(axis=0)\n\n# Calculer la valeur liquide relative par rapport au jour 0\nmarket_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n\n# Calculer la valeur cumulée liquide relative du portefeuille\nmarket_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n\n# Afficher le DataFrame résultant\nprint(market_value_df)\n\n   market_value  relative value  cumulative value\n0  1.091699e+10    1.000000e+00               0.0\n1  2.183399e+09    2.000001e-01               0.2\n2  2.183399e+09    2.000001e-01               0.4\n3  2.183399e+09    2.000001e-01               0.6\n4  2.183399e+09    2.000001e-01               0.8\n5  2.183392e+09    1.999994e-01               1.0\n6  3.893920e+03    3.566844e-07               1.0\n7  0.000000e+00    0.000000e+00               1.0\n\n\n\nimport matplotlib.pyplot as plt\nmarket_value_df = market_value_df.iloc[1:]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,  # Center text\n        height,  # Position slightly above the bar\n        f'{height:.2f}',  # Format with 2 decimal places\n        ha='center',  # Center horizontally\n        va='bottom',  # Position text at the bottom\n        fontsize=10, color=\"black\"\n    )\n\n# Set labels and title\nplt.xlabel(\"Days\")\nplt.ylabel(\"Cumulative Value (%)\")\nplt.title(\"Profil de liquidation du portefeuille\")\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nPour voir ce qui arrive au profil d’écoulement lorsque les quantités varient, on va utiliser un facteur de modulation de la quantité. Cela permet de déterminer quelle est la taille cible du portefeuille qui permet d’avoir la liquidité pour un certain niveau en nombre de jours qu’on se fixe. Cet exercice est fait une seule fois à l’initialisation du portefeuille.\nLa liquidité d’un portefeuille dépend de la liquidité intrinsèque des titres et la quantité de titres.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumulées.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (clé = actif, valeur = prix).\n        selected_assets (list): Liste des actifs sélectionnés.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantités.\n        plot_graphs (bool): Indique si les graphiques doivent être affichés.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumulées et relatives.\n    \"\"\"\n    # Initialisation des quantités liquidées\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantités liquidées par jour\n    for _ in range(1, nb_liquidation+1):\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        ADV[\"Quantity liquidated\"] += liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des résultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de marché initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de marché cumulée (à partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empilé)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empilé\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de départ pour empiler les barres\n                label=selected_assets[asset]  # Légende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"Déformation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Création du DataFrame des valeurs de marché\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumulées\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumulée\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=0.5\nnb_liquidation=6\n\nnew_market_value_df, new_market_value, new_weights = liquidation_profile(ADV, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew_market_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\nAC.PA\n5.646475e+07\n2.258589e+07\n2.258589e+07\n1.129297e+07\n0.0\n0.0\n0.0\n\n\nAI.PA\n2.209477e+07\n8.837873e+06\n8.837873e+06\n4.419021e+06\n0.0\n0.0\n0.0\n\n\nAIR.PA\n1.091290e+07\n4.365125e+06\n4.365125e+06\n2.182646e+06\n0.0\n0.0\n0.0\n\n\nMT.AS\n2.589255e+06\n1.035713e+06\n1.035713e+06\n5.178291e+05\n0.0\n0.0\n0.0\n\n\nCS.PA\n1.152904e+07\n4.611617e+06\n4.611617e+06\n2.305808e+06\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nSous conditions stressées avec déformation\nPour avoir des conditions stressées, on joue sur la quantité liquidable en un jour et de fait sur la profondeur de marché. Pour des conditions stressées à la baisse, on divise la profondeur de marché par 2. Pour des conditions stressées à la hausse, on multiplie la profondeur de marché par 2.\n\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\n\nADV_stressed = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\n\n# Génération des quantités\nnp.random.seed(42)\nADV_stressed[\"Quantity\"] =  round(1.5 * np.random.uniform(0, 1, size=len(ADV)) * ADV[\"ADV\"])\n\n# Quantité journalière\nmarket_depth = (20/100)/2  # On stresse la liquidité\nADV_stressed[\"Quantity in 1day\"] = round(ADV_stressed[\"Quantity\"] * market_depth)\n\n# Calcul du nombre de jours de liquidation\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Quantity\"]/ADV_stressed[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\n\nprint(f\"Temps de liquidation du portefeuille : {ADV_stressed['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 10 jours\n\n\n\nfact_modulation=1\nnb_liquidation=12\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#absence-de-déformation-du-portefeuille-pro-forma",
    "href": "3A/gestion_actifs/profil_liquid.html#absence-de-déformation-du-portefeuille-pro-forma",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "Absence de déformation du portefeuille (pro forma)",
    "text": "Absence de déformation du portefeuille (pro forma)\nL’objectif est de conserver la distribution du portefeuille à mesure qu’il se liquide. Tout d’abord, on estime la quantité liquidable à un jour de chacun des titres comme fait précédemment. Celà permet d’avoir le pourcentage liquidable en un jour.\nSi on veut que le portefeuille se liquide à la même vitesse, il faut aller à la vitesse du titre le plus lent. On peut calculer le pourcentage liquidable en un jour pour chaque titre. On prendra le minimum de ces pourcentages pour déterminer le pourcentage liquidable en un jour du portefeuille.\nLe portefeuille prend ainsi plus de temps à se liquider et fatalement, le portefeuille finit par se déformer.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile_pro_forma(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumulées.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (clé = actif, valeur = prix).\n        selected_assets (list): Liste des actifs sélectionnés.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantités.\n        plot_graphs (bool): Indique si les graphiques doivent être affichés.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumulées et relatives.\n    \"\"\"\n    # Initialisation des quantités liquidées\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantités liquidées par jour\n    for _ in range(1, nb_liquidation+1):        \n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        min_liquidated_today = (liquidated_today/ADV[\"Quantity in 1day\"]).min() # On liquide à la vitesse de l'actif le moins liquide\n        ADV[\"Quantity liquidated\"] += min_liquidated_today*liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des résultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de marché initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de marché cumulée (à partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empilé)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empilé\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de départ pour empiler les barres\n                label=selected_assets[asset]  # Légende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"Déformation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Création du DataFrame des valeurs de marché\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumulées\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumulée\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=1\nnb_liquidation=10\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile_pro_forma(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour gérer la liquidité d’un portefeuille et donc préserver la qualité du portefeuille, on peut suspendre les souscriptions et les rachats par des mécanismes émis par la loi. Les régulateurs des SGP annoncent que les investisseurs annoncent que les indivdus ne peuvent plus souscrire ou faire un rachat.\nMécanismes de gestion de la liquidité:\n\nLes Gates consistent à plafonner les rachats. Si les rachats totaux sont supérieures à 5% de l’actif net, la SGP a le droit et non l’obligatoire ne pas honorer les rachats de plus de 5%. Elle limite donc les rachats en un jour à 5% et ventiler le reste sur les jours suivants en fonction des conditions du marché. Cela permet de ne pas impacter le prix de façon considérable. C’est une mesure de protection des investisseurs restants. Les gates restent quand même un signal négatif pour les investisseurs restants. Ils permettent toutefois de mettre de l’ordre dans le portefeuille. L’AMF le fait figurer dans le prospectus, sauf si la SGP arrive à justifier qu’elle n’a pas besoin de le faire. Il n’en demeure pas moins que l’activation des gates est optionnelle"
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "",
    "text": "L’analyse financière constitue l’ensemble des outils permettant de donner un avis objectif d’une organisation (entreprises, fondations, etc.) sur la santé finanière et les risques financiers auxquels elle sera confrontée. Il s’agit de determiner quels sont les critères d’une santé financière, qu’est le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le gère-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financière d’une entreprise. Il s’agit du bilan et du compte de résultat. Ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise. Comprendre comment ils fonctionnent permet de mieux appréhender la situation financière d’une banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un état des lieux de la situation patrimoniale de l’entreprise à un moment donné. Il est composé de deux parties : l’actif et le passif. L’actif ou l’emploi regroupe l’ensemble des biens et des droits de l’entreprise tandis que le passif regroupe l’ensemble des ressources de l’entreprise (d’où vient l’argent et où peut-on s’en procurer). Le bilan est équilibré en valeur nette, c’est-à-dire que l’actif est égal au passif.\nLe compte de résultats, quant à lui, est un document qui permet de faire un état des lieux des performances de l’entreprise sur une période donnée (il résume les bénéfices ou pertes générées). Il est composé du détail des produits et des charges de l’entreprise. Les produits sont les éléments qui génèrent des revenus pour l’entreprise tandis que les charges sont les éléments qui génèrent des dépenses pour l’entreprise. Le compte de résultat alimente par ailleurs la partie “résultat de l’exercice” du bilan comptable.\nLe coeur de l’entreprise à analyser comme ressources supplémentaires dans le compte de résultat est l’ensembles des charges financières & exceptionnelles ainsi que l’ensemble des produits d’exploitation et financiers. Ces éléments clés permettent de déterminer la rentabilité de l’entreprise. En effet, si les charges sont supérieures aux produits, l’entreprise est en perte. Si les produits sont supérieurs aux charges, l’entreprise est en bénéfice.\nIl est important de noter que ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d’analyser la situation financière d’une entreprise, il faut donc le remodeler en un bilan “fonctionnel” pour pouvoir l’analyser. Le bilan fonctionnel est un document qui permet de faire un état des lieux de la situation financière de l’entreprise en fonction de son activité, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d’investissement à long terme\nEmplois stables\n\nactifs immobilisés en valeur brute\n\nCycle de financement à long terme\nRessources stables\n\nCapitaux propres,\nEmprunts à long terme,\nAmortissements et dépréciation,\nProvisions pour risques\n\n\n\nCycle d’exploitation\nEmplois d’exploitation\n\nStocks et encours\nCréances\n\nCycle d’exploitation\nRessources d’exploitation\n\nDettes circulantes\n\n\n\nTrésorerie active\n\nDisponibilités\n\nTrésorerie passive\n\nDécouverts bancaires\n\n\n\n\nLes ressources stables font référence aux ressources saines du bilan etfont face aux emplois stables. La trésorerie passive fait référence aux découverts bancaires. Il est important de souligner qu’une trésorerie passive est perçue négativement dans le bilan fonctionnel. En effet, une trésorerie passive signifie que l’entreprise a des dettes à court terme qui ne sont pas couvertes par des actifs à court terme d’où la nécessité d’avoir des découverts bancaires.\nNb : La provision pour le risque peuve être considérée comme une ressource stable ou une ressource d’exploitation en fonction de l’entreprise. Tout dépend de la longevité des provisions.\n\nEquilibre financier\nNous dirons qu’il y a équilibre financier lorsque :\n\nLes emplois stables soient entièrement financés par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a necéssité d’un fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) dépend du cycle d’exploitation (entre autre, la rapidité de rotation des stocks et des créances). Il doit couvrir les besoins de financement du cycle d’exploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, créances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la trésorerie (\\(\\text{Trésorerie}=FDR-BFR\\)). Si la trésorerie est positive, il y a équilibre financier. Celà signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d’exploitation. Lorsqu’il est négatif, il faut trouver des ressources pour financer le cycle d’exploitation. Si la trésorerie est nulle, il y a équilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-même le cycle d’exploitation de l’entreprise. C’est ce qu’on appelle le crédit fournisseur. Il est important de noter que le crédit fournisseur est une source de financement gratuite pour l’entreprise. C’est le cas des E-commerce où les acteurs encaissent leurs clients avant même d’acheter les stocks auprès des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transformé en ressources en fonds de roulement, celà est une situation très favorable pour l’entreprise et est appelée “crédit inter-entreprises”."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Analyse du compte de résultat",
    "text": "Analyse du compte de résultat\nNous pouvons faire les mêmes critiques faites au bilan comptable sur le compte de résultat. En effet, le compte de résultat est conçu de sorte à fournir des informations au seul détenteur du capital, à savoir les actionnaires. Il fait apparaitre uniquement le bénéfice ou la perte. C’est un document d’intérêt pour l’Etat pour déterminer si un pays est en croissance ou en récession. Pour en faire un vrai diagnostic financier, il faut le découper en sous-soldes appelés “soldes intermédiaires de gestion” (SIG). Les SIG permettent de déterminer la rentabilité de l’entreprise, sa capacité d’autofinancement, sa capacité de remboursement, sa capacité de financement, etc.\nIl existe 9 soldes intermédiaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajoutée\nL’excédent brut d’exploitation (EBE)\nLe résultat d’exploitation\nLe résultat courant avant impôt\nLe résultat exceptionnel\nLe résultat net\nLa plus ou moins value de cession\n\nSelon la théorie de prise de décisions, il y a deux grands types de décisions : des décisions qui permettent de créer de la riches (Marge co., production et valeur ajoutée) et des décisions qui permettent de distribuer/dépenser de la richesse (EBE, résultat d’exploitation, résultat courant avant impôt, résultat exceptionnel, résultat net et plus ou moins value de cession). Lorsqu’on dépense la riches, il faudrait qu’elle soit bien dépensée.\n\nSoldes de création de richesse\nLes soldes qui contribuent à la création de richesse sont la marge commerciale, la production et la valeur ajoutée :\n\nLa marge commerciale est la différence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C’est un solde des entreprises commerciales (par exemple, les supermarchés). Pour une entreprise qui n’ont pas de marchandises, le marge commerciale est nulle.\nLa production de l’exercice est la somme des produits vendus(\\(\\pm\\) les produits stockées) et des produits immobilisées par l’entreprise (certaines entreprises peuvent se vendre des produits à elles-mêmes). C’est un solde des entreprises industrielles.\nLa valeur ajoutée est la richesse créée par l’entreprise. C’est la somme des marges commerciales, de la production de l’exercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services extérieurs).\n\nLa valeur ajouté est un indicateur très suivi par l’Etat pour déterminer le produit intérieur brut (PIB) afin de déterminer si un pays est en croissance ou en récession. Par ailleurs, la valeur ajoutée divisée par le nombre de salariés permet de déterminer le niveau de technicité de l’entreprise. Plus la valeur ajoutée par salarié est élevée, plus l’entreprise est techniquement avancée.\n\n\nLa richesse dédiée à l’activité économique\nIl existe 5 tiers à qui l’entreprise redistribue la VA (rangée par ordre de priorité) :\n\nLe personnel (à travers les salaires),\nL’Etat (à travers les impôts),\nLe capital technique (via les amortissements),\nLes banques (via les intérêts),\nLes actionnaires ou les associés (via le bénéfice comptable)\n\nLes soldes qui permettent de financer l’activité économique (Etat, personnel, capital technique) sont l’excédent brut d’exploitation et le résultat d’exploitation :\n\nLe solde EBE rémunère le personnel et l’Etat. Il représente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d’entreprise pour l’Etat. Un EBE positif signifie que l’entreprise est capable de rémunérer le personnel et l’Etat, et donc de financer l’emploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajoutée} - \\text{Impôts, tâxes et versements assimilés} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de résultat d’exploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de rémunérer le capital technique (machines etc.) et appartient à tout ceux qui dépendent du capital financier et mesure les performances industrielles et commerciales de l’entreprise.\n\n\\[\\begin{align*}\n\\text{Résultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse dédiée à l’activité financière\nLes soldes qui permettent de financer l’activité financière (banques, actionnaires) sont le résultat courant avant impôt, le résultat exceptionnel, le résultat net et la plus ou moins value de cession :\n\nLe résultat courant avant impôt est le solde qui permet de rémunérer les banques. Il est un indicateur de la capacité de l’entreprise à rembourser ses dettes et est un témoin de l’incidence de la politique financière de l’entreprise sur son résultat. Il faut distinguer les intérêts à long terme et ceux de court terme. Plus ceux ci sont liés à des dettes de court terme (ex. : découverts), on peut dire que l’entreprise est en difficulté financière tandis que l’endettement à long terme est un signe de bonne santé financière, car il est voulu plutôt que subi. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat courant avant impôt} &= \\text{Résultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financières}\n\\end{align*}\\]\n\nLe résultat exceptionnel est le solde qui est le moins analysé car il est souvent lié à des évènements exceptionnels (ex. : vente d’un bien immobilier). Il est calculé comme étant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe résultat net est le solde qui permet de rémunérer les actionnaires. C’est le solde en bas du compte de résultat. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat net} &= \\text{Résultat courant avant impôt} + \\text{Résultat exceptionnel} \\\\\n&- \\text{participations des salariés} - \\text{Impôts sur les bénéfices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu’une entreprise vend une immobilisation. Ce ratio permet de déterminer si l’entreprise a vendu une immobilisation à un prix supérieur ou inférieur à sa valeur comptable. Celà constitue un temoin d’alerte sur la santé de l’entreprise et permet de déterminer si l’entreprise est en difficulté financière (car rien ne l’oblige à vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "La capacité d’autofinancement",
    "text": "La capacité d’autofinancement\nLa capacité d’autofinancement (CAF) est un indicateur qui permet de déterminer si l’entreprise est capable de financer ses investissements sans recourir à des financements extérieurs. Elle regroupe la capacité à dégager de la liquidité. Il n’y a pas de correspondance entre la trésorerie et le bénéfice. En effet, une entreprise peut être en bénéfice mais en difficulté financière. Pour la calculer, il faut éliminer les sommes non encaissanles et non décaissables (ex. : Dotations, provision, reprise sur amortissements, les écritures exceptionnelles).\nPour passer du bénéfice à la CAF, on ne conserve que les éléments qui sont encaissables et décaissables et est calculée comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges décaissables (intérêt bancaire, impôt sur bénéfice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu versé par l’entreprise à ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas à elle toute seule de déterminer l’autofinancement de l’entreprise. Dans le cadre légal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu’à 95% du bénéfice comptable imposé par l’Etat et garder 5% à l’entreprise. C’est ce qu’on appelle le “dividende légal”. Au delà de 10%, les actionnaires peuvent retirer jusqu’à 100% du bénéfice comptable. C’est ce qu’on appelle le “dividende statutaire”.\nAinsi, l’autofinancement est la somme qui reste de la CAF après le dividende légal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaissés par les actionnaires détermine la politique d’autofinancement de l’entreprise.\nL’autofinancement est essentiel pour l’entreprise car il permet de:\n\nrembourser les emprunts,\naméliorer la trésorerie,\ncouvrir les risque de l’entreprise (provisions pour risque),\nfinancer l’exploitation (stocks & créances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html",
    "href": "3A/Apprentisage-stat/Tp3.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "AdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\nHence, for M weak learners, the final prediction is given by \\(g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)\\) where \\(g_m(x)\\) is the m-th weak learner and \\(\\alpha_m\\) is the weight associated with the m-th weak learner. The optimisation problem of AdaBoost is given by:\n\\[ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) \\]\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function \\(l(y,f(x))=\\exp(-yf(x))\\), \\(y \\in \\{-1,+1\\}\\). It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n\nInitialize the observation weights \\(w_i^{(1)} = 1/n\\) for \\(i=1,\\dots,n\\)\n\n\nFor m=1 to M:\n\n\n\nFit a weak learner \\(g_m(x)\\) to the training data using weights \\(w_i^{(m)}\\)\n\n\nCompute the error rate \\(err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))\\) where \\(I\\) is the indicator function\n\n\nCompute the weight \\(\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)\\)\n\n\nUpdate the weights \\(w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)\\)\n\n\n\nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems. It is a generalization of the AdaBoost algorithm to multi-class classification problems.\nTo inspect how the errors and the weights vary with the number of iterations, we will the function make_gaussian_quantiles from sklearn. This function generates a multi-dimensional standard normale distribution with a given number of samples \\(n\\) per class \\(K\\). We will generate a dataset of size \\(n=2000\\) with \\(K=3\\) classes and \\(d=10\\) features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "href": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "title": "Gradient boosting",
    "section": "",
    "text": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "href": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "title": "Gradient boosting",
    "section": "",
    "text": "# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html",
    "href": "3A/Apprentisage-stat/Tp2.html",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear.\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable. In particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. The optimization problem of KRR is given by: \\[\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n\\] where \\(x_i\\) is the \\(i\\)-th row of the matrix \\(X\\) and \\(y_i\\) is the \\(i\\)-th element of the vector \\(y\\). The parameter \\(\\lambda\\) is the regularization parameter. The solution of the optimization problem is given by:\n\\[\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n\\]\nwhere \\(I_d\\) and \\(I_n\\) are the identity matrix.\nIn prediction, the target variable is given by: \\[\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n\\] where \\(\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}\\). We easily see that the prediction is a linear combination of the scalar product between the test point \\(x^*\\) and the training points \\(x_i\\), we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\\[\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n\\]\nwhere \\(K(x_i, x^*)\\) is the kernel function.\n\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinusoïdal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let’s try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Now we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinusoïdal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let’s try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "href": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Linear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "title": "Kernel Trick and SVM",
    "section": "I. Toy dataset",
    "text": "I. Toy dataset\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n\nText(0.5, 1.0, 'Two moons dataset')\n\n\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n\n\n1. K-nearest neighbors\n\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nBest parameters by CV : {'n_neighbors': 1}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Logistic regression\n\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n\n\n\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. SVM\n\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 0.030888435964774818, 'gamma': 3.727593720314938}\n\n\n\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Conclusion\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\nWhenever we have a classification problem, it is hence always useful to try the SVM and the KNN."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "title": "Kernel Trick and SVM",
    "section": "II. Image dataset",
    "text": "II. Image dataset\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255). We are going to use the SVM with the gaussian kernel to classify the images.\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n\nDescription of train dataset : \n\n\n\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n\n\n\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n\n\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n\n\n\n\n\n\n\n\n\n\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n\n\n\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html",
    "href": "3A/Apprentisage-stat/Tp4.html",
    "title": "Features selection",
    "section": "",
    "text": "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features.\nThe usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\nIn this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n\\[I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)\\]\nwhere \\(p(x,y)\\) is the joint probability distribution function of X and Y, and \\(p(x)\\) and \\(p(y)\\) are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\nIn order to have confidence in this measure, we will consider a bivariate gaussian variable \\(Z=(X,Y)\\) with mean \\(\\mu = (0,0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}\\). We will compute the mutual information between X and Y for a grid a \\(\\rho\\) between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to \\(-\\frac{1}{2} \\log(1-\\rho^2)\\).\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset.\n\n\n\nWe will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n\n\n\n\nWe can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "href": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "title": "Features selection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "href": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "title": "Features selection",
    "section": "",
    "text": "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "href": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "title": "Features selection",
    "section": "",
    "text": "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "href": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "title": "Features selection",
    "section": "",
    "text": "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/gestion_actifs/mesures_rsq.html",
    "href": "3A/gestion_actifs/mesures_rsq.html",
    "title": "Gestion de risques de portefeuille",
    "section": "",
    "text": "Pour gérer les risques, on procède en trois étapes : 1. Identification : Nous avons un portefeuille d’action, donc le risque auquel on fait face est le risque de marché action.\n\nMetrique de risque : Volatilité ex-ante, Value at Risk ex-ante, Tracking error ex-ante (i.e. par anticipation, on se base sur l’état du portefeuille à l’instant t et non aux instants passés - ex-post)\nEncadrement\n\nDans notre cas, on va constituer le portefeuille avec 10 actifs du CAC 40 de notre choix et leur allouer des poids aléatoires :\n\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_data(start_date, end_date, index_ticker, tickers):\n    \"\"\"\n    Extraction de données de cours d'actions\n    Args:\n        start_date (str): Date de début au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des prix historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    # Extraction des prix historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=2*365)\n\nselected_assets = {\n    \"SAN.PA\" : \"Sanofi\",\n    \"GLE.PA\" : \"Société générale\",\n    \"HO.PA\" : \"Thales\",\n    \"ENGI.PA\" : \"Engie\",\n    \"CAP.PA\" : \"Capgemini\",\n    \"CA.PA\" : \"Carrefour\",\n    \"ORA.PA\" : \"Orange\",\n    \"AC.PA\" : \"Accor\",\n    \"OR.PA\" : \"L'Oreal\",\n    \"ACA.PA\" : \"Crédit agricole\"\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker)\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nCA.PA\nCAP.PA\nENGI.PA\nGLE.PA\nHO.PA\nOR.PA\nORA.PA\nSAN.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-02-08\n29.441587\n9.554356\n15.175895\n177.961441\n10.856310\n23.891726\n113.740341\n359.421448\n8.415674\n81.924522\n\n\n2023-02-09\n29.232716\n9.961541\n15.267509\n179.503082\n10.846358\n23.828888\n113.931816\n362.677155\n8.366665\n82.563179\n\n\n2023-02-10\n27.998465\n9.850800\n14.928535\n176.419815\n10.950842\n23.564072\n116.804039\n359.660858\n8.478687\n82.100380\n\n\n2023-02-13\n28.425705\n9.833763\n14.745306\n177.672379\n10.887820\n23.743610\n119.676277\n373.114624\n8.490064\n81.128510\n\n\n2023-02-14\n28.606094\n9.850800\n15.043054\n178.202316\n10.979035\n23.797468\n122.021935\n371.295227\n8.686104\n81.554283\n\n\n\n\n\n\n\n\nbenchmark_data = data[\"benchmark_data\"]\nbenchmark_data.head()\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2023-02-08\n7119.830078\n\n\n2023-02-09\n7188.359863\n\n\n2023-02-10\n7129.729980\n\n\n2023-02-13\n7208.589844\n\n\n2023-02-14\n7213.810059\n\n\n\n\n\n\n\n\n# On attribue des poids équitables pour chaque action\nweights_by_asset = {ticker: 1 / len(assets_ticker) for ticker in assets_ticker}\n\nOn souhaite connaitre la valeur totale du actifs du portefeuille, i.e. l’asset under management(AUM) :\n\\[\nAUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times P_i(T_n)\n\\]\n\naum_series = portfolio_data.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\naum_series\n\nAUM = pd.DataFrame(aum_series, columns=[\"AUM\"])\n\n\nAUM.head()\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-02-08\n83.038330\n\n\n2023-02-09\n83.617891\n\n\n2023-02-10\n83.075649\n\n\n2023-02-13\n84.771806\n\n\n2023-02-14\n85.003632\n\n\n\n\n\n\n\n\n# Evolution de la valeur totale du portefeuille\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.plot(AUM, label=\"AUM\")\nplt.title(\"Evolution de l'actif sous gestion\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# On s'interesse aux variations/rendements de l'AUM\n\nAUM[\"Variation\"] = AUM[\"AUM\"].pct_change()\nAUM[\"Variation\"].head()\n\nDate\n2023-02-08         NaN\n2023-02-09    0.006979\n2023-02-10   -0.006485\n2023-02-13    0.020417\n2023-02-14    0.002735\nName: Variation, dtype: float64\n\n\n\nEstimation de la volatilité\nPour estimer la volatilité du portefeuille, on peut calculer l’écart-type des variations de l’AUM. On fait le choix de calculer une volatilité ex-ante en se basant sur les variation historiques des prix des actifs avec une profondeur historique de 2 ans. Vu qu’on a une volatilité quotidienne, on va l’annualiser en multipliant par \\(\\sqrt{252}\\).\nEn général, sur le marché action, la volatilité quotidienne est environ de 1% et la volatilité annuelle est entre 10% et 20%.\n\n# Calcul de la volatilité du portefeuille\nvolatility_portfolio = np.std(AUM[\"Variation\"])\nannualized_volatility_portfolio = volatility_portfolio * np.sqrt(252)\nprint(f\"Volatilité de la performance quotidienne : {volatility_portfolio : .2%}\")\nprint(f\"Volatilité de la performance annuelle : {annualized_volatility_portfolio : .2%}\")\n\nVolatilité de la performance quotidienne :  0.87%\nVolatilité de la performance annuelle :  13.80%\n\n\n\n# Calcul de la volatilité de l'indice CAC 40\n\nbenchmark_data[\"Variation\"] = benchmark_data[\"^FCHI\"].pct_change()\nvolatility_benchmark = np.std(benchmark_data[\"Variation\"])\nannualized_volatility_benchmark = volatility_benchmark * np.sqrt(252)\n\nprint(f\"Volatilité de l'indice CAC 40 : {volatility_benchmark : .2%}\")\nprint(f\"Volatilité de l'indice CAC 40 annuelle : {annualized_volatility_benchmark : .2%}\")\n\nVolatilité de l'indice CAC 40 :  0.84%\nVolatilité de l'indice CAC 40 annuelle :  13.34%\n\n\nOn retrouve sur à peu près la même volatilité du portefeuille et celle du CAC 40. Il y a donc une certaine homogénéité.\n\n\nEstimation de la tracking error/erreur de suivi\nLa tracking error est une mesure de l’écart entre la performance d’un portefeuille et celle de son indice de référence. Elle est calculée comme la volatilité de la différence entre les rendements du portefeuille et de l’indice de référence :\n\\[\nTE = \\sqrt{Var(R_p - R_b)}\n\\]\nLa tracking error mesure l’incercitude du portefeuille par rapport à l’indice de référence, c’est une mesure relative. Plus la tracking error est élevée, plus le portefeuille est risqué. On ne souhaite sous ou sur-performer l’indice de référence. On souhaite suivre véritablement l’indice de référence.\nPour l’annualiser, on multiplie par \\(\\sqrt{252}\\) en supposant que les performances quotidiennes sont indépendantes et donc un utilise l’additivité des variances.\n\nperformance_relative = AUM[\"Variation\"] - benchmark_data[\"Variation\"]\n\nplt.figure(figsize=(12, 4))\nplt.plot(performance_relative, label=\"Performance\")\nplt.title(\"Performance du portefeuille par rapport à l'indice CAC 40\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Performance\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcul de la tracking error\n\nTE = np.std(AUM[\"Variation\"] - benchmark_data[\"Variation\"]) \nprint(f\"Tracking error : {TE : .2%}\")\n\nTE_annualized = TE * np.sqrt(252)\nprint(f\"Tracking error annualisé : {TE_annualized : .2%}\")\n\nTracking error :  0.52%\nTracking error annualisé :  8.20%\n\n\n\n\nEstimation de la Value-at-Risk (VaR)\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l’on peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donné. Par exemple, une VaR à 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut également raisonner en terme de gain, i.e. Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements passés selon l’horizon fixé pour estimer la VaR, à l’aide d’un quantile empirique d’ordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la méthode de rescaling, i.e. \\(VaR = \\sigma \\times \\Phi^{-1}(\\alpha)\\). 2. Approche paramétrique : On suppose que les rendements suivent une loi normale. 3. Approche Monte Carlo : On simule les rendements futurs.\n\n# VaR historique\nseuil = 99/100\n\nVaR_hist_portfolio = np.percentile(AUM[\"Variation\"].dropna(), 100*(1- seuil))\nprint(f\"VaR historique sur le portefeuille : {- VaR_hist_portfolio : .2%}\")\nprint(f\"VaR historique sur 20 jours sur le portefeuille : {-VaR_hist_portfolio*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\nVaR_hist_benchmark = np.percentile(benchmark_data[\"Variation\"].dropna(), 100*(1 - seuil))\nprint(f\"VaR historique sur l'indice CAC 40 : {-VaR_hist_benchmark : .2%}\")\nprint(f\"VaR historique sur 20 jours sur l'indice CAC 40 : {-VaR_hist_benchmark*np.sqrt(20) : .2%}\")\n\nVaR historique sur le portefeuille :  2.29%\nVaR historique sur 20 jours sur le portefeuille :  10.23%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nVaR historique sur l'indice CAC 40 :  2.13%\nVaR historique sur 20 jours sur l'indice CAC 40 :  9.51%\n\n\n\n# VaR paramétrique\n# PnL ~ N(mu, sigma) ==&gt; PnL = mu + sigma * Z, où Z ~ N(0,1)\n# P(PnL &gt; -VaR) = alpha &lt;=&gt; P(mu + sigma * Z &gt; -VaR) = alpha &lt;=&gt; P(Z &lt; (-VaR - mu) / sigma) = 1 - alpha\n# Donc, -VaR = mu + sigma * quantile(1 - alpha), où quantile(1 - alpha) est le quantile de la loi normale standard\n\nfrom scipy.stats import norm\n\nmu = np.mean(AUM[\"Variation\"].dropna())\nprint(f\"mu sur le portefeuille : {mu : .2}\")\nsigma = np.std(AUM[\"Variation\"].dropna())\nprint(f\"sigma sur le portefeuille : {sigma : .2}\")\n\nVaR_param_portfolio  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique sur le portefeuille : {VaR_param_portfolio : .2%}\")\nprint(f\"VaR paramétrique sur 20 jours sur le portefeuille : {VaR_param_portfolio * np.sqrt(20): .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(benchmark_data[\"Variation\"].dropna())\nprint(f\"mu sur le benchmark: {mu : .2}\")\nsigma = np.std(benchmark_data[\"Variation\"].dropna())\nprint(f\"sigma sur le benchmark : {sigma : .2}\")\n\nVaR_param_benchmark  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique sur le portefeuille : {VaR_param_benchmark : .2%}\")\nprint(f\"VaR paramétrique sur 20 jours sur le portefeuille : {VaR_param_benchmark * np.sqrt(20): .2%}\")\n\nmu sur le portefeuille :  0.00024\nsigma sur le portefeuille :  0.0087\nVaR paramétrique sur le portefeuille :  2.00%\nVaR paramétrique sur 20 jours sur le portefeuille :  8.94%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu sur le benchmark:  0.00027\nsigma sur le benchmark :  0.0084\nVaR paramétrique sur le portefeuille :  1.93%\nVaR paramétrique sur 20 jours sur le portefeuille :  8.63%\n\n\nLa VaR relative suit une philosophie proche du tracking error. Elle se calcule sur les écarts entre le portefeuille et le benchmark. Elle sert à mesurer de combien mon portefeuille sous-performe par rapport à l’indice de référence.\n\nperformance_relative\n\n\nVaR_hist_relative = np.percentile(performance_relative.dropna(), 100*(1- seuil))\nprint(f\"VaR historique relative : {- VaR_hist_relative : .2%}\")\nprint(f\"VaR historique relative sur 20 jours : {-VaR_hist_relative*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(performance_relative.dropna())\nprint(f\"mu des performances relatives: {mu : .2}\")\nsigma = np.std(performance_relative.dropna())\nprint(f\"sigma des performances relatives : {sigma : .2}\")\n\nVaR_param_relative  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique relative : {VaR_param_relative : .2%}\")\nprint(f\"VaR paramétrique relative sur 20 jours : {VaR_param_relative * np.sqrt(20): .2%}\")\n\nVaR historique relative :  1.07%\nVaR historique relative sur 20 jours :  4.80%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu des performances relatives: -2.5e-05\nsigma des performances relatives :  0.0052\nVaR paramétrique relative :  1.20%\nVaR paramétrique relative sur 20 jours :  5.39%\n\n\n\n\nStress test\nLes stress test permettent de tester les performances du portefeuille dans des conditions extrêmes. Ils sont de deux natures : 1. Stress test historique : On soumet le portefeuille à une période historique ou on estime avoir eu une condition extrême (Covid, Subprime crisis). On rejoue un scénario qui s’est déjà passé.\n\nStress test hypothétique : On joue un scénario qui ne s’est jamais réalisé. Exemple, si les actions chutent de 40%, notre portefeuille d’action chute de 40%.\n\nNote : bp = 0,01%\n\n# Recuperons les prix des actifs le 19/02/2020 et le 18/03/2020\n# On va valoriser notre portefeuille à ces dates et calculer les performances\n# A ces dates, le CAC 40 a connu de fortes pertes pendant la COVID-19\n# data_1902 = get_data\n\nstart_date = pd.to_datetime(\"19-02-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1902 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1902=data_1902[\"portfolio_data\"]\nbenchmark_data_1902=data_1902['benchmark_data']\n\nstart_date = pd.to_datetime(\"18-03-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1803 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1803=data_1803[\"portfolio_data\"]\nbenchmark_data_1803=data_1803['benchmark_data']\n\n# Concaténer les données des deux dates pour le portefeuille et le benchmark\nportfolio_data_stress = pd.concat([portfolio_data_1902, portfolio_data_1803], ignore_index=False)\nbenchmark_data_stress = pd.concat([benchmark_data_1902, benchmark_data_1803], ignore_index=False)\n\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n[                       0%                       ][                       0%                       ][                       0%                       ][*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Stress test covid sur le portefeuille\naum_series_stress = portfolio_data_stress.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\n\nAUM_stress = pd.DataFrame(aum_series_stress, columns=[\"AUM\"])\nAUM_stress[\"Variation\"] = AUM_stress[\"AUM\"].pct_change()\n\nAUM_stress\n\n\n\n\n\n\n\n\nAUM\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n63.108800\nNaN\n\n\n2020-03-18\n43.740069\n-0.30691\n\n\n\n\n\n\n\n\nbenchmark_data_stress[\"Variation\"]=benchmark_data_stress[\"^FCHI\"].pct_change()\nbenchmark_data_stress\n\n\n\n\n\n\n\nTicker\n^FCHI\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n6111.240234\nNaN\n\n\n2020-03-18\n3754.840088\n-0.385585\n\n\n\n\n\n\n\nNotre portefeuille permet de mieux resister au stress test covid que le CAC 40.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html",
    "href": "3A/proc_stochastique/modele_bs.html",
    "title": "Calibration du modèle Black-Scholes",
    "section": "",
    "text": "Le modèle de Black-Scholes est un modèle mathématique qui permet de déterminer le prix d’une option à partir de plusieurs paramètres. Il est basé sur l’hypothèse que le prix de l’actif sous-jacent suit un mouvement brownien géométrique :\n\\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t\n\\]\nAvec \\(S_t\\) le prix de l’actif, \\(\\mu\\) le taux de rendement moyen, \\(\\sigma\\) la volatilité et \\(W_t\\) un mouvement brownien.\nDe ce fait, le prix d’une option européenne peut être calculé par la formule de Black-Scholes :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\nAvec \\(C\\) le prix de l’option, \\(S_t\\) le prix de l’actif sous-jacent, \\(K\\) le prix d’exercice de l’option, \\(T\\) la maturité de l’option, \\(r\\) le taux d’intérêt sans risque, \\(\\sigma\\) la volatilité de l’actif, \\(N\\) la fonction de répartition de la loi normale centrée réduite, et :\n\\[\nd_1 = \\frac{1}{\\sigma \\sqrt{T}} \\left( \\ln \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T \\right)\n\\]\n\\[\nd_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\n\n# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu’on a pas accès à une formule analytique pour le prix d’une option, on peut utiliser la méthode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l’actif sous-jacent, et on calcule la valeur de l’option à chaque date de maturité. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l’option.\nLe prix de l’actif sous-jacent suit un mouvement brownien géométrique, et on peut simuler ce mouvement en utilisant la formule d’Ito :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturité {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturité 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturité {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturité 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturité 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturité 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturité {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturité {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturité 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturité 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturité 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturité 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux méthodes permettent d’avoir des résultats similaires. De plus, plus le nombre de simulations est grand, plus la précision de l’estimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilité du prix d’une option à différents paramètres. Les principaux greeks sont :\n\nDelta : mesure la sensibilité du prix de l’option par rapport au prix de l’actif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilité du delta par rapport au prix de l’actif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilité du prix de l’option par rapport à la volatilité de l’actif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d’autres, mais ces trois-là sont les plus couramment utilisés.\nAvec le modèle de Black-Scholes, on peut calculer ces greeks de manière analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu’on a pas accès à une formule analytique pour le prix de l’option, on peut utiliser la méthode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l’option pour une petite variation de chaque paramètre, et on fait la différence entre ces deux prix pour obtenir une estimation du greek. On peut également utiliser la méthode des différences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# Mérhode de différencee finie basé sur la méthode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par différences finies.\n\n    Paramètres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'intérêt sans risque (annualisé)\n    tau : float - Temps jusqu'à la maturité (en années)\n    sigma : float - Volatilité de l'actif sous-jacent (annualisée)\n    epsilon : float - Petit incrément pour les différences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL’intérêt du modèle de Black-Scholes est qu’il permet de calculer la volatilité implicite d’un actif à partir du prix de l’option. En effet, si on connait le prix de l’option, le prix de l’actif sous-jacent, le prix d’exercice de l’option, la maturité de l’option et le taux d’intérêt sans risque, on peut calculer la volatilité implicite en résolvant l’équation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilité d'une option d'achat à la monnaie de maturité 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilité implicite d'un call est de maturité {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilité implicite d'un call est de maturité 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilité d'une option d'achat à la monnaie de maturité 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilité implicite d'un call est de maturité {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilité implicite d'un call est de maturité 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "href": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "title": "Calibration du modèle Black-Scholes",
    "section": "",
    "text": "# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu’on a pas accès à une formule analytique pour le prix d’une option, on peut utiliser la méthode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l’actif sous-jacent, et on calcule la valeur de l’option à chaque date de maturité. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l’option.\nLe prix de l’actif sous-jacent suit un mouvement brownien géométrique, et on peut simuler ce mouvement en utilisant la formule d’Ito :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturité {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturité 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturité {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturité 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturité 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturité 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturité {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturité {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturité 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturité 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturité 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturité 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux méthodes permettent d’avoir des résultats similaires. De plus, plus le nombre de simulations est grand, plus la précision de l’estimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilité du prix d’une option à différents paramètres. Les principaux greeks sont :\n\nDelta : mesure la sensibilité du prix de l’option par rapport au prix de l’actif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilité du delta par rapport au prix de l’actif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilité du prix de l’option par rapport à la volatilité de l’actif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d’autres, mais ces trois-là sont les plus couramment utilisés.\nAvec le modèle de Black-Scholes, on peut calculer ces greeks de manière analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu’on a pas accès à une formule analytique pour le prix de l’option, on peut utiliser la méthode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l’option pour une petite variation de chaque paramètre, et on fait la différence entre ces deux prix pour obtenir une estimation du greek. On peut également utiliser la méthode des différences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# Mérhode de différencee finie basé sur la méthode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par différences finies.\n\n    Paramètres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'intérêt sans risque (annualisé)\n    tau : float - Temps jusqu'à la maturité (en années)\n    sigma : float - Volatilité de l'actif sous-jacent (annualisée)\n    epsilon : float - Petit incrément pour les différences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturité {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturité 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL’intérêt du modèle de Black-Scholes est qu’il permet de calculer la volatilité implicite d’un actif à partir du prix de l’option. En effet, si on connait le prix de l’option, le prix de l’actif sous-jacent, le prix d’exercice de l’option, la maturité de l’option et le taux d’intérêt sans risque, on peut calculer la volatilité implicite en résolvant l’équation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilité d'une option d'achat à la monnaie de maturité 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilité implicite d'un call est de maturité {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilité implicite d'un call est de maturité 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilité d'une option d'achat à la monnaie de maturité 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilité implicite d'un call est de maturité {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilité implicite d'un call est de maturité 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le modèle à volatilité stochastique de Taylor est un modèle de volatilité stochastique qui est utilisé pour modéliser la volatilité des actifs financiers. Le modèle est défini par l’équation suivante :\n\\[\n\\begin{aligned}\nr_t &= \\exp(x_t/2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim \\text{N}(0,1) \\\\\nx_t &= \\mu + \\phi \\cdot x_{t-1} + \\sigma_t \\cdot \\eta_t\n\\end{aligned}\n\\]\noù \\(r_t\\) est le rendement de l’actif financier à l’instant \\(t\\), \\(x_t\\) est la volatilité de l’actif financier à l’instant \\(t\\), \\(\\mu\\) est la moyenne de la volatilité, \\(\\phi\\) est le coefficient d’autorégression, \\(\\sigma_t\\) est l’écart-type de la volatilité à l’instant \\(t\\), \\(\\eta_t\\) est un bruit blanc gaussien, et \\(\\varepsilon_t\\) est un bruit blanc gaussien.\nPour extraire la volatilité, nous utilisons le filtre de Kalman sur le logarithme des rendements au carré \\(y_t = \\log(r_t^2)\\), afin de linéariser le modèle. Le modèle linéarisé est défini par l’équation suivante :\n\\[\ny_t = x_t + \\varepsilon_t\n\\]\noù \\(y_t\\) est le logarithme des rendements au carré à l’instant \\(t\\), \\(x_t\\) est la volatilité de l’actif financier à l’instant \\(t\\), et \\(\\varepsilon_t\\) est un bruit blanc de loi log-\\(\\chi^2\\).\nDe ce fait, nous pouvons utiliser le filtre de Kalman pour estimer la volatilité de l’actif financier en utilisant les rendements observés. En effet, le filtre de Kalman est un algorithme récursif qui permet d’estimer l’état caché d’un système dynamique à partir d’observations bruitées. Il s’applique à des modèles linéaires dont le bruit est gaussien. Dans notre cas, nous avons linéarisé le modèle pour qu’il soit compatible avec le filtre de Kalman. Cependant, le bruit n’est pas gaussien, mais log-\\(\\chi^2\\). L’objectif de ce notebook est d’observer le comportement du filtre de Kalman où le bruit n’est pas gaussien.\nNous possédons déjà d’un fichier avec les rendements de l’actif financier et la vraie volatilité simulés avec les paramètres suivants \\(\\mu = -0.8\\), \\(\\phi = 0.9\\), \\(\\sigma = 0.09\\). Nous allons donc utiliser ces données pour estimer la volatilité de l’actif financier en utilisant le filtre de Kalman. Néanmoins, le code est également fourni pour simuler les données si vous souhaitez tester le filtre de Kalman sur d’autres paramètres.\n\n# Simulation d'un modèle à vol stochastique de Taylor\n\n# r_t = exp(x_t/2)*eps_t (eps_t iid N(0,1))\n# x_t = mu + phi * x_{t-1} + sigma_t * eta_t  (eta_t iid N(0,1))\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Paramètres\nn = 252\nmu = -0.8\nphi = 0.9\nsigma_squared = 0.09\n\n# Simulation\n# np.random.seed(0)\n\n# x = np.zeros(n)\n# r = np.zeros(n)\n\n# for t in range(0, n):\n#     if t == 0:\n#         x[t] = np.random.normal(loc= mu/(1-phi), scale=np.sqrt(sigma_squared/(1-phi**2))) # Densité de transition stationnaire de x_t\n#     else:\n#         x[t] = mu + phi * x[t-1] + np.sqrt(sigma_squared) * np.random.normal(loc=0, scale=1)\n#     r[t] = np.exp(x[t]/2) * np.random.normal(loc=0, scale=1)\n\ndata  = pd.read_csv('true_sv_taylor.csv')\nr = data['r']\nx = data['x']\n\n\n# Affichage des trajectoires\nimport matplotlib.pyplot as plt\n\nplt.plot(r, color=\"black\")\nplt.title(\"Trajectoire des rendements\")\nplt.show()\n\nplt.plot(x, color='red')\nplt.title(\"Trajectoire de log-volatilité\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Construction du modèle log-SV (modèle de Taylor)\n\n# Pour appliquer le filtre de Kalman, il faut que les bruits soient centrés.\nmu_r_squared = -1.27 # car log(eps**2) suit une log chi-deux\ny = np.log(r**2) - mu_r_squared\n\n\n\nLe filtre de Kalman fonctionne en deux étapes : prédiction et mise à jour. La prédiction consiste à prédire l’état caché du système à l’instant \\(t\\) en utilisant les observations jusqu’à l’instant \\(t-1\\). La mise à jour consiste à mettre à jour l’estimation de l’état caché en utilisant l’observation à l’instant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l’estimation de la volatilité à l’instant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l’estimation de la volatilité à l’instant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la prédiction de la volatilité à l’instant \\(t\\),\n\\(P_m\\) la matrice de covariance de la prédiction de la volatilité à l’instant \\(t\\),\n\\(y[t]\\) l’observation à l’instant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Prédiction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise à jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseillé de connaitre la loi stationnaire de la volatilité. En effet, la volatilité suit une loi normale stationnaire de paramètre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par conséquent, nous pouvons initialiser le filtre de Kalman avec ces paramètres. En ce qui concerne la matrice de covariance de l’état initial, nous pouvons la fixer à une valeur élevée, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations différentes : une initialisation avec les paramètres stationnaires et une initialisation avec des paramètres aléatoires.\n\n# Script de filtre de kalman pour estimer la volatilité à chaque instant t en supposant les paramètres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance à l'apriori sur la valeur de la volatilité\n\n    for t in range(1, n):\n        # Prédiction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise à jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation à 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilité estimée')\n\n\n\n\n\n\n\n\n\n# Initialisation à x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilité estimée')\nplt.plot(x, color='grey', label = 'Volatilité réelle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation à mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilité estimée')\nplt.plot(x, color='grey', label = 'Volatilité réelle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cas où les paramètres du modèle sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l’algorithme EM pour estimer les paramètres du modèle. L’algorithme EM est un algorithme itératif qui permet d’estimer les paramètres d’un modèle en maximisant la vraisemblance des données observées. Dans notre cas, nous allons utiliser l’algorithme EM pour estimer les paramètres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du modèle.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-paramètres-connus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-paramètres-connus",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le filtre de Kalman fonctionne en deux étapes : prédiction et mise à jour. La prédiction consiste à prédire l’état caché du système à l’instant \\(t\\) en utilisant les observations jusqu’à l’instant \\(t-1\\). La mise à jour consiste à mettre à jour l’estimation de l’état caché en utilisant l’observation à l’instant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l’estimation de la volatilité à l’instant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l’estimation de la volatilité à l’instant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la prédiction de la volatilité à l’instant \\(t\\),\n\\(P_m\\) la matrice de covariance de la prédiction de la volatilité à l’instant \\(t\\),\n\\(y[t]\\) l’observation à l’instant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Prédiction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise à jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseillé de connaitre la loi stationnaire de la volatilité. En effet, la volatilité suit une loi normale stationnaire de paramètre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par conséquent, nous pouvons initialiser le filtre de Kalman avec ces paramètres. En ce qui concerne la matrice de covariance de l’état initial, nous pouvons la fixer à une valeur élevée, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations différentes : une initialisation avec les paramètres stationnaires et une initialisation avec des paramètres aléatoires.\n\n# Script de filtre de kalman pour estimer la volatilité à chaque instant t en supposant les paramètres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance à l'apriori sur la valeur de la volatilité\n\n    for t in range(1, n):\n        # Prédiction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise à jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation à 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilité estimée')\n\n\n\n\n\n\n\n\n\n# Initialisation à x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilité estimée')\nplt.plot(x, color='grey', label = 'Volatilité réelle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation à mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilité estimée')\nplt.plot(x, color='grey', label = 'Volatilité réelle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-paramètres-inconnus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-paramètres-inconnus",
    "title": "Calibration du modèle à volatilité stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Dans le cas où les paramètres du modèle sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l’algorithme EM pour estimer les paramètres du modèle. L’algorithme EM est un algorithme itératif qui permet d’estimer les paramètres d’un modèle en maximisant la vraisemblance des données observées. Dans notre cas, nous allons utiliser l’algorithme EM pour estimer les paramètres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du modèle.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html",
    "href": "3A/proc_stochastique/pj_courbe_tx.html",
    "title": "Modèles de courbe de taux",
    "section": "",
    "text": "La courbe interbancaire est une courbe de taux qui représente les taux d’intérêt auxquels les banques se prêtent de l’argent entre elles. Elle est utilisée pour déterminer les taux d’intérêt des prêts et des emprunts à court terme. Elle est construite sur le court terme (maturité&lt;6M) à partir des taux du marchés monétaire (Money Market) basés sur les dépots non garantis entre banques. Sur le moyen terme (6m - 3y) elle est construite à partir des contrats futures, i.e. des forwards sur un marché OTC (Over The Counter) et sur le long terme (&gt;3y) elle est construite à partir des contrats de swap euribor (Euro Interbank Offered Rate) 3M ou 6M.\nCi dessous, nous disposons de ces données de taux de marché cotés sur le marché interbancaire. Nous allons essayer de reconstituer la courbe de taux zero coupon implicite, qui ne cote pas directement sur le marché. Le fichier de données contient trois variables : - Type d’instruments (Money Market, Futures, Swap) - Maturité (en années) - Taux d’intérêt\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_excel('data/Data_tx.xlsx', sheet_name='tx_marche')\ndata.columns = ['type', 'T', 'tx']\ndata\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n5\nFUT\n1.50\n0.977094\n\n\n6\nFUT\n1.75\n0.974981\n\n\n7\nFUT\n2.00\n0.972911\n\n\n8\nFUT\n2.25\n0.970984\n\n\n9\nFUT\n2.50\n0.969711\n\n\n10\nFUT\n2.75\n0.968436\n\n\n11\nSWAP\n3.00\n0.026112\n\n\n12\nSWAP\n4.00\n0.028117\n\n\n13\nSWAP\n5.00\n0.029680\n\n\n14\nSWAP\n6.00\n0.031107\n\n\n15\nSWAP\n7.00\n0.032313\n\n\n16\nSWAP\n8.00\n0.033382\n\n\n17\nSWAP\n9.00\n0.034385\n\n\n18\nSWAP\n10.00\n0.035312\n\n\n19\nSWAP\n11.00\n0.036197\n\n\n20\nSWAP\n12.00\n0.037003\n\n\n21\nSWAP\n13.00\n0.037668\n\n\n22\nSWAP\n14.00\n0.038201\n\n\n23\nSWAP\n15.00\n0.038624\n\n\n24\nSWAP\n20.00\n0.039380\n\n\n25\nSWAP\n25.00\n0.038501\n\n\n26\nSWAP\n30.00\n0.037668\n\n\n\n\n\n\n\nEn l’absence d’oppotunité d’arbitrage, les valorisations des instruments de marché s’expriment en fonction des taux zéro coupon implicites suivantes : - Sur le segment Money Market, on cote en taux monétaires :\n$$\nL_t(T,T+\\delta) = \\frac{1}{\\delta} \\left( \\frac{B(t,T)}{B(t,T+\\delta)} - 1 \\right),\n$$\n\navec t le temps courant, T la maturité et $\\delta$ la période de capitalisation.\nDans notre cas, t=T=0 et $\\delta$ varie en fonction de la maturité.\n\nSur le segment Future, on côte en 1 - tx forward :\n\\[\n  future(T, T+\\delta) = 1 - L_t(T,T+\\delta).\n  \\] Dans notre cas, t=0, T= maturité - 3m et \\(\\delta = 3m\\).\nSur le segment Swap, on côte en taux swap :\n\\[\n  Swap(t, T_0, T_n) = B(t, T_0)−B(t, T_n)−K ×lvl(t),\n  \\]\navec K le taux fixe du swap qui égalise la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de marché à la maturité \\(T_n\\). Dans notre cas, la date de départ est le spot, i.e. \\(T_0=0\\) et \\(T_n\\) est la maturité du swap, et t=0 (vu d’aujourd’hui).\n\nDe ce fait, le données ne sont pas homogènes en taux du fait de la différente cotations des instruments. Nous allons donc les transformer en taux monétaires pour les homogénéiser.\nRemarques préliminaires : - En zone EURO, les swaps standards côtés sur le marché ont une fréquence de paiement semestrielle pour la patte variable et annuelle pour la patte fixe. Ainsi pour le calcul du level du swap, \\(\\delta=1\\) et on ajoute progressivement les taux de marché. - Pour simplifier les calculs, nous supposerons que les dates de départ des taux monétaires et des taux de swap sont spot (i.e. T0 = 0 et non 1 ou 2 jours).\nMethode de bootstrapping & stripping :\nPour extraire les taux zéro coupon implicites, nous allons utiliser la méthode de bootstrapping. Cette méthode consiste à calculer les taux zéro coupon implicites à partir des taux de marché. Pour cela, nous allons utiliser les formules des taux monétaires présentées ci-dessus, qui sont vu comme des fonctions de taux zéro coupon implicites.\nComme les taux de swap ne sont pas nécessairement disponibles pour toutes les maturités annuelles, il faut interpoler les taux intermédiaires. Cela permettra de simplifier la méthode de bootstrapping. Nous allons utiliser une interpolation par spline cubic afin d’avoir des taux swap par an. Une interpolation par spline permet d’avoir des bonnes propriétés en terme de dérivabilité et de continuité de la courbe de taux.\nIl s’agira donc de construire une nouvelle courbe de taux de marché discrète avec des cotations annuelles de taux swap à l’aide d’une méthode d’interpolation par spline, en plus des autres instruments. Par la suite, on supposera que cette nouvelle courbe est la courbe de marché de référence, i.e. la courbe utilisée pour impliciter les taux zéro coupon.\nEnfin, nous allons faire du stripping afin de reconstituer une courbe de taux zero coupon implicite plus lisse à l’aide de différentes méthodes d’interpolations (linéaire, spline, etc).\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\n\ndef interpolate_and_update_df(data, col_x, col_y, kind='cubic', start=3, end=30, step=1):\n    \"\"\"\n    Interpole les taux SWAP et met à jour le DataFrame avec les nouvelles valeurs interpolées.\n\n    Paramètres :\n    - data : DataFrame d'origine contenant une colonne 'type' avec 'SWAP', 'T' et 'tx'.\n    - kind : Type d'interpolation (par défaut 'cubic', peut être 'linear', 'quadratic', etc.).\n    - start : Valeur minimale de T pour l'interpolation (par défaut 3).\n    - end : Valeur maximale de T pour l'interpolation (par défaut 31).\n    - step : Pas d'incrémentation pour la grille interpolée (par défaut 1).\n\n    Retourne :\n    - new_df : DataFrame mis à jour avec les taux SWAP interpolés.\n    \"\"\"\n\n    data = data.copy()\n    x, y = data[col_x].values, data[col_y].values\n\n    f = interp1d(x, y, kind=kind)\n    xnew = np.arange(start, end+step, step)\n    tx_new = f(xnew)\n    df = pd.DataFrame({col_x: xnew, col_y: tx_new})\n\n    return df\n\ndf_interp = interpolate_and_update_df(data, 'T', 'tx') \ndf_interp['type'] = 'SWAP'\ndf_interp.head()\n\n\n\n\n\n\n\n\nT\ntx\ntype\n\n\n\n\n0\n3\n0.026112\nSWAP\n\n\n1\n4\n0.028117\nSWAP\n\n\n2\n5\n0.029680\nSWAP\n\n\n3\n6\n0.031107\nSWAP\n\n\n4\n7\n0.032313\nSWAP\n\n\n\n\n\n\n\n\n# Le rajoiuter dans le df\nnew_df = pd.concat([data[data[\"type\"] != \"SWAP\"], df_interp], ignore_index=True)\nnew_df.head()\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n\n\n\n\n\n\n# Affichage d'une courbe homogène de taux de marché en fonction de la maturité\nnew_df['tx_h'] = new_df.apply(lambda x: 1 - x['tx'] if x['type'] == 'FUT' else x['tx'], axis=1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], marker='o', linestyle='-', color='b')\nplt.xlabel('Maturité')\nplt.ylabel('Taux de marché')\nplt.title('Courbe des taux de marché')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLes taux zéro-coupon continus sont définis par la formule suivante :\n\\[\nr(t,T) = -\\frac{1}{T-t} \\ln B(t,T),\n\\]\noù B(t,T) est le facteur d’actualisation (\\(B(t,T) = exp(-r(t,T) \\times  T)\\) ), i.e. le prix d’une obligation zéro-coupon de maturité T à la date t.\nIls sont la brique de base pour la valorisation des produits dérivés et des obligations. De ce fait, nous allons essayer de reconstituer la courbe des taux zéro-coupon implicite à partir de la courbe des taux de marché à l’aide de la méthode du bootstrapping. Cette méthode consiste à calculer les taux zéro-coupon implicites à partir des taux de marché en utilisant la formule suivante selon le segment Money Market :\n\\[\nL_0(0,\\delta) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0,\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d’actualisation est : \\[\nB(0,\\delta) = \\frac{1}{1 + \\delta L_0(0,\\delta)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Money Market\nmm = new_df[new_df['type'] == 'MM']\n\nmm.loc[:, 'B'] = 1 / (1 + mm['tx'] * mm['T'])\nmm.loc[:, 'R'] = - np.log(mm['B']) / mm['T']\n\ndf_ZC = mm\n\n\n\n\nSur le segment Future, on a :\n\\[\nFuture = 1 - L_0(T,T+\\delta) = 1 - \\frac{1}{\\delta} \\left( \\frac{B(0,T)}{B(0,T+\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d’actualisation est : \\[\nB(0,T+\\delta) = \\frac{B(0,T)}{1 + \\delta (1- Future)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Futures\n\nfut = new_df[new_df['type'] == 'FUT']\n\n# concat à mm\ndf_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\nmm_len = len(mm)\n\nfor i in range(mm_len, len(df_ZC)):\n    df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, 'tx'])* 0.25)\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, 'T']\n\n\n\n\nPour le segment swap payeur, on a :\n\\[\nSwap(t, T_0, T_n) = B(t, T_0)−B(t, T_n)−K ×lvl(t) = 0,\n\\]\navec K le taux fixe du swap qui fait que la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de marché à la maturité \\(T_n\\). De ce fait, le facteur d’actualisation est : \\[\nB(0,T_n) = \\frac{1 - K \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1 + K}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Swaps\nswap = new_df[new_df['type'] == 'SWAP']\nfut_len = len(fut)\n\ndf_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\nfor i in range(mm_len+fut_len, len(df_ZC)):\n    T_n = df_ZC.loc[i, 'T']  # Récupère la valeur de T actuelle\n    mask = (df_ZC['T'] &lt; T_n) & (df_ZC['T'] % 1 == 0)  # Sélectionne uniquement les T entiers &lt; T_n\n    df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, 'tx'] * sum(df_ZC.loc[mask, 'B'].fillna(0)))/(1+df_ZC.loc[i, 'tx'])\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC['T'], df_ZC['R'], label='Courbe de taux zéro coupon', marker='o')\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de marché', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux zéro coupon discrétisée')\n\nText(0.5, 1.0, 'Courbe de taux zéro coupon discrétisée')\n\n\n\n\n\n\n\n\n\nComme on peut le constater, le mode d’interpolation a un impact significatif sur le calcul des taux de marché car il affecte la forme de la courbe des taux et donc la valorisation des instruments financiers.\n\n\n\n\nA partir de la courbe des taux zéro-coupon issue de la méthode de bootstrapping, nous souhaitons tracer la courbe des taux forwards de tenor 3M en fonction de la maturité à l’aide des méthodes d’interpolation linéaire et par spline, avec une discrétisation de 0.1 an.\nPour tracer la courbe taux forward, on utilisera la formule suivante pour calculer les taux forward :\n\\[\nL_0(T,T+\\delta) = \\frac{1}{\\delta} (\\frac{B(0,T)}{B(0,T+\\delta)} - 1)\n\\] avec \\(\\delta = 0.25\\).\nPour le segment swap, il s’agira d’interpoler les taux zéro-coupon implicites pour avoir des tx forwards 3M. # changer la discretisatio à 1an ce qui est différent du ténor.\n\ndf_ZC = pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC], ignore_index=True)\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\n\ndef compute_forward_rates(R, T_range, tau):\n    \"\"\" Calcule les taux forward pour chaque maturité \"\"\"\n    fwd_rates = []\n    T_values = []\n    for i in range(len(T_range)-1):\n        T = T_range[i]\n        T_tau = T + tau\n        if T_tau &gt;= max(T_range):\n            break  # Éviter d'extrapoler au-delà des données disponibles\n        B_T = np.exp(-R[i] * T)\n        R_T_tau = np.interp(T_tau, T_range, R) \n        B_T_tau = np.exp(-R_T_tau * T_tau)\n\n        # Formule du taux forward instantané\n        fwd_rate = (B_T / B_T_tau - 1) / tau\n        fwd_rates.append(fwd_rate)\n        T_values.append(T)\n\n    return pd.DataFrame({\"T\": T_values, \"tx_fwd\": fwd_rates})\n\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation linéaire')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation linéaire')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nLorsqu’on utilise une interpolation linéaire, on obtient une courbe plus discontinue. La courbe a une structure en marches d’escalier. Il y a des sauts brusques lorsque l’on passe d’un intervalle à un autre. En effet, par nature, l’interpolation linéaire qui ne prend pas en compte les points intermédiaires. En interpolant avec une fonction spline, on obtient une courbe plus lisse et continue.Elle est plus cohérente avec l’évolution naturelle des taux d’intérêt. En effet, la fonction spline est une fonction polynomiale qui passe par tous les points de la courbe. Elle est plus flexible et permet de mieux capturer les variations des taux d’intérêt.\nNous sommes intéressés à ce qui pourrait se passer lorsque nous shiftons le taux de swap 5Y de 10 points de base. Cela permet de déterminer la sensibilité de la courbe des taux forward aux variations des taux de swap et donc donner des indications sur comment hedger ce risque.\nNous allons donc calculer le taux forward 3M pour les deux courbes de taux forward et comparer les résultats.\n\nchoc = 10/10000\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx_h']+choc if x['T'] == 5 else x['tx_h'], axis=1)\n\n# plot\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_s\"], label='Taux de marché shiftés', color=\"r\")\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de marché', color=\"b\")\nplt.xlabel('Maturité')\nplt.ylabel('Taux de marché')\nplt.title('Courbe des taux de marché')\nplt.legend()\nplt.grid()\nplt.show()\n\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx']+choc if x['T'] == 5 else x['tx'], axis=1)\n\n\n\n\n\n\n\n\n\n# Extraction des facteurs d'actualisation pour les Money Market\nimport numpy as np\nimport pandas as pd\n\ndef compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx\"):\n    \"\"\"\n    Calcule les facteurs d'actualisation (B) et les taux zéro-coupon (R) \n    à partir des taux du marché pour les instruments MM, FUT et SWAP.\n\n    Paramètres :\n    - new_df : DataFrame contenant les taux du marché avec les colonnes spécifiées.\n    - col_T : Nom de la colonne contenant les maturités (ex: \"T\").\n    - col_tx : Nom de la colonne contenant les taux du marché (ex: \"tx\").\n\n    Retourne :\n    - df_ZC : DataFrame contenant les facteurs d'actualisation et les taux zéro-coupon.\n    \"\"\"\n\n    # --- Extraction des données du marché monétaire (MM) ---\n    mm = new_df[new_df['type'] == 'MM'].copy()\n    mm.loc[:, 'B'] = 1 / (1 + mm[col_tx] * mm[col_T])\n    mm.loc[:, 'R'] = - np.log(mm['B']) / mm[col_T]\n\n    df_ZC = mm.copy()\n\n    # --- Extraction des données Futures (FUT) ---\n    fut = new_df[new_df['type'] == 'FUT'].copy()\n    df_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\n    mm_len = len(mm)\n\n    # --- Calcul des facteurs d'actualisation pour les Futures ---\n    for i in range(mm_len, len(df_ZC)):\n        df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, col_tx]) * 0.25)\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, col_T]\n\n    # --- Extraction des données Swaps (SWAP) ---\n    swap = new_df[new_df['type'] == 'SWAP'].copy()\n    fut_len = len(fut)\n    df_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\n    # --- Calcul des facteurs d'actualisation pour les Swaps ---\n    for i in range(mm_len + fut_len, len(df_ZC)):\n        T_n = df_ZC.loc[i, col_T]  # Maturité actuelle\n        mask = (df_ZC[col_T] &lt; T_n) & (df_ZC[col_T] % 1 == 0)  # Sélection des T entiers &lt; T_n\n\n        sum_B = sum(df_ZC.loc[mask, 'B'].fillna(0))\n        df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, col_tx] * sum_B) / (1 + df_ZC.loc[i, col_tx])\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n    return df_ZC\n\n\ndf_ZC_s = compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx_s\")\ndf_ZC_s=pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC_s], ignore_index=True)\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC_s.loc[1:,'T'], df_ZC_s.loc[1:,'R'], label='Courbe de taux zéro coupon shifté', marker='o')\nplt.plot(df_ZC.loc[1:,\"T\"], df_ZC.loc[1:,\"R\"], label='Courbe de taux zéro coupon non shifté', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux zéro coupon discrétisée')\n\nText(0.5, 1.0, 'Courbe de taux zéro coupon discrétisée')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nEn shiftant le taux de swap 5Y, la courbe de forward baisse brusquement pour T=5Y. Il y a une déformation locale de la courbe des taux forward. Cela signifie que la courbe des taux forward est sensible aux variations des taux de swap. En effet, les taux swap sont des instruments financiers qui permettent de se couvrir contre les variations des taux d’intérêt."
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "title": "Modèles de courbe de taux",
    "section": "",
    "text": "La courbe interbancaire est une courbe de taux qui représente les taux d’intérêt auxquels les banques se prêtent de l’argent entre elles. Elle est utilisée pour déterminer les taux d’intérêt des prêts et des emprunts à court terme. Elle est construite sur le court terme (maturité&lt;6M) à partir des taux du marchés monétaire (Money Market) basés sur les dépots non garantis entre banques. Sur le moyen terme (6m - 3y) elle est construite à partir des contrats futures, i.e. des forwards sur un marché OTC (Over The Counter) et sur le long terme (&gt;3y) elle est construite à partir des contrats de swap euribor (Euro Interbank Offered Rate) 3M ou 6M.\nCi dessous, nous disposons de ces données de taux de marché cotés sur le marché interbancaire. Nous allons essayer de reconstituer la courbe de taux zero coupon implicite, qui ne cote pas directement sur le marché. Le fichier de données contient trois variables : - Type d’instruments (Money Market, Futures, Swap) - Maturité (en années) - Taux d’intérêt\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_excel('data/Data_tx.xlsx', sheet_name='tx_marche')\ndata.columns = ['type', 'T', 'tx']\ndata\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n5\nFUT\n1.50\n0.977094\n\n\n6\nFUT\n1.75\n0.974981\n\n\n7\nFUT\n2.00\n0.972911\n\n\n8\nFUT\n2.25\n0.970984\n\n\n9\nFUT\n2.50\n0.969711\n\n\n10\nFUT\n2.75\n0.968436\n\n\n11\nSWAP\n3.00\n0.026112\n\n\n12\nSWAP\n4.00\n0.028117\n\n\n13\nSWAP\n5.00\n0.029680\n\n\n14\nSWAP\n6.00\n0.031107\n\n\n15\nSWAP\n7.00\n0.032313\n\n\n16\nSWAP\n8.00\n0.033382\n\n\n17\nSWAP\n9.00\n0.034385\n\n\n18\nSWAP\n10.00\n0.035312\n\n\n19\nSWAP\n11.00\n0.036197\n\n\n20\nSWAP\n12.00\n0.037003\n\n\n21\nSWAP\n13.00\n0.037668\n\n\n22\nSWAP\n14.00\n0.038201\n\n\n23\nSWAP\n15.00\n0.038624\n\n\n24\nSWAP\n20.00\n0.039380\n\n\n25\nSWAP\n25.00\n0.038501\n\n\n26\nSWAP\n30.00\n0.037668\n\n\n\n\n\n\n\nEn l’absence d’oppotunité d’arbitrage, les valorisations des instruments de marché s’expriment en fonction des taux zéro coupon implicites suivantes : - Sur le segment Money Market, on cote en taux monétaires :\n$$\nL_t(T,T+\\delta) = \\frac{1}{\\delta} \\left( \\frac{B(t,T)}{B(t,T+\\delta)} - 1 \\right),\n$$\n\navec t le temps courant, T la maturité et $\\delta$ la période de capitalisation.\nDans notre cas, t=T=0 et $\\delta$ varie en fonction de la maturité.\n\nSur le segment Future, on côte en 1 - tx forward :\n\\[\n  future(T, T+\\delta) = 1 - L_t(T,T+\\delta).\n  \\] Dans notre cas, t=0, T= maturité - 3m et \\(\\delta = 3m\\).\nSur le segment Swap, on côte en taux swap :\n\\[\n  Swap(t, T_0, T_n) = B(t, T_0)−B(t, T_n)−K ×lvl(t),\n  \\]\navec K le taux fixe du swap qui égalise la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de marché à la maturité \\(T_n\\). Dans notre cas, la date de départ est le spot, i.e. \\(T_0=0\\) et \\(T_n\\) est la maturité du swap, et t=0 (vu d’aujourd’hui).\n\nDe ce fait, le données ne sont pas homogènes en taux du fait de la différente cotations des instruments. Nous allons donc les transformer en taux monétaires pour les homogénéiser.\nRemarques préliminaires : - En zone EURO, les swaps standards côtés sur le marché ont une fréquence de paiement semestrielle pour la patte variable et annuelle pour la patte fixe. Ainsi pour le calcul du level du swap, \\(\\delta=1\\) et on ajoute progressivement les taux de marché. - Pour simplifier les calculs, nous supposerons que les dates de départ des taux monétaires et des taux de swap sont spot (i.e. T0 = 0 et non 1 ou 2 jours).\nMethode de bootstrapping & stripping :\nPour extraire les taux zéro coupon implicites, nous allons utiliser la méthode de bootstrapping. Cette méthode consiste à calculer les taux zéro coupon implicites à partir des taux de marché. Pour cela, nous allons utiliser les formules des taux monétaires présentées ci-dessus, qui sont vu comme des fonctions de taux zéro coupon implicites.\nComme les taux de swap ne sont pas nécessairement disponibles pour toutes les maturités annuelles, il faut interpoler les taux intermédiaires. Cela permettra de simplifier la méthode de bootstrapping. Nous allons utiliser une interpolation par spline cubic afin d’avoir des taux swap par an. Une interpolation par spline permet d’avoir des bonnes propriétés en terme de dérivabilité et de continuité de la courbe de taux.\nIl s’agira donc de construire une nouvelle courbe de taux de marché discrète avec des cotations annuelles de taux swap à l’aide d’une méthode d’interpolation par spline, en plus des autres instruments. Par la suite, on supposera que cette nouvelle courbe est la courbe de marché de référence, i.e. la courbe utilisée pour impliciter les taux zéro coupon.\nEnfin, nous allons faire du stripping afin de reconstituer une courbe de taux zero coupon implicite plus lisse à l’aide de différentes méthodes d’interpolations (linéaire, spline, etc).\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\n\ndef interpolate_and_update_df(data, col_x, col_y, kind='cubic', start=3, end=30, step=1):\n    \"\"\"\n    Interpole les taux SWAP et met à jour le DataFrame avec les nouvelles valeurs interpolées.\n\n    Paramètres :\n    - data : DataFrame d'origine contenant une colonne 'type' avec 'SWAP', 'T' et 'tx'.\n    - kind : Type d'interpolation (par défaut 'cubic', peut être 'linear', 'quadratic', etc.).\n    - start : Valeur minimale de T pour l'interpolation (par défaut 3).\n    - end : Valeur maximale de T pour l'interpolation (par défaut 31).\n    - step : Pas d'incrémentation pour la grille interpolée (par défaut 1).\n\n    Retourne :\n    - new_df : DataFrame mis à jour avec les taux SWAP interpolés.\n    \"\"\"\n\n    data = data.copy()\n    x, y = data[col_x].values, data[col_y].values\n\n    f = interp1d(x, y, kind=kind)\n    xnew = np.arange(start, end+step, step)\n    tx_new = f(xnew)\n    df = pd.DataFrame({col_x: xnew, col_y: tx_new})\n\n    return df\n\ndf_interp = interpolate_and_update_df(data, 'T', 'tx') \ndf_interp['type'] = 'SWAP'\ndf_interp.head()\n\n\n\n\n\n\n\n\nT\ntx\ntype\n\n\n\n\n0\n3\n0.026112\nSWAP\n\n\n1\n4\n0.028117\nSWAP\n\n\n2\n5\n0.029680\nSWAP\n\n\n3\n6\n0.031107\nSWAP\n\n\n4\n7\n0.032313\nSWAP\n\n\n\n\n\n\n\n\n# Le rajoiuter dans le df\nnew_df = pd.concat([data[data[\"type\"] != \"SWAP\"], df_interp], ignore_index=True)\nnew_df.head()\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n\n\n\n\n\n\n# Affichage d'une courbe homogène de taux de marché en fonction de la maturité\nnew_df['tx_h'] = new_df.apply(lambda x: 1 - x['tx'] if x['type'] == 'FUT' else x['tx'], axis=1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], marker='o', linestyle='-', color='b')\nplt.xlabel('Maturité')\nplt.ylabel('Taux de marché')\nplt.title('Courbe des taux de marché')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLes taux zéro-coupon continus sont définis par la formule suivante :\n\\[\nr(t,T) = -\\frac{1}{T-t} \\ln B(t,T),\n\\]\noù B(t,T) est le facteur d’actualisation (\\(B(t,T) = exp(-r(t,T) \\times  T)\\) ), i.e. le prix d’une obligation zéro-coupon de maturité T à la date t.\nIls sont la brique de base pour la valorisation des produits dérivés et des obligations. De ce fait, nous allons essayer de reconstituer la courbe des taux zéro-coupon implicite à partir de la courbe des taux de marché à l’aide de la méthode du bootstrapping. Cette méthode consiste à calculer les taux zéro-coupon implicites à partir des taux de marché en utilisant la formule suivante selon le segment Money Market :\n\\[\nL_0(0,\\delta) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0,\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d’actualisation est : \\[\nB(0,\\delta) = \\frac{1}{1 + \\delta L_0(0,\\delta)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Money Market\nmm = new_df[new_df['type'] == 'MM']\n\nmm.loc[:, 'B'] = 1 / (1 + mm['tx'] * mm['T'])\nmm.loc[:, 'R'] = - np.log(mm['B']) / mm['T']\n\ndf_ZC = mm\n\n\n\n\nSur le segment Future, on a :\n\\[\nFuture = 1 - L_0(T,T+\\delta) = 1 - \\frac{1}{\\delta} \\left( \\frac{B(0,T)}{B(0,T+\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d’actualisation est : \\[\nB(0,T+\\delta) = \\frac{B(0,T)}{1 + \\delta (1- Future)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Futures\n\nfut = new_df[new_df['type'] == 'FUT']\n\n# concat à mm\ndf_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\nmm_len = len(mm)\n\nfor i in range(mm_len, len(df_ZC)):\n    df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, 'tx'])* 0.25)\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, 'T']\n\n\n\n\nPour le segment swap payeur, on a :\n\\[\nSwap(t, T_0, T_n) = B(t, T_0)−B(t, T_n)−K ×lvl(t) = 0,\n\\]\navec K le taux fixe du swap qui fait que la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de marché à la maturité \\(T_n\\). De ce fait, le facteur d’actualisation est : \\[\nB(0,T_n) = \\frac{1 - K \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1 + K}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Swaps\nswap = new_df[new_df['type'] == 'SWAP']\nfut_len = len(fut)\n\ndf_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\nfor i in range(mm_len+fut_len, len(df_ZC)):\n    T_n = df_ZC.loc[i, 'T']  # Récupère la valeur de T actuelle\n    mask = (df_ZC['T'] &lt; T_n) & (df_ZC['T'] % 1 == 0)  # Sélectionne uniquement les T entiers &lt; T_n\n    df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, 'tx'] * sum(df_ZC.loc[mask, 'B'].fillna(0)))/(1+df_ZC.loc[i, 'tx'])\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC['T'], df_ZC['R'], label='Courbe de taux zéro coupon', marker='o')\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de marché', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux zéro coupon discrétisée')\n\nText(0.5, 1.0, 'Courbe de taux zéro coupon discrétisée')\n\n\n\n\n\n\n\n\n\nComme on peut le constater, le mode d’interpolation a un impact significatif sur le calcul des taux de marché car il affecte la forme de la courbe des taux et donc la valorisation des instruments financiers.\n\n\n\n\nA partir de la courbe des taux zéro-coupon issue de la méthode de bootstrapping, nous souhaitons tracer la courbe des taux forwards de tenor 3M en fonction de la maturité à l’aide des méthodes d’interpolation linéaire et par spline, avec une discrétisation de 0.1 an.\nPour tracer la courbe taux forward, on utilisera la formule suivante pour calculer les taux forward :\n\\[\nL_0(T,T+\\delta) = \\frac{1}{\\delta} (\\frac{B(0,T)}{B(0,T+\\delta)} - 1)\n\\] avec \\(\\delta = 0.25\\).\nPour le segment swap, il s’agira d’interpoler les taux zéro-coupon implicites pour avoir des tx forwards 3M. # changer la discretisatio à 1an ce qui est différent du ténor.\n\ndf_ZC = pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC], ignore_index=True)\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\n\ndef compute_forward_rates(R, T_range, tau):\n    \"\"\" Calcule les taux forward pour chaque maturité \"\"\"\n    fwd_rates = []\n    T_values = []\n    for i in range(len(T_range)-1):\n        T = T_range[i]\n        T_tau = T + tau\n        if T_tau &gt;= max(T_range):\n            break  # Éviter d'extrapoler au-delà des données disponibles\n        B_T = np.exp(-R[i] * T)\n        R_T_tau = np.interp(T_tau, T_range, R) \n        B_T_tau = np.exp(-R_T_tau * T_tau)\n\n        # Formule du taux forward instantané\n        fwd_rate = (B_T / B_T_tau - 1) / tau\n        fwd_rates.append(fwd_rate)\n        T_values.append(T)\n\n    return pd.DataFrame({\"T\": T_values, \"tx_fwd\": fwd_rates})\n\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation linéaire')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation linéaire')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nLorsqu’on utilise une interpolation linéaire, on obtient une courbe plus discontinue. La courbe a une structure en marches d’escalier. Il y a des sauts brusques lorsque l’on passe d’un intervalle à un autre. En effet, par nature, l’interpolation linéaire qui ne prend pas en compte les points intermédiaires. En interpolant avec une fonction spline, on obtient une courbe plus lisse et continue.Elle est plus cohérente avec l’évolution naturelle des taux d’intérêt. En effet, la fonction spline est une fonction polynomiale qui passe par tous les points de la courbe. Elle est plus flexible et permet de mieux capturer les variations des taux d’intérêt.\nNous sommes intéressés à ce qui pourrait se passer lorsque nous shiftons le taux de swap 5Y de 10 points de base. Cela permet de déterminer la sensibilité de la courbe des taux forward aux variations des taux de swap et donc donner des indications sur comment hedger ce risque.\nNous allons donc calculer le taux forward 3M pour les deux courbes de taux forward et comparer les résultats.\n\nchoc = 10/10000\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx_h']+choc if x['T'] == 5 else x['tx_h'], axis=1)\n\n# plot\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_s\"], label='Taux de marché shiftés', color=\"r\")\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de marché', color=\"b\")\nplt.xlabel('Maturité')\nplt.ylabel('Taux de marché')\nplt.title('Courbe des taux de marché')\nplt.legend()\nplt.grid()\nplt.show()\n\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx']+choc if x['T'] == 5 else x['tx'], axis=1)\n\n\n\n\n\n\n\n\n\n# Extraction des facteurs d'actualisation pour les Money Market\nimport numpy as np\nimport pandas as pd\n\ndef compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx\"):\n    \"\"\"\n    Calcule les facteurs d'actualisation (B) et les taux zéro-coupon (R) \n    à partir des taux du marché pour les instruments MM, FUT et SWAP.\n\n    Paramètres :\n    - new_df : DataFrame contenant les taux du marché avec les colonnes spécifiées.\n    - col_T : Nom de la colonne contenant les maturités (ex: \"T\").\n    - col_tx : Nom de la colonne contenant les taux du marché (ex: \"tx\").\n\n    Retourne :\n    - df_ZC : DataFrame contenant les facteurs d'actualisation et les taux zéro-coupon.\n    \"\"\"\n\n    # --- Extraction des données du marché monétaire (MM) ---\n    mm = new_df[new_df['type'] == 'MM'].copy()\n    mm.loc[:, 'B'] = 1 / (1 + mm[col_tx] * mm[col_T])\n    mm.loc[:, 'R'] = - np.log(mm['B']) / mm[col_T]\n\n    df_ZC = mm.copy()\n\n    # --- Extraction des données Futures (FUT) ---\n    fut = new_df[new_df['type'] == 'FUT'].copy()\n    df_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\n    mm_len = len(mm)\n\n    # --- Calcul des facteurs d'actualisation pour les Futures ---\n    for i in range(mm_len, len(df_ZC)):\n        df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, col_tx]) * 0.25)\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, col_T]\n\n    # --- Extraction des données Swaps (SWAP) ---\n    swap = new_df[new_df['type'] == 'SWAP'].copy()\n    fut_len = len(fut)\n    df_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\n    # --- Calcul des facteurs d'actualisation pour les Swaps ---\n    for i in range(mm_len + fut_len, len(df_ZC)):\n        T_n = df_ZC.loc[i, col_T]  # Maturité actuelle\n        mask = (df_ZC[col_T] &lt; T_n) & (df_ZC[col_T] % 1 == 0)  # Sélection des T entiers &lt; T_n\n\n        sum_B = sum(df_ZC.loc[mask, 'B'].fillna(0))\n        df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, col_tx] * sum_B) / (1 + df_ZC.loc[i, col_tx])\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n    return df_ZC\n\n\ndf_ZC_s = compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx_s\")\ndf_ZC_s=pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC_s], ignore_index=True)\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC_s.loc[1:,'T'], df_ZC_s.loc[1:,'R'], label='Courbe de taux zéro coupon shifté', marker='o')\nplt.plot(df_ZC.loc[1:,\"T\"], df_ZC.loc[1:,\"R\"], label='Courbe de taux zéro coupon non shifté', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux zéro coupon discrétisée')\n\nText(0.5, 1.0, 'Courbe de taux zéro coupon discrétisée')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nEn shiftant le taux de swap 5Y, la courbe de forward baisse brusquement pour T=5Y. Il y a une déformation locale de la courbe des taux forward. Cela signifie que la courbe des taux forward est sensible aux variations des taux de swap. En effet, les taux swap sont des instruments financiers qui permettent de se couvrir contre les variations des taux d’intérêt."
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "title": "Modèles de courbe de taux",
    "section": "II. Valorisation de swaptions et de caplets",
    "text": "II. Valorisation de swaptions et de caplets\n\nPour coter les caplets/floorlets et swaptions, le modèle de Black est souvent utilisé. Ce modèle est basé sur l’hypothèse que les taux d’intérêt sont log-normalement distribués. Il permet de calculer le prix d’un caplet/floorlet et d’un swaption en fonction des taux d’intérêt et de la volatilité implicite.\nL’EDS (Equation Différentielle Stochastique) de Black est donnée par :\n\n\\[\ndL(t) = \\sigma L(t) dW(t)\n\\]\navec \\(L(t)\\) le taux, \\(\\sigma\\) la volatilité du taux et \\(W(t)\\) un mouvement brownien. En utilisant le changement de numéraire, ce taux est une martingale sous la mesure du numeraire (probabilité risque neutre). Cela permet de calculer le prix d’un caplet/floorlet ou d’une swaption.\nCaplets et Floorlets\n\nPour un caplet, le prix est donné par la formule suivante :\n\n\\[\nCaplet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ L_i(t) \\phi(d) - K \\phi (d - \\sigma_i \\sqrt{T_{i-1}-t} )\\right]\n\\]\navec \\(d = \\frac{1}{\\sigma \\sqrt{T_{i-1}-t}} \\left( \\ln \\left( \\frac{L_i(t)}{K} \\right) + \\frac{\\sigma^2(T_{i-1}-t)}{2} \\right)\\), \\(L_i(t)\\) le taux forward 3M à la date t, \\(K\\) le strike du caplet, \\(N\\) le nominal, \\(\\delta_i\\) la période de capitalisation, \\(B(t,T_i)\\) le facteur d’actualisation à la maturité \\(T_i\\), \\(\\sigma_i\\) la volatilité du taux forward 3M à la maturité \\(T_i\\) et \\(\\phi\\) la fonction de répartition de la loi normale standard.\nPour un floorlet, le prix est donné par la formule suivante :\n\\[\nFloorlet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ K \\phi (d - \\sigma_i \\sqrt(T_{i-1}-t) ) - L_i(t) \\phi(d) \\right]\n\\]\nSwaptions\nPour un swaption donneur, le prix est donné par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ F_S(t) \\Phi(d) - K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) \\right]\n\\]\navec \\(F_S(t)\\) le taux swap à la date t, \\(K\\) le strike du swaption, \\(N\\) le nominal, \\(\\delta\\) la période de capitalisation, \\(B(t,T_j)\\) le facteur d’actualisation à la maturité \\(T_j\\), \\(\\sigma_S\\) la volatilité du taux swap et \\(\\Phi\\) la fonction de répartition de la loi normale standard.\nd est donné par la formule suivante :\n\\[\nd = \\frac{1}{\\sigma_S \\sqrt{T_0 - t}} \\left( \\ln \\left( \\frac{F_S(t)}{K} \\right) + \\frac{\\sigma_S^2(T_0 - t)}{2} \\right)\n\\]\nPour un swaption receveur, le prix est donné par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) - F_S(t) \\Phi(d) \\right]\n\\]\nIl s’agit, à partir des cotations décrites dans le tableau ci-dessous et de la courbe des taux zéro-coupon construite précédemment, calculer les prix de marché de caplets sur euribor12M, ce qui implique une période de capitalisation annuelle, de maturité T = 5Y, i.e. payé à 6Y, et de strikes K associés au tableau. Nous souhaitons ainsi calculer : - Le prix des caplets Caplet(t, 5Y, 6Y) pour les strikes du tableau ci-dessous. - Le prix des swaptions Swaption(t, 5Y, 6Y) pour les strikes du tableau ci-dessous.\n\nvol_data = pd.read_excel('data/Data_tx.xlsx', sheet_name='vol')\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\n\n\n\n\n0\n-100\n0.311859\n0.311859\n\n\n1\n-50\n0.283274\n0.283274\n\n\n2\n-25\n0.265921\n0.265921\n\n\n3\n0\n0.250000\n0.250000\n\n\n4\n25\n0.243451\n0.243451\n\n\n5\n50\n0.249019\n0.249019\n\n\n6\n100\n0.271828\n0.271828\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet'):\n    \"\"\"\n    Calcule la valeur d'un caplet selon le modèle de Black.\n\n    Paramètres :\n    - N : Notional\n    - delta_i : Période du caplet \n    - B_t_Ti : Facteur d'actualisation B(t, Ti)\n    - L_i_t : Taux forward Li(t)\n    - K : Strike du caplet\n    - sigma_i : Volatilité implicite\n    - Ti_1 : Date de début de la période\n    - Ti : Date de fin de la période\n    - t : Temps actuel\n\n    Retourne :\n    - Valeur du caplet\n    \"\"\"\n    Ti_1 = Ti - delta_i\n    d1 = (np.log(L_i_t / K) + 0.5 * sigma_i**2 * (Ti_1 - t)) / (sigma_i * np.sqrt(Ti_1 - t))\n    d2 = sigma_i * np.sqrt(Ti_1 - t) - d1\n    if option_type == 'caplet':\n        price = N * delta_i * B_t_Ti * (L_i_t * norm.cdf(d1) - K * norm.cdf(-d2))\n    elif option_type == 'floorlet':\n        price = N * delta_i * B_t_Ti * (K * norm.cdf(d2) - L_i_t * norm.cdf(-d1))\n    return price\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # Période du caplet\n    delta_i = 1 \n\n    # Maturité du caplet\n    T=5 \n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == T, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    # Volatilité implicite\n    sigma_i = vol_data.loc[i, \"Vols Caplets\"]  \n\n    # Date de début de la période\n    Ti = T + delta_i\n\n    # Temps actuel\n    t = 0  \n\n    caplet_price = price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet')\n    vol_data.loc[i, 'Caplet Price MKT'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n\n\n\n\n\n\n\n\n# pricer les swaptions"
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#iii.-modèle-de-hull-white",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#iii.-modèle-de-hull-white",
    "title": "Modèles de courbe de taux",
    "section": "III. Modèle de Hull-White",
    "text": "III. Modèle de Hull-White\n\nIII.1 Du modèle HJM vers le modèle Hull&White\nPour une maturité \\(T\\) fixée, Heath, Jarrow et Morton ont supposé que le taux forward instantané évolue selon la dynamique suivante :\n\\[\ndf(t, T) = \\alpha(t, T)\\, dt + \\sigma(t, T)\\, dW_t \\quad (1)\n\\]\nLa dynamique (1) ne se place pas forcément dans un cadre sans opportunité d’arbitrage. Les auteurs ont montré que le processus \\(\\alpha\\) ne pouvait pas être choisi arbitrairement et que, pour qu’il existe une unique mesure martingale équivalente, \\(\\alpha\\) devait être lié à la volatilité du zéro coupon.\nSupposons donc l’existence d’une unique mesure martingale équivalente \\(\\mathbb{Q}\\) (mesure risque-neutre) dont le numéraire est l’actif sans risque.\nOn suppose que le prix du zéro coupon (payant une unité de devise en date \\(T\\)) évolue sous \\(\\mathbb{Q}\\) selon l’EDS :\n\\[\n\\frac{dB(t, T)}{B(t, T)} = r_t\\, dt + \\Gamma(t, T)\\, dW_t^Q \\quad (2)\n\\]\nPar définition, on sait que $ B(t, T) = e^{-_t^T f(t,s), ds}$ et que $ f(t, T) = -_T (B(t, T)). $\nEn appliquant le lemme d’Itô, on obtient : $ df(t, T) = (t, T), _T (t, T), dt - _T (t, T), dW_t^Q. $\nEn posant $ -_T (t, T) = (t, T), $ nous obtenons :\n\\[\ndf(t, T) = \\gamma(t, T) \\int_t^T \\gamma(t, u)\\, du \\, dt + \\gamma(t, T)\\, dW_t^Q \\quad (3)\n\\]\nAprès intégration, on retrouve finalement :\n\\[\nf(t, T) = f(0, T) + \\int_0^t \\gamma(s, T) \\left( \\int_s^T \\gamma(s, u)\\, du \\right) ds + \\int_0^t \\gamma(s, T)\\, dW_s^Q \\quad (4)\n\\]\n\n\nIII.2 Hypothèses du modèle Hull&White\nOn suppose que le modèle HJM est gaussien, linéaire et calibrable. Ces hypothèses permettent d’écrire :\n\\[\n\\gamma(t, T) = \\sigma(t)\\, e^{-\\lambda (T-t)}  \\quad \\text{ et} \\quad \\Gamma(t, T) = \\frac{\\sigma(t)}{\\lambda}\\Bigl(e^{-\\lambda (T-t)} - 1\\Bigr)\n\\]\noù la fonction de volatilité instantanée \\(\\sigma(t)\\) est constante par morceaux.\n\n\nIII.3 Construction de la formule zéro-coupon\nDans le cadre du modèle Hull&White, la dynamique du taux court instantané \\(r_t\\) s’écrit :\n\\[\ndr_t = \\left[\\lambda\\bigl(f(0,t) - r_t\\bigr) + \\partial_t f(0,t) + \\int_0^t \\sigma^2(s)\\, e^{-2\\lambda (t-s)} ds \\right] dt + \\sigma(t)\\, dW_t^Q \\quad (5)\n\\]\nOn introduit alors une nouvelle variable d’état : $ X_t = r_t - f(0,t)$\nLa dynamique de \\(X_t\\) devient :\n\\[\ndX_t = \\left[\\varphi(t) - \\lambda X_t\\right] dt + \\sigma(t)\\, dW_t^Q \\quad (8)\n\\]\navec $ (t) = _0^t ^2(s), e^{-2(t-s)} ds. $\nLa formule du prix du zéro coupon s’exprime alors comme une fonction déterministe de \\(X_t\\) :\n\\[\nB(t, T) = \\frac{B(0,T)}{B(0,t)} \\exp\\!\\Biggl\\{ -\\frac{1}{2\\beta^2(t,T)} \\varphi(t) - \\beta(t,T) X_t \\Biggr\\} \\quad (9)\n\\]\noù $ (t, T) = . $\n\nA quelle catégorie de modèle appartient le modèle Hull&White? Justifier.\n\nLe modèle Hull & White est un modèle à structure à terme aﬃne, i.e. un modèle de taux d’intérêt pour lequel le taux zéro-coupon continu R(t, T ) est une fonction aﬃne du taux court r (t).\nIl ressemble à un processus d’Ornstein-Uhlenbeck ou mean reversing process, qui est un processus gaussien définit de la manière suivante :\n\\[\ndY_t = - \\theta \\left[Y_t - \\mu \\right] dt + \\sigma dW_t,\n\\]\noù \\(\\theta, \\mu, \\sigma\\) sont des paramètres déterministes et \\(W_t\\) est le processus de Wiener.\nDans notre cas, on a \\(\\theta = \\lambda\\), \\(\\mu = \\frac{\\phi(t)}{\\lambda}\\) et \\(\\sigma = \\sigma(t)\\). De ce fait, la moyenne et la variance dépend du temps et le paramètre de vitesse de retour à la moyenne est constant.\n\nDéterminer la loi du processus \\(X_t|X_s\\)?\n\nSous la probabilité risque neute \\(\\mathbb{Q}\\), le processus \\(X_t\\) s’ecrit :\n\\[\ndX_t = \\left(\\phi(t)- \\lambda X_t \\right) dt + \\sigma(t) dW_t^Q,\n\\]\navec \\(\\phi(t) = \\int_0^t \\sigma^2(s) e^{-2\\lambda(t-s)} ds\\).\nPosons \\(K_t = e^{\\lambda t} X_t  = f(X_t, t)\\implies X_t = e^{-\\lambda t} K_t\\), par la formule d’Itô, on a :\n\\[\\begin{aligned}\ndf(X_t, t) &= e^{\\lambda t} dX_t + de^{\\lambda t} X_t \\\\\n&= e^{\\lambda t} \\left( \\phi(t) - \\lambda X_t \\right) dt + e^{\\lambda t} \\sigma(t) dW_t^Q + e^{\\lambda t} X_t dt \\\\\n&= e^{\\lambda t}\\phi(t) dt + e^{\\lambda t}\\sigma(t)dW_t^Q \\\\\n&\\implies f(X_t, t) = K_t = e^{- \\lambda t}K_s  + \\int_s^t e^{-\\lambda(t-u)}\\phi(u) du + \\int_s^t e^{-\\lambda(t-u)}\\sigma(u)dW_u^Q \\\\\n&\\Leftrightarrow X_t = X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\phi(u)  du + \\int_s^t e^{-\\lambda (t-u)} \\sigma(u) dW_u^T\n\\end{aligned}\\]\nDe ce fait, on en déduit que $X_t|X_s ( X_s e^{-(t-s)} + _s^t e^{-(t-u)} (u) du, _s^t e^{-2 (t-u)} (u)^2 d ) $.\n\n\nIII.4 Dynamique des taux forwards\nOn note ensuite \\(L_i(t)\\) le taux LIBOR forward à la date \\(t\\) qui fixe en \\(T_i\\) et paie en \\(T_{i+1}\\). Sous l’hypothèse d’absence d’opportunité d’arbitrage, ce taux s’exprime à partir de la courbe de taux : Î \\[\nL_i(t) = \\frac{1}{\\delta_i}\\left(\\frac{B(t, T_i)}{B(t, T_{i+1})} - 1\\right)  = \\frac{1}{\\delta_i}\\left(Z_t- 1\\right) ,\n\\]\nPour connaitre la dynamique des taux forwards, on applique le lemme d’Itô au processus :\n\\[\nZ_t = \\frac{B(t, T_i)}{B(t, T_{i+1})}.\n\\]\n\nRappel du lemme d’Itô : Considérons deux actifs \\(X\\) et \\(Y\\) et posons \\(Z = \\frac{X}{Y}\\) (la valeur de \\(X\\) exprimée en numéraire \\(Y\\)). Le lemme d’Itô nous donne l’évolution de \\(Z\\) par :\n\\[\n\\frac{dZ}{Z} = \\left(\\frac{dX}{X} - \\frac{dY}{Y}\\right) - \\left\\langle \\frac{dX}{X} - \\frac{dY}{Y},\\, \\frac{dY}{Y}\\right\\rangle.\n\\]\n\nEn appliquant le lemme d’Itô à \\(Z_t\\), on obtient :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= \\frac{dB(t, T_i)}{B(t, T_i)} - \\frac{dB(t, T_{i+1})}{B(t, T_{i+1})} \\\\\n&- \\frac{1}{\\cancel{B(t,T_{i+1})^2}} \\cancel{B(t,T_{i+1})^2} \\, \\Gamma(t,T_i,T_{i+1})^2 \\, dt \\\\\n&-  \\frac{-1}{\\cancel{B(t,T_i)B(t,T_{i+1})}} \\cancel{B(t,T_i)B(t,T_{i+1})}\\,\\Gamma(t,T_i)\\Gamma(t,T_{i+1})\\,dt\\\\\n&= \\Gamma(t,T_i,T_{i+1})\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i)\\right)dt + \\left(\\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\right)dW_t^Q\\\\\n&\\implies \\frac{dZ_t}{Z_t} = \\mu(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\n\\end{aligned}\\]\navec\n\\[\\begin{cases}\n\\sigma(t,T_i,T_{i+1}) = \\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\\\\n\\mu(t,T_i,T_{i+1}) = \\Gamma(t,T_i)\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i) \\right) = - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\n\\end{cases}\\]\nDe ce fait, on a :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\\\\\n&= \\sigma(t,T_i,T_{i+1}) \\underbrace{\\left( - \\Gamma(t,T_i)\\,dt + dW_t^Q \\right)}_{d\\tilde{W_t}}\\\\\n&= \\sigma(t,T_i,T_{i+1}) d\\tilde{W_t}\n\\end{aligned}\\]\noù \\(d\\tilde{W_t}\\) est un mouvement brownien selon le théorème de Girsanov.\nLa diffusion de \\(Z_t\\) est une loi log-normale, sans drift sous la probabilité risque forward. De ce fait, il suit le modèle de Black pour la valorisation des options.\nOn peut écrire ainsi la dynamique du taux forward \\(L_i(t)\\) sous la probabilité risque forward :\n\\[\\begin{aligned}\ndL_i(t) &= \\frac{Z_t}{\\delta_i} \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}  \\\\\n&= (L_i(t) + \\frac{1}{\\delta_i}) \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}\n\\end{aligned}\\]\n\n\nIII.5 Valorisation des instruments de calibration\n\nPayoff d’un caplet vanille :\nLe payoff d’un caplet sur le taux LIBOR \\(L_i(T_i)\\), de maturité \\(T_i\\), avec paiement en \\(T_{i+1}\\) et de strike \\(K\\) est donné par :\n\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr).\n\\]\n\nFormule de valorisation dans le cadre du modèle H&W :\nIl peut être démontré que la formule de valorisation de ce caplet s’exprime de la manière suivante :\n\n\\[\nC\\Bigl( Z_t,\\, \\tilde{K},\\, T_i,\\, \\sigma_i^*,\\, B(t, T_{i+1}) \\Bigr) \\quad (11)\n\\]\navec :\n\n$ Z_t = $,\n$ (_i^*)^2 = t^{T_i} ( (s, T_i) - (s, T{i+1}) )^2 ds = ^2(T_i, T_{i+1}), (T_i) $,\n$ = 1 + _i K $,\n$ C() $ désigne le prix d’un Call selon le cadre Black, en fonction du forward, du strike, de la maturité, de la volatilité et du facteur d’actualisation.\n\nEn effet, on peut réécrire le payoff d’un caplet sous la forme d’un Call sur \\(Z_t\\) qui suit un modèle de Black :\n\\[\\begin{aligned}\n\\text{Payoff} &= \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t}{\\delta_i} - 1 - K; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t - 1 - \\delta_i K}{\\delta_i}; 0 \\Bigr) \\\\\n&= \\max\\Bigl(Z_t - 1 - \\delta_i K; 0 \\Bigr) \\\\\n\\text{Payoff} &= \\max\\Bigl( Z_t - \\tilde{K},\\; 0 \\Bigr) \\\\\n\\end{aligned}\\]\n\n\nIII.6. Calibration du modèle\nLe modèle de Hull White permet d’avoir une formule fermée pour le prix des caplets. De fait, puisqu’on a calculé les prix de marché de caplets sur euribor12M, ce qui implique une période de capitalisation annuelle, de maturité T = 5Y, nous pouvons desormais calibrer le paramètre de volatilité \\(\\sigma_i^*\\) avec la méthode de dichotomie et aussi extraire de manière analytique la volatilité instantanée \\(\\sigma(t)\\) du modèle Hull&White, qu’on supposera constante, i.e. \\(\\sigma(t) = \\sigma\\).\nOn pose également, pour la calibration, \\(\\lambda = 5\\%\\).\nPour extraire la volatilité spot, nous utiliserons uniquement le prix de marche ATM.\n\ncaplet_price_MKT = vol_data.loc[3, 'Caplet Price MKT']\nprint(f\"Le prix de marché caplet sur euribor 12M de maturité T=5Y est de {caplet_price_MKT:.4%}\")\n\nLe prix de marché caplet sur euribor 12M de maturité T=5Y est de 0.7137%\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW'):\n    if model == \"HW\" :\n        Z_t = delta_i * L_i_t + 1\n        K = 1 + delta_i * K\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=Z_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n    else:\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=L_i_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n\n\ndef Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT,option_type='caplet', model=\"HW\",tol=1e-6, sigma_low=1/10000, sigma_high=1):\n    \"\"\"\n    Extrait la volatilité implicite sigma en utilisant la méthode de dichotomie.\n    \"\"\"\n    fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n    fmax = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_high,option_type=option_type, model=model)\n    price = caplet_price_MKT\n    if fmin&gt;price :\n        return sigma_low\n    elif fmax&lt;price :\n        return sigma_high\n    else:\n        while sigma_high-sigma_low&gt;tol:\n            sigma_mid = (sigma_low + sigma_high) / 2\n            fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n            fmid = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_mid,option_type=option_type, model=model)\n            if ((fmin - price) * (fmid - price) &gt; 0) : # jette la moitié de gauche\n                sigma_low = sigma_mid\n            else: # jette la moitié de droite\n                sigma_high = sigma_mid\n        sigma_mid = (sigma_low + sigma_high) / 2\n        return sigma_mid\n    \n\n# Notional\nN = 1  \n\n# Période du caplet\ndelta_i = 1 \n\n# Maturité du caplet\nTi=6\n\n# Facteur d'actualisation B(t, Ti)\nB_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n# Taux forward Li(t)\nL_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n# Strike du caplet\nK = L_i_t + vol_data.loc[3, 'Strike en bps et en rel. / fwd']/10000\n\nlambda_ = 5/100\n\nsigma_i = Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT, option_type='caplet',model=\"HW\")\nprint(\"Volatilité implicite (en %) :\", sigma_i*100) \n\nVolatilité implicite (en %) : 0.9271045541763303\n\n\n\nbeta_Ti_Ti_1 =  ((1 - np.exp(- lambda_ * delta_i))/lambda_)**2\nTi_1 = Ti - delta_i\nphi = (1 - np.exp(-2*lambda_*(Ti_1-t)))/(2*lambda_)\nsigma = np.sqrt((sigma_i**2 * Ti_1)/ (beta_Ti_Ti_1 * phi))\n\nprint(\"Volatilité instantanée (en %) :\", sigma*100) \n\nVolatilité instantanée (en %) : 1.071446257025021\n\n\n\nCette volatilité spot nous permet de valoriiser les caplets pour des strikes différents de l’ATM à l’aide de la formule de valorisation fermée du modèle Hull et White.\n\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # Période du caplet\n    delta_i = 1 \n\n    # Maturité du caplet\n    Ti=6\n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    sigma_i = sigma_i\n\n    lambda_ = 5/100\n\n    caplet_price = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW')\n    vol_data.loc[i, 'Caplet Price HW'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n\n\n\n\n\n\n\nL’une des faiblesses du modèle de Hull et White est le fait qu’il n’arrive pas à capter le smile de volatilité. En effet, la volatilité implicite extraite est un skew. Pour constater ce phénomène, nous inverserons la formule de Black pour les caplets et nous en déduirons la volatilité implicite pour chaque strike. Nous utiliserons toujours la méthode de dichotomie pour trouver la volatilité implicite.\n\nfor i in vol_data.index:\n    N = 1\n    delta_i = 1\n    Ti = 6\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n    L_i_t = (1 / delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0] / df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd'] / 10000\n    lambda_ = 5 / 100\n    caplet_price_MKT = vol_data.loc[i, 'Caplet Price HW']\n\n    sigma_extracted =  Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, caplet_price_MKT, option_type='caplet', model='Black')\n    vol_data.loc[i, 'Sigma_HW'] = sigma_extracted\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\nSigma_HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n0.288669\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n0.267386\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n0.258287\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n0.250009\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n0.242433\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n0.235466\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n0.223060\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Sigma_HW\"], label='Volatilité extraite', marker='o')\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Vols Caplets\"], label='Volatilité de marché', marker='o')\nplt.legend()\nplt.title('Comparaison de la volatilité extraite par le modèle HW et la volatilité de marché')\nplt.xlabel('Strike en bps et en rel. / fwd ')\nplt.ylabel('Volatilité')\n\nText(0, 0.5, 'Volatilité')\n\n\n\n\n\n\n\n\n\n\nATM  = L_i_t\n\n\n\n3.7 Valorisation d’un produit structuré\n\nRemarques préliminaires :\n\nNous garderons dans un premier temps la calibration ATM effectuée avec \\(\\lambda = 5\\%\\).\nPour la partie Monte-Carlo, nous admettrons que l’EDS pour le processus \\(X_t\\) sous la probabilité forward neutre \\(Q^T\\) associée au numéraire \\(B(t,T)\\) s’écrit comme :\n\n\\[\ndX_t = \\left[\\phi(t) + \\sigma(t)\\Gamma(t,T) - \\lambda X_t\\right] dt + \\sigma(t) dW_t^T \\tag{12}\n\\]\nOù \\(W_t^T\\) est un brownien sous \\(Q^T\\).\n\nNous souhaitons valoriser un caplet de strike \\(K\\), de dates de fixing \\(T_i = 5Y\\) et de paiement \\(T_{i+1} = 6Y\\) et de barrière désactivante \\(B\\) (avec \\(B &gt; K\\)).\n\nÉcrire le payoff de l’option et tracer la fonction de payoff en fonction de \\(L_i(T_i)\\). Cette option est-elle plus ou moins chère qu’un simple caplet de strike \\(K\\) ?\n\nLe payoff de l’option est donné par :\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &lt; B}\n\\]\n\n\n\nimage-2.png\n\n\nIl est possible de décomposer le payoff à partir d’options vanilles et digitales :\n\\[\n\\text{Payoff} = C_K - CB - (B-K) \\times D_B\n\\]\n\nUne option digitale est une option qui paie 1 si le sous-jacent est au-dessus d’un certain seuil et 0 sinon. De ce fait, le payoff de l’option est donné par :\n\\[\nD_B = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &gt; K}\n\\]\n\n\n\nimage.png\n\n\nUne option vanille est un contrat financier standardisé qui donne le droit, mais non l’obligation, d’acheter (call) ou de vendre (put) un actif sous-jacent à un prix fixé (strike) à une date donnée (maturité). De ce fait, dans le cas d’un call, le payoff de l’option est donné par :\n\\[\nC_K = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr)\n\\]\n\n\n\nCapture d’écran 2025-03-05 à 00.17.09.png\n\n\n\n\nRappeler les principes du pricing par méthode de Monte-Carlo.\n\nLa méthode de Monte-Carlo est une méthode numérique qui permet de pricer des produits financiers complexes lorsque les formules fermées ne sont pas disponibles. Elle consiste à simuler un grand nombre \\(N\\) de trajectoires du processus stochastique et à calculer la moyenne empiriques des payoffs actualisés pour obtenir le prix de l’option.\n\nRappeler comment on simule une loi gaussienne à partir d’une loi uniforme.\n\nPour simuler une variable aléatoire suivant une loi gaussienne standard \\(\\mathcal{N}(0,1)\\) à partir d’une variable uniforme \\(U\\) sur \\([0,1]\\), on applique l’inverse de la fonction de répartition de la loi gaussienne standard (aussi appelée la fonction quantile de la loi normale) :\n\\[\nX = F^{-1}(U)\n\\]\noù \\(F\\) est la fonction de répartition de la loi normale standard.\n\nOn considère un caplet sur euribor12M à barrière désactivante de strike $ K = ATM - 100 bps$, de barrière \\(B = ATM + 100 bps\\) et de maturité \\(T_i = 5Y\\). Pour valoriser cette option, nous allons utiliser une méthode numérique de type Monte-Carlo. Pour cela, il est necessaire de connaire la loi de X_t sachant X_s. En nous aidant de la question précédente, on peut déduire que la loi de \\(X_t|X_s\\) est une loi normale de paramètres :\n\n\\[\nX_t|X_s \\sim \\mathcal{N}\\left( X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\left( \\phi(u) - \\sigma \\Gamma(u,T) \\right) du, \\quad \\int_s^t e^{-2 \\lambda (t-u)} \\sigma^2 d \\right)\n\\]\nPour valoriser cette option, nous pouvons directement utiliser la loi de \\(X_5|X_0\\) pour simuler les trajectoires du taux court et calculer le payoff de l’option ou diffuser progressivement le taux court en utilisant la loi de \\(X_t|X_s\\) pour chaque pas de temps. Ensuite, il s’agira de calculer le payoff de l’option à chaque date, en faire la moyenne et l’actualiser pour obtenir le prix de l’option.\n\nMéthode 1 : Simulation de la loi de \\(X_5|X_0\\)\n\nimport numpy as np\nfrom scipy.integrate import quad\n\n# Fonction phi(t) - variance cumulée\ndef phi(t, sigma, lambda_):\n    return (sigma**2 / (2 * lambda_)) * (1 - np.exp(-2 * lambda_ * t))\n\n# Fonction beta(t, T)\ndef beta(t, T, lambda_):\n    return (1 - np.exp(-lambda_ * (T - t))) / lambda_\n\n# Fonction gamma(t, T)\ndef gamma(t, T, sigma, lambda_):\n    return (sigma / lambda_) * (np.exp(-lambda_ * (T - t)) - 1)\n\n# Fonction B(t, T)\ndef B_t_T(t, T, B0_T, B0_t, X_t, sigma, lambda_):\n    beta_t_T = beta(t, T, lambda_)**2\n    phi_t = phi(t, sigma, lambda_)\n    exponent = -0.5 * beta_t_T * phi_t - beta_t_T * X_t\n    return (B0_T / B0_t) * np.exp(exponent)\n\n# Fonction d'intégration avec paramètres supplémentaires\ndef integrand_mean(u, t, Xs, s, sigma, lambda_, T):\n    # t = borne sup\n    # s = borne inf\n    # T = maturité\n    Xt = Xs * np.exp(-lambda_ * (t - s))\n    exp_part = np.exp(-lambda_ * (t - u))\n    return Xt + exp_part * (phi(u, sigma, lambda_) + sigma * gamma(u, T, sigma, lambda_))\n\n# Paramètres\nT = Ti_1  = 5\nXs = X0 = 0  \ns = 0\nt = Ti_1 \n\n# Calcul de la moyenne conditionnelle\nmean_5_given_0, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\nprint(f\"Moyenne conditionnelle de X_5 | X_0 : {mean_5_given_0:.6f}\")\n\n# Calcul de la variance conditionnelle (indépendant de gamma ici)\n\ndef compute_variance(sigma, lambda_, t, s):\n    return (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nvar_5_given_0 = compute_variance(sigma, lambda_, t, s)\n# var_5_given_0 = (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nprint(f\"Variance conditionnelle de X_5 | X_0 : {var_5_given_0:.6f}\")\n\nMoyenne conditionnelle de X_5 | X_0 : 0.000000\nVariance conditionnelle de X_5 | X_0 : 0.000452\n\n\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # Définition du strike\n    strike = ATM - bp\n\n    # Barrière\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.002985\n\n\n\n\nMéthode 2 : Methode de diffusion\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[i-1]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # Définition du strike\n    strike = ATM - bp\n\n    # Barrière\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003075\n\n\nNous constatons qu’avec les deux méthodes, nous obtenons des prix d’options similaires (différent de 0.2bps). Cela confirme que les deux méthodes convergent vers le même résultat.\nEn dégénérant le produit en faisant tendre la barrière à \\(+\\infty\\), nous constatons que le prix de l’option call est égal au prix de marché du forward. En dégénérant le produit en faisant tendre la barrière à 0, nous constatons que le prix de l’option call est égal à 0.\nCela est cohérent car lorsque la barrière est très élevée, le produit est équivalent à un forward et lorsque la barrière est nulle, le produit est équivalent à un call classique. La fonction que nous avons implémenté est donc cohérente et bien implémentée.\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # Définition du strike\n    strike = ATM # - bp\n\n    # Barrière\n    B = np.inf#ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.006961\n\n\nNous rendons la barrière ‘bermudéenne’ en étendant la condition de désactivation aux dates 1Y, 2Y, 3Y, 4Y et 5Y. De ce fait, le payoff de cette option s’écrit :\n\\[\n\\text{Payoff} = \\delta_i \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{max_{i=1,\\dots,5}(L_i(T_i) &lt; B)}\n\\]\n\n# Option bermudienne\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n    L_i_t = np.zeros(Ti_1)\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[s]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n        # Calcul du prix B(5,6) selon Hull-White\n        Bi_t = df_ZC.loc[df_ZC['T'] == t, 'B'].values[0] # B(0,6)\n        Bi_s = df_ZC.loc[df_ZC['T'] == s, 'B'].values[0] # B(0,5)\n\n        B_s_t = B_t_T(s, t, Bi_t, Bi_s, X[i], sigma, lambda_)\n        B_s_s = B_t_T(s, s, Bi_s, Bi_s, X[i], sigma, lambda_)\n\n        # Calcul du taux forward L_i_t\n        L_i_t[i-1] = (1 / (t-s)) * ((B_s_s / B_s_t) - 1)\n    \n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n\n    # Définition du strike\n    strike = ATM - bp\n\n    # Barrière\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_t - strike, 0) * (np.max(L_i_t) &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003079\n\n\nL’un des paramètres important du modèle de Hull et White est la mean reversion \\(\\lambda\\), qui caractérise la force de rappel à la moyenne du processus. Ce paramètre a un impact positive sur la valorisation de l’option, comme nous pouvons l’observer dans la figure ci dessous .\n\n# Liste des lambda à tester\nlambdas = np.linspace(0.01, 1, 15)  # Exemple de grille de lambda\nresults = []\n\nTi_1 = 5\nTi = 6\n# Boucle principale sur les lambdas\nfor lambda_ in lambdas:\n    n_touched = 0\n    payoffs = np.zeros(n_simulations)\n\n    for sim in range(n_simulations):\n        phi_ = phi(Ti_1, sigma, lambda_)\n        gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n        # Moyenne\n        mu_X = mean_5_given_0\n\n        # Ecart-type\n        sigma_X = np.sqrt(var_5_given_0)\n\n        # X_5|X_0\n        X = np.random.normal(mu_X, sigma_X)  \n\n        # Calcul du prix B(5,6) selon Hull-White\n        B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n        B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n        B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n        B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n        L_i_t = ((B_5_5 / B_5_6) - 1)\n        bp = 100 / 10000\n        strike = ATM - bp\n        B = ATM + bp\n\n        payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n        payoffs[sim] = payoff\n\n        # Vérification de la barrière\n        if np.any(L_i_t&gt;= B):\n            n_touched += 1\n\n    prob_toucher_barriere = n_touched / n_simulations\n\n    call_price = B0_6 * np.mean(payoffs)\n    results.append((lambda_, call_price,prob_toucher_barriere))\n\n\n# Optionnel : Graphique de la sensibilité\nimport matplotlib.pyplot as plt\n\nlambdas, prices, probabilities = zip(*results)\nplt.plot(lambdas, prices, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Prix de l\\'option')\nplt.title('Sensibilité du prix à la mean reversion (λ)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nDe plus, plus le paramètre de mean reversion \\(\\lambda\\) est élevé, plus la probabilité de toucher la barrière est faible. Inversement, une faible mean reversion laisse plus de liberté au processus pour explorer des valeurs extrêmes, augmentant ainsi la probabilité de franchir la barrière.\n\nplt.plot(lambdas, probabilities, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Probabilité de toucher la barrière')\nplt.title('Probabilité de toucher la barrière en fonction de la mean reversion (λ)')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "3A/risque_def.html",
    "href": "3A/risque_def.html",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "",
    "text": "En finance, le risque peut être défini comme la survenance d’un événement incertain qui peut avoir des conséquences négatives sur le bilan, ou le compte de résultat d’une banque. Par exemple, une fraude aura un impact négative sur la réputation d’une banque ce qui peut entrainer des pertes importants ayant un impact négatif sur le résultat net de celle-ci. En économie, le risque est un événement probabilisable tandis que l’incertitude est non probabilisable.\nNous pouvons caractériser 3 grands types de risques établis par le comité de Bâle qui veille au renforcement et à la stabilité du système financier. (rangés par ordre d’importance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la conséquence. Toutefois, certains risque sont difficiles à distinguer. Ils se trouvent à la frontière entre le risque de marché, de crédit et le risque opérationnel.\nIl est important de noter que le but d’une banque n’est pas de prendre le moins de risque, mais d’atteindre une rentabilité maximale pour un risque donné. La théorie financière nous apprend que seul le risque est rémunéré. La banque procède donc à une arbitrage entre risque et rentabilité. C’est pourquoi la gestion des risques est un élément clé de la stratégie de décision de la banque. La mesure du risque intervient pour calculer les fonds propres nécessaires pour assurer chaque opération financière. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "3A/risque_def.html#les-mesures-de-risque",
    "href": "3A/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al. ont défini les propriétés que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  cohérente. Une mesure de risque est une fonction qui permet de quantifier le risque d’un portefeuille. Elle est cohérente si elle satisfait les propriétés suivantes :\n\nsous-additivité : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomogénéité positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivité signifie que le risque d’un portefeuille est inférieur ou égal à la somme des risques des actifs qui le composent. Ce phénomène est appelé effet de diversification. En effet, la diversification permet de réduire le risque d’un portefeuille en investissant dans des actifs non corrélés. Ainsi, en agrégeant deux porte-feuilles, il n’y a pas de création de risque supplémentaire.\nL’homogénéité positive signifie que le risque d’un portefeuille est proportionnel à la taille du portefeuille. Cette propriété ignore les problèmes de liquidité.\nL’invariance par translation signifie que l’addition au portefeuille initiale un montant sûr rémunéré au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d’immobiliser des fonds propres égaux à la mesure du risque.\nLa monotonie signifie que le risque d’un portefeuille est inférieur ou égal au risque d’un autre portefeuille si la distribution de probabilité de la perte potentielle du premier portefeuille est inférieure ou égale à celle du deuxième portefeuille. Celà traduit l’ordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d’un portefeuille à un instant t donné, le risque est la variation négative de ce portefeuille dans le futur. Le risque se caractérisait donc par une perte relativfe (par rapport à la valeur initiale du portefeuille à un instant t). Pendant très longtemps, les banques utilisaient la volatilité (écart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup évoluée et celle qui est la plus répandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d’un portefeuille à un horizon \\(h\\) donné et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\noù F est la distribution de probabilité de la perte potentielle du portefeuille.\nPar exemple, une VaR à \\(\\alpha=1\\%\\) de 1 million d’euros signifie que la probabilité que la banque perde plus de 1 million d’euros est égale à 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d’euros sur une période donnée (C’est la perte maximale encourue par la banque avec un intervalle de confiance à 99%). Nous allons préférer la deuxième formulation de l’interprétation.\nDeux éléments sont nécessaires pour déterminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance à un indicateur de tolérance pour le risque. Une couverture à 99% est beaucoup plus exigente et donc plus coûteuse qu’une couverture à 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait définir l’horizon h. Par exemple, une couverture à 1 jour est moins coûteuse qu’une couverture à 1 mois. C’est la combinaison de ces deux éléments qui détermine le degré de la couverture qui peut être exprimé en temps de retour 1 \\(t°\\)qui est la durée moyenne entre deux dépassements de la VaR. Il permet de caractériser la rareté d’un évènement (dont la probabilité d’occurence est petite)\n\\[t°= \\frac{h}{1-\\alpha}\\]\nLorsqu’on entend parler de gestion de risque décennal, celà revient à considérer une valeur en risque (VaR) journalière (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non cohérente car elle ne respecte pas la propriété de sous-additivité. De nombreux professionnels recommanderaient alors l’utilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque cohérente. La CVAR est l’espérance de la perte au delà de la VaR. Toutefois, la VaR reste une mesure de risque très utilisée en pratique, qui ne respecte pas la propriété de sous-additivité que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilités sont principalement localisées dans les quantiles extrêmes.\n\n\n\nD’autres mesures de risque\nD’autres mesures peuvent être définis comme celle de la perte exceptionnelle (Unexpected Loss - UL) définie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] où L est la distribution de la perte potentielle.\nIl s’agit là de la différence entre la VaR et la perte moyenne (expected loss - EL). Il y a également le regret espéré défini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donné représentant le montant de la perte tolérable par l’institut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l’institut financier. Lorsque H est endogène, c’est-à-dire dépendant de la distribution de la perte potentielle, et égale à la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l’espérance de la perte au delà de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au delà d’un certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au delà de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c’est la semi variance."
  },
  {
    "objectID": "3A/risque_def.html#footnotes",
    "href": "3A/risque_def.html#footnotes",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npériode de retour doit être interprétée comme la probabilité statistique qu’un évènement se produise↩︎"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html",
    "href": "3A/value-at-risk/var_classiques.html",
    "title": "TP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "",
    "text": "Ce TP est fait dans le but de modéliser la Value at Risk qui est une mesure de risque financier. La Value at Risk (VaR) est une mesure essentielle du risque de marché qui estime la perte potentielle maximale d’un portefeuille sur un horizon h donné, avec un certain niveau de confiance \\(\\alpha\\). La VaR est utilisée par les institutions financières et les gestionnaires de risques pour évaluer l’exposition aux pertes extrêmes et ajuster leurs stratégies d’investissement. Elle est définie comme suit :\n\\[VaR_{\\alpha}(h) = inf \\{ l \\in \\mathbb{R} | P(PnL \\geq -l) \\geq 1 - \\alpha \\}\\]\nLe PnL représente le profit and loss, c’est-à-dire la variation de la valeur du portefeuille. Dans notre cas, nous utilserons les rendements logarithmiques des actifs financiers, qui est stationnaire, pour calculer la VaR. Les rendements logarithmiques sont calculés comme suit :\n\\[r_t = log(\\frac{P_t}{P_{t-1}}) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}.\\]\nCette mesure est préférée aux rendements simples car sa décomposition en termes additifs permet de mieux modéliser les variations de prix des actifs financiers. De plus, en supposant la distribution identique et indépendante des rendements, on peut facilement déterminer sa loi de probabilité et calculer la VaR.\nDans ce projet, nous explorerons plusieurs méthodes pour calculer la VaR, en tenant compte de la nature des rendements financiers et des hypothèses sous-jacentes :\n# Définition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tqdm import tqdm\nfrom scipy.stats import bootstrap\n\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des données du CAC 40\ndata = yf.download(\"^FCHI\")\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la première ligne\ndata = data.dropna()\ndata.head()\n\n\n\n\n\n\n\nPrice\nClose\nHigh\nLow\nOpen\nVolume\nlog_return\n\n\nTicker\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n1990-03-02\n1860.0\n1860.0\n1831.0\n1831.0\n0\n0.015168\n\n\n1990-03-05\n1874.0\n1874.0\n1862.0\n1866.0\n0\n0.007499\n\n\n1990-03-06\n1872.0\n1875.0\n1866.0\n1869.0\n0\n-0.001068\n\n\n1990-03-07\n1880.0\n1881.0\n1874.0\n1874.0\n0\n0.004264\n\n\n1990-03-08\n1917.0\n1923.0\n1891.0\n1891.0\n0\n0.019490"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "href": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "title": "TP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.1. Statistiques descriptives",
    "text": "II.1. Statistiques descriptives\nNous constatons que dans la période d’entrainement est plus volatile (std=1.39%). Cela s’explique par le fait que la période d’entrainement prend en compte deux crises majeures : la crise des subprimes et la crise du Covid-19. Cela peut également se voir à travers les clusters de volatilité observables à la suite de ces crises. Dans la période de test, aucun évènement majeur n’est observé, avec une plus faible volatilité observée (std=0.08%).\nOn s’attend à ce que la VaR entrainée sur la période d’entrainement performe très bien sur la période de test, mais on s’attend également à ce que la VaR entrainée sur la période de test performe moins bien sur la période d’entrainement.\n\nplt.figure(figsize=(10, 4))\nplt.plot(data_train, label='Train', color='grey')\nplt.plot(data_test, label='Test', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Données d'entrainement et de test\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_train.describe()\n\ncount    3523.000000\nmean        0.000153\nstd         0.013953\nmin        -0.130983\n25%        -0.006099\n50%         0.000580\n75%         0.006855\nmax         0.096169\nName: log_return, dtype: float64\n\n\n\ndata_test.describe()\n\ncount    586.000000\nmean       0.000292\nstd        0.008947\nmin       -0.036484\n25%       -0.004763\n50%        0.000642\n75%        0.005612\nmax        0.041504\nName: log_return, dtype: float64"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-paramétrique",
    "href": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-paramétrique",
    "title": "TP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.2. VaR non paramétrique",
    "text": "II.2. VaR non paramétrique\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l’on peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donné. Par exemple, une VaR à 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut également raisonner en terme de gain, i.e. Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements passés selon l’horizon fixé pour estimer la VaR, à l’aide d’un quantile empirique d’ordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la méthode de scaling. 2. Approche bootstrap : On tire aléatoirement des échantillons de rendements passés avec remise, puis on prend le quantile empirique d’ordre \\(\\alpha\\) pour calculer la VaR de chaque échantillon. La VaR finale est la moyenne des VaR obtenues.\n\nII.2.1. Historique\n\n# Objectif : implémenter une fonction calculant la VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1- alpha))\n\n\n# Calcul de la VaR historique sur l'échantillon d'entrainement pour h=1j et alpha=0.99\nalpha = 0.99\nvar_hist_train = historical_var(data_train, alpha=alpha)\nprint(f\"La VaR historique pour h=1j et alpha=0.99 est : {var_hist_train:.4%}\")\n\nLa VaR historique pour h=1j et alpha=0.99 est : 4.0850%\n\n\nOn constate que la VaR historique pour une horizon de 1 jour et un niveau de confiance de 99% est de -4,09%. De ce fait, la perte maximale que l’on peut subir avec un niveau de confiance de 99% sur un jour est de 4,09%. Autrement dit, il y a 1 chance sur 100 que la perte soit supérieure à 4,09%. Cette perte peut se produire 2 à 3 ans fois en une année (252 jours de trading).\n\n\nII.2.2. Bootstrap\nPour l’implémentation de la VaR bootstrap, nous faisons le choix de faire un tirage de taille n=la taille de la série des rendements, avec remise. Ce choix est fait pour des raisons de simplicité. En ce qui concerne le choix du nombre d’échantillons, nous allons observer l’évolution de de l’estimation de la VaR en fonction du nombre d’échantillons. Nous limiterons à des échantillons compris entre 1000 et 10000, pour des raisons de temps computationnels, en ayant conscience que plus le nombre d’échantillons est grand, plus l’estimation de la VaR sera précise.\n\n# Objectif : implémenter une fonction calculant la VaR bootstrap et un IC\n\ndef bootstrap_var(data, alpha=0.99, M=1000, seuil=0.05):\n    \"\"\"\n    Calcul de la VaR bootstrap\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance de la VaR\n    n : le nombre de simulations\n    seuil : le seuil de l'intervalle de confiance\n    \"\"\"\n    # set seed\n    np.random.seed(42)\n\n    # Initialisation du vecteur des VaR\n    var = np.zeros(M)\n\n    # Calcul de la VaR bootstrap\n    for i in range(M):\n        sample = np.random.choice(data, size=len(data), replace=True)\n        var[i] = -np.percentile(sample, 100*(1- alpha))\n\n    # Calcul de l'intervalle de confiance\n    lower = np.percentile(var, 100*(1-seuil)/2)\n    upper = np.percentile(var, 100*(seuil + (1-seuil)/2))\n\n    return np.mean(var), lower, upper\n\n\n# Observer la variation de la VaR en fonction de M\nM_values = np.arange(1000, 10000, 10)\nvar_bs_values = []\n\nfor M in tqdm(M_values):\n    var_bs_train, _, _ = bootstrap_var(data_train, alpha=alpha, M=M)\n    var_bs_values.append(var_bs_train)\n\n  0%|          | 0/900 [00:00&lt;?, ?it/s]  0%|          | 2/900 [00:00&lt;01:04, 14.01it/s]  0%|          | 4/900 [00:00&lt;01:04, 13.90it/s]  1%|          | 6/900 [00:00&lt;01:04, 13.86it/s]  1%|          | 8/900 [00:00&lt;01:05, 13.68it/s]  1%|          | 10/900 [00:00&lt;01:05, 13.50it/s]  1%|▏         | 12/900 [00:00&lt;01:06, 13.32it/s]  2%|▏         | 14/900 [00:01&lt;01:07, 13.12it/s]  2%|▏         | 16/900 [00:01&lt;01:08, 12.93it/s]  2%|▏         | 18/900 [00:01&lt;01:09, 12.72it/s]  2%|▏         | 20/900 [00:01&lt;01:10, 12.55it/s]  2%|▏         | 22/900 [00:01&lt;01:10, 12.37it/s]  3%|▎         | 24/900 [00:01&lt;01:11, 12.19it/s]  3%|▎         | 26/900 [00:02&lt;01:12, 12.01it/s]  3%|▎         | 28/900 [00:02&lt;01:13, 11.83it/s]  3%|▎         | 30/900 [00:02&lt;01:14, 11.65it/s]  4%|▎         | 32/900 [00:02&lt;01:15, 11.44it/s]  4%|▍         | 34/900 [00:02&lt;01:16, 11.28it/s]  4%|▍         | 36/900 [00:02&lt;01:17, 11.12it/s]  4%|▍         | 38/900 [00:03&lt;01:18, 10.95it/s]  4%|▍         | 40/900 [00:03&lt;01:20, 10.74it/s]  5%|▍         | 42/900 [00:03&lt;01:20, 10.61it/s]  5%|▍         | 44/900 [00:03&lt;01:21, 10.47it/s]  5%|▌         | 46/900 [00:03&lt;01:23, 10.29it/s]  5%|▌         | 48/900 [00:04&lt;01:23, 10.16it/s]  6%|▌         | 50/900 [00:04&lt;01:24, 10.03it/s]  6%|▌         | 52/900 [00:04&lt;01:25,  9.91it/s]  6%|▌         | 53/900 [00:04&lt;01:26,  9.84it/s]  6%|▌         | 54/900 [00:04&lt;01:26,  9.76it/s]  6%|▌         | 55/900 [00:04&lt;01:27,  9.67it/s]  6%|▌         | 56/900 [00:04&lt;01:28,  9.57it/s]  6%|▋         | 57/900 [00:05&lt;01:28,  9.48it/s]  6%|▋         | 58/900 [00:05&lt;01:29,  9.40it/s]  7%|▋         | 59/900 [00:05&lt;01:30,  9.29it/s]  7%|▋         | 60/900 [00:05&lt;01:31,  9.22it/s]  7%|▋         | 61/900 [00:05&lt;01:31,  9.17it/s]  7%|▋         | 62/900 [00:05&lt;01:32,  9.11it/s]  7%|▋         | 63/900 [00:05&lt;01:32,  9.05it/s]  7%|▋         | 64/900 [00:05&lt;01:32,  8.99it/s]  7%|▋         | 65/900 [00:05&lt;01:33,  8.93it/s]  7%|▋         | 66/900 [00:06&lt;01:34,  8.79it/s]  7%|▋         | 67/900 [00:06&lt;01:35,  8.74it/s]  8%|▊         | 68/900 [00:06&lt;01:35,  8.72it/s]  8%|▊         | 69/900 [00:06&lt;01:35,  8.69it/s]  8%|▊         | 70/900 [00:06&lt;01:35,  8.65it/s]  8%|▊         | 71/900 [00:06&lt;01:36,  8.59it/s]  8%|▊         | 72/900 [00:06&lt;01:36,  8.55it/s]  8%|▊         | 73/900 [00:06&lt;01:37,  8.50it/s]  8%|▊         | 74/900 [00:07&lt;01:37,  8.47it/s]  8%|▊         | 75/900 [00:07&lt;01:37,  8.42it/s]  8%|▊         | 76/900 [00:07&lt;01:38,  8.35it/s]  9%|▊         | 77/900 [00:07&lt;01:38,  8.31it/s]  9%|▊         | 78/900 [00:07&lt;01:39,  8.27it/s]  9%|▉         | 79/900 [00:07&lt;01:39,  8.23it/s]  9%|▉         | 80/900 [00:07&lt;01:40,  8.18it/s]  9%|▉         | 81/900 [00:07&lt;01:40,  8.14it/s]  9%|▉         | 82/900 [00:07&lt;01:41,  8.10it/s]  9%|▉         | 83/900 [00:08&lt;01:41,  8.06it/s]  9%|▉         | 84/900 [00:08&lt;01:41,  8.02it/s]  9%|▉         | 85/900 [00:08&lt;01:42,  7.95it/s] 10%|▉         | 86/900 [00:08&lt;01:42,  7.91it/s] 10%|▉         | 87/900 [00:08&lt;01:43,  7.87it/s] 10%|▉         | 88/900 [00:08&lt;01:43,  7.83it/s] 10%|▉         | 89/900 [00:08&lt;01:44,  7.79it/s] 10%|█         | 90/900 [00:09&lt;01:44,  7.75it/s] 10%|█         | 91/900 [00:09&lt;01:44,  7.71it/s] 10%|█         | 92/900 [00:09&lt;01:45,  7.67it/s] 10%|█         | 93/900 [00:09&lt;01:45,  7.62it/s] 10%|█         | 94/900 [00:09&lt;01:46,  7.57it/s] 11%|█         | 95/900 [00:09&lt;01:46,  7.52it/s] 11%|█         | 96/900 [00:09&lt;01:47,  7.49it/s] 11%|█         | 97/900 [00:09&lt;01:47,  7.44it/s] 11%|█         | 98/900 [00:10&lt;01:48,  7.39it/s] 11%|█         | 99/900 [00:10&lt;01:49,  7.34it/s] 11%|█         | 100/900 [00:10&lt;01:49,  7.32it/s] 11%|█         | 101/900 [00:10&lt;01:49,  7.30it/s] 11%|█▏        | 102/900 [00:10&lt;01:51,  7.17it/s] 11%|█▏        | 103/900 [00:10&lt;01:51,  7.15it/s] 12%|█▏        | 104/900 [00:10&lt;01:51,  7.14it/s] 12%|█▏        | 105/900 [00:11&lt;01:51,  7.10it/s] 12%|█▏        | 106/900 [00:11&lt;01:52,  7.08it/s] 12%|█▏        | 107/900 [00:11&lt;01:52,  7.03it/s] 12%|█▏        | 108/900 [00:11&lt;01:53,  7.01it/s] 12%|█▏        | 109/900 [00:11&lt;01:53,  6.99it/s] 12%|█▏        | 110/900 [00:11&lt;01:53,  6.96it/s] 12%|█▏        | 111/900 [00:11&lt;01:53,  6.94it/s] 12%|█▏        | 112/900 [00:12&lt;01:54,  6.89it/s] 13%|█▎        | 113/900 [00:12&lt;01:54,  6.86it/s] 13%|█▎        | 114/900 [00:12&lt;01:54,  6.84it/s] 13%|█▎        | 115/900 [00:12&lt;01:55,  6.81it/s] 13%|█▎        | 116/900 [00:12&lt;01:55,  6.78it/s] 13%|█▎        | 117/900 [00:12&lt;01:55,  6.75it/s] 13%|█▎        | 118/900 [00:12&lt;01:56,  6.73it/s] 13%|█▎        | 119/900 [00:13&lt;01:56,  6.70it/s] 13%|█▎        | 120/900 [00:13&lt;01:57,  6.66it/s] 13%|█▎        | 121/900 [00:13&lt;01:57,  6.64it/s] 14%|█▎        | 122/900 [00:13&lt;01:57,  6.61it/s] 14%|█▎        | 123/900 [00:13&lt;01:58,  6.58it/s] 14%|█▍        | 124/900 [00:13&lt;01:58,  6.53it/s] 14%|█▍        | 125/900 [00:14&lt;01:59,  6.51it/s] 14%|█▍        | 126/900 [00:14&lt;01:59,  6.48it/s] 14%|█▍        | 127/900 [00:14&lt;01:59,  6.46it/s] 14%|█▍        | 128/900 [00:14&lt;02:00,  6.43it/s] 14%|█▍        | 129/900 [00:14&lt;02:00,  6.38it/s] 14%|█▍        | 130/900 [00:14&lt;02:01,  6.36it/s] 15%|█▍        | 131/900 [00:15&lt;02:01,  6.34it/s] 15%|█▍        | 132/900 [00:15&lt;02:01,  6.31it/s] 15%|█▍        | 133/900 [00:15&lt;02:02,  6.29it/s] 15%|█▍        | 134/900 [00:15&lt;02:02,  6.26it/s] 15%|█▌        | 135/900 [00:15&lt;02:02,  6.23it/s] 15%|█▌        | 136/900 [00:15&lt;02:03,  6.20it/s] 15%|█▌        | 137/900 [00:15&lt;02:03,  6.17it/s] 15%|█▌        | 138/900 [00:16&lt;02:04,  6.13it/s] 15%|█▌        | 139/900 [00:16&lt;02:04,  6.11it/s] 16%|█▌        | 140/900 [00:16&lt;02:04,  6.09it/s] 16%|█▌        | 141/900 [00:16&lt;02:04,  6.08it/s] 16%|█▌        | 142/900 [00:16&lt;02:05,  6.05it/s] 16%|█▌        | 143/900 [00:16&lt;02:05,  6.03it/s] 16%|█▌        | 144/900 [00:17&lt;02:05,  6.00it/s] 16%|█▌        | 145/900 [00:17&lt;02:06,  5.98it/s] 16%|█▌        | 146/900 [00:17&lt;02:06,  5.95it/s] 16%|█▋        | 147/900 [00:17&lt;02:06,  5.93it/s] 16%|█▋        | 148/900 [00:17&lt;02:07,  5.91it/s] 17%|█▋        | 149/900 [00:17&lt;02:07,  5.88it/s] 17%|█▋        | 150/900 [00:18&lt;02:07,  5.86it/s] 17%|█▋        | 151/900 [00:18&lt;02:08,  5.84it/s] 17%|█▋        | 152/900 [00:18&lt;02:08,  5.82it/s] 17%|█▋        | 153/900 [00:18&lt;02:08,  5.80it/s] 17%|█▋        | 154/900 [00:18&lt;02:09,  5.78it/s] 17%|█▋        | 155/900 [00:19&lt;02:09,  5.75it/s] 17%|█▋        | 156/900 [00:19&lt;02:09,  5.73it/s] 17%|█▋        | 157/900 [00:19&lt;02:10,  5.71it/s] 18%|█▊        | 158/900 [00:19&lt;02:10,  5.68it/s] 18%|█▊        | 159/900 [00:19&lt;02:10,  5.66it/s] 18%|█▊        | 160/900 [00:19&lt;02:11,  5.62it/s] 18%|█▊        | 161/900 [00:20&lt;02:12,  5.59it/s] 18%|█▊        | 162/900 [00:20&lt;02:12,  5.57it/s] 18%|█▊        | 163/900 [00:20&lt;02:13,  5.54it/s] 18%|█▊        | 164/900 [00:20&lt;02:13,  5.52it/s] 18%|█▊        | 165/900 [00:20&lt;02:13,  5.50it/s] 18%|█▊        | 166/900 [00:21&lt;02:13,  5.49it/s] 19%|█▊        | 167/900 [00:21&lt;02:15,  5.42it/s] 19%|█▊        | 168/900 [00:21&lt;02:15,  5.41it/s] 19%|█▉        | 169/900 [00:21&lt;02:15,  5.40it/s] 19%|█▉        | 170/900 [00:21&lt;02:15,  5.38it/s] 19%|█▉        | 171/900 [00:21&lt;02:15,  5.37it/s] 19%|█▉        | 172/900 [00:22&lt;02:16,  5.34it/s] 19%|█▉        | 173/900 [00:22&lt;02:16,  5.33it/s] 19%|█▉        | 174/900 [00:22&lt;02:16,  5.31it/s] 19%|█▉        | 175/900 [00:22&lt;02:16,  5.29it/s] 20%|█▉        | 176/900 [00:22&lt;02:16,  5.28it/s] 20%|█▉        | 177/900 [00:23&lt;02:17,  5.27it/s] 20%|█▉        | 178/900 [00:23&lt;02:17,  5.26it/s] 20%|█▉        | 179/900 [00:23&lt;02:17,  5.24it/s] 20%|██        | 180/900 [00:23&lt;02:17,  5.22it/s] 20%|██        | 181/900 [00:23&lt;02:18,  5.20it/s] 20%|██        | 182/900 [00:24&lt;02:18,  5.19it/s] 20%|██        | 183/900 [00:24&lt;02:18,  5.17it/s] 20%|██        | 184/900 [00:24&lt;02:18,  5.15it/s] 21%|██        | 185/900 [00:24&lt;02:19,  5.14it/s] 21%|██        | 186/900 [00:24&lt;02:19,  5.12it/s] 21%|██        | 187/900 [00:25&lt;02:19,  5.09it/s] 21%|██        | 188/900 [00:25&lt;02:20,  5.06it/s] 21%|██        | 189/900 [00:25&lt;02:20,  5.05it/s] 21%|██        | 190/900 [00:25&lt;02:20,  5.04it/s] 21%|██        | 191/900 [00:25&lt;02:21,  5.02it/s] 21%|██▏       | 192/900 [00:26&lt;02:21,  5.00it/s] 21%|██▏       | 193/900 [00:26&lt;02:21,  4.99it/s] 22%|██▏       | 194/900 [00:26&lt;02:22,  4.97it/s] 22%|██▏       | 195/900 [00:26&lt;02:23,  4.92it/s] 22%|██▏       | 196/900 [00:26&lt;02:24,  4.87it/s] 22%|██▏       | 197/900 [00:27&lt;02:24,  4.87it/s] 22%|██▏       | 198/900 [00:27&lt;02:26,  4.78it/s] 22%|██▏       | 199/900 [00:27&lt;02:26,  4.78it/s] 22%|██▏       | 200/900 [00:27&lt;02:26,  4.78it/s] 22%|██▏       | 201/900 [00:27&lt;02:25,  4.79it/s] 22%|██▏       | 202/900 [00:28&lt;02:25,  4.79it/s] 23%|██▎       | 203/900 [00:28&lt;02:26,  4.77it/s] 23%|██▎       | 204/900 [00:28&lt;02:25,  4.77it/s] 23%|██▎       | 205/900 [00:28&lt;02:26,  4.75it/s] 23%|██▎       | 206/900 [00:28&lt;02:26,  4.73it/s] 23%|██▎       | 207/900 [00:29&lt;02:26,  4.73it/s] 23%|██▎       | 208/900 [00:29&lt;02:26,  4.72it/s] 23%|██▎       | 209/900 [00:29&lt;02:26,  4.72it/s] 23%|██▎       | 210/900 [00:29&lt;02:26,  4.71it/s] 23%|██▎       | 211/900 [00:30&lt;02:26,  4.69it/s] 24%|██▎       | 212/900 [00:30&lt;02:27,  4.67it/s] 24%|██▎       | 213/900 [00:30&lt;02:27,  4.66it/s] 24%|██▍       | 214/900 [00:30&lt;02:27,  4.65it/s] 24%|██▍       | 215/900 [00:30&lt;02:27,  4.63it/s] 24%|██▍       | 216/900 [00:31&lt;02:28,  4.62it/s] 24%|██▍       | 217/900 [00:31&lt;02:28,  4.60it/s] 24%|██▍       | 218/900 [00:31&lt;02:28,  4.59it/s] 24%|██▍       | 219/900 [00:31&lt;02:28,  4.57it/s] 24%|██▍       | 220/900 [00:31&lt;02:29,  4.56it/s] 25%|██▍       | 221/900 [00:32&lt;02:29,  4.55it/s] 25%|██▍       | 222/900 [00:32&lt;02:29,  4.52it/s] 25%|██▍       | 223/900 [00:32&lt;02:29,  4.52it/s] 25%|██▍       | 224/900 [00:32&lt;02:30,  4.49it/s] 25%|██▌       | 225/900 [00:33&lt;02:30,  4.48it/s] 25%|██▌       | 226/900 [00:33&lt;02:30,  4.47it/s] 25%|██▌       | 227/900 [00:33&lt;02:30,  4.47it/s] 25%|██▌       | 228/900 [00:33&lt;02:30,  4.45it/s] 25%|██▌       | 229/900 [00:34&lt;02:31,  4.44it/s] 26%|██▌       | 230/900 [00:34&lt;02:31,  4.43it/s] 26%|██▌       | 231/900 [00:34&lt;02:31,  4.42it/s] 26%|██▌       | 232/900 [00:34&lt;02:32,  4.39it/s] 26%|██▌       | 233/900 [00:34&lt;02:32,  4.38it/s] 26%|██▌       | 234/900 [00:35&lt;02:32,  4.37it/s] 26%|██▌       | 235/900 [00:35&lt;02:32,  4.36it/s] 26%|██▌       | 236/900 [00:35&lt;02:33,  4.33it/s] 26%|██▋       | 237/900 [00:35&lt;02:33,  4.33it/s] 26%|██▋       | 238/900 [00:36&lt;02:33,  4.32it/s] 27%|██▋       | 239/900 [00:36&lt;02:33,  4.30it/s] 27%|██▋       | 240/900 [00:36&lt;02:33,  4.29it/s] 27%|██▋       | 241/900 [00:36&lt;02:33,  4.28it/s] 27%|██▋       | 242/900 [00:37&lt;02:33,  4.27it/s] 27%|██▋       | 243/900 [00:37&lt;02:34,  4.26it/s] 27%|██▋       | 244/900 [00:37&lt;02:34,  4.25it/s] 27%|██▋       | 245/900 [00:37&lt;02:34,  4.23it/s] 27%|██▋       | 246/900 [00:37&lt;02:35,  4.22it/s] 27%|██▋       | 247/900 [00:38&lt;02:35,  4.21it/s] 28%|██▊       | 248/900 [00:38&lt;02:35,  4.20it/s] 28%|██▊       | 249/900 [00:38&lt;02:35,  4.19it/s] 28%|██▊       | 250/900 [00:38&lt;02:35,  4.18it/s] 28%|██▊       | 251/900 [00:39&lt;02:36,  4.15it/s] 28%|██▊       | 252/900 [00:39&lt;02:36,  4.15it/s] 28%|██▊       | 253/900 [00:39&lt;02:36,  4.14it/s] 28%|██▊       | 254/900 [00:39&lt;02:36,  4.13it/s] 28%|██▊       | 255/900 [00:40&lt;02:37,  4.11it/s] 28%|██▊       | 256/900 [00:40&lt;02:37,  4.09it/s] 29%|██▊       | 257/900 [00:40&lt;02:38,  4.07it/s] 29%|██▊       | 258/900 [00:40&lt;02:38,  4.05it/s] 29%|██▉       | 259/900 [00:41&lt;02:38,  4.05it/s] 29%|██▉       | 260/900 [00:41&lt;02:38,  4.05it/s] 29%|██▉       | 261/900 [00:41&lt;02:38,  4.04it/s] 29%|██▉       | 262/900 [00:41&lt;02:38,  4.04it/s] 29%|██▉       | 263/900 [00:42&lt;02:38,  4.03it/s] 29%|██▉       | 264/900 [00:42&lt;02:39,  4.00it/s] 29%|██▉       | 265/900 [00:42&lt;02:38,  4.00it/s] 30%|██▉       | 266/900 [00:42&lt;02:39,  3.97it/s] 30%|██▉       | 267/900 [00:43&lt;02:39,  3.97it/s] 30%|██▉       | 268/900 [00:43&lt;02:39,  3.96it/s] 30%|██▉       | 269/900 [00:43&lt;02:39,  3.95it/s] 30%|███       | 270/900 [00:43&lt;02:39,  3.95it/s] 30%|███       | 271/900 [00:44&lt;02:39,  3.94it/s] 30%|███       | 272/900 [00:44&lt;02:39,  3.93it/s] 30%|███       | 273/900 [00:44&lt;02:40,  3.91it/s] 30%|███       | 274/900 [00:44&lt;02:41,  3.89it/s] 31%|███       | 275/900 [00:45&lt;02:40,  3.89it/s] 31%|███       | 276/900 [00:45&lt;02:40,  3.88it/s] 31%|███       | 277/900 [00:45&lt;02:41,  3.87it/s] 31%|███       | 278/900 [00:45&lt;02:40,  3.86it/s] 31%|███       | 279/900 [00:46&lt;02:41,  3.85it/s] 31%|███       | 280/900 [00:46&lt;02:41,  3.83it/s] 31%|███       | 281/900 [00:46&lt;02:41,  3.82it/s] 31%|███▏      | 282/900 [00:47&lt;02:41,  3.82it/s] 31%|███▏      | 283/900 [00:47&lt;02:41,  3.81it/s] 32%|███▏      | 284/900 [00:47&lt;02:42,  3.79it/s] 32%|███▏      | 285/900 [00:47&lt;02:42,  3.78it/s] 32%|███▏      | 286/900 [00:48&lt;02:42,  3.78it/s] 32%|███▏      | 287/900 [00:48&lt;02:42,  3.76it/s] 32%|███▏      | 288/900 [00:48&lt;02:42,  3.76it/s] 32%|███▏      | 289/900 [00:48&lt;02:43,  3.74it/s] 32%|███▏      | 290/900 [00:49&lt;02:43,  3.72it/s] 32%|███▏      | 291/900 [00:49&lt;02:45,  3.68it/s] 32%|███▏      | 292/900 [00:49&lt;02:45,  3.68it/s] 33%|███▎      | 293/900 [00:49&lt;02:45,  3.67it/s] 33%|███▎      | 294/900 [00:50&lt;02:45,  3.67it/s] 33%|███▎      | 295/900 [00:50&lt;02:45,  3.66it/s] 33%|███▎      | 296/900 [00:50&lt;02:45,  3.65it/s] 33%|███▎      | 297/900 [00:51&lt;02:44,  3.66it/s] 33%|███▎      | 298/900 [00:51&lt;02:44,  3.66it/s] 33%|███▎      | 299/900 [00:51&lt;02:44,  3.65it/s] 33%|███▎      | 300/900 [00:51&lt;02:44,  3.65it/s] 33%|███▎      | 301/900 [00:52&lt;02:45,  3.63it/s] 34%|███▎      | 302/900 [00:52&lt;02:44,  3.63it/s] 34%|███▎      | 303/900 [00:52&lt;02:44,  3.62it/s] 34%|███▍      | 304/900 [00:53&lt;02:45,  3.60it/s] 34%|███▍      | 305/900 [00:53&lt;02:46,  3.58it/s] 34%|███▍      | 306/900 [00:53&lt;02:45,  3.58it/s] 34%|███▍      | 307/900 [00:53&lt;02:45,  3.58it/s] 34%|███▍      | 308/900 [00:54&lt;02:46,  3.56it/s] 34%|███▍      | 309/900 [00:54&lt;02:46,  3.55it/s] 34%|███▍      | 310/900 [00:54&lt;02:46,  3.55it/s] 35%|███▍      | 311/900 [00:55&lt;02:46,  3.54it/s] 35%|███▍      | 312/900 [00:55&lt;02:46,  3.52it/s] 35%|███▍      | 313/900 [00:55&lt;02:46,  3.52it/s] 35%|███▍      | 314/900 [00:55&lt;02:46,  3.52it/s] 35%|███▌      | 315/900 [00:56&lt;02:46,  3.52it/s] 35%|███▌      | 316/900 [00:56&lt;02:46,  3.51it/s] 35%|███▌      | 317/900 [00:56&lt;02:46,  3.50it/s] 35%|███▌      | 318/900 [00:57&lt;02:46,  3.50it/s] 35%|███▌      | 319/900 [00:57&lt;02:46,  3.49it/s] 36%|███▌      | 320/900 [00:57&lt;02:46,  3.48it/s] 36%|███▌      | 321/900 [00:57&lt;02:46,  3.48it/s] 36%|███▌      | 322/900 [00:58&lt;02:46,  3.47it/s] 36%|███▌      | 323/900 [00:58&lt;02:46,  3.46it/s] 36%|███▌      | 324/900 [00:58&lt;02:46,  3.45it/s] 36%|███▌      | 325/900 [00:59&lt;02:46,  3.45it/s] 36%|███▌      | 326/900 [00:59&lt;02:46,  3.44it/s] 36%|███▋      | 327/900 [00:59&lt;02:47,  3.41it/s] 36%|███▋      | 328/900 [00:59&lt;02:47,  3.41it/s] 37%|███▋      | 329/900 [01:00&lt;02:47,  3.40it/s] 37%|███▋      | 330/900 [01:00&lt;02:47,  3.40it/s] 37%|███▋      | 331/900 [01:00&lt;02:47,  3.39it/s] 37%|███▋      | 332/900 [01:01&lt;02:48,  3.37it/s] 37%|███▋      | 333/900 [01:01&lt;02:48,  3.37it/s] 37%|███▋      | 334/900 [01:01&lt;02:48,  3.37it/s] 37%|███▋      | 335/900 [01:01&lt;02:48,  3.36it/s] 37%|███▋      | 336/900 [01:02&lt;02:48,  3.36it/s] 37%|███▋      | 337/900 [01:02&lt;02:48,  3.34it/s] 38%|███▊      | 338/900 [01:02&lt;02:49,  3.33it/s] 38%|███▊      | 339/900 [01:03&lt;02:48,  3.32it/s] 38%|███▊      | 340/900 [01:03&lt;02:48,  3.32it/s] 38%|███▊      | 341/900 [01:03&lt;02:48,  3.32it/s] 38%|███▊      | 342/900 [01:04&lt;02:48,  3.31it/s] 38%|███▊      | 343/900 [01:04&lt;02:48,  3.30it/s] 38%|███▊      | 344/900 [01:04&lt;02:48,  3.30it/s] 38%|███▊      | 345/900 [01:05&lt;02:48,  3.29it/s] 38%|███▊      | 346/900 [01:05&lt;02:48,  3.29it/s] 39%|███▊      | 347/900 [01:05&lt;02:48,  3.28it/s] 39%|███▊      | 348/900 [01:05&lt;02:48,  3.27it/s] 39%|███▉      | 349/900 [01:06&lt;02:48,  3.26it/s] 39%|███▉      | 350/900 [01:06&lt;02:49,  3.25it/s] 39%|███▉      | 351/900 [01:06&lt;02:49,  3.25it/s] 39%|███▉      | 352/900 [01:07&lt;02:49,  3.24it/s] 39%|███▉      | 353/900 [01:07&lt;02:49,  3.23it/s] 39%|███▉      | 354/900 [01:07&lt;02:52,  3.16it/s] 39%|███▉      | 355/900 [01:08&lt;02:51,  3.18it/s] 40%|███▉      | 356/900 [01:08&lt;02:51,  3.17it/s] 40%|███▉      | 357/900 [01:08&lt;02:50,  3.18it/s] 40%|███▉      | 358/900 [01:09&lt;02:50,  3.18it/s] 40%|███▉      | 359/900 [01:09&lt;02:50,  3.18it/s] 40%|████      | 360/900 [01:09&lt;02:50,  3.17it/s] 40%|████      | 361/900 [01:10&lt;02:50,  3.17it/s] 40%|████      | 362/900 [01:10&lt;02:50,  3.16it/s] 40%|████      | 363/900 [01:10&lt;02:50,  3.16it/s] 40%|████      | 364/900 [01:10&lt;02:50,  3.15it/s] 41%|████      | 365/900 [01:11&lt;02:50,  3.14it/s] 41%|████      | 366/900 [01:11&lt;02:50,  3.14it/s] 41%|████      | 367/900 [01:11&lt;02:50,  3.13it/s] 41%|████      | 368/900 [01:12&lt;02:50,  3.13it/s] 41%|████      | 369/900 [01:12&lt;02:50,  3.11it/s] 41%|████      | 370/900 [01:12&lt;02:50,  3.10it/s] 41%|████      | 371/900 [01:13&lt;02:50,  3.10it/s] 41%|████▏     | 372/900 [01:13&lt;02:51,  3.09it/s] 41%|████▏     | 373/900 [01:13&lt;02:51,  3.08it/s] 42%|████▏     | 374/900 [01:14&lt;02:51,  3.07it/s] 42%|████▏     | 375/900 [01:14&lt;02:51,  3.06it/s] 42%|████▏     | 376/900 [01:14&lt;02:51,  3.06it/s] 42%|████▏     | 377/900 [01:15&lt;02:51,  3.06it/s] 42%|████▏     | 378/900 [01:15&lt;02:51,  3.05it/s] 42%|████▏     | 379/900 [01:15&lt;02:51,  3.04it/s] 42%|████▏     | 380/900 [01:16&lt;02:51,  3.03it/s] 42%|████▏     | 381/900 [01:16&lt;02:52,  3.02it/s] 42%|████▏     | 382/900 [01:16&lt;02:51,  3.02it/s] 43%|████▎     | 383/900 [01:17&lt;02:51,  3.02it/s] 43%|████▎     | 384/900 [01:17&lt;02:51,  3.01it/s] 43%|████▎     | 385/900 [01:17&lt;02:51,  3.01it/s] 43%|████▎     | 386/900 [01:18&lt;02:50,  3.01it/s] 43%|████▎     | 387/900 [01:18&lt;02:50,  3.00it/s] 43%|████▎     | 388/900 [01:18&lt;02:50,  3.00it/s] 43%|████▎     | 389/900 [01:19&lt;02:51,  2.99it/s] 43%|████▎     | 390/900 [01:19&lt;02:50,  2.98it/s] 43%|████▎     | 391/900 [01:19&lt;02:51,  2.97it/s] 44%|████▎     | 392/900 [01:20&lt;02:51,  2.96it/s] 44%|████▎     | 393/900 [01:20&lt;02:51,  2.95it/s] 44%|████▍     | 394/900 [01:20&lt;02:51,  2.95it/s] 44%|████▍     | 395/900 [01:21&lt;02:51,  2.95it/s] 44%|████▍     | 396/900 [01:21&lt;02:51,  2.94it/s] 44%|████▍     | 397/900 [01:21&lt;02:51,  2.94it/s] 44%|████▍     | 398/900 [01:22&lt;02:51,  2.94it/s] 44%|████▍     | 399/900 [01:22&lt;02:51,  2.92it/s] 44%|████▍     | 400/900 [01:22&lt;02:52,  2.91it/s] 45%|████▍     | 401/900 [01:23&lt;02:51,  2.91it/s] 45%|████▍     | 402/900 [01:23&lt;02:51,  2.90it/s] 45%|████▍     | 403/900 [01:24&lt;02:51,  2.89it/s] 45%|████▍     | 404/900 [01:24&lt;02:51,  2.89it/s] 45%|████▌     | 405/900 [01:24&lt;02:51,  2.89it/s] 45%|████▌     | 406/900 [01:25&lt;02:51,  2.88it/s] 45%|████▌     | 407/900 [01:25&lt;02:55,  2.81it/s] 45%|████▌     | 408/900 [01:25&lt;03:01,  2.71it/s] 45%|████▌     | 409/900 [01:26&lt;02:58,  2.76it/s] 46%|████▌     | 410/900 [01:26&lt;02:55,  2.78it/s] 46%|████▌     | 411/900 [01:26&lt;02:56,  2.77it/s] 46%|████▌     | 412/900 [01:27&lt;02:57,  2.75it/s] 46%|████▌     | 413/900 [01:27&lt;02:55,  2.78it/s] 46%|████▌     | 414/900 [01:27&lt;02:53,  2.79it/s] 46%|████▌     | 415/900 [01:28&lt;02:53,  2.80it/s] 46%|████▌     | 416/900 [01:28&lt;02:52,  2.81it/s] 46%|████▋     | 417/900 [01:29&lt;02:51,  2.81it/s] 46%|████▋     | 418/900 [01:29&lt;02:51,  2.81it/s] 47%|████▋     | 419/900 [01:29&lt;02:51,  2.81it/s] 47%|████▋     | 420/900 [01:30&lt;02:51,  2.80it/s] 47%|████▋     | 421/900 [01:30&lt;02:50,  2.80it/s] 47%|████▋     | 422/900 [01:30&lt;02:50,  2.80it/s] 47%|████▋     | 423/900 [01:31&lt;02:51,  2.79it/s] 47%|████▋     | 424/900 [01:31&lt;02:51,  2.78it/s] 47%|████▋     | 425/900 [01:31&lt;02:51,  2.77it/s] 47%|████▋     | 426/900 [01:32&lt;02:51,  2.77it/s] 47%|████▋     | 427/900 [01:32&lt;02:51,  2.76it/s] 48%|████▊     | 428/900 [01:32&lt;02:51,  2.75it/s] 48%|████▊     | 429/900 [01:33&lt;02:51,  2.75it/s] 48%|████▊     | 430/900 [01:33&lt;02:55,  2.67it/s] 48%|████▊     | 431/900 [01:34&lt;02:54,  2.69it/s] 48%|████▊     | 432/900 [01:34&lt;02:52,  2.71it/s] 48%|████▊     | 433/900 [01:34&lt;02:51,  2.72it/s] 48%|████▊     | 434/900 [01:35&lt;02:51,  2.72it/s] 48%|████▊     | 435/900 [01:35&lt;02:50,  2.72it/s] 48%|████▊     | 436/900 [01:35&lt;02:50,  2.72it/s] 49%|████▊     | 437/900 [01:36&lt;02:51,  2.71it/s] 49%|████▊     | 438/900 [01:36&lt;02:50,  2.71it/s] 49%|████▉     | 439/900 [01:37&lt;02:50,  2.71it/s] 49%|████▉     | 440/900 [01:37&lt;02:50,  2.70it/s] 49%|████▉     | 441/900 [01:37&lt;02:49,  2.70it/s] 49%|████▉     | 442/900 [01:38&lt;02:49,  2.70it/s] 49%|████▉     | 443/900 [01:38&lt;02:49,  2.69it/s] 49%|████▉     | 444/900 [01:38&lt;02:49,  2.69it/s] 49%|████▉     | 445/900 [01:39&lt;02:49,  2.68it/s] 50%|████▉     | 446/900 [01:39&lt;02:49,  2.68it/s] 50%|████▉     | 447/900 [01:40&lt;02:49,  2.67it/s] 50%|████▉     | 448/900 [01:40&lt;02:49,  2.67it/s] 50%|████▉     | 449/900 [01:40&lt;02:49,  2.67it/s] 50%|█████     | 450/900 [01:41&lt;02:49,  2.65it/s] 50%|█████     | 451/900 [01:41&lt;02:50,  2.63it/s] 50%|█████     | 452/900 [01:41&lt;02:50,  2.63it/s] 50%|█████     | 453/900 [01:42&lt;02:50,  2.63it/s] 50%|█████     | 454/900 [01:42&lt;02:49,  2.63it/s] 51%|█████     | 455/900 [01:43&lt;02:49,  2.63it/s] 51%|█████     | 456/900 [01:43&lt;02:49,  2.62it/s] 51%|█████     | 457/900 [01:43&lt;02:49,  2.62it/s] 51%|█████     | 458/900 [01:44&lt;02:48,  2.62it/s] 51%|█████     | 459/900 [01:44&lt;02:48,  2.61it/s] 51%|█████     | 460/900 [01:45&lt;02:48,  2.61it/s] 51%|█████     | 461/900 [01:45&lt;02:48,  2.60it/s] 51%|█████▏    | 462/900 [01:45&lt;02:48,  2.60it/s] 51%|█████▏    | 463/900 [01:46&lt;02:48,  2.59it/s] 52%|█████▏    | 464/900 [01:46&lt;02:48,  2.59it/s] 52%|█████▏    | 465/900 [01:46&lt;02:48,  2.59it/s] 52%|█████▏    | 466/900 [01:47&lt;02:48,  2.58it/s] 52%|█████▏    | 467/900 [01:47&lt;02:47,  2.58it/s] 52%|█████▏    | 468/900 [01:48&lt;02:47,  2.57it/s] 52%|█████▏    | 469/900 [01:48&lt;02:47,  2.57it/s] 52%|█████▏    | 470/900 [01:48&lt;02:47,  2.56it/s] 52%|█████▏    | 471/900 [01:49&lt;02:47,  2.56it/s] 52%|█████▏    | 472/900 [01:49&lt;02:47,  2.55it/s] 53%|█████▎    | 473/900 [01:50&lt;02:47,  2.55it/s] 53%|█████▎    | 474/900 [01:50&lt;02:47,  2.55it/s] 53%|█████▎    | 475/900 [01:50&lt;02:47,  2.54it/s] 53%|█████▎    | 476/900 [01:51&lt;02:47,  2.54it/s] 53%|█████▎    | 477/900 [01:51&lt;02:47,  2.53it/s] 53%|█████▎    | 478/900 [01:52&lt;02:46,  2.53it/s] 53%|█████▎    | 479/900 [01:52&lt;02:47,  2.52it/s] 53%|█████▎    | 480/900 [01:52&lt;02:47,  2.51it/s] 53%|█████▎    | 481/900 [01:53&lt;02:46,  2.51it/s] 54%|█████▎    | 482/900 [01:53&lt;02:46,  2.51it/s] 54%|█████▎    | 483/900 [01:54&lt;02:46,  2.50it/s] 54%|█████▍    | 484/900 [01:54&lt;02:46,  2.50it/s] 54%|█████▍    | 485/900 [01:54&lt;02:47,  2.48it/s] 54%|█████▍    | 486/900 [01:55&lt;02:46,  2.48it/s] 54%|█████▍    | 487/900 [01:55&lt;02:46,  2.47it/s] 54%|█████▍    | 488/900 [01:56&lt;02:47,  2.46it/s] 54%|█████▍    | 489/900 [01:56&lt;02:46,  2.46it/s] 54%|█████▍    | 490/900 [01:56&lt;02:46,  2.47it/s] 55%|█████▍    | 491/900 [01:57&lt;02:45,  2.47it/s] 55%|█████▍    | 492/900 [01:57&lt;02:45,  2.47it/s] 55%|█████▍    | 493/900 [01:58&lt;02:45,  2.46it/s] 55%|█████▍    | 494/900 [01:58&lt;02:45,  2.45it/s] 55%|█████▌    | 495/900 [01:58&lt;02:45,  2.44it/s] 55%|█████▌    | 496/900 [01:59&lt;02:45,  2.44it/s] 55%|█████▌    | 497/900 [01:59&lt;02:45,  2.44it/s] 55%|█████▌    | 498/900 [02:00&lt;02:45,  2.44it/s] 55%|█████▌    | 499/900 [02:00&lt;02:45,  2.42it/s] 56%|█████▌    | 500/900 [02:01&lt;02:45,  2.42it/s] 56%|█████▌    | 501/900 [02:01&lt;02:44,  2.42it/s] 56%|█████▌    | 502/900 [02:01&lt;02:44,  2.42it/s] 56%|█████▌    | 503/900 [02:02&lt;02:44,  2.42it/s] 56%|█████▌    | 504/900 [02:02&lt;02:44,  2.41it/s] 56%|█████▌    | 505/900 [02:03&lt;02:43,  2.41it/s] 56%|█████▌    | 506/900 [02:03&lt;02:44,  2.40it/s] 56%|█████▋    | 507/900 [02:03&lt;02:44,  2.39it/s] 56%|█████▋    | 508/900 [02:04&lt;02:44,  2.39it/s] 57%|█████▋    | 509/900 [02:04&lt;02:43,  2.39it/s] 57%|█████▋    | 510/900 [02:05&lt;02:43,  2.39it/s] 57%|█████▋    | 511/900 [02:05&lt;02:42,  2.39it/s] 57%|█████▋    | 512/900 [02:06&lt;02:42,  2.38it/s] 57%|█████▋    | 513/900 [02:06&lt;02:42,  2.38it/s] 57%|█████▋    | 514/900 [02:06&lt;02:42,  2.38it/s] 57%|█████▋    | 515/900 [02:07&lt;02:42,  2.37it/s] 57%|█████▋    | 516/900 [02:07&lt;02:41,  2.37it/s] 57%|█████▋    | 517/900 [02:08&lt;02:41,  2.37it/s] 58%|█████▊    | 518/900 [02:08&lt;02:41,  2.36it/s] 58%|█████▊    | 519/900 [02:08&lt;02:41,  2.36it/s] 58%|█████▊    | 520/900 [02:09&lt;02:41,  2.36it/s] 58%|█████▊    | 521/900 [02:09&lt;02:41,  2.35it/s] 58%|█████▊    | 522/900 [02:10&lt;02:41,  2.34it/s] 58%|█████▊    | 523/900 [02:10&lt;02:41,  2.34it/s] 58%|█████▊    | 524/900 [02:11&lt;02:40,  2.34it/s] 58%|█████▊    | 525/900 [02:11&lt;02:40,  2.34it/s] 58%|█████▊    | 526/900 [02:11&lt;02:40,  2.33it/s] 59%|█████▊    | 527/900 [02:12&lt;02:40,  2.32it/s] 59%|█████▊    | 528/900 [02:12&lt;02:40,  2.32it/s] 59%|█████▉    | 529/900 [02:13&lt;02:40,  2.32it/s] 59%|█████▉    | 530/900 [02:13&lt;02:39,  2.32it/s] 59%|█████▉    | 531/900 [02:14&lt;02:40,  2.30it/s] 59%|█████▉    | 532/900 [02:14&lt;02:39,  2.30it/s] 59%|█████▉    | 533/900 [02:15&lt;02:40,  2.29it/s] 59%|█████▉    | 534/900 [02:15&lt;02:39,  2.29it/s] 59%|█████▉    | 535/900 [02:15&lt;02:39,  2.29it/s] 60%|█████▉    | 536/900 [02:16&lt;02:38,  2.29it/s] 60%|█████▉    | 537/900 [02:16&lt;02:38,  2.29it/s] 60%|█████▉    | 538/900 [02:17&lt;02:38,  2.29it/s] 60%|█████▉    | 539/900 [02:17&lt;02:38,  2.28it/s] 60%|██████    | 540/900 [02:18&lt;02:37,  2.28it/s] 60%|██████    | 541/900 [02:18&lt;02:37,  2.28it/s] 60%|██████    | 542/900 [02:18&lt;02:37,  2.28it/s] 60%|██████    | 543/900 [02:19&lt;02:37,  2.27it/s] 60%|██████    | 544/900 [02:19&lt;02:36,  2.27it/s] 61%|██████    | 545/900 [02:20&lt;02:36,  2.26it/s] 61%|██████    | 546/900 [02:20&lt;02:37,  2.25it/s] 61%|██████    | 547/900 [02:21&lt;02:36,  2.25it/s] 61%|██████    | 548/900 [02:21&lt;02:36,  2.25it/s] 61%|██████    | 549/900 [02:22&lt;02:35,  2.25it/s] 61%|██████    | 550/900 [02:22&lt;02:35,  2.25it/s] 61%|██████    | 551/900 [02:22&lt;02:35,  2.25it/s] 61%|██████▏   | 552/900 [02:23&lt;02:35,  2.24it/s] 61%|██████▏   | 553/900 [02:23&lt;02:35,  2.23it/s] 62%|██████▏   | 554/900 [02:24&lt;02:35,  2.23it/s] 62%|██████▏   | 555/900 [02:24&lt;02:35,  2.22it/s] 62%|██████▏   | 556/900 [02:25&lt;02:35,  2.21it/s] 62%|██████▏   | 557/900 [02:25&lt;02:41,  2.12it/s] 62%|██████▏   | 558/900 [02:26&lt;02:39,  2.15it/s] 62%|██████▏   | 559/900 [02:26&lt;02:37,  2.16it/s] 62%|██████▏   | 560/900 [02:27&lt;02:39,  2.13it/s] 62%|██████▏   | 561/900 [02:27&lt;02:37,  2.15it/s] 62%|██████▏   | 562/900 [02:28&lt;02:35,  2.17it/s] 63%|██████▎   | 563/900 [02:28&lt;02:34,  2.18it/s] 63%|██████▎   | 564/900 [02:28&lt;02:33,  2.18it/s] 63%|██████▎   | 565/900 [02:29&lt;02:33,  2.18it/s] 63%|██████▎   | 566/900 [02:29&lt;02:32,  2.18it/s] 63%|██████▎   | 567/900 [02:30&lt;02:32,  2.18it/s] 63%|██████▎   | 568/900 [02:30&lt;02:32,  2.18it/s] 63%|██████▎   | 569/900 [02:31&lt;02:31,  2.18it/s] 63%|██████▎   | 570/900 [02:31&lt;02:31,  2.18it/s] 63%|██████▎   | 571/900 [02:32&lt;02:31,  2.18it/s] 64%|██████▎   | 572/900 [02:32&lt;02:30,  2.17it/s] 64%|██████▎   | 573/900 [02:33&lt;02:30,  2.17it/s] 64%|██████▍   | 574/900 [02:33&lt;02:30,  2.17it/s] 64%|██████▍   | 575/900 [02:34&lt;02:30,  2.16it/s] 64%|██████▍   | 576/900 [02:34&lt;02:29,  2.16it/s] 64%|██████▍   | 577/900 [02:34&lt;02:29,  2.16it/s] 64%|██████▍   | 578/900 [02:35&lt;02:29,  2.16it/s] 64%|██████▍   | 579/900 [02:35&lt;02:29,  2.15it/s] 64%|██████▍   | 580/900 [02:36&lt;02:29,  2.14it/s] 65%|██████▍   | 581/900 [02:36&lt;02:29,  2.14it/s] 65%|██████▍   | 582/900 [02:37&lt;02:29,  2.13it/s] 65%|██████▍   | 583/900 [02:37&lt;02:28,  2.13it/s] 65%|██████▍   | 584/900 [02:38&lt;02:28,  2.13it/s] 65%|██████▌   | 585/900 [02:38&lt;02:27,  2.13it/s] 65%|██████▌   | 586/900 [02:39&lt;02:27,  2.13it/s] 65%|██████▌   | 587/900 [02:39&lt;02:27,  2.13it/s] 65%|██████▌   | 588/900 [02:40&lt;02:26,  2.12it/s] 65%|██████▌   | 589/900 [02:40&lt;02:26,  2.12it/s] 66%|██████▌   | 590/900 [02:41&lt;02:26,  2.11it/s] 66%|██████▌   | 591/900 [02:41&lt;02:26,  2.10it/s] 66%|██████▌   | 592/900 [02:42&lt;02:26,  2.10it/s] 66%|██████▌   | 593/900 [02:42&lt;02:26,  2.10it/s] 66%|██████▌   | 594/900 [02:42&lt;02:25,  2.10it/s] 66%|██████▌   | 595/900 [02:43&lt;02:25,  2.10it/s] 66%|██████▌   | 596/900 [02:43&lt;02:25,  2.09it/s] 66%|██████▋   | 597/900 [02:44&lt;02:25,  2.09it/s] 66%|██████▋   | 598/900 [02:44&lt;02:25,  2.08it/s] 67%|██████▋   | 599/900 [02:45&lt;02:24,  2.08it/s] 67%|██████▋   | 600/900 [02:45&lt;02:24,  2.08it/s] 67%|██████▋   | 601/900 [02:46&lt;02:23,  2.08it/s] 67%|██████▋   | 602/900 [02:46&lt;02:23,  2.08it/s] 67%|██████▋   | 603/900 [02:47&lt;02:23,  2.08it/s] 67%|██████▋   | 604/900 [02:47&lt;02:22,  2.07it/s] 67%|██████▋   | 605/900 [02:48&lt;02:22,  2.07it/s] 67%|██████▋   | 606/900 [02:48&lt;02:22,  2.07it/s] 67%|██████▋   | 607/900 [02:49&lt;02:22,  2.06it/s] 68%|██████▊   | 608/900 [02:49&lt;02:21,  2.06it/s] 68%|██████▊   | 609/900 [02:50&lt;02:21,  2.05it/s] 68%|██████▊   | 610/900 [02:50&lt;02:21,  2.05it/s] 68%|██████▊   | 611/900 [02:51&lt;02:21,  2.05it/s] 68%|██████▊   | 612/900 [02:51&lt;02:20,  2.05it/s] 68%|██████▊   | 613/900 [02:52&lt;02:20,  2.05it/s] 68%|██████▊   | 614/900 [02:52&lt;02:20,  2.04it/s] 68%|██████▊   | 615/900 [02:53&lt;02:19,  2.04it/s] 68%|██████▊   | 616/900 [02:53&lt;02:19,  2.04it/s] 69%|██████▊   | 617/900 [02:54&lt;02:19,  2.03it/s] 69%|██████▊   | 618/900 [02:54&lt;02:18,  2.03it/s] 69%|██████▉   | 619/900 [02:55&lt;02:18,  2.03it/s] 69%|██████▉   | 620/900 [02:55&lt;02:18,  2.03it/s] 69%|██████▉   | 621/900 [02:56&lt;02:18,  2.02it/s] 69%|██████▉   | 622/900 [02:56&lt;02:17,  2.02it/s] 69%|██████▉   | 623/900 [02:57&lt;02:17,  2.02it/s] 69%|██████▉   | 624/900 [02:57&lt;02:16,  2.01it/s] 69%|██████▉   | 625/900 [02:58&lt;02:16,  2.01it/s] 70%|██████▉   | 626/900 [02:58&lt;02:16,  2.01it/s] 70%|██████▉   | 627/900 [02:59&lt;02:15,  2.01it/s] 70%|██████▉   | 628/900 [02:59&lt;02:15,  2.01it/s] 70%|██████▉   | 629/900 [03:00&lt;02:15,  2.00it/s] 70%|███████   | 630/900 [03:00&lt;02:15,  2.00it/s] 70%|███████   | 631/900 [03:01&lt;02:14,  2.00it/s] 70%|███████   | 632/900 [03:01&lt;02:14,  2.00it/s] 70%|███████   | 633/900 [03:02&lt;02:13,  1.99it/s] 70%|███████   | 634/900 [03:02&lt;02:13,  1.99it/s] 71%|███████   | 635/900 [03:03&lt;02:13,  1.99it/s] 71%|███████   | 636/900 [03:03&lt;02:12,  1.99it/s] 71%|███████   | 637/900 [03:04&lt;02:12,  1.98it/s] 71%|███████   | 638/900 [03:04&lt;02:12,  1.98it/s] 71%|███████   | 639/900 [03:05&lt;02:11,  1.98it/s] 71%|███████   | 640/900 [03:05&lt;02:11,  1.98it/s] 71%|███████   | 641/900 [03:06&lt;02:11,  1.97it/s] 71%|███████▏  | 642/900 [03:06&lt;02:11,  1.96it/s] 71%|███████▏  | 643/900 [03:07&lt;02:10,  1.96it/s] 72%|███████▏  | 644/900 [03:07&lt;02:10,  1.96it/s] 72%|███████▏  | 645/900 [03:08&lt;02:10,  1.96it/s] 72%|███████▏  | 646/900 [03:08&lt;02:10,  1.95it/s] 72%|███████▏  | 647/900 [03:09&lt;02:09,  1.95it/s] 72%|███████▏  | 648/900 [03:09&lt;02:09,  1.94it/s] 72%|███████▏  | 649/900 [03:10&lt;02:09,  1.93it/s] 72%|███████▏  | 650/900 [03:10&lt;02:09,  1.94it/s] 72%|███████▏  | 651/900 [03:11&lt;02:08,  1.94it/s] 72%|███████▏  | 652/900 [03:11&lt;02:08,  1.93it/s] 73%|███████▎  | 653/900 [03:12&lt;02:07,  1.93it/s] 73%|███████▎  | 654/900 [03:12&lt;02:07,  1.93it/s] 73%|███████▎  | 655/900 [03:13&lt;02:09,  1.89it/s] 73%|███████▎  | 656/900 [03:13&lt;02:08,  1.90it/s] 73%|███████▎  | 657/900 [03:14&lt;02:07,  1.90it/s] 73%|███████▎  | 658/900 [03:15&lt;02:06,  1.91it/s] 73%|███████▎  | 659/900 [03:15&lt;02:06,  1.91it/s] 73%|███████▎  | 660/900 [03:16&lt;02:05,  1.91it/s] 73%|███████▎  | 661/900 [03:16&lt;02:04,  1.92it/s] 74%|███████▎  | 662/900 [03:17&lt;02:04,  1.91it/s] 74%|███████▎  | 663/900 [03:17&lt;02:03,  1.91it/s] 74%|███████▍  | 664/900 [03:18&lt;02:03,  1.91it/s] 74%|███████▍  | 665/900 [03:18&lt;02:04,  1.89it/s] 74%|███████▍  | 666/900 [03:19&lt;02:08,  1.82it/s] 74%|███████▍  | 667/900 [03:19&lt;02:06,  1.84it/s] 74%|███████▍  | 668/900 [03:20&lt;02:06,  1.84it/s] 74%|███████▍  | 669/900 [03:20&lt;02:05,  1.83it/s] 74%|███████▍  | 670/900 [03:21&lt;02:04,  1.85it/s] 75%|███████▍  | 671/900 [03:21&lt;02:03,  1.85it/s] 75%|███████▍  | 672/900 [03:22&lt;02:03,  1.85it/s] 75%|███████▍  | 673/900 [03:23&lt;02:02,  1.86it/s] 75%|███████▍  | 674/900 [03:23&lt;02:01,  1.86it/s] 75%|███████▌  | 675/900 [03:24&lt;02:00,  1.86it/s] 75%|███████▌  | 676/900 [03:24&lt;02:01,  1.85it/s] 75%|███████▌  | 677/900 [03:25&lt;02:01,  1.84it/s] 75%|███████▌  | 678/900 [03:25&lt;02:00,  1.83it/s] 75%|███████▌  | 679/900 [03:26&lt;02:01,  1.82it/s] 76%|███████▌  | 680/900 [03:26&lt;02:00,  1.83it/s] 76%|███████▌  | 681/900 [03:27&lt;01:59,  1.83it/s] 76%|███████▌  | 682/900 [03:28&lt;02:05,  1.73it/s] 76%|███████▌  | 683/900 [03:28&lt;02:03,  1.75it/s] 76%|███████▌  | 684/900 [03:29&lt;02:02,  1.76it/s] 76%|███████▌  | 685/900 [03:29&lt;02:01,  1.77it/s] 76%|███████▌  | 686/900 [03:30&lt;02:01,  1.77it/s] 76%|███████▋  | 687/900 [03:30&lt;02:00,  1.77it/s] 76%|███████▋  | 688/900 [03:31&lt;02:01,  1.75it/s] 77%|███████▋  | 689/900 [03:32&lt;02:00,  1.76it/s] 77%|███████▋  | 690/900 [03:32&lt;01:59,  1.76it/s] 77%|███████▋  | 691/900 [03:33&lt;01:58,  1.76it/s] 77%|███████▋  | 692/900 [03:33&lt;01:57,  1.77it/s] 77%|███████▋  | 693/900 [03:34&lt;01:57,  1.77it/s] 77%|███████▋  | 694/900 [03:34&lt;01:56,  1.76it/s] 77%|███████▋  | 695/900 [03:35&lt;01:57,  1.75it/s] 77%|███████▋  | 696/900 [03:35&lt;01:55,  1.77it/s] 77%|███████▋  | 697/900 [03:36&lt;01:54,  1.78it/s] 78%|███████▊  | 698/900 [03:37&lt;01:52,  1.79it/s] 78%|███████▊  | 699/900 [03:37&lt;01:51,  1.80it/s] 78%|███████▊  | 700/900 [03:38&lt;01:51,  1.80it/s] 78%|███████▊  | 701/900 [03:38&lt;01:50,  1.79it/s] 78%|███████▊  | 702/900 [03:39&lt;01:50,  1.79it/s] 78%|███████▊  | 703/900 [03:39&lt;01:49,  1.79it/s] 78%|███████▊  | 704/900 [03:40&lt;01:49,  1.79it/s] 78%|███████▊  | 705/900 [03:40&lt;01:48,  1.79it/s] 78%|███████▊  | 706/900 [03:41&lt;01:48,  1.79it/s] 79%|███████▊  | 707/900 [03:42&lt;01:47,  1.79it/s] 79%|███████▊  | 708/900 [03:42&lt;01:47,  1.79it/s] 79%|███████▉  | 709/900 [03:43&lt;01:46,  1.79it/s] 79%|███████▉  | 710/900 [03:43&lt;01:46,  1.79it/s] 79%|███████▉  | 711/900 [03:44&lt;01:46,  1.77it/s] 79%|███████▉  | 712/900 [03:44&lt;01:45,  1.77it/s] 79%|███████▉  | 713/900 [03:45&lt;01:45,  1.77it/s] 79%|███████▉  | 714/900 [03:46&lt;01:45,  1.77it/s] 79%|███████▉  | 715/900 [03:46&lt;01:44,  1.76it/s] 80%|███████▉  | 716/900 [03:47&lt;01:45,  1.75it/s] 80%|███████▉  | 717/900 [03:47&lt;01:44,  1.75it/s] 80%|███████▉  | 718/900 [03:48&lt;01:43,  1.76it/s] 80%|███████▉  | 719/900 [03:48&lt;01:42,  1.76it/s] 80%|████████  | 720/900 [03:49&lt;01:42,  1.76it/s] 80%|████████  | 721/900 [03:50&lt;01:41,  1.76it/s] 80%|████████  | 722/900 [03:50&lt;01:41,  1.76it/s] 80%|████████  | 723/900 [03:51&lt;01:40,  1.75it/s] 80%|████████  | 724/900 [03:51&lt;01:40,  1.76it/s] 81%|████████  | 725/900 [03:52&lt;01:39,  1.76it/s] 81%|████████  | 726/900 [03:52&lt;01:39,  1.75it/s] 81%|████████  | 727/900 [03:53&lt;01:38,  1.75it/s] 81%|████████  | 728/900 [03:54&lt;01:38,  1.75it/s] 81%|████████  | 729/900 [03:54&lt;01:37,  1.75it/s] 81%|████████  | 730/900 [03:55&lt;01:37,  1.75it/s] 81%|████████  | 731/900 [03:55&lt;01:39,  1.70it/s] 81%|████████▏ | 732/900 [03:56&lt;01:39,  1.70it/s] 81%|████████▏ | 733/900 [03:57&lt;01:38,  1.69it/s] 82%|████████▏ | 734/900 [03:57&lt;01:39,  1.68it/s] 82%|████████▏ | 735/900 [03:58&lt;01:37,  1.68it/s] 82%|████████▏ | 736/900 [03:58&lt;01:37,  1.68it/s] 82%|████████▏ | 737/900 [03:59&lt;01:37,  1.67it/s] 82%|████████▏ | 738/900 [04:00&lt;01:36,  1.67it/s] 82%|████████▏ | 739/900 [04:00&lt;01:36,  1.67it/s] 82%|████████▏ | 740/900 [04:01&lt;01:35,  1.68it/s] 82%|████████▏ | 741/900 [04:01&lt;01:35,  1.66it/s] 82%|████████▏ | 742/900 [04:02&lt;01:34,  1.67it/s] 83%|████████▎ | 743/900 [04:02&lt;01:33,  1.68it/s] 83%|████████▎ | 744/900 [04:03&lt;01:33,  1.67it/s] 83%|████████▎ | 745/900 [04:04&lt;01:32,  1.67it/s] 83%|████████▎ | 746/900 [04:04&lt;01:32,  1.66it/s] 83%|████████▎ | 747/900 [04:05&lt;01:32,  1.65it/s] 83%|████████▎ | 748/900 [04:06&lt;01:32,  1.64it/s] 83%|████████▎ | 749/900 [04:06&lt;01:32,  1.64it/s] 83%|████████▎ | 750/900 [04:07&lt;01:30,  1.65it/s] 83%|████████▎ | 751/900 [04:07&lt;01:29,  1.67it/s] 84%|████████▎ | 752/900 [04:08&lt;01:29,  1.66it/s] 84%|████████▎ | 753/900 [04:09&lt;01:28,  1.67it/s] 84%|████████▍ | 754/900 [04:09&lt;01:28,  1.66it/s] 84%|████████▍ | 755/900 [04:10&lt;01:27,  1.66it/s] 84%|████████▍ | 756/900 [04:10&lt;01:26,  1.66it/s] 84%|████████▍ | 757/900 [04:11&lt;01:25,  1.66it/s] 84%|████████▍ | 758/900 [04:12&lt;01:25,  1.66it/s] 84%|████████▍ | 759/900 [04:12&lt;01:25,  1.64it/s] 84%|████████▍ | 760/900 [04:13&lt;01:25,  1.64it/s] 85%|████████▍ | 761/900 [04:13&lt;01:24,  1.65it/s] 85%|████████▍ | 762/900 [04:14&lt;01:23,  1.65it/s] 85%|████████▍ | 763/900 [04:15&lt;01:22,  1.66it/s] 85%|████████▍ | 764/900 [04:15&lt;01:21,  1.66it/s] 85%|████████▌ | 765/900 [04:16&lt;01:21,  1.66it/s] 85%|████████▌ | 766/900 [04:16&lt;01:20,  1.67it/s] 85%|████████▌ | 767/900 [04:17&lt;01:19,  1.66it/s] 85%|████████▌ | 768/900 [04:18&lt;01:19,  1.66it/s] 85%|████████▌ | 769/900 [04:18&lt;01:19,  1.64it/s] 86%|████████▌ | 770/900 [04:19&lt;01:18,  1.65it/s] 86%|████████▌ | 771/900 [04:19&lt;01:18,  1.65it/s] 86%|████████▌ | 772/900 [04:20&lt;01:17,  1.64it/s] 86%|████████▌ | 773/900 [04:21&lt;01:17,  1.64it/s] 86%|████████▌ | 774/900 [04:21&lt;01:16,  1.65it/s] 86%|████████▌ | 775/900 [04:22&lt;01:15,  1.65it/s] 86%|████████▌ | 776/900 [04:22&lt;01:15,  1.65it/s] 86%|████████▋ | 777/900 [04:23&lt;01:14,  1.65it/s] 86%|████████▋ | 778/900 [04:24&lt;01:14,  1.64it/s] 87%|████████▋ | 779/900 [04:24&lt;01:14,  1.62it/s] 87%|████████▋ | 780/900 [04:25&lt;01:19,  1.51it/s] 87%|████████▋ | 781/900 [04:26&lt;01:23,  1.42it/s] 87%|████████▋ | 782/900 [04:27&lt;01:23,  1.41it/s] 87%|████████▋ | 783/900 [04:27&lt;01:20,  1.45it/s] 87%|████████▋ | 784/900 [04:28&lt;01:17,  1.49it/s] 87%|████████▋ | 785/900 [04:29&lt;01:15,  1.52it/s] 87%|████████▋ | 786/900 [04:29&lt;01:13,  1.54it/s] 87%|████████▋ | 787/900 [04:30&lt;01:14,  1.51it/s] 88%|████████▊ | 788/900 [04:30&lt;01:13,  1.53it/s] 88%|████████▊ | 789/900 [04:31&lt;01:11,  1.56it/s] 88%|████████▊ | 790/900 [04:32&lt;01:11,  1.53it/s] 88%|████████▊ | 791/900 [04:32&lt;01:12,  1.49it/s] 88%|████████▊ | 792/900 [04:33&lt;01:10,  1.53it/s] 88%|████████▊ | 793/900 [04:34&lt;01:09,  1.54it/s] 88%|████████▊ | 794/900 [04:34&lt;01:07,  1.56it/s] 88%|████████▊ | 795/900 [04:35&lt;01:06,  1.58it/s] 88%|████████▊ | 796/900 [04:36&lt;01:10,  1.47it/s] 89%|████████▊ | 797/900 [04:37&lt;01:15,  1.37it/s] 89%|████████▊ | 798/900 [04:37&lt;01:11,  1.43it/s] 89%|████████▉ | 799/900 [04:38&lt;01:08,  1.48it/s] 89%|████████▉ | 800/900 [04:38&lt;01:05,  1.52it/s] 89%|████████▉ | 801/900 [04:39&lt;01:04,  1.55it/s] 89%|████████▉ | 802/900 [04:40&lt;01:02,  1.56it/s] 89%|████████▉ | 803/900 [04:40&lt;01:01,  1.58it/s] 89%|████████▉ | 804/900 [04:41&lt;01:00,  1.58it/s] 89%|████████▉ | 805/900 [04:42&lt;01:00,  1.58it/s] 90%|████████▉ | 806/900 [04:42&lt;00:59,  1.59it/s] 90%|████████▉ | 807/900 [04:43&lt;00:58,  1.59it/s] 90%|████████▉ | 808/900 [04:44&lt;00:59,  1.54it/s] 90%|████████▉ | 809/900 [04:44&lt;00:59,  1.53it/s] 90%|█████████ | 810/900 [04:45&lt;00:58,  1.54it/s] 90%|█████████ | 811/900 [04:45&lt;00:57,  1.55it/s] 90%|█████████ | 812/900 [04:46&lt;00:56,  1.55it/s] 90%|█████████ | 813/900 [04:47&lt;00:55,  1.56it/s] 90%|█████████ | 814/900 [04:47&lt;00:54,  1.57it/s] 91%|█████████ | 815/900 [04:48&lt;00:54,  1.55it/s] 91%|█████████ | 816/900 [04:49&lt;00:54,  1.55it/s] 91%|█████████ | 817/900 [04:49&lt;00:53,  1.55it/s] 91%|█████████ | 818/900 [04:50&lt;00:52,  1.55it/s] 91%|█████████ | 819/900 [04:51&lt;00:52,  1.56it/s] 91%|█████████ | 820/900 [04:51&lt;00:51,  1.54it/s] 91%|█████████ | 821/900 [04:52&lt;00:50,  1.55it/s] 91%|█████████▏| 822/900 [04:53&lt;00:50,  1.56it/s] 91%|█████████▏| 823/900 [04:53&lt;00:49,  1.55it/s] 92%|█████████▏| 824/900 [04:54&lt;00:48,  1.55it/s] 92%|█████████▏| 825/900 [04:54&lt;00:48,  1.56it/s] 92%|█████████▏| 826/900 [04:55&lt;00:47,  1.54it/s] 92%|█████████▏| 827/900 [04:56&lt;00:47,  1.54it/s] 92%|█████████▏| 828/900 [04:56&lt;00:47,  1.53it/s] 92%|█████████▏| 829/900 [04:57&lt;00:46,  1.54it/s] 92%|█████████▏| 830/900 [04:58&lt;00:45,  1.54it/s] 92%|█████████▏| 831/900 [04:58&lt;00:44,  1.54it/s] 92%|█████████▏| 832/900 [04:59&lt;00:44,  1.54it/s] 93%|█████████▎| 833/900 [05:00&lt;00:43,  1.53it/s] 93%|█████████▎| 834/900 [05:00&lt;00:44,  1.49it/s] 93%|█████████▎| 835/900 [05:01&lt;00:43,  1.50it/s] 93%|█████████▎| 836/900 [05:02&lt;00:42,  1.52it/s] 93%|█████████▎| 837/900 [05:02&lt;00:41,  1.53it/s] 93%|█████████▎| 838/900 [05:03&lt;00:40,  1.54it/s] 93%|█████████▎| 839/900 [05:04&lt;00:39,  1.54it/s] 93%|█████████▎| 840/900 [05:04&lt;00:38,  1.54it/s] 93%|█████████▎| 841/900 [05:05&lt;00:38,  1.54it/s] 94%|█████████▎| 842/900 [05:06&lt;00:37,  1.54it/s] 94%|█████████▎| 843/900 [05:06&lt;00:37,  1.53it/s] 94%|█████████▍| 844/900 [05:07&lt;00:36,  1.53it/s] 94%|█████████▍| 845/900 [05:08&lt;00:35,  1.53it/s] 94%|█████████▍| 846/900 [05:08&lt;00:35,  1.53it/s] 94%|█████████▍| 847/900 [05:09&lt;00:34,  1.53it/s] 94%|█████████▍| 848/900 [05:10&lt;00:34,  1.52it/s] 94%|█████████▍| 849/900 [05:10&lt;00:33,  1.52it/s] 94%|█████████▍| 850/900 [05:11&lt;00:32,  1.52it/s] 95%|█████████▍| 851/900 [05:12&lt;00:32,  1.52it/s] 95%|█████████▍| 852/900 [05:12&lt;00:31,  1.52it/s] 95%|█████████▍| 853/900 [05:13&lt;00:31,  1.50it/s] 95%|█████████▍| 854/900 [05:14&lt;00:30,  1.50it/s] 95%|█████████▌| 855/900 [05:14&lt;00:29,  1.51it/s] 95%|█████████▌| 856/900 [05:15&lt;00:29,  1.51it/s] 95%|█████████▌| 857/900 [05:16&lt;00:28,  1.50it/s] 95%|█████████▌| 858/900 [05:16&lt;00:28,  1.50it/s] 95%|█████████▌| 859/900 [05:17&lt;00:27,  1.50it/s] 96%|█████████▌| 860/900 [05:18&lt;00:26,  1.50it/s] 96%|█████████▌| 861/900 [05:18&lt;00:25,  1.50it/s] 96%|█████████▌| 862/900 [05:19&lt;00:25,  1.50it/s] 96%|█████████▌| 863/900 [05:20&lt;00:24,  1.50it/s] 96%|█████████▌| 864/900 [05:20&lt;00:24,  1.50it/s] 96%|█████████▌| 865/900 [05:21&lt;00:23,  1.50it/s] 96%|█████████▌| 866/900 [05:22&lt;00:22,  1.48it/s] 96%|█████████▋| 867/900 [05:22&lt;00:23,  1.43it/s] 96%|█████████▋| 868/900 [05:23&lt;00:22,  1.44it/s] 97%|█████████▋| 869/900 [05:24&lt;00:21,  1.45it/s] 97%|█████████▋| 870/900 [05:24&lt;00:20,  1.44it/s] 97%|█████████▋| 871/900 [05:25&lt;00:20,  1.45it/s] 97%|█████████▋| 872/900 [05:26&lt;00:19,  1.45it/s] 97%|█████████▋| 873/900 [05:26&lt;00:18,  1.44it/s] 97%|█████████▋| 874/900 [05:27&lt;00:18,  1.43it/s] 97%|█████████▋| 875/900 [05:28&lt;00:17,  1.43it/s] 97%|█████████▋| 876/900 [05:29&lt;00:16,  1.43it/s] 97%|█████████▋| 877/900 [05:29&lt;00:15,  1.44it/s] 98%|█████████▊| 878/900 [05:30&lt;00:15,  1.45it/s] 98%|█████████▊| 879/900 [05:31&lt;00:14,  1.45it/s] 98%|█████████▊| 880/900 [05:31&lt;00:13,  1.44it/s] 98%|█████████▊| 881/900 [05:32&lt;00:13,  1.43it/s] 98%|█████████▊| 882/900 [05:33&lt;00:12,  1.44it/s] 98%|█████████▊| 883/900 [05:33&lt;00:11,  1.44it/s] 98%|█████████▊| 884/900 [05:34&lt;00:11,  1.43it/s] 98%|█████████▊| 885/900 [05:35&lt;00:10,  1.41it/s] 98%|█████████▊| 886/900 [05:36&lt;00:09,  1.43it/s] 99%|█████████▊| 887/900 [05:36&lt;00:09,  1.42it/s] 99%|█████████▊| 888/900 [05:37&lt;00:08,  1.43it/s] 99%|█████████▉| 889/900 [05:38&lt;00:07,  1.42it/s] 99%|█████████▉| 890/900 [05:38&lt;00:06,  1.43it/s] 99%|█████████▉| 891/900 [05:39&lt;00:06,  1.43it/s] 99%|█████████▉| 892/900 [05:40&lt;00:05,  1.43it/s] 99%|█████████▉| 893/900 [05:40&lt;00:04,  1.44it/s] 99%|█████████▉| 894/900 [05:41&lt;00:04,  1.44it/s] 99%|█████████▉| 895/900 [05:42&lt;00:03,  1.44it/s]100%|█████████▉| 896/900 [05:42&lt;00:02,  1.44it/s]100%|█████████▉| 897/900 [05:43&lt;00:02,  1.45it/s]100%|█████████▉| 898/900 [05:44&lt;00:01,  1.45it/s]100%|█████████▉| 899/900 [05:45&lt;00:00,  1.45it/s]100%|██████████| 900/900 [05:45&lt;00:00,  1.45it/s]100%|██████████| 900/900 [05:45&lt;00:00,  2.60it/s]\n\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(M_values, var_bs_values, marker='o', color='red')\nplt.xlabel('Nombre de simulations')\nplt.ylabel('VaR bootstrap')\nplt.title(\"Variation de la VaR bootstrap en fonction du nombre de simulations\")\n\nText(0.5, 1.0, 'Variation de la VaR bootstrap en fonction du nombre de simulations')\n\n\n\n\n\n\n\n\n\nPour la taille de l’echantillon bootstrap, nous allons prendre M=8000 étant donné que la courbe semble se stabiliser à partir de cette valeur. Avec ce choix, la VaR estimé est de 4.11%% avec un intervalle de confiance à 5% de [4.05%, 4.09%]. De plus, en ce qui concerne la VaR historique, nous constatons que l’estimation est contenu dans l’intervalle de confiance.\n\nM=8000\nvar_bs_train, lower_ic,upper_ic = bootstrap_var(data_train, alpha=alpha, M=M)\nprint(f\"La VaR bootstrap pour h=1j et alpha=0.99 est : {var_bs_train:.4%}\")\nprint(f\"L'intervalle de confiance est : [{lower_ic:.4%}, {upper_ic:.4%}]\")\n\nLa VaR bootstrap pour h=1j et alpha=0.99 est : 4.1065%\nL'intervalle de confiance est : [4.0536%, 4.0939%]\n\n\n\n\nII.2.3. Backtest\nPour l’exercice de backtest, il s’agit de : 1. Déterminer si la proportion \\(p\\) de violations de la VaR est cohérente avec le niveau de confiance, i.e. égale à \\(1-\\alpha\\). Cela permet de vérifier si la mesure de risque est bien calibrée. Pour cela, nous pouvons avoir recours à un test de proportion ou un test de ratio de vraisemblance.\n**Unconditional coverage test** :\nSoit I la variable indicatrice de violation de la VaR, i.e. $I=1$ si la perte est supérieure à la VaR, et $I=0$ sinon. La proportion de violations de la VaR est donnée par :\n\n$$p = \\frac{1}{n} \\sum_{i=1}^{n} I_i = \\frac{Z}{n}$$\n\nSous H0, i.e. p=1-$\\alpha$, Z $\\sim$ Binomiale(n, 1-$\\alpha$). En supposant que n est suffisamment grand, on peut approximer Z par une loi normale. Ainsi donc :\n$$\\frac{Z - n (1-\\alpha)}{\\sqrt{\\alpha (1-\\alpha) n}} \\sim \\mathcal{N}(n(1-\\alpha), n\\alpha(1-\\alpha))$$\n\nSous cette hypothèse asymptotique, on peut calculer la statistique du ratio de vraisemblance suivant :\n$$LR = -2 ln \\left( \\frac{L(H1)}{L(H0)} \\right) =-2 ln \\left( 1- (1-\\alpha))^{n-e}(1-\\alpha)^e \\right) + 2 ln \\left( (1-\\frac{e}{n})^{n-e} (\\frac{e}{n})^e  \\right)  \\sim \\chi^2(1)$$\n\noù e est le nombre de violations de la VaR. On rejette H0 si LR &gt; $\\chi^2(1-\\alpha)$.\n\n**Test de proportion** :\n$$\nH_0 : p = p_0 = 1-\\alpha \\\\ H_1 : p &gt; 1-\\alpha\n$$\nOn peut également utiliser un test binomial pour tester si la proportion de violations de la VaR est égale à $1-\\alpha$. On peut calculer la statistique du test suivant :\n$$Z = \\frac{p - p_0}{\\sqrt{p_0 (1-p_0) / n}} \\sim \\mathcal{N}(0,1)$$\n\nOn rejette H0 si Z &gt; $p_0+ \\phi^{-1}(1-\\alpha) \\sqrt{p_0 (1-p_0)/n}$, où $\\phi$ est la quantile de la loi normale standard.\n\nDéterminer si, lorsqu’il y en a, les violations de VaR à deux différents jours sont indépendantes. Cela permet si la mesure de risque est capable de réagir aux chocs de marché affectant la volatilité des rendements. Pour cela, nous utilisons un conditional coverage test.\nConditional coverage test : y revenir\n\n\nimport scipy.stats as stats\n\n# Objectif : implémenter une fonction calculant le nombre d'exception sur l'échantillon test\ndef exceptions(data, var):\n    \"\"\"\n    Calcul du nombre d'exception\n    data : les rendements logarithmiques\n    var : la VaR\n    \"\"\"\n    return np.sum(data &lt; -var)\n\n\n# Objectif : test de proportion binomiale\n\ndef binomial_test(n, p, p0 = 0.01, alpha=0.05):\n    \"\"\"\n    Test de proportion binomiale\n    H0 : p = p0\n    H1 : p &gt; p0\n    n : le nombre d'essais\n    p : la proportion\n    alpha : le niveau de confiance\n    \"\"\"\n\n    z = (p - p0) / np.sqrt(p0 * (1 - p0) / n)\n    #reject_zone = p0 + stats.norm.ppf(1 - alpha) * np.sqrt(p0 * (1 - p0) / n)\n    p_value = 1 - stats.norm.cdf(z)\n    reject = p_value &lt; alpha\n\n    # Calcul des IC\n    lower = p - stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n    upper = p + stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n\n    return p_value, reject, lower, upper\n\n# Unconditionnal coverage test ==&gt; to do.\n\n\n# Backtest de la VaR historique\n\nexceptions_test = exceptions(data_test, var_hist_train)\nprint(f\"Le nombre d'exceptions sur l'échantillon de test est : {exceptions_test}\")\n\nprint(\"=\"*80)\nn = len(data_test)\np = exceptions_test / n\np_value, reject, lower,upper = binomial_test(n, p)\nprint(f\"H0 : le nombre d'exceptions est inférieur ou égale à {1-alpha:.2%}\")\nprint(f\"IC : [{lower:.2%},{upper:.2%}]\")\nprint(f\"La p-value du test de proportion binomiale est : {p_value:.4f}\")\nprint(f\"Rejet de l'hypothèse nulle : {reject}\")\nprint(\"=\"*80)\n\nLe nombre d'exceptions sur l'échantillon de test est : 0\n================================================================================\nH0 : le nombre d'exceptions est inférieur ou égale à 1.00%\nIC : [0.00%,0.00%]\nLa p-value du test de proportion binomiale est : 0.9925\nRejet de l'hypothèse nulle : False\n================================================================================"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.3.-var-paramétrique",
    "href": "3A/value-at-risk/var_classiques.html#ii.3.-var-paramétrique",
    "title": "TP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.3. VaR paramétrique",
    "text": "II.3. VaR paramétrique\n\nII.3.1. Validation ex-ante\nVisuellement, les données ne semblent pas suivre une loi normale. En effet, les quantiles théoriques d’une loi normale ne collent pas avec les quantiles empiriques des rendements. Cela peut être dû à la présence de queues épaisses observables sur l’estimation de la densité des rendements sur l’échantillon d’apprentissage, de pics, de clusters de volatilité que nous avons observées plus haut. De plus, le skewness est négatif ce qui indique une asymétrie négative des rendements. Enfin, le kurtosis est supérieur à 3, ce qui indique une distribution leptokurtique des rendements.\nNous allons tout de même implémenter une VaR gaussienne pour voir comment elle se comporte dans le backtest.\n\n# Test visuel d'adéquation de la loi normale\n\n# Créer un Q-Q plot\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(data_train, dist=\"norm\", plot=ax)\n\n# Personnalisation du graphique\nax.set_title(\"Q-Q Plot (Normal Distribution)\")\nax.set_xlabel(\"Theoretical Quantiles\")\nax.set_ylabel(\"Sample Quantiles\")\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\n# Densité de l'echantillon train et l'échantillon de test\n\nplt.figure(figsize=(10, 4))\ndata_train.plot(kind='kde', label='Train', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Densité de l'échantillon d'entrainement\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Skewness et kurtosis\nprint(\"=\"*80)\nprint(\"Skewness de l'échantillon d'entrainement : \", data_train.skew())\nprint(\"Kurtosis de l'échantillon d'entrainement : \", data_train.kurt())\nprint(\"=\"*80)\n\n================================================================================\nSkewness de l'échantillon d'entrainement :  -0.2981820421484688\nKurtosis de l'échantillon d'entrainement :  7.353960005618779\n================================================================================\n\n\n\nfrom scipy.stats import kstest\n\n# Test de Kolmogorov-Smirnov\nks_stat, ks_p_value = kstest(data_train, 'norm')\nprint(\"=\"*80)\nprint(\"H0 : Les données suivent une loi normale\")\nprint(f\"Statistique de test : {ks_stat:.4f}\")\nprint(f\"P-value : {ks_p_value:.4f}\")\nprint(\"=\"*80)\n\n================================================================================\nH0 : Les données suivent une loi normale\nStatistique de test : 0.4775\nP-value : 0.0000\n================================================================================\n\n\n\n\nII.3.2. Implémentation de la VaR\n\na. Méthode scaling\n\n# Objectif : écrire une fonction qui calcule la VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n\nvar_gauss_train = gaussian_var(data_train, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=1j et alpha={alpha} est : {var_gauss_train:.4%}\")\nprint(f\"La VaR historique pour h=10j et alpha={alpha} est : {np.sqrt(10)*var_gauss_train:.4%}\")\n\nLa VaR gaussienne pour h=1j et alpha=0.99 est : 3.2302%\nLa VaR historique pour h=10j et alpha=0.99 est : 10.2148%\n\n\n\n\nb. Méthode de diffusion d’un actif\nPour calculer la VaR gaussienne à 10 jours par méthode de diffusion d’un actif, nous allons suivre les étapes suivantes :\nL’évolution du prix d’un actif suit un processus de type mouvement brownien géométrique : \\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t &lt;=&gt; S_t = S_{t-1} e^{(\\mu - \\frac{1}{2} \\sigma^2) + \\sigma W_t}\n\\] où $ S_t$ est le prix de l’actif à l’instant$ t$, $ $ est le rendement moyen estimé (drift), $ $ est la volatilité du rendement, $ dW_t$ est un mouvement brownien standard.\nOn peut de ce fait calculer plusieurs trajectoires de rendements de \\(S_0\\) et \\(S_{10}\\), puis calculer la VaR à partir de la série des rendements $ r_{10j} = (S_{10} / S_{0}) $ obtenus avec ces trajectoires.\nEn utilisant la méthode de scaling et la méthode de diffusion, nous obtenons sensiblement la même VaR.\n\nimport numpy as np\n\nmu = np.mean(data_train)\nsigma = np.std(data_train)\n\nreturn_sim = []\nT = 10\nM = 1000\nS0 = train['Close'].iloc[-1]\n\nfor i in range(M):\n    S = np.zeros(T+1)\n    S[0] = S0\n    for t in range(1,T+1):\n        Wt = np.random.normal()\n        S[t] = S[t-1] * np.exp((mu - 0.5 * sigma**2) + sigma * Wt)\n\n    return_sim.append(np.log(S[T]/S0))\n\nvar_gauss_diffusion = gaussian_var(return_sim, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=10j et alpha={alpha} est : {var_gauss_diffusion:.4%}\")\n\nLa VaR gaussienne pour h=10j et alpha=0.99 est : 10.0053%\n\n\n\n\nc. Méthode EWMA\nLa VaR EWMA est une méthode qui permet de calculer la VaR en surpondérant les rendements les plus récents. Cela permet de donner plus de poids aux rendements les plus récents, et donc de mieux capturer les changements de volatilité. La VaR EWMA est donnée par la formule suivante :\n\\[\nVaR_{t+1} = \\mu + \\sigma \\times z_{\\alpha}\n\\]\n\n# VaR la méthode EWMA (Exponential Weighting Moving Average)\n\ndef gaussian_var_ewma(data, alpha, lambda_=0.94):\n    \"\"\"\n    Calcul de la VaR gaussienne EWMA\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    lambda_ : le paramètre de lissage\n    \"\"\"\n\n    weights = np.array([(1-lambda_)*(lambda_**i) for i in range(len(data))])\n    weights = weights / np.sum(weights)\n\n    mu = np.sum(data[::-1] * weights)\n    sigma = np.sqrt(np.sum(weights * (data[::-1] - mu)**2))\n\n    return -(mu + sigma * stats.norm.ppf(1 - alpha)), mu, sigma\n\n# y revenir\n\n\nlambdas = [0.9, 0.95, 0.99]\nalpha = 0.99\nimport scipy.stats as stats\nfor l in lambdas:\n    print(\"=\"*80)\n    print(\"Lambda : \", l)\n    print(\"-\"*15)\n    var_gauss_emwa, mu, sigma = gaussian_var_ewma(data_train, alpha=alpha, lambda_=l)\n    print(f\"La VaR gaussienne EWMA pour h=1j et alpha={alpha} est : {var_gauss_emwa:.4%}\")\n    print(f\"La moyenne est : {mu:.4%}\")\n    print(f\"L'écart-type est : {sigma:.4%}\")\n\n    n_exceptions = exceptions(data_test, var_gauss_emwa)\n    print(f\"Le nombre d'exceptions sur l'échantillon de test est : {n_exceptions}\")\n\n================================================================================\nLambda :  0.9\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.3751%\nLa moyenne est : 0.1898%\nL'écart-type est : 1.1025%\nLe nombre d'exceptions sur l'échantillon de test est : 5\n================================================================================\nLambda :  0.95\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.8969%\nLa moyenne est : 0.0735%\nL'écart-type est : 1.2768%\nLe nombre d'exceptions sur l'échantillon de test est : 4\n================================================================================\nLambda :  0.99\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 3.3511%\nLa moyenne est : -0.0321%\nL'écart-type est : 1.4267%\nLe nombre d'exceptions sur l'échantillon de test est : 1\n\n\nAvec la méthode EWMA, nous observons une que la VaR diminue plus \\(\\lambda\\) augmente. Cela est dû au fait que plus \\(\\lambda\\) est grand, plus les rendements les plus récents sont surpondérés, et donc la volatilité est plus faible, en raison de la fin de la période d’apprentissage."
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "href": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "title": "TP1:Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.4. VaR skew-student",
    "text": "II.4. VaR skew-student\n\nII.4.1. Validation ex-ante\n\n# Ecrire une fonction permettant d’estimer les paramètres d’une loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n\n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les paramètres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n\n    return - loglik\n\n# Optimisation des paramètres avec contraintes de positivité sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des paramètres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(data_train)\nprint(\"=\"*80)\nprint(\"Les paramètres estimés de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0],\n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n================================================================================\nLes paramètres estimés de la loi de Skew Student sont : \n---------------\nMu :  0.0023251952411471218\nSigma :  0.008823931435233275\nGamma :  -0.23188477391476636\nNu :  2.9618199934116825\n================================================================================\n\n\n\n# Superposition de la densité théorique et des données\n\nx_values = np.linspace(min(data_train), max(data_train), 1000)\n\ntheoretical_density = skew_student_pdf(x_values, **params_sstd)\nplt.figure(figsize = (10,4))\nplt.hist(data_train, bins=30, density=True, alpha=0.5, label='Données empiriques')\nplt.plot(x_values, theoretical_density, label='Densité Skew Student', color='red')\n# Densité normale\nplt.plot(x_values, stats.norm.pdf(x_values, np.mean(data_train), np.std(data_train)), label='Densité normale', color='blue')\nplt.xlabel('Rendements logarithmiques')\nplt.ylabel('Densité')\nplt.title(\"Comparaison entre les données et la densité théorique d'une loi de Skew Student\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densité théorique de la skew-student semble bien s’ajuster aux données. La loi skew-student de paramètres (\\(\\mu = 0.002, \\sigma = 0.009, \\gamma = -0.23, \\nu = 2.96\\)). Le \\(\\mu\\) est le rendement moyen, \\(\\sigma\\) est l’écart-type, \\(\\gamma\\) est le coefficient d’asymétrie et \\(\\nu\\) est le degré de liberté. Le skewness est négatif, ce qui indique une asymétrie négative des rendements, comme ce qu’on a observé plus haut.\nNous allons appuyer cette validation en utilisant le QQ-plot. La fonction quantile d’une loi skew-student n’est pas analytique. Pour ce faire, nous allons construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l’inverse de cette fonction de repartition. Nous allons ensuite comparer les quantiles théoriques de la skew-student avec les quantiles empiriques des rendements.\nEn observant le QQ-plot, on constate que les quantiles théoriques de la skew-student collent bien avec les quantiles empiriques des rendements. Cela confirme que la skew-student est une bonne approximation de la distribution des rendements. Pour une validation plus rigoureuse, on peut utiliser un test de Kolmogorov-Smirnov pour tester si les rendements suivent une loi skew-student.\n\n## Intégration de la fonction de densité\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\nquantiles_empiriques = np.quantile(data_train, niveaux_quantiles)\nquantiles_theoriques = [skew_student_quantile(alpha,**params_sstd) for alpha in tqdm(niveaux_quantiles)]\n\n  0%|          | 0/999 [00:00&lt;?, ?it/s]  0%|          | 1/999 [00:00&lt;05:35,  2.97it/s]  0%|          | 2/999 [00:00&lt;05:26,  3.05it/s]  0%|          | 3/999 [00:00&lt;05:30,  3.02it/s]  0%|          | 4/999 [00:01&lt;06:18,  2.63it/s]  1%|          | 5/999 [00:01&lt;06:32,  2.53it/s]  1%|          | 6/999 [00:02&lt;06:37,  2.50it/s]  1%|          | 7/999 [00:02&lt;06:34,  2.52it/s]  1%|          | 8/999 [00:03&lt;06:49,  2.42it/s]  1%|          | 9/999 [00:03&lt;06:48,  2.43it/s]  1%|          | 10/999 [00:03&lt;06:58,  2.36it/s]  1%|          | 11/999 [00:04&lt;06:43,  2.45it/s]  1%|          | 12/999 [00:04&lt;06:49,  2.41it/s]  1%|▏         | 13/999 [00:05&lt;06:58,  2.36it/s]  1%|▏         | 14/999 [00:05&lt;07:08,  2.30it/s]  2%|▏         | 15/999 [00:06&lt;06:57,  2.36it/s]  2%|▏         | 16/999 [00:06&lt;06:59,  2.34it/s]  2%|▏         | 17/999 [00:06&lt;06:54,  2.37it/s]  2%|▏         | 18/999 [00:07&lt;06:56,  2.36it/s]  2%|▏         | 19/999 [00:07&lt;06:58,  2.34it/s]  2%|▏         | 20/999 [00:08&lt;06:46,  2.41it/s]  2%|▏         | 21/999 [00:08&lt;06:44,  2.42it/s]  2%|▏         | 22/999 [00:09&lt;06:54,  2.36it/s]  2%|▏         | 23/999 [00:09&lt;06:52,  2.37it/s]  2%|▏         | 24/999 [00:09&lt;06:45,  2.41it/s]  3%|▎         | 25/999 [00:10&lt;06:50,  2.37it/s]  3%|▎         | 26/999 [00:10&lt;07:13,  2.25it/s]  3%|▎         | 27/999 [00:11&lt;07:05,  2.29it/s]  3%|▎         | 28/999 [00:11&lt;06:59,  2.31it/s]  3%|▎         | 29/999 [00:12&lt;07:03,  2.29it/s]  3%|▎         | 30/999 [00:12&lt;06:59,  2.31it/s]  3%|▎         | 31/999 [00:12&lt;06:58,  2.32it/s]  3%|▎         | 32/999 [00:13&lt;07:06,  2.26it/s]  3%|▎         | 33/999 [00:13&lt;07:24,  2.17it/s]  3%|▎         | 34/999 [00:14&lt;07:34,  2.12it/s]  4%|▎         | 35/999 [00:14&lt;07:34,  2.12it/s]  4%|▎         | 36/999 [00:15&lt;07:39,  2.10it/s]  4%|▎         | 37/999 [00:15&lt;07:42,  2.08it/s]  4%|▍         | 38/999 [00:16&lt;07:58,  2.01it/s]  4%|▍         | 39/999 [00:16&lt;08:08,  1.96it/s]  4%|▍         | 40/999 [00:17&lt;07:59,  2.00it/s]  4%|▍         | 41/999 [00:17&lt;07:53,  2.02it/s]  4%|▍         | 42/999 [00:18&lt;07:53,  2.02it/s]  4%|▍         | 43/999 [00:18&lt;07:49,  2.04it/s]  4%|▍         | 44/999 [00:19&lt;07:44,  2.06it/s]  5%|▍         | 45/999 [00:19&lt;07:43,  2.06it/s]  5%|▍         | 46/999 [00:20&lt;07:46,  2.04it/s]  5%|▍         | 47/999 [00:20&lt;07:40,  2.07it/s]  5%|▍         | 48/999 [00:21&lt;07:50,  2.02it/s]  5%|▍         | 49/999 [00:21&lt;08:00,  1.98it/s]  5%|▌         | 50/999 [00:22&lt;07:49,  2.02it/s]  5%|▌         | 51/999 [00:22&lt;07:48,  2.02it/s]  5%|▌         | 52/999 [00:23&lt;08:03,  1.96it/s]  5%|▌         | 53/999 [00:23&lt;08:03,  1.96it/s]  5%|▌         | 54/999 [00:24&lt;07:50,  2.01it/s]  6%|▌         | 55/999 [00:24&lt;07:59,  1.97it/s]  6%|▌         | 56/999 [00:25&lt;07:53,  1.99it/s]  6%|▌         | 57/999 [00:25&lt;07:58,  1.97it/s]  6%|▌         | 58/999 [00:26&lt;08:02,  1.95it/s]  6%|▌         | 59/999 [00:26&lt;07:43,  2.03it/s]  6%|▌         | 60/999 [00:27&lt;07:45,  2.02it/s]  6%|▌         | 61/999 [00:27&lt;08:01,  1.95it/s]  6%|▌         | 62/999 [00:28&lt;08:07,  1.92it/s]  6%|▋         | 63/999 [00:28&lt;08:08,  1.92it/s]  6%|▋         | 64/999 [00:29&lt;08:07,  1.92it/s]  7%|▋         | 65/999 [00:29&lt;07:53,  1.97it/s]  7%|▋         | 66/999 [00:30&lt;07:41,  2.02it/s]  7%|▋         | 67/999 [00:30&lt;07:55,  1.96it/s]  7%|▋         | 68/999 [00:31&lt;07:41,  2.02it/s]  7%|▋         | 69/999 [00:31&lt;07:47,  1.99it/s]  7%|▋         | 70/999 [00:32&lt;07:38,  2.03it/s]  7%|▋         | 71/999 [00:32&lt;07:47,  1.99it/s]  7%|▋         | 72/999 [00:33&lt;07:53,  1.96it/s]  7%|▋         | 73/999 [00:34&lt;07:58,  1.94it/s]  7%|▋         | 74/999 [00:34&lt;07:51,  1.96it/s]  8%|▊         | 75/999 [00:35&lt;07:59,  1.93it/s]  8%|▊         | 76/999 [00:35&lt;08:03,  1.91it/s]  8%|▊         | 77/999 [00:36&lt;07:51,  1.95it/s]  8%|▊         | 78/999 [00:36&lt;08:02,  1.91it/s]  8%|▊         | 79/999 [00:37&lt;08:06,  1.89it/s]  8%|▊         | 80/999 [00:37&lt;08:01,  1.91it/s]  8%|▊         | 81/999 [00:38&lt;07:47,  1.96it/s]  8%|▊         | 82/999 [00:38&lt;07:46,  1.97it/s]  8%|▊         | 83/999 [00:39&lt;07:37,  2.00it/s]  8%|▊         | 84/999 [00:39&lt;07:39,  1.99it/s]  9%|▊         | 85/999 [00:40&lt;07:41,  1.98it/s]  9%|▊         | 86/999 [00:40&lt;07:39,  1.99it/s]  9%|▊         | 87/999 [00:41&lt;07:42,  1.97it/s]  9%|▉         | 88/999 [00:41&lt;07:31,  2.02it/s]  9%|▉         | 89/999 [00:42&lt;07:26,  2.04it/s]  9%|▉         | 90/999 [00:42&lt;07:11,  2.11it/s]  9%|▉         | 91/999 [00:43&lt;07:16,  2.08it/s]  9%|▉         | 92/999 [00:43&lt;07:15,  2.08it/s]  9%|▉         | 93/999 [00:43&lt;06:51,  2.20it/s]  9%|▉         | 94/999 [00:44&lt;07:07,  2.11it/s] 10%|▉         | 95/999 [00:44&lt;07:23,  2.04it/s] 10%|▉         | 96/999 [00:45&lt;07:25,  2.03it/s] 10%|▉         | 97/999 [00:45&lt;07:35,  1.98it/s] 10%|▉         | 98/999 [00:46&lt;07:38,  1.96it/s] 10%|▉         | 99/999 [00:47&lt;07:37,  1.97it/s] 10%|█         | 100/999 [00:47&lt;07:47,  1.92it/s] 10%|█         | 101/999 [00:48&lt;07:42,  1.94it/s] 10%|█         | 102/999 [00:48&lt;07:54,  1.89it/s] 10%|█         | 103/999 [00:49&lt;07:43,  1.93it/s] 10%|█         | 104/999 [00:49&lt;07:34,  1.97it/s] 11%|█         | 105/999 [00:50&lt;07:23,  2.01it/s] 11%|█         | 106/999 [00:50&lt;07:29,  1.99it/s] 11%|█         | 107/999 [00:51&lt;07:27,  1.99it/s] 11%|█         | 108/999 [00:51&lt;07:42,  1.92it/s] 11%|█         | 109/999 [00:52&lt;07:42,  1.92it/s] 11%|█         | 110/999 [00:52&lt;07:36,  1.95it/s] 11%|█         | 111/999 [00:53&lt;07:37,  1.94it/s] 11%|█         | 112/999 [00:53&lt;07:46,  1.90it/s] 11%|█▏        | 113/999 [00:54&lt;07:52,  1.88it/s] 11%|█▏        | 114/999 [00:54&lt;07:46,  1.90it/s] 12%|█▏        | 115/999 [00:55&lt;07:46,  1.90it/s] 12%|█▏        | 116/999 [00:55&lt;07:40,  1.92it/s] 12%|█▏        | 117/999 [00:56&lt;07:42,  1.91it/s] 12%|█▏        | 118/999 [00:56&lt;07:43,  1.90it/s] 12%|█▏        | 119/999 [00:57&lt;07:48,  1.88it/s] 12%|█▏        | 120/999 [00:57&lt;07:39,  1.91it/s] 12%|█▏        | 121/999 [00:58&lt;07:43,  1.90it/s] 12%|█▏        | 122/999 [00:59&lt;08:00,  1.83it/s] 12%|█▏        | 123/999 [00:59&lt;07:57,  1.83it/s] 12%|█▏        | 124/999 [01:00&lt;08:07,  1.80it/s] 13%|█▎        | 125/999 [01:00&lt;08:03,  1.81it/s] 13%|█▎        | 126/999 [01:01&lt;08:09,  1.78it/s] 13%|█▎        | 127/999 [01:01&lt;08:04,  1.80it/s] 13%|█▎        | 128/999 [01:02&lt;07:53,  1.84it/s] 13%|█▎        | 129/999 [01:02&lt;07:47,  1.86it/s] 13%|█▎        | 130/999 [01:03&lt;07:50,  1.85it/s] 13%|█▎        | 131/999 [01:04&lt;07:48,  1.85it/s] 13%|█▎        | 132/999 [01:04&lt;07:37,  1.89it/s] 13%|█▎        | 133/999 [01:04&lt;07:29,  1.93it/s] 13%|█▎        | 134/999 [01:05&lt;07:19,  1.97it/s] 14%|█▎        | 135/999 [01:06&lt;07:23,  1.95it/s] 14%|█▎        | 136/999 [01:06&lt;07:29,  1.92it/s] 14%|█▎        | 137/999 [01:07&lt;07:33,  1.90it/s] 14%|█▍        | 138/999 [01:07&lt;07:24,  1.94it/s] 14%|█▍        | 139/999 [01:08&lt;07:39,  1.87it/s] 14%|█▍        | 140/999 [01:08&lt;07:44,  1.85it/s] 14%|█▍        | 141/999 [01:09&lt;07:38,  1.87it/s] 14%|█▍        | 142/999 [01:09&lt;07:29,  1.91it/s] 14%|█▍        | 143/999 [01:10&lt;07:19,  1.95it/s] 14%|█▍        | 144/999 [01:10&lt;07:12,  1.98it/s] 15%|█▍        | 145/999 [01:11&lt;07:07,  2.00it/s] 15%|█▍        | 146/999 [01:11&lt;07:05,  2.00it/s] 15%|█▍        | 147/999 [01:12&lt;07:16,  1.95it/s] 15%|█▍        | 148/999 [01:12&lt;07:45,  1.83it/s] 15%|█▍        | 149/999 [01:13&lt;07:45,  1.83it/s] 15%|█▌        | 150/999 [01:14&lt;07:54,  1.79it/s] 15%|█▌        | 151/999 [01:14&lt;07:58,  1.77it/s] 15%|█▌        | 152/999 [01:15&lt;07:43,  1.83it/s] 15%|█▌        | 153/999 [01:15&lt;07:46,  1.81it/s] 15%|█▌        | 154/999 [01:16&lt;07:46,  1.81it/s] 16%|█▌        | 155/999 [01:16&lt;07:54,  1.78it/s] 16%|█▌        | 156/999 [01:17&lt;08:07,  1.73it/s] 16%|█▌        | 157/999 [01:18&lt;08:14,  1.70it/s] 16%|█▌        | 158/999 [01:18&lt;07:59,  1.75it/s] 16%|█▌        | 159/999 [01:19&lt;07:58,  1.75it/s] 16%|█▌        | 160/999 [01:19&lt;08:07,  1.72it/s] 16%|█▌        | 161/999 [01:20&lt;08:05,  1.73it/s] 16%|█▌        | 162/999 [01:20&lt;08:09,  1.71it/s] 16%|█▋        | 163/999 [01:21&lt;08:13,  1.69it/s] 16%|█▋        | 164/999 [01:22&lt;08:23,  1.66it/s] 17%|█▋        | 165/999 [01:22&lt;08:22,  1.66it/s] 17%|█▋        | 166/999 [01:23&lt;08:20,  1.67it/s] 17%|█▋        | 167/999 [01:23&lt;08:19,  1.66it/s] 17%|█▋        | 168/999 [01:24&lt;08:19,  1.66it/s] 17%|█▋        | 169/999 [01:25&lt;08:08,  1.70it/s] 17%|█▋        | 170/999 [01:25&lt;08:12,  1.68it/s] 17%|█▋        | 171/999 [01:26&lt;08:05,  1.70it/s] 17%|█▋        | 172/999 [01:26&lt;08:20,  1.65it/s] 17%|█▋        | 173/999 [01:27&lt;08:15,  1.67it/s] 17%|█▋        | 174/999 [01:28&lt;08:06,  1.69it/s] 18%|█▊        | 175/999 [01:28&lt;08:11,  1.68it/s] 18%|█▊        | 176/999 [01:29&lt;08:13,  1.67it/s] 18%|█▊        | 177/999 [01:29&lt;08:23,  1.63it/s] 18%|█▊        | 178/999 [01:30&lt;08:15,  1.66it/s] 18%|█▊        | 179/999 [01:31&lt;08:13,  1.66it/s] 18%|█▊        | 180/999 [01:31&lt;08:03,  1.69it/s] 18%|█▊        | 181/999 [01:32&lt;08:05,  1.68it/s] 18%|█▊        | 182/999 [01:32&lt;08:02,  1.69it/s] 18%|█▊        | 183/999 [01:33&lt;08:23,  1.62it/s] 18%|█▊        | 184/999 [01:34&lt;08:12,  1.66it/s] 19%|█▊        | 185/999 [01:34&lt;08:20,  1.63it/s] 19%|█▊        | 186/999 [01:35&lt;08:34,  1.58it/s] 19%|█▊        | 187/999 [01:36&lt;08:31,  1.59it/s] 19%|█▉        | 188/999 [01:36&lt;08:38,  1.56it/s] 19%|█▉        | 189/999 [01:37&lt;08:30,  1.59it/s] 19%|█▉        | 190/999 [01:37&lt;08:26,  1.60it/s] 19%|█▉        | 191/999 [01:38&lt;08:10,  1.65it/s] 19%|█▉        | 192/999 [01:39&lt;08:18,  1.62it/s] 19%|█▉        | 193/999 [01:39&lt;08:03,  1.67it/s] 19%|█▉        | 194/999 [01:40&lt;07:45,  1.73it/s] 20%|█▉        | 195/999 [01:40&lt;07:36,  1.76it/s] 20%|█▉        | 196/999 [01:41&lt;07:40,  1.74it/s] 20%|█▉        | 197/999 [01:41&lt;07:50,  1.70it/s] 20%|█▉        | 198/999 [01:42&lt;07:42,  1.73it/s] 20%|█▉        | 199/999 [01:43&lt;07:36,  1.75it/s] 20%|██        | 200/999 [01:43&lt;07:51,  1.69it/s] 20%|██        | 201/999 [01:44&lt;07:43,  1.72it/s] 20%|██        | 202/999 [01:44&lt;07:41,  1.73it/s] 20%|██        | 203/999 [01:45&lt;07:36,  1.74it/s] 20%|██        | 204/999 [01:45&lt;07:13,  1.83it/s] 21%|██        | 205/999 [01:46&lt;07:06,  1.86it/s] 21%|██        | 206/999 [01:46&lt;07:08,  1.85it/s] 21%|██        | 207/999 [01:47&lt;07:24,  1.78it/s] 21%|██        | 208/999 [01:48&lt;07:26,  1.77it/s] 21%|██        | 209/999 [01:48&lt;07:48,  1.69it/s] 21%|██        | 210/999 [01:49&lt;07:58,  1.65it/s] 21%|██        | 211/999 [01:50&lt;08:06,  1.62it/s] 21%|██        | 212/999 [01:50&lt;08:02,  1.63it/s] 21%|██▏       | 213/999 [01:51&lt;08:08,  1.61it/s] 21%|██▏       | 214/999 [01:51&lt;07:57,  1.64it/s] 22%|██▏       | 215/999 [01:52&lt;07:56,  1.65it/s] 22%|██▏       | 216/999 [01:53&lt;07:50,  1.67it/s] 22%|██▏       | 217/999 [01:53&lt;07:39,  1.70it/s] 22%|██▏       | 218/999 [01:54&lt;07:40,  1.70it/s] 22%|██▏       | 219/999 [01:54&lt;07:33,  1.72it/s] 22%|██▏       | 220/999 [01:55&lt;07:37,  1.70it/s] 22%|██▏       | 221/999 [01:56&lt;07:41,  1.69it/s] 22%|██▏       | 222/999 [01:56&lt;07:31,  1.72it/s] 22%|██▏       | 223/999 [01:57&lt;07:21,  1.76it/s] 22%|██▏       | 224/999 [01:57&lt;07:26,  1.73it/s] 23%|██▎       | 225/999 [01:58&lt;07:42,  1.67it/s] 23%|██▎       | 226/999 [01:58&lt;07:40,  1.68it/s] 23%|██▎       | 227/999 [01:59&lt;07:26,  1.73it/s] 23%|██▎       | 228/999 [02:00&lt;07:27,  1.72it/s] 23%|██▎       | 229/999 [02:00&lt;07:17,  1.76it/s] 23%|██▎       | 230/999 [02:01&lt;07:30,  1.71it/s] 23%|██▎       | 231/999 [02:01&lt;07:26,  1.72it/s] 23%|██▎       | 232/999 [02:02&lt;07:28,  1.71it/s] 23%|██▎       | 233/999 [02:02&lt;07:33,  1.69it/s] 23%|██▎       | 234/999 [02:03&lt;07:29,  1.70it/s] 24%|██▎       | 235/999 [02:04&lt;07:28,  1.70it/s] 24%|██▎       | 236/999 [02:04&lt;07:32,  1.69it/s] 24%|██▎       | 237/999 [02:05&lt;07:37,  1.67it/s] 24%|██▍       | 238/999 [02:06&lt;07:55,  1.60it/s] 24%|██▍       | 239/999 [02:06&lt;08:02,  1.57it/s] 24%|██▍       | 240/999 [02:07&lt;08:10,  1.55it/s] 24%|██▍       | 241/999 [02:08&lt;08:06,  1.56it/s] 24%|██▍       | 242/999 [02:08&lt;08:12,  1.54it/s] 24%|██▍       | 243/999 [02:09&lt;08:23,  1.50it/s] 24%|██▍       | 244/999 [02:10&lt;08:25,  1.49it/s] 25%|██▍       | 245/999 [02:10&lt;08:22,  1.50it/s] 25%|██▍       | 246/999 [02:11&lt;08:28,  1.48it/s] 25%|██▍       | 247/999 [02:12&lt;08:23,  1.49it/s] 25%|██▍       | 248/999 [02:12&lt;08:31,  1.47it/s] 25%|██▍       | 249/999 [02:13&lt;08:32,  1.46it/s] 25%|██▌       | 250/999 [02:14&lt;08:32,  1.46it/s] 25%|██▌       | 251/999 [02:14&lt;08:28,  1.47it/s] 25%|██▌       | 252/999 [02:15&lt;08:24,  1.48it/s] 25%|██▌       | 253/999 [02:16&lt;08:32,  1.45it/s] 25%|██▌       | 254/999 [02:16&lt;08:45,  1.42it/s] 26%|██▌       | 255/999 [02:17&lt;08:30,  1.46it/s] 26%|██▌       | 256/999 [02:18&lt;08:19,  1.49it/s] 26%|██▌       | 257/999 [02:18&lt;08:08,  1.52it/s] 26%|██▌       | 258/999 [02:19&lt;08:10,  1.51it/s] 26%|██▌       | 259/999 [02:20&lt;07:59,  1.54it/s] 26%|██▌       | 260/999 [02:20&lt;07:56,  1.55it/s] 26%|██▌       | 261/999 [02:21&lt;07:49,  1.57it/s] 26%|██▌       | 262/999 [02:21&lt;07:32,  1.63it/s] 26%|██▋       | 263/999 [02:22&lt;07:30,  1.63it/s] 26%|██▋       | 264/999 [02:23&lt;07:42,  1.59it/s] 27%|██▋       | 265/999 [02:23&lt;07:42,  1.59it/s] 27%|██▋       | 266/999 [02:24&lt;07:35,  1.61it/s] 27%|██▋       | 267/999 [02:25&lt;07:33,  1.61it/s] 27%|██▋       | 268/999 [02:25&lt;07:45,  1.57it/s] 27%|██▋       | 269/999 [02:26&lt;07:42,  1.58it/s] 27%|██▋       | 270/999 [02:27&lt;07:48,  1.56it/s] 27%|██▋       | 271/999 [02:27&lt;07:38,  1.59it/s] 27%|██▋       | 272/999 [02:28&lt;07:45,  1.56it/s] 27%|██▋       | 273/999 [02:28&lt;07:49,  1.55it/s] 27%|██▋       | 274/999 [02:29&lt;07:44,  1.56it/s] 28%|██▊       | 275/999 [02:30&lt;07:48,  1.55it/s] 28%|██▊       | 276/999 [02:30&lt;07:48,  1.54it/s] 28%|██▊       | 277/999 [02:31&lt;07:40,  1.57it/s] 28%|██▊       | 278/999 [02:32&lt;07:41,  1.56it/s] 28%|██▊       | 279/999 [02:32&lt;07:41,  1.56it/s] 28%|██▊       | 280/999 [02:33&lt;07:54,  1.52it/s] 28%|██▊       | 281/999 [02:34&lt;07:56,  1.51it/s] 28%|██▊       | 282/999 [02:34&lt;08:04,  1.48it/s] 28%|██▊       | 283/999 [02:35&lt;08:11,  1.46it/s] 28%|██▊       | 284/999 [02:36&lt;08:23,  1.42it/s] 29%|██▊       | 285/999 [02:37&lt;08:33,  1.39it/s] 29%|██▊       | 286/999 [02:37&lt;08:23,  1.42it/s] 29%|██▊       | 287/999 [02:38&lt;08:30,  1.39it/s] 29%|██▉       | 288/999 [02:39&lt;08:16,  1.43it/s] 29%|██▉       | 289/999 [02:39&lt;08:29,  1.39it/s] 29%|██▉       | 290/999 [02:40&lt;08:27,  1.40it/s] 29%|██▉       | 291/999 [02:41&lt;08:21,  1.41it/s] 29%|██▉       | 292/999 [02:42&lt;08:17,  1.42it/s] 29%|██▉       | 293/999 [02:42&lt;08:11,  1.44it/s] 29%|██▉       | 294/999 [02:43&lt;08:09,  1.44it/s] 30%|██▉       | 295/999 [02:44&lt;08:12,  1.43it/s] 30%|██▉       | 296/999 [02:44&lt;08:21,  1.40it/s] 30%|██▉       | 297/999 [02:45&lt;08:23,  1.39it/s] 30%|██▉       | 298/999 [02:46&lt;08:34,  1.36it/s] 30%|██▉       | 299/999 [02:47&lt;08:17,  1.41it/s] 30%|███       | 300/999 [02:47&lt;08:17,  1.41it/s] 30%|███       | 301/999 [02:48&lt;08:22,  1.39it/s] 30%|███       | 302/999 [02:49&lt;08:19,  1.39it/s] 30%|███       | 303/999 [02:49&lt;08:19,  1.39it/s] 30%|███       | 304/999 [02:50&lt;08:16,  1.40it/s] 31%|███       | 305/999 [02:51&lt;08:14,  1.40it/s] 31%|███       | 306/999 [02:52&lt;08:04,  1.43it/s] 31%|███       | 307/999 [02:52&lt;07:59,  1.44it/s] 31%|███       | 308/999 [02:53&lt;07:49,  1.47it/s] 31%|███       | 309/999 [02:54&lt;07:55,  1.45it/s] 31%|███       | 310/999 [02:54&lt;08:00,  1.43it/s] 31%|███       | 311/999 [02:55&lt;08:03,  1.42it/s] 31%|███       | 312/999 [02:56&lt;08:11,  1.40it/s] 31%|███▏      | 313/999 [02:56&lt;08:11,  1.40it/s] 31%|███▏      | 314/999 [02:57&lt;08:04,  1.41it/s] 32%|███▏      | 315/999 [02:58&lt;08:06,  1.41it/s] 32%|███▏      | 316/999 [02:59&lt;08:05,  1.41it/s] 32%|███▏      | 317/999 [02:59&lt;08:03,  1.41it/s] 32%|███▏      | 318/999 [03:00&lt;07:56,  1.43it/s] 32%|███▏      | 319/999 [03:01&lt;08:07,  1.39it/s] 32%|███▏      | 320/999 [03:01&lt;08:11,  1.38it/s] 32%|███▏      | 321/999 [03:02&lt;08:24,  1.34it/s] 32%|███▏      | 322/999 [03:03&lt;08:10,  1.38it/s] 32%|███▏      | 323/999 [03:04&lt;08:09,  1.38it/s] 32%|███▏      | 324/999 [03:04&lt;08:10,  1.38it/s] 33%|███▎      | 325/999 [03:05&lt;07:55,  1.42it/s] 33%|███▎      | 326/999 [03:06&lt;07:52,  1.42it/s] 33%|███▎      | 327/999 [03:06&lt;08:04,  1.39it/s] 33%|███▎      | 328/999 [03:07&lt;07:56,  1.41it/s] 33%|███▎      | 329/999 [03:08&lt;07:56,  1.41it/s] 33%|███▎      | 330/999 [03:09&lt;07:52,  1.41it/s] 33%|███▎      | 331/999 [03:09&lt;08:01,  1.39it/s] 33%|███▎      | 332/999 [03:10&lt;07:52,  1.41it/s] 33%|███▎      | 333/999 [03:11&lt;07:51,  1.41it/s] 33%|███▎      | 334/999 [03:11&lt;07:47,  1.42it/s] 34%|███▎      | 335/999 [03:12&lt;07:43,  1.43it/s] 34%|███▎      | 336/999 [03:13&lt;07:50,  1.41it/s] 34%|███▎      | 337/999 [03:14&lt;07:47,  1.41it/s] 34%|███▍      | 338/999 [03:14&lt;07:55,  1.39it/s] 34%|███▍      | 339/999 [03:15&lt;07:43,  1.42it/s] 34%|███▍      | 340/999 [03:16&lt;07:35,  1.45it/s] 34%|███▍      | 341/999 [03:16&lt;07:41,  1.43it/s] 34%|███▍      | 342/999 [03:17&lt;07:35,  1.44it/s] 34%|███▍      | 343/999 [03:18&lt;07:34,  1.44it/s] 34%|███▍      | 344/999 [03:18&lt;07:38,  1.43it/s] 35%|███▍      | 345/999 [03:19&lt;07:31,  1.45it/s] 35%|███▍      | 346/999 [03:20&lt;07:34,  1.44it/s] 35%|███▍      | 347/999 [03:21&lt;07:36,  1.43it/s] 35%|███▍      | 348/999 [03:21&lt;07:40,  1.41it/s] 35%|███▍      | 349/999 [03:22&lt;07:31,  1.44it/s] 35%|███▌      | 350/999 [03:23&lt;07:31,  1.44it/s] 35%|███▌      | 351/999 [03:23&lt;07:37,  1.42it/s] 35%|███▌      | 352/999 [03:24&lt;07:38,  1.41it/s] 35%|███▌      | 353/999 [03:25&lt;07:42,  1.40it/s] 35%|███▌      | 354/999 [03:25&lt;07:40,  1.40it/s] 36%|███▌      | 355/999 [03:26&lt;07:32,  1.42it/s] 36%|███▌      | 356/999 [03:27&lt;07:26,  1.44it/s] 36%|███▌      | 357/999 [03:28&lt;07:23,  1.45it/s] 36%|███▌      | 358/999 [03:28&lt;07:19,  1.46it/s] 36%|███▌      | 359/999 [03:29&lt;07:27,  1.43it/s] 36%|███▌      | 360/999 [03:30&lt;07:22,  1.44it/s] 36%|███▌      | 361/999 [03:30&lt;07:36,  1.40it/s] 36%|███▌      | 362/999 [03:31&lt;07:30,  1.42it/s] 36%|███▋      | 363/999 [03:32&lt;07:20,  1.44it/s] 36%|███▋      | 364/999 [03:32&lt;07:15,  1.46it/s] 37%|███▋      | 365/999 [03:33&lt;07:17,  1.45it/s] 37%|███▋      | 366/999 [03:34&lt;07:04,  1.49it/s] 37%|███▋      | 367/999 [03:34&lt;07:07,  1.48it/s] 37%|███▋      | 368/999 [03:35&lt;07:07,  1.48it/s] 37%|███▋      | 369/999 [03:36&lt;07:08,  1.47it/s] 37%|███▋      | 370/999 [03:36&lt;07:06,  1.47it/s] 37%|███▋      | 371/999 [03:37&lt;07:04,  1.48it/s] 37%|███▋      | 372/999 [03:38&lt;06:59,  1.49it/s] 37%|███▋      | 373/999 [03:38&lt;06:56,  1.50it/s] 37%|███▋      | 374/999 [03:39&lt;06:55,  1.50it/s] 38%|███▊      | 375/999 [03:40&lt;06:49,  1.53it/s] 38%|███▊      | 376/999 [03:40&lt;06:48,  1.53it/s] 38%|███▊      | 377/999 [03:41&lt;06:48,  1.52it/s] 38%|███▊      | 378/999 [03:42&lt;06:52,  1.51it/s] 38%|███▊      | 379/999 [03:42&lt;06:50,  1.51it/s] 38%|███▊      | 380/999 [03:43&lt;06:59,  1.48it/s] 38%|███▊      | 381/999 [03:44&lt;07:05,  1.45it/s] 38%|███▊      | 382/999 [03:44&lt;06:53,  1.49it/s] 38%|███▊      | 383/999 [03:45&lt;06:50,  1.50it/s] 38%|███▊      | 384/999 [03:46&lt;07:00,  1.46it/s] 39%|███▊      | 385/999 [03:46&lt;07:01,  1.46it/s] 39%|███▊      | 386/999 [03:47&lt;07:01,  1.45it/s] 39%|███▊      | 387/999 [03:48&lt;06:45,  1.51it/s] 39%|███▉      | 388/999 [03:48&lt;06:52,  1.48it/s] 39%|███▉      | 389/999 [03:49&lt;06:51,  1.48it/s] 39%|███▉      | 390/999 [03:50&lt;06:50,  1.49it/s] 39%|███▉      | 391/999 [03:51&lt;06:52,  1.47it/s] 39%|███▉      | 392/999 [03:51&lt;06:58,  1.45it/s] 39%|███▉      | 393/999 [03:52&lt;07:01,  1.44it/s] 39%|███▉      | 394/999 [03:53&lt;06:57,  1.45it/s] 40%|███▉      | 395/999 [03:53&lt;06:57,  1.45it/s] 40%|███▉      | 396/999 [03:54&lt;07:02,  1.43it/s] 40%|███▉      | 397/999 [03:55&lt;07:04,  1.42it/s] 40%|███▉      | 398/999 [03:56&lt;07:12,  1.39it/s] 40%|███▉      | 399/999 [03:56&lt;07:19,  1.36it/s] 40%|████      | 400/999 [03:57&lt;07:20,  1.36it/s] 40%|████      | 401/999 [03:58&lt;07:17,  1.37it/s] 40%|████      | 402/999 [03:58&lt;07:11,  1.38it/s] 40%|████      | 403/999 [03:59&lt;07:43,  1.29it/s] 40%|████      | 404/999 [04:00&lt;07:26,  1.33it/s] 41%|████      | 405/999 [04:01&lt;07:14,  1.37it/s] 41%|████      | 406/999 [04:01&lt;07:01,  1.41it/s] 41%|████      | 407/999 [04:02&lt;06:55,  1.42it/s] 41%|████      | 408/999 [04:03&lt;06:52,  1.43it/s] 41%|████      | 409/999 [04:03&lt;06:56,  1.42it/s] 41%|████      | 410/999 [04:04&lt;06:53,  1.42it/s] 41%|████      | 411/999 [04:05&lt;07:00,  1.40it/s] 41%|████      | 412/999 [04:06&lt;06:48,  1.44it/s] 41%|████▏     | 413/999 [04:06&lt;06:42,  1.46it/s] 41%|████▏     | 414/999 [04:07&lt;06:42,  1.45it/s] 42%|████▏     | 415/999 [04:08&lt;06:44,  1.44it/s] 42%|████▏     | 416/999 [04:08&lt;06:53,  1.41it/s] 42%|████▏     | 417/999 [04:09&lt;06:59,  1.39it/s] 42%|████▏     | 418/999 [04:10&lt;06:55,  1.40it/s] 42%|████▏     | 419/999 [04:11&lt;07:03,  1.37it/s] 42%|████▏     | 420/999 [04:11&lt;07:06,  1.36it/s] 42%|████▏     | 421/999 [04:12&lt;07:06,  1.36it/s] 42%|████▏     | 422/999 [04:13&lt;07:09,  1.34it/s] 42%|████▏     | 423/999 [04:14&lt;07:09,  1.34it/s] 42%|████▏     | 424/999 [04:14&lt;07:06,  1.35it/s] 43%|████▎     | 425/999 [04:15&lt;07:09,  1.34it/s] 43%|████▎     | 426/999 [04:16&lt;07:03,  1.35it/s] 43%|████▎     | 427/999 [04:17&lt;07:12,  1.32it/s] 43%|████▎     | 428/999 [04:17&lt;07:04,  1.35it/s] 43%|████▎     | 429/999 [04:18&lt;07:16,  1.31it/s] 43%|████▎     | 430/999 [04:19&lt;07:02,  1.35it/s] 43%|████▎     | 431/999 [04:19&lt;06:45,  1.40it/s] 43%|████▎     | 432/999 [04:20&lt;06:35,  1.43it/s] 43%|████▎     | 433/999 [04:21&lt;06:32,  1.44it/s] 43%|████▎     | 434/999 [04:22&lt;06:41,  1.41it/s] 44%|████▎     | 435/999 [04:22&lt;06:37,  1.42it/s] 44%|████▎     | 436/999 [04:23&lt;06:44,  1.39it/s] 44%|████▎     | 437/999 [04:24&lt;06:49,  1.37it/s] 44%|████▍     | 438/999 [04:24&lt;06:43,  1.39it/s] 44%|████▍     | 439/999 [04:25&lt;06:40,  1.40it/s] 44%|████▍     | 440/999 [04:26&lt;06:45,  1.38it/s] 44%|████▍     | 441/999 [04:27&lt;06:39,  1.40it/s] 44%|████▍     | 442/999 [04:27&lt;06:38,  1.40it/s] 44%|████▍     | 443/999 [04:28&lt;06:38,  1.39it/s] 44%|████▍     | 444/999 [04:29&lt;06:23,  1.45it/s] 45%|████▍     | 445/999 [04:29&lt;06:29,  1.42it/s] 45%|████▍     | 446/999 [04:30&lt;06:32,  1.41it/s] 45%|████▍     | 447/999 [04:31&lt;06:33,  1.40it/s] 45%|████▍     | 448/999 [04:32&lt;06:23,  1.44it/s] 45%|████▍     | 449/999 [04:32&lt;06:25,  1.43it/s] 45%|████▌     | 450/999 [04:33&lt;06:32,  1.40it/s] 45%|████▌     | 451/999 [04:34&lt;06:32,  1.40it/s] 45%|████▌     | 452/999 [04:34&lt;06:43,  1.36it/s] 45%|████▌     | 453/999 [04:35&lt;06:46,  1.34it/s] 45%|████▌     | 454/999 [04:36&lt;06:47,  1.34it/s] 46%|████▌     | 455/999 [04:37&lt;06:40,  1.36it/s] 46%|████▌     | 456/999 [04:37&lt;06:35,  1.37it/s] 46%|████▌     | 457/999 [04:38&lt;06:31,  1.38it/s] 46%|████▌     | 458/999 [04:39&lt;06:36,  1.36it/s] 46%|████▌     | 459/999 [04:40&lt;06:41,  1.34it/s] 46%|████▌     | 460/999 [04:40&lt;06:39,  1.35it/s] 46%|████▌     | 461/999 [04:41&lt;06:49,  1.31it/s] 46%|████▌     | 462/999 [04:42&lt;06:35,  1.36it/s] 46%|████▋     | 463/999 [04:43&lt;06:32,  1.37it/s] 46%|████▋     | 464/999 [04:43&lt;06:36,  1.35it/s] 47%|████▋     | 465/999 [04:44&lt;06:37,  1.34it/s] 47%|████▋     | 466/999 [04:45&lt;06:40,  1.33it/s] 47%|████▋     | 467/999 [04:46&lt;06:27,  1.37it/s] 47%|████▋     | 468/999 [04:46&lt;06:21,  1.39it/s] 47%|████▋     | 469/999 [04:47&lt;06:15,  1.41it/s] 47%|████▋     | 470/999 [04:48&lt;06:20,  1.39it/s] 47%|████▋     | 471/999 [04:48&lt;06:26,  1.37it/s] 47%|████▋     | 472/999 [04:49&lt;06:31,  1.35it/s] 47%|████▋     | 473/999 [04:50&lt;06:36,  1.33it/s] 47%|████▋     | 474/999 [04:51&lt;06:21,  1.38it/s] 48%|████▊     | 475/999 [04:51&lt;06:16,  1.39it/s] 48%|████▊     | 476/999 [04:52&lt;06:09,  1.41it/s] 48%|████▊     | 477/999 [04:53&lt;06:06,  1.42it/s] 48%|████▊     | 478/999 [04:53&lt;06:12,  1.40it/s] 48%|████▊     | 479/999 [04:54&lt;06:15,  1.38it/s] 48%|████▊     | 480/999 [04:55&lt;06:16,  1.38it/s] 48%|████▊     | 481/999 [04:56&lt;06:26,  1.34it/s] 48%|████▊     | 482/999 [04:56&lt;06:23,  1.35it/s] 48%|████▊     | 483/999 [04:57&lt;06:24,  1.34it/s] 48%|████▊     | 484/999 [04:58&lt;06:09,  1.39it/s] 49%|████▊     | 485/999 [04:59&lt;06:07,  1.40it/s] 49%|████▊     | 486/999 [04:59&lt;05:59,  1.43it/s] 49%|████▊     | 487/999 [05:00&lt;06:05,  1.40it/s] 49%|████▉     | 488/999 [05:01&lt;06:04,  1.40it/s] 49%|████▉     | 489/999 [05:01&lt;06:01,  1.41it/s] 49%|████▉     | 490/999 [05:02&lt;05:51,  1.45it/s] 49%|████▉     | 491/999 [05:03&lt;05:44,  1.48it/s] 49%|████▉     | 492/999 [05:03&lt;05:40,  1.49it/s] 49%|████▉     | 493/999 [05:04&lt;05:46,  1.46it/s] 49%|████▉     | 494/999 [05:05&lt;05:52,  1.43it/s] 50%|████▉     | 495/999 [05:05&lt;05:51,  1.43it/s] 50%|████▉     | 496/999 [05:06&lt;05:56,  1.41it/s] 50%|████▉     | 497/999 [05:07&lt;06:00,  1.39it/s] 50%|████▉     | 498/999 [05:08&lt;06:16,  1.33it/s] 50%|████▉     | 499/999 [05:08&lt;05:53,  1.41it/s] 50%|█████     | 500/999 [05:09&lt;05:48,  1.43it/s] 50%|█████     | 501/999 [05:10&lt;05:37,  1.47it/s] 50%|█████     | 502/999 [05:10&lt;05:51,  1.41it/s] 50%|█████     | 503/999 [05:11&lt;05:42,  1.45it/s] 50%|█████     | 504/999 [05:12&lt;05:31,  1.49it/s] 51%|█████     | 505/999 [05:12&lt;05:29,  1.50it/s] 51%|█████     | 506/999 [05:13&lt;05:35,  1.47it/s] 51%|█████     | 507/999 [05:14&lt;05:38,  1.45it/s] 51%|█████     | 508/999 [05:14&lt;05:25,  1.51it/s] 51%|█████     | 509/999 [05:15&lt;05:22,  1.52it/s] 51%|█████     | 510/999 [05:16&lt;05:30,  1.48it/s] 51%|█████     | 511/999 [05:16&lt;05:20,  1.52it/s] 51%|█████▏    | 512/999 [05:17&lt;05:14,  1.55it/s] 51%|█████▏    | 513/999 [05:18&lt;05:11,  1.56it/s] 51%|█████▏    | 514/999 [05:18&lt;05:12,  1.55it/s] 52%|█████▏    | 515/999 [05:19&lt;05:17,  1.53it/s] 52%|█████▏    | 516/999 [05:20&lt;05:13,  1.54it/s] 52%|█████▏    | 517/999 [05:20&lt;05:13,  1.54it/s] 52%|█████▏    | 518/999 [05:21&lt;05:13,  1.53it/s] 52%|█████▏    | 519/999 [05:22&lt;05:10,  1.55it/s] 52%|█████▏    | 520/999 [05:22&lt;05:05,  1.57it/s] 52%|█████▏    | 521/999 [05:23&lt;05:07,  1.55it/s] 52%|█████▏    | 522/999 [05:23&lt;05:06,  1.56it/s] 52%|█████▏    | 523/999 [05:24&lt;05:00,  1.59it/s] 52%|█████▏    | 524/999 [05:25&lt;05:06,  1.55it/s] 53%|█████▎    | 525/999 [05:26&lt;05:21,  1.47it/s] 53%|█████▎    | 526/999 [05:26&lt;05:22,  1.46it/s] 53%|█████▎    | 527/999 [05:27&lt;05:15,  1.49it/s] 53%|█████▎    | 528/999 [05:28&lt;05:16,  1.49it/s] 53%|█████▎    | 529/999 [05:28&lt;05:13,  1.50it/s] 53%|█████▎    | 530/999 [05:29&lt;05:24,  1.44it/s] 53%|█████▎    | 531/999 [05:30&lt;05:24,  1.44it/s] 53%|█████▎    | 532/999 [05:30&lt;05:26,  1.43it/s] 53%|█████▎    | 533/999 [05:31&lt;05:31,  1.41it/s] 53%|█████▎    | 534/999 [05:32&lt;05:26,  1.43it/s] 54%|█████▎    | 535/999 [05:32&lt;05:19,  1.45it/s] 54%|█████▎    | 536/999 [05:33&lt;05:18,  1.45it/s] 54%|█████▍    | 537/999 [05:34&lt;05:17,  1.45it/s] 54%|█████▍    | 538/999 [05:34&lt;05:11,  1.48it/s] 54%|█████▍    | 539/999 [05:35&lt;05:06,  1.50it/s] 54%|█████▍    | 540/999 [05:36&lt;05:01,  1.52it/s] 54%|█████▍    | 541/999 [05:36&lt;05:00,  1.53it/s] 54%|█████▍    | 542/999 [05:37&lt;04:57,  1.53it/s] 54%|█████▍    | 543/999 [05:38&lt;05:07,  1.49it/s] 54%|█████▍    | 544/999 [05:38&lt;05:09,  1.47it/s] 55%|█████▍    | 545/999 [05:39&lt;05:12,  1.45it/s] 55%|█████▍    | 546/999 [05:40&lt;05:10,  1.46it/s] 55%|█████▍    | 547/999 [05:41&lt;05:12,  1.45it/s] 55%|█████▍    | 548/999 [05:41&lt;05:08,  1.46it/s] 55%|█████▍    | 549/999 [05:42&lt;05:02,  1.49it/s] 55%|█████▌    | 550/999 [05:43&lt;05:01,  1.49it/s] 55%|█████▌    | 551/999 [05:43&lt;05:08,  1.45it/s] 55%|█████▌    | 552/999 [05:44&lt;05:02,  1.48it/s] 55%|█████▌    | 553/999 [05:45&lt;05:02,  1.47it/s] 55%|█████▌    | 554/999 [05:45&lt;05:05,  1.46it/s] 56%|█████▌    | 555/999 [05:46&lt;05:07,  1.44it/s] 56%|█████▌    | 556/999 [05:47&lt;05:06,  1.45it/s] 56%|█████▌    | 557/999 [05:47&lt;05:08,  1.43it/s] 56%|█████▌    | 558/999 [05:48&lt;05:07,  1.43it/s] 56%|█████▌    | 559/999 [05:49&lt;05:09,  1.42it/s] 56%|█████▌    | 560/999 [05:49&lt;05:07,  1.43it/s] 56%|█████▌    | 561/999 [05:50&lt;05:03,  1.44it/s] 56%|█████▋    | 562/999 [05:51&lt;05:02,  1.44it/s] 56%|█████▋    | 563/999 [05:52&lt;05:11,  1.40it/s] 56%|█████▋    | 564/999 [05:52&lt;05:05,  1.42it/s] 57%|█████▋    | 565/999 [05:53&lt;05:06,  1.42it/s] 57%|█████▋    | 566/999 [05:54&lt;05:00,  1.44it/s] 57%|█████▋    | 567/999 [05:54&lt;04:57,  1.45it/s] 57%|█████▋    | 568/999 [05:55&lt;04:59,  1.44it/s] 57%|█████▋    | 569/999 [05:56&lt;05:04,  1.41it/s] 57%|█████▋    | 570/999 [05:57&lt;05:03,  1.42it/s] 57%|█████▋    | 571/999 [05:57&lt;05:02,  1.42it/s] 57%|█████▋    | 572/999 [05:58&lt;04:56,  1.44it/s] 57%|█████▋    | 573/999 [05:59&lt;04:53,  1.45it/s] 57%|█████▋    | 574/999 [05:59&lt;04:57,  1.43it/s] 58%|█████▊    | 575/999 [06:00&lt;04:49,  1.47it/s] 58%|█████▊    | 576/999 [06:01&lt;04:43,  1.49it/s] 58%|█████▊    | 577/999 [06:01&lt;04:46,  1.47it/s] 58%|█████▊    | 578/999 [06:02&lt;04:43,  1.48it/s] 58%|█████▊    | 579/999 [06:03&lt;04:39,  1.50it/s] 58%|█████▊    | 580/999 [06:03&lt;04:36,  1.52it/s] 58%|█████▊    | 581/999 [06:04&lt;04:45,  1.46it/s] 58%|█████▊    | 582/999 [06:05&lt;04:41,  1.48it/s] 58%|█████▊    | 583/999 [06:05&lt;04:41,  1.48it/s] 58%|█████▊    | 584/999 [06:06&lt;04:44,  1.46it/s] 59%|█████▊    | 585/999 [06:07&lt;04:45,  1.45it/s] 59%|█████▊    | 586/999 [06:07&lt;04:51,  1.42it/s] 59%|█████▉    | 587/999 [06:08&lt;04:53,  1.41it/s] 59%|█████▉    | 588/999 [06:09&lt;04:54,  1.40it/s] 59%|█████▉    | 589/999 [06:10&lt;04:49,  1.42it/s] 59%|█████▉    | 590/999 [06:10&lt;04:51,  1.40it/s] 59%|█████▉    | 591/999 [06:11&lt;04:55,  1.38it/s] 59%|█████▉    | 592/999 [06:12&lt;04:51,  1.40it/s] 59%|█████▉    | 593/999 [06:12&lt;04:49,  1.40it/s] 59%|█████▉    | 594/999 [06:13&lt;05:00,  1.35it/s] 60%|█████▉    | 595/999 [06:14&lt;05:04,  1.32it/s] 60%|█████▉    | 596/999 [06:15&lt;05:04,  1.32it/s] 60%|█████▉    | 597/999 [06:16&lt;05:04,  1.32it/s] 60%|█████▉    | 598/999 [06:16&lt;04:57,  1.35it/s] 60%|█████▉    | 599/999 [06:17&lt;04:59,  1.34it/s] 60%|██████    | 600/999 [06:18&lt;04:55,  1.35it/s] 60%|██████    | 601/999 [06:18&lt;04:50,  1.37it/s] 60%|██████    | 602/999 [06:19&lt;04:46,  1.39it/s] 60%|██████    | 603/999 [06:20&lt;04:40,  1.41it/s] 60%|██████    | 604/999 [06:21&lt;04:50,  1.36it/s] 61%|██████    | 605/999 [06:21&lt;04:47,  1.37it/s] 61%|██████    | 606/999 [06:22&lt;04:52,  1.34it/s] 61%|██████    | 607/999 [06:23&lt;04:44,  1.38it/s] 61%|██████    | 608/999 [06:23&lt;04:36,  1.41it/s] 61%|██████    | 609/999 [06:24&lt;04:38,  1.40it/s] 61%|██████    | 610/999 [06:25&lt;04:36,  1.41it/s] 61%|██████    | 611/999 [06:26&lt;04:35,  1.41it/s] 61%|██████▏   | 612/999 [06:26&lt;04:37,  1.39it/s] 61%|██████▏   | 613/999 [06:27&lt;04:31,  1.42it/s] 61%|██████▏   | 614/999 [06:28&lt;04:26,  1.44it/s] 62%|██████▏   | 615/999 [06:28&lt;04:28,  1.43it/s] 62%|██████▏   | 616/999 [06:29&lt;04:37,  1.38it/s] 62%|██████▏   | 617/999 [06:30&lt;04:31,  1.40it/s] 62%|██████▏   | 618/999 [06:31&lt;04:27,  1.42it/s] 62%|██████▏   | 619/999 [06:31&lt;04:21,  1.45it/s] 62%|██████▏   | 620/999 [06:32&lt;04:20,  1.45it/s] 62%|██████▏   | 621/999 [06:33&lt;04:18,  1.46it/s] 62%|██████▏   | 622/999 [06:33&lt;04:19,  1.45it/s] 62%|██████▏   | 623/999 [06:34&lt;04:23,  1.43it/s] 62%|██████▏   | 624/999 [06:35&lt;04:14,  1.47it/s] 63%|██████▎   | 625/999 [06:35&lt;04:14,  1.47it/s] 63%|██████▎   | 626/999 [06:36&lt;04:12,  1.48it/s] 63%|██████▎   | 627/999 [06:37&lt;04:05,  1.51it/s] 63%|██████▎   | 628/999 [06:37&lt;04:09,  1.49it/s] 63%|██████▎   | 629/999 [06:38&lt;04:09,  1.48it/s] 63%|██████▎   | 630/999 [06:39&lt;04:11,  1.47it/s] 63%|██████▎   | 631/999 [06:39&lt;04:08,  1.48it/s] 63%|██████▎   | 632/999 [06:40&lt;04:08,  1.48it/s] 63%|██████▎   | 633/999 [06:41&lt;04:04,  1.50it/s] 63%|██████▎   | 634/999 [06:41&lt;04:06,  1.48it/s] 64%|██████▎   | 635/999 [06:42&lt;04:04,  1.49it/s] 64%|██████▎   | 636/999 [06:43&lt;04:03,  1.49it/s] 64%|██████▍   | 637/999 [06:43&lt;03:59,  1.51it/s] 64%|██████▍   | 638/999 [06:44&lt;04:04,  1.47it/s] 64%|██████▍   | 639/999 [06:45&lt;04:04,  1.47it/s] 64%|██████▍   | 640/999 [06:45&lt;04:05,  1.46it/s] 64%|██████▍   | 641/999 [06:46&lt;04:05,  1.46it/s] 64%|██████▍   | 642/999 [06:47&lt;04:06,  1.45it/s] 64%|██████▍   | 643/999 [06:48&lt;04:05,  1.45it/s] 64%|██████▍   | 644/999 [06:48&lt;04:04,  1.45it/s] 65%|██████▍   | 645/999 [06:49&lt;03:57,  1.49it/s] 65%|██████▍   | 646/999 [06:50&lt;04:02,  1.45it/s] 65%|██████▍   | 647/999 [06:50&lt;04:03,  1.45it/s] 65%|██████▍   | 648/999 [06:51&lt;03:55,  1.49it/s] 65%|██████▍   | 649/999 [06:52&lt;03:51,  1.51it/s] 65%|██████▌   | 650/999 [06:52&lt;03:46,  1.54it/s] 65%|██████▌   | 651/999 [06:53&lt;03:45,  1.54it/s] 65%|██████▌   | 652/999 [06:53&lt;03:51,  1.50it/s] 65%|██████▌   | 653/999 [06:54&lt;03:52,  1.49it/s] 65%|██████▌   | 654/999 [06:55&lt;03:43,  1.54it/s] 66%|██████▌   | 655/999 [06:55&lt;03:38,  1.58it/s] 66%|██████▌   | 656/999 [06:56&lt;03:47,  1.51it/s] 66%|██████▌   | 657/999 [06:57&lt;03:49,  1.49it/s] 66%|██████▌   | 658/999 [06:57&lt;03:42,  1.53it/s] 66%|██████▌   | 659/999 [06:58&lt;03:42,  1.53it/s] 66%|██████▌   | 660/999 [06:59&lt;03:44,  1.51it/s] 66%|██████▌   | 661/999 [06:59&lt;03:42,  1.52it/s] 66%|██████▋   | 662/999 [07:00&lt;03:45,  1.49it/s] 66%|██████▋   | 663/999 [07:01&lt;03:48,  1.47it/s] 66%|██████▋   | 664/999 [07:01&lt;03:49,  1.46it/s] 67%|██████▋   | 665/999 [07:02&lt;03:45,  1.48it/s] 67%|██████▋   | 666/999 [07:03&lt;03:41,  1.50it/s] 67%|██████▋   | 667/999 [07:03&lt;03:36,  1.53it/s] 67%|██████▋   | 668/999 [07:04&lt;03:39,  1.51it/s] 67%|██████▋   | 669/999 [07:05&lt;03:43,  1.48it/s] 67%|██████▋   | 670/999 [07:05&lt;03:43,  1.47it/s] 67%|██████▋   | 671/999 [07:06&lt;03:36,  1.51it/s] 67%|██████▋   | 672/999 [07:07&lt;03:31,  1.54it/s] 67%|██████▋   | 673/999 [07:07&lt;03:32,  1.53it/s] 67%|██████▋   | 674/999 [07:08&lt;03:27,  1.57it/s] 68%|██████▊   | 675/999 [07:09&lt;03:29,  1.55it/s] 68%|██████▊   | 676/999 [07:09&lt;03:28,  1.55it/s] 68%|██████▊   | 677/999 [07:10&lt;03:27,  1.55it/s] 68%|██████▊   | 678/999 [07:11&lt;03:28,  1.54it/s] 68%|██████▊   | 679/999 [07:11&lt;03:27,  1.54it/s] 68%|██████▊   | 680/999 [07:12&lt;03:27,  1.54it/s] 68%|██████▊   | 681/999 [07:13&lt;03:24,  1.56it/s] 68%|██████▊   | 682/999 [07:13&lt;03:19,  1.59it/s] 68%|██████▊   | 683/999 [07:14&lt;03:24,  1.54it/s] 68%|██████▊   | 684/999 [07:14&lt;03:27,  1.52it/s] 69%|██████▊   | 685/999 [07:15&lt;03:28,  1.50it/s] 69%|██████▊   | 686/999 [07:16&lt;03:28,  1.50it/s] 69%|██████▉   | 687/999 [07:17&lt;03:32,  1.47it/s] 69%|██████▉   | 688/999 [07:17&lt;03:32,  1.46it/s] 69%|██████▉   | 689/999 [07:18&lt;03:31,  1.46it/s] 69%|██████▉   | 690/999 [07:19&lt;03:28,  1.48it/s] 69%|██████▉   | 691/999 [07:19&lt;03:26,  1.49it/s] 69%|██████▉   | 692/999 [07:20&lt;03:23,  1.51it/s] 69%|██████▉   | 693/999 [07:21&lt;03:24,  1.50it/s] 69%|██████▉   | 694/999 [07:21&lt;03:31,  1.44it/s] 70%|██████▉   | 695/999 [07:22&lt;03:32,  1.43it/s] 70%|██████▉   | 696/999 [07:23&lt;03:38,  1.39it/s] 70%|██████▉   | 697/999 [07:24&lt;03:38,  1.38it/s] 70%|██████▉   | 698/999 [07:24&lt;03:38,  1.38it/s] 70%|██████▉   | 699/999 [07:25&lt;03:37,  1.38it/s] 70%|███████   | 700/999 [07:26&lt;03:34,  1.39it/s] 70%|███████   | 701/999 [07:26&lt;03:33,  1.39it/s] 70%|███████   | 702/999 [07:27&lt;03:41,  1.34it/s] 70%|███████   | 703/999 [07:28&lt;03:39,  1.35it/s] 70%|███████   | 704/999 [07:29&lt;03:34,  1.37it/s] 71%|███████   | 705/999 [07:29&lt;03:42,  1.32it/s] 71%|███████   | 706/999 [07:30&lt;03:38,  1.34it/s] 71%|███████   | 707/999 [07:31&lt;03:32,  1.37it/s] 71%|███████   | 708/999 [07:32&lt;03:32,  1.37it/s] 71%|███████   | 709/999 [07:32&lt;03:30,  1.38it/s] 71%|███████   | 710/999 [07:33&lt;03:34,  1.35it/s] 71%|███████   | 711/999 [07:34&lt;03:32,  1.35it/s] 71%|███████▏  | 712/999 [07:35&lt;03:31,  1.36it/s] 71%|███████▏  | 713/999 [07:35&lt;03:25,  1.39it/s] 71%|███████▏  | 714/999 [07:36&lt;03:29,  1.36it/s] 72%|███████▏  | 715/999 [07:37&lt;03:28,  1.36it/s] 72%|███████▏  | 716/999 [07:37&lt;03:27,  1.37it/s] 72%|███████▏  | 717/999 [07:38&lt;03:33,  1.32it/s] 72%|███████▏  | 718/999 [07:39&lt;03:31,  1.33it/s] 72%|███████▏  | 719/999 [07:40&lt;03:33,  1.31it/s] 72%|███████▏  | 720/999 [07:41&lt;03:38,  1.28it/s] 72%|███████▏  | 721/999 [07:41&lt;03:41,  1.26it/s] 72%|███████▏  | 722/999 [07:42&lt;03:38,  1.27it/s] 72%|███████▏  | 723/999 [07:43&lt;03:34,  1.28it/s] 72%|███████▏  | 724/999 [07:44&lt;03:31,  1.30it/s] 73%|███████▎  | 725/999 [07:45&lt;03:32,  1.29it/s] 73%|███████▎  | 726/999 [07:45&lt;03:30,  1.29it/s] 73%|███████▎  | 727/999 [07:46&lt;03:27,  1.31it/s] 73%|███████▎  | 728/999 [07:47&lt;03:28,  1.30it/s] 73%|███████▎  | 729/999 [07:48&lt;03:26,  1.31it/s] 73%|███████▎  | 730/999 [07:48&lt;03:32,  1.26it/s] 73%|███████▎  | 731/999 [07:49&lt;03:32,  1.26it/s] 73%|███████▎  | 732/999 [07:50&lt;03:27,  1.29it/s] 73%|███████▎  | 733/999 [07:51&lt;03:24,  1.30it/s] 73%|███████▎  | 734/999 [07:51&lt;03:18,  1.33it/s] 74%|███████▎  | 735/999 [07:52&lt;03:21,  1.31it/s] 74%|███████▎  | 736/999 [07:53&lt;03:28,  1.26it/s] 74%|███████▍  | 737/999 [07:54&lt;03:25,  1.27it/s] 74%|███████▍  | 738/999 [07:55&lt;03:38,  1.19it/s] 74%|███████▍  | 739/999 [07:56&lt;03:44,  1.16it/s] 74%|███████▍  | 740/999 [07:57&lt;03:45,  1.15it/s] 74%|███████▍  | 741/999 [07:57&lt;03:42,  1.16it/s] 74%|███████▍  | 742/999 [07:58&lt;03:41,  1.16it/s] 74%|███████▍  | 743/999 [07:59&lt;03:37,  1.18it/s] 74%|███████▍  | 744/999 [08:00&lt;03:40,  1.15it/s] 75%|███████▍  | 745/999 [08:01&lt;03:37,  1.17it/s] 75%|███████▍  | 746/999 [08:02&lt;03:36,  1.17it/s] 75%|███████▍  | 747/999 [08:03&lt;03:33,  1.18it/s] 75%|███████▍  | 748/999 [08:03&lt;03:37,  1.15it/s] 75%|███████▍  | 749/999 [08:04&lt;03:36,  1.15it/s] 75%|███████▌  | 750/999 [08:05&lt;03:38,  1.14it/s] 75%|███████▌  | 751/999 [08:06&lt;03:38,  1.14it/s] 75%|███████▌  | 752/999 [08:07&lt;03:33,  1.16it/s] 75%|███████▌  | 753/999 [08:08&lt;03:31,  1.16it/s] 75%|███████▌  | 754/999 [08:09&lt;03:31,  1.16it/s] 76%|███████▌  | 755/999 [08:10&lt;03:34,  1.14it/s] 76%|███████▌  | 756/999 [08:10&lt;03:31,  1.15it/s] 76%|███████▌  | 757/999 [08:11&lt;03:31,  1.15it/s] 76%|███████▌  | 758/999 [08:12&lt;03:28,  1.15it/s] 76%|███████▌  | 759/999 [08:13&lt;03:27,  1.16it/s] 76%|███████▌  | 760/999 [08:14&lt;03:26,  1.16it/s] 76%|███████▌  | 761/999 [08:15&lt;03:28,  1.14it/s] 76%|███████▋  | 762/999 [08:16&lt;03:25,  1.15it/s] 76%|███████▋  | 763/999 [08:17&lt;03:29,  1.12it/s] 76%|███████▋  | 764/999 [08:17&lt;03:27,  1.13it/s] 77%|███████▋  | 765/999 [08:18&lt;03:21,  1.16it/s] 77%|███████▋  | 766/999 [08:19&lt;03:20,  1.16it/s] 77%|███████▋  | 767/999 [08:20&lt;03:16,  1.18it/s] 77%|███████▋  | 768/999 [08:21&lt;03:15,  1.18it/s] 77%|███████▋  | 769/999 [08:22&lt;03:20,  1.15it/s] 77%|███████▋  | 770/999 [08:23&lt;03:14,  1.18it/s] 77%|███████▋  | 771/999 [08:23&lt;03:10,  1.20it/s] 77%|███████▋  | 772/999 [08:24&lt;03:09,  1.20it/s] 77%|███████▋  | 773/999 [08:25&lt;03:07,  1.20it/s] 77%|███████▋  | 774/999 [08:26&lt;03:05,  1.21it/s] 78%|███████▊  | 775/999 [08:27&lt;03:05,  1.20it/s] 78%|███████▊  | 776/999 [08:27&lt;03:06,  1.20it/s] 78%|███████▊  | 777/999 [08:28&lt;03:08,  1.17it/s] 78%|███████▊  | 778/999 [08:29&lt;03:11,  1.15it/s] 78%|███████▊  | 779/999 [08:30&lt;03:07,  1.17it/s] 78%|███████▊  | 780/999 [08:31&lt;03:05,  1.18it/s] 78%|███████▊  | 781/999 [08:32&lt;03:04,  1.18it/s] 78%|███████▊  | 782/999 [08:33&lt;03:03,  1.18it/s] 78%|███████▊  | 783/999 [08:33&lt;03:03,  1.18it/s] 78%|███████▊  | 784/999 [08:34&lt;03:00,  1.19it/s] 79%|███████▊  | 785/999 [08:35&lt;03:08,  1.14it/s] 79%|███████▊  | 786/999 [08:36&lt;03:09,  1.12it/s] 79%|███████▉  | 787/999 [08:37&lt;03:08,  1.12it/s] 79%|███████▉  | 788/999 [08:38&lt;03:07,  1.13it/s] 79%|███████▉  | 789/999 [08:39&lt;03:09,  1.11it/s] 79%|███████▉  | 790/999 [08:40&lt;03:07,  1.11it/s] 79%|███████▉  | 791/999 [08:41&lt;03:04,  1.13it/s] 79%|███████▉  | 792/999 [08:42&lt;03:04,  1.12it/s] 79%|███████▉  | 793/999 [08:42&lt;03:05,  1.11it/s] 79%|███████▉  | 794/999 [08:43&lt;03:05,  1.11it/s] 80%|███████▉  | 795/999 [08:44&lt;03:02,  1.12it/s] 80%|███████▉  | 796/999 [08:45&lt;02:59,  1.13it/s] 80%|███████▉  | 797/999 [08:46&lt;02:59,  1.13it/s] 80%|███████▉  | 798/999 [08:47&lt;03:00,  1.11it/s] 80%|███████▉  | 799/999 [08:48&lt;03:02,  1.10it/s] 80%|████████  | 800/999 [08:49&lt;03:01,  1.10it/s] 80%|████████  | 801/999 [08:50&lt;02:58,  1.11it/s] 80%|████████  | 802/999 [08:51&lt;02:59,  1.10it/s] 80%|████████  | 803/999 [08:52&lt;02:59,  1.09it/s] 80%|████████  | 804/999 [08:52&lt;02:59,  1.09it/s] 81%|████████  | 805/999 [08:53&lt;02:57,  1.09it/s] 81%|████████  | 806/999 [08:54&lt;02:57,  1.08it/s] 81%|████████  | 807/999 [08:55&lt;02:56,  1.09it/s] 81%|████████  | 808/999 [08:56&lt;02:53,  1.10it/s] 81%|████████  | 809/999 [08:57&lt;02:53,  1.09it/s] 81%|████████  | 810/999 [08:58&lt;02:50,  1.11it/s] 81%|████████  | 811/999 [08:59&lt;02:47,  1.12it/s] 81%|████████▏ | 812/999 [09:00&lt;02:45,  1.13it/s] 81%|████████▏ | 813/999 [09:01&lt;02:47,  1.11it/s] 81%|████████▏ | 814/999 [09:02&lt;02:52,  1.07it/s] 82%|████████▏ | 815/999 [09:03&lt;02:52,  1.07it/s] 82%|████████▏ | 816/999 [09:03&lt;02:49,  1.08it/s] 82%|████████▏ | 817/999 [09:04&lt;02:48,  1.08it/s] 82%|████████▏ | 818/999 [09:05&lt;02:47,  1.08it/s] 82%|████████▏ | 819/999 [09:06&lt;02:55,  1.03it/s] 82%|████████▏ | 820/999 [09:07&lt;02:55,  1.02it/s] 82%|████████▏ | 821/999 [09:08&lt;02:55,  1.01it/s] 82%|████████▏ | 822/999 [09:09&lt;02:54,  1.01it/s] 82%|████████▏ | 823/999 [09:10&lt;02:54,  1.01it/s] 82%|████████▏ | 824/999 [09:11&lt;02:54,  1.00it/s] 83%|████████▎ | 825/999 [09:12&lt;02:56,  1.01s/it] 83%|████████▎ | 826/999 [09:13&lt;02:50,  1.01it/s] 83%|████████▎ | 827/999 [09:14&lt;02:43,  1.05it/s] 83%|████████▎ | 828/999 [09:15&lt;02:42,  1.05it/s] 83%|████████▎ | 829/999 [09:16&lt;02:42,  1.05it/s] 83%|████████▎ | 830/999 [09:17&lt;02:38,  1.06it/s] 83%|████████▎ | 831/999 [09:18&lt;02:33,  1.09it/s] 83%|████████▎ | 832/999 [09:19&lt;02:33,  1.09it/s] 83%|████████▎ | 833/999 [09:20&lt;02:25,  1.14it/s] 83%|████████▎ | 834/999 [09:20&lt;02:26,  1.12it/s] 84%|████████▎ | 835/999 [09:21&lt;02:27,  1.11it/s] 84%|████████▎ | 836/999 [09:22&lt;02:25,  1.12it/s] 84%|████████▍ | 837/999 [09:23&lt;02:24,  1.12it/s] 84%|████████▍ | 838/999 [09:24&lt;02:25,  1.11it/s] 84%|████████▍ | 839/999 [09:25&lt;02:23,  1.12it/s] 84%|████████▍ | 840/999 [09:26&lt;02:20,  1.13it/s] 84%|████████▍ | 841/999 [09:27&lt;02:16,  1.15it/s] 84%|████████▍ | 842/999 [09:28&lt;02:17,  1.14it/s] 84%|████████▍ | 843/999 [09:28&lt;02:15,  1.16it/s] 84%|████████▍ | 844/999 [09:29&lt;02:13,  1.16it/s] 85%|████████▍ | 845/999 [09:30&lt;02:11,  1.17it/s] 85%|████████▍ | 846/999 [09:31&lt;02:10,  1.17it/s] 85%|████████▍ | 847/999 [09:32&lt;02:09,  1.18it/s] 85%|████████▍ | 848/999 [09:33&lt;02:08,  1.17it/s] 85%|████████▍ | 849/999 [09:34&lt;02:10,  1.15it/s] 85%|████████▌ | 850/999 [09:34&lt;02:08,  1.16it/s] 85%|████████▌ | 851/999 [09:35&lt;02:09,  1.14it/s] 85%|████████▌ | 852/999 [09:36&lt;02:08,  1.14it/s] 85%|████████▌ | 853/999 [09:37&lt;02:08,  1.14it/s] 85%|████████▌ | 854/999 [09:38&lt;02:04,  1.16it/s] 86%|████████▌ | 855/999 [09:39&lt;02:05,  1.15it/s] 86%|████████▌ | 856/999 [09:40&lt;02:02,  1.17it/s] 86%|████████▌ | 857/999 [09:40&lt;01:58,  1.19it/s] 86%|████████▌ | 858/999 [09:41&lt;01:58,  1.19it/s] 86%|████████▌ | 859/999 [09:42&lt;01:59,  1.18it/s] 86%|████████▌ | 860/999 [09:43&lt;01:58,  1.18it/s] 86%|████████▌ | 861/999 [09:44&lt;02:01,  1.14it/s] 86%|████████▋ | 862/999 [09:45&lt;01:57,  1.17it/s] 86%|████████▋ | 863/999 [09:46&lt;01:59,  1.13it/s] 86%|████████▋ | 864/999 [09:47&lt;01:58,  1.14it/s] 87%|████████▋ | 865/999 [09:47&lt;01:58,  1.13it/s] 87%|████████▋ | 866/999 [09:48&lt;01:56,  1.14it/s] 87%|████████▋ | 867/999 [09:49&lt;01:57,  1.13it/s] 87%|████████▋ | 868/999 [09:50&lt;01:54,  1.14it/s] 87%|████████▋ | 869/999 [09:51&lt;01:51,  1.17it/s] 87%|████████▋ | 870/999 [09:52&lt;01:49,  1.18it/s] 87%|████████▋ | 871/999 [09:53&lt;01:50,  1.16it/s] 87%|████████▋ | 872/999 [09:54&lt;01:52,  1.13it/s] 87%|████████▋ | 873/999 [09:54&lt;01:51,  1.13it/s] 87%|████████▋ | 874/999 [09:55&lt;01:51,  1.12it/s] 88%|████████▊ | 875/999 [09:56&lt;01:52,  1.11it/s] 88%|████████▊ | 876/999 [09:57&lt;01:51,  1.10it/s] 88%|████████▊ | 877/999 [09:58&lt;01:52,  1.08it/s] 88%|████████▊ | 878/999 [09:59&lt;01:49,  1.10it/s] 88%|████████▊ | 879/999 [10:00&lt;01:49,  1.09it/s] 88%|████████▊ | 880/999 [10:01&lt;01:49,  1.09it/s] 88%|████████▊ | 881/999 [10:02&lt;01:47,  1.09it/s] 88%|████████▊ | 882/999 [10:03&lt;01:46,  1.10it/s] 88%|████████▊ | 883/999 [10:04&lt;01:46,  1.09it/s] 88%|████████▊ | 884/999 [10:04&lt;01:43,  1.11it/s] 89%|████████▊ | 885/999 [10:05&lt;01:39,  1.15it/s] 89%|████████▊ | 886/999 [10:06&lt;01:39,  1.13it/s] 89%|████████▉ | 887/999 [10:07&lt;01:38,  1.13it/s] 89%|████████▉ | 888/999 [10:08&lt;01:41,  1.09it/s] 89%|████████▉ | 889/999 [10:09&lt;01:40,  1.09it/s] 89%|████████▉ | 890/999 [10:10&lt;01:37,  1.12it/s] 89%|████████▉ | 891/999 [10:11&lt;01:34,  1.15it/s] 89%|████████▉ | 892/999 [10:11&lt;01:31,  1.17it/s] 89%|████████▉ | 893/999 [10:12&lt;01:30,  1.17it/s] 89%|████████▉ | 894/999 [10:13&lt;01:30,  1.16it/s] 90%|████████▉ | 895/999 [10:14&lt;01:29,  1.17it/s] 90%|████████▉ | 896/999 [10:15&lt;01:28,  1.16it/s] 90%|████████▉ | 897/999 [10:16&lt;01:27,  1.17it/s] 90%|████████▉ | 898/999 [10:17&lt;01:26,  1.16it/s] 90%|████████▉ | 899/999 [10:17&lt;01:25,  1.17it/s] 90%|█████████ | 900/999 [10:18&lt;01:24,  1.17it/s] 90%|█████████ | 901/999 [10:19&lt;01:22,  1.18it/s] 90%|█████████ | 902/999 [10:20&lt;01:22,  1.18it/s] 90%|█████████ | 903/999 [10:21&lt;01:19,  1.21it/s] 90%|█████████ | 904/999 [10:22&lt;01:19,  1.19it/s] 91%|█████████ | 905/999 [10:23&lt;01:20,  1.16it/s] 91%|█████████ | 906/999 [10:23&lt;01:19,  1.17it/s] 91%|█████████ | 907/999 [10:24&lt;01:18,  1.17it/s] 91%|█████████ | 908/999 [10:25&lt;01:16,  1.18it/s] 91%|█████████ | 909/999 [10:26&lt;01:16,  1.18it/s] 91%|█████████ | 910/999 [10:27&lt;01:16,  1.17it/s] 91%|█████████ | 911/999 [10:28&lt;01:15,  1.16it/s] 91%|█████████▏| 912/999 [10:29&lt;01:15,  1.15it/s] 91%|█████████▏| 913/999 [10:29&lt;01:14,  1.15it/s] 91%|█████████▏| 914/999 [10:30&lt;01:13,  1.15it/s] 92%|█████████▏| 915/999 [10:31&lt;01:13,  1.14it/s] 92%|█████████▏| 916/999 [10:32&lt;01:12,  1.14it/s] 92%|█████████▏| 917/999 [10:33&lt;01:12,  1.14it/s] 92%|█████████▏| 918/999 [10:34&lt;01:10,  1.15it/s] 92%|█████████▏| 919/999 [10:35&lt;01:09,  1.15it/s] 92%|█████████▏| 920/999 [10:35&lt;01:06,  1.18it/s] 92%|█████████▏| 921/999 [10:36&lt;01:05,  1.19it/s] 92%|█████████▏| 922/999 [10:37&lt;01:03,  1.21it/s] 92%|█████████▏| 923/999 [10:38&lt;01:03,  1.20it/s] 92%|█████████▏| 924/999 [10:39&lt;01:02,  1.20it/s] 93%|█████████▎| 925/999 [10:40&lt;01:02,  1.18it/s] 93%|█████████▎| 926/999 [10:40&lt;01:01,  1.19it/s] 93%|█████████▎| 927/999 [10:41&lt;01:01,  1.17it/s] 93%|█████████▎| 928/999 [10:42&lt;01:00,  1.17it/s] 93%|█████████▎| 929/999 [10:43&lt;01:00,  1.16it/s] 93%|█████████▎| 930/999 [10:44&lt;00:59,  1.15it/s] 93%|█████████▎| 931/999 [10:45&lt;00:59,  1.15it/s] 93%|█████████▎| 932/999 [10:46&lt;00:58,  1.14it/s] 93%|█████████▎| 933/999 [10:47&lt;00:58,  1.13it/s] 93%|█████████▎| 934/999 [10:47&lt;00:56,  1.16it/s] 94%|█████████▎| 935/999 [10:48&lt;00:54,  1.17it/s] 94%|█████████▎| 936/999 [10:49&lt;00:54,  1.16it/s] 94%|█████████▍| 937/999 [10:50&lt;00:55,  1.11it/s] 94%|█████████▍| 938/999 [10:51&lt;00:53,  1.14it/s] 94%|█████████▍| 939/999 [10:52&lt;00:53,  1.13it/s] 94%|█████████▍| 940/999 [10:53&lt;00:50,  1.16it/s] 94%|█████████▍| 941/999 [10:54&lt;00:50,  1.15it/s] 94%|█████████▍| 942/999 [10:54&lt;00:49,  1.15it/s] 94%|█████████▍| 943/999 [10:55&lt;00:48,  1.16it/s] 94%|█████████▍| 944/999 [10:56&lt;00:47,  1.16it/s] 95%|█████████▍| 945/999 [10:57&lt;00:45,  1.17it/s] 95%|█████████▍| 946/999 [10:58&lt;00:45,  1.17it/s] 95%|█████████▍| 947/999 [10:59&lt;00:43,  1.20it/s] 95%|█████████▍| 948/999 [10:59&lt;00:41,  1.22it/s] 95%|█████████▍| 949/999 [11:00&lt;00:41,  1.21it/s] 95%|█████████▌| 950/999 [11:01&lt;00:41,  1.19it/s] 95%|█████████▌| 951/999 [11:02&lt;00:41,  1.17it/s] 95%|█████████▌| 952/999 [11:03&lt;00:40,  1.15it/s] 95%|█████████▌| 953/999 [11:04&lt;00:40,  1.14it/s] 95%|█████████▌| 954/999 [11:05&lt;00:40,  1.12it/s] 96%|█████████▌| 955/999 [11:06&lt;00:39,  1.11it/s] 96%|█████████▌| 956/999 [11:07&lt;00:39,  1.10it/s] 96%|█████████▌| 957/999 [11:08&lt;00:38,  1.08it/s] 96%|█████████▌| 958/999 [11:08&lt;00:37,  1.08it/s] 96%|█████████▌| 959/999 [11:09&lt;00:37,  1.07it/s] 96%|█████████▌| 960/999 [11:10&lt;00:35,  1.09it/s] 96%|█████████▌| 961/999 [11:11&lt;00:35,  1.08it/s] 96%|█████████▋| 962/999 [11:12&lt;00:33,  1.09it/s] 96%|█████████▋| 963/999 [11:13&lt;00:33,  1.07it/s] 96%|█████████▋| 964/999 [11:14&lt;00:33,  1.04it/s] 97%|█████████▋| 965/999 [11:15&lt;00:32,  1.05it/s] 97%|█████████▋| 966/999 [11:16&lt;00:31,  1.06it/s] 97%|█████████▋| 967/999 [11:17&lt;00:30,  1.06it/s] 97%|█████████▋| 968/999 [11:18&lt;00:28,  1.07it/s] 97%|█████████▋| 969/999 [11:19&lt;00:28,  1.05it/s] 97%|█████████▋| 970/999 [11:20&lt;00:27,  1.06it/s] 97%|█████████▋| 971/999 [11:21&lt;00:26,  1.06it/s] 97%|█████████▋| 972/999 [11:22&lt;00:24,  1.08it/s] 97%|█████████▋| 973/999 [11:23&lt;00:24,  1.08it/s] 97%|█████████▋| 974/999 [11:24&lt;00:23,  1.05it/s] 98%|█████████▊| 975/999 [11:24&lt;00:22,  1.05it/s] 98%|█████████▊| 976/999 [11:25&lt;00:21,  1.09it/s] 98%|█████████▊| 977/999 [11:26&lt;00:20,  1.06it/s] 98%|█████████▊| 978/999 [11:27&lt;00:19,  1.06it/s] 98%|█████████▊| 979/999 [11:28&lt;00:19,  1.04it/s] 98%|█████████▊| 980/999 [11:29&lt;00:18,  1.05it/s] 98%|█████████▊| 981/999 [11:30&lt;00:17,  1.03it/s] 98%|█████████▊| 982/999 [11:31&lt;00:16,  1.02it/s] 98%|█████████▊| 983/999 [11:32&lt;00:15,  1.03it/s] 98%|█████████▊| 984/999 [11:33&lt;00:14,  1.06it/s] 99%|█████████▊| 985/999 [11:34&lt;00:13,  1.03it/s] 99%|█████████▊| 986/999 [11:35&lt;00:12,  1.06it/s] 99%|█████████▉| 987/999 [11:36&lt;00:11,  1.07it/s] 99%|█████████▉| 988/999 [11:37&lt;00:10,  1.04it/s] 99%|█████████▉| 989/999 [11:38&lt;00:09,  1.04it/s] 99%|█████████▉| 990/999 [11:39&lt;00:08,  1.05it/s] 99%|█████████▉| 991/999 [11:40&lt;00:07,  1.05it/s] 99%|█████████▉| 992/999 [11:41&lt;00:06,  1.07it/s] 99%|█████████▉| 993/999 [11:42&lt;00:05,  1.04it/s] 99%|█████████▉| 994/999 [11:43&lt;00:04,  1.02it/s]100%|█████████▉| 995/999 [11:44&lt;00:03,  1.04it/s]100%|█████████▉| 996/999 [11:45&lt;00:02,  1.04it/s]100%|█████████▉| 997/999 [11:46&lt;00:01,  1.04it/s]100%|█████████▉| 998/999 [11:47&lt;00:00,  1.03it/s]100%|██████████| 999/999 [11:47&lt;00:00,  1.04it/s]100%|██████████| 999/999 [11:47&lt;00:00,  1.41it/s]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', label='Première bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles théoriques')\nplt.xlabel('Quantiles théoriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# # kstest y revenir\n# ks_stat, ks_p_value = kstest(data_train, skew_student_pdf, args=(**params_sstd,))\n\n# print(\"=\"*80)\n# print(\"H0 : Les données suivent une loi de Skew Student\")\n# print(f\"Statistique de test : {ks_stat:.4f}\")\n# print(f\"P-value : {ks_p_value:.4f}\")\n# print(\"=\"*80)\n# A revoir\n\n\n\nII.4.2. Calcul de la VaR Skew Student\n\n# Objectif : écrire une fonction qui calcule la VaR skew-student\n\ndef sstd_var_fct(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\nsstd_var = sstd_var_fct(alpha, params_sstd)\nprint(f\"La VaR skew student pour h=1j et alpha={alpha} est : {sstd_var:.4%}\")\n\nLa VaR skew student pour h=1j et alpha=0.99 est : 4.2576%"
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html",
    "href": "3A/value-at-risk/var_evt.html",
    "title": "TP2:Méthodes basées sur la théorie des valeurs extrêmes]",
    "section": "",
    "text": "Ce TP est une continuité du TP-1 dans lequel on souhaitait implémenter la VaR (Value at Risk) et l’ES (Expected Shortfall) en utilisant les méthodes classiques proposées dans la réglementation bâloise, i.e. la méthode historique, paramétrique et bootstrap. Cependant, une limite de ces méthodes est qu’elles ne prennent pas en compte la queue de distribution de la perte. Pour remédier à cela, on peut utiliser des méthodes avec la théorie des valeurs extrêmes, i.e. l’approche Block Maxima et l’approche Peaks Over Threshold.\n# Définition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des données du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la première ligne\ndata = data.dropna()\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test"
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.-implémentation-de-la-var-avec-la-théorie-des-valeurs-extrêmes",
    "href": "3A/value-at-risk/var_evt.html#i.-implémentation-de-la-var-avec-la-théorie-des-valeurs-extrêmes",
    "title": "TP2:Méthodes basées sur la théorie des valeurs extrêmes]",
    "section": "I. Implémentation de la VaR avec la théorie des valeurs extrêmes",
    "text": "I. Implémentation de la VaR avec la théorie des valeurs extrêmes\n\nI.1. VaR TVE : Approche Maxima par bloc\nL’approche des Block Maxima (BM) est une méthode modélise les maxima des rendements sur des blocs de taille fixe \\(s\\) en utilisant la distribution GEV. Le seuil de confiance \\(\\alpha_{\\text{GEV}}\\) est ajusté pour correspondre à l’horizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nNous allons dans ce projet une taille de blocs \\(s = 21\\) jours ouvrés comme ce qui souvent utilisé en pratique. De ce fait, nous parvenons à construire 239 blocs de taille 21 et un bloc de taille De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ _{}$ est :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\noù G est la fonction de répartition de la GEV (\\(\\hat \\mu, \\hat \\sigma, \\hat \\xi\\)) estimée.\n\n\nI.1.1. Construction de l’échantillon de maxima sur data_train\n\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extrêmes d'une série de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associées aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # Sélectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # Récupérer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des données suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extrêmes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n\n\n\n\n\n\n\n\nPour avoir une idée de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour \\(s=21\\), nous utilisons un Gumbel plot qui est un outil graphique pour juger de l’hypothèse \\(\\xi=0\\), i.e. la distribution GEV se réduit à la distribution de Gumbel.\nPour le construire, nous devons suivre les étapes suivantes :\n\ncalculer l’abscisse avec la série des maximas ordonées \\(R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}\\).\ncalculer l’ordonnée de la manière suivante :\n\n\\[\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n\\]\nLorsque la distribution adaptée est celle de Gumbel alors le Gumbel plot est linéaire. Dans notre cas, nous constatons une courbure ce qui nous pousse à conclure qu’une distribution Gumbel n’est pas adaptée dans la modélisation des maxima des pertes de rendements du CAC 40. Une distribution Fréchet ou de Weibull serait plus adaptée.\n\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.1.2. Estimation des paramètres de la loi de GEV\nEn estimant les paramètres de la loi GEV, nous utilisons la méthode du maximum de vraisemblance. Les paramètres estimés par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = -0.15, \\mu=0.02, \\sigma=0.01\\). Nous constatons par ailleurs que le paramètre de forme \\(\\xi\\) est négatif ce qui est cohérent avec notre observation précédente.\n\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les paramètres estimés\nprint(\"=\"*50)\nprint(\"Paramètres estimés de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParamètres estimés de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\nPour accorder plus de poids à cette observation, nous avons calculé un intervalle de confiance profilé à 95% pour le paramètre de forme \\(\\xi\\). Pour ce faire, nous avons suivi les étapes suivantes : 1. Estimation des paramètres par maximum de vraisemblance : Nous avons estimé \\(\\hat{\\xi}\\), \\(\\hat{\\mu}\\) et \\(\\hat{\\sigma}\\) en maximisant la log-vraisemblance de la loi GEV.\n\nConstruction du profil de vraisemblance : Nous avons fixé \\(\\xi\\) à différentes valeurs autour de \\(\\hat{\\xi}\\) et, pour chacune, réestimé \\(\\mu\\) et \\(\\sigma\\) afin d’obtenir une log-vraisemblance profilée.\nSeuil basé sur le test du rapport de vraisemblance : Le seuil critique est déterminé par la statistique $ ^2(1) $ :\n\\[\n\\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n\\]\nDétermination des bornes de l’IC : L’intervalle est formé par les valeurs de \\(\\xi\\) pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l’incertitude en évitant les approximations asymptotiques classiques. La modélisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adaptée.\nNous obtenons ainsi un intervalle de confiance à 95% pour le paramètre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\). Comme 0 n’appartient pas à cet intervalle, nous pouvons rejeter l’hypothèse \\(\\xi=0\\). De ce fait, la distribution de Weibull est plus adaptée pour modéliser les maxima des pertes de rendements du CAC 40 car \\(\\xi\\) est négatif.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance négative de la distribution GEV\n    en fixant le paramètre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale &lt;= 0:  # Contrainte pour éviter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profilés pour le paramètre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estimée\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # Réoptimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' à la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# Déterminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood &gt;= chi2_threshold)[0]\nif len(valid_points) &gt; 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profilé pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de déterminer des IC profilés avec les paramètres actuels.\")\n\n# Tracé du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n\nIC profilé pour shape: [-0.287, -0.042]\n\n\n\n\n\n\n\n\n\n\na. Validation ex-ante\nOn remarque la loi GEV estimée par une weibull semble coller à la distribution des rendements extrêmes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles théoriques de la GEV-Weibull et empiriques sembelnt alignés sauf pour les quantiles élévés où l’on constate un décrochage.\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observées')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajustée')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi GEV\")\nplt.xlabel('Quantiles théoriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nb. Calcul de la VaR TVE par MB\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\noù G est la fonction de répartition de la GEV\\((\\hat \\mu, \\hat \\sigma, \\hat \\xi)\\) estimée, et \\(\\alpha_{\\text{GEV}}\\) est ajusté pour correspondre à l’horizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nPour convertir la VaR à horizon 1jour en VaR à horizon T jours, la méthode de scaling soulève quelques questions, car elle repose essentiellement sur la normalité et l’indépendance des rendements ce qui n’est pas le cas en pratique. De ce fait, nous utiliserons la méthode alternative reposant sur la théorie des valeurs extrêmes.\ny revenir\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n\n\n\n\n\nI.1.3. Estimation des paramètres de la loi de EV\nBien que l’intervalle de confiance à 95% pour le paramètre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\) ne contienne pas 0, nous avons tout de même estimé les paramètres de la loi EV pour comparer les résultats avec ceux de la loi GEV. En estimant tout de même les paramètres de la loi EV, nous obtenons les paramètres suivants : \\(\\mu=0.02, \\sigma=0.01, \\xi=0\\).\nNous constatons que la loi EV ne semble pas mal s’adapter à la distribution des rendements extrêmes du CAC 40.\n\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les paramètres estimés\nprint(\"=\"*50)\nprint(\"Paramètres estimés de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParamètres estimés de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observées')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densité GEV ajustée\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajustée')\n\n# Densité Gumbel ajustée\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajustée')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi GEV Weibull\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title(\"QQ Plot d'une modélisation par loi Gumbel\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n\n\nDe plus, les résultats en terme de VaR sont très proches entre les deux modèles."
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "href": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "title": "TP2:Méthodes basées sur la théorie des valeurs extrêmes]",
    "section": "I.2. VaR TVE : Approche Peak over threshold",
    "text": "I.2. VaR TVE : Approche Peak over threshold\n\nI.2.1. Choix du seuil u\nCette méthode est basée sur la modélisation de la distribution des excès au-dessus d’un seuil élevé de log-rendement négatif (\\(u\\)), seuil déterminé de manière subjective à partir de l’analyse du mean residual life plot, en ajustant une distribution de Pareto généralisée (GPD). Dans le mean residual life plot, si les excès au-delà de 𝒖 suivent une loi GPD, alors le mean-excess plot a un comportement linéaire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est linéaire. Nous ne privilégions pas les seuils \\(u\\) élevés puisque la moyenne est faite sur peu d’observations.\nNous allons choisir un seuil \\(u = 0.03\\) pour lequel le mean residual life plot est linéaire. Nous allons ensuite ajuster une distribution GPD pour les excès au-dessus de ce seuil.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Paramètres :\n    - data : array-like, données d'entrée.\n    - tlim : tuple (min, max), limites des seuils (si None, calculé automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils à considérer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les données\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn &lt;= 5:\n        raise ValueError(\"Les données contiennent trop peu de valeurs valides.\")\n\n    # Définition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # Évite les 4 plus grandes valeurs\n\n    if np.all(data &lt;= tlim[1]):\n        raise ValueError(\"La borne supérieure du seuil est trop élevée.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data &lt;= tlim[0]), np.mean(data &lt;= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les résultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data &gt; u] - u  # Excès au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Tracé du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile à 5%\n\n\n\n\n\n\n\n\n\n\nI.2.2. Estimation des paramètres de la loi GPD\nEn estimant les paramètres de la loi GPD, nous utilisons la méthode du maximum de vraisemblance. Les paramètres estimés par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01\\). De ce fait, la distribution de Pareto généralisée est adaptée pour modéliser les excès au-dessus du seuil \\(u = 0.03\\).\n\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les paramètres estimés\nprint(\"Paramètres estimés de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n\nParamètres estimés de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n\n\n\n\nI.2.3. Validation ex-ante\nEn comparant la distribution GPD estimée et la distribution empirique des excès, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estimé indique que les quantiles théoriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observés dans notre distribution des excès. Nous concluons que la distribution GPD n’est pas adaptée pour modéliser les excès au-dessus du seuil \\(u = 0.03\\). Cela peut être dû à un mauvais choix du seuil \\(u\\), une analyse plus aprofondie aurait été nécessaire pour choisir un seuil plus adapté.\n\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Données observées des excès')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densité GPD ajustée\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajustée')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une modélisation par loi GPD\")\nplt.xlabel('Quantiles théoriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance \\({\\alpha}\\) est alors obtenue par la formule :\n\\[\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n\\]\noù \\(\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})\\) est la fonction de répartition de la GPD(\\(\\hat{\\sigma},\\hat{\\xi}\\)) estimée, \\(\\alpha_{\\text{POT}}\\) est le quantile ajusté, nécessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\nComme on ne se concentre que sur l’échantillon des excès dans cette modélisation, l’estimation de la VaR à partir de la GPD ne doit pas se faire au niveau \\(\\alpha\\), mais à un niveau ajusté \\(\\alpha_{\\text{POT}}\\) défini par la relation suivante :\n\\[\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n\\]\noù \\(n\\) représente le nombre total d’observations, \\(N_u\\) correspond au nombre d’excès au-delà du seuil \\(u\\),\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Cheryl KOUADIO\n\n\nEtudiante en gestion des risques financiers\n\n\nEmail : cheryl.s.kouadio@gmail.com\n\n\nCV (Français/English) : cv_cheryl_kouadio_fr.pdf / cv_cheryl_kouadio_en.pdf\n\n\nGithub :  github.com/cheryl-kdio\n\n\nLinkedin : /in/cheryl-kouadio-251815206 \n\n\nMedium :  medium.com/@cheryl.s.kouadio\n\n\n\n\n\nA propos de moi\n\nJe suis Cheryl Kouadio, étudiante en gestion des risques bancaires à l’ENSAI. Je me spécialise dans la modélisation des risques financiers, en particulier les risques de marché et de crédit, ainsi que dans l’étude des normes financières, comptables et réglementaires encadrant le système bancaire (Bâle III, IFRS 9, etc.).\nCe site web, conçu et généré avec Quarto, s’adresse principalement aux étudiants de l’ENSAI et a pour vocation d’offrir un soutien à ceux qui, comme moi, ont été confrontés à des défis académiques au cours de leur formation, notamment en deuxième et troisième année. Il ne prétend en aucun cas se substituer à l’enseignement dispensé par nos professeurs, dont la rigueur et l’expertise sont essentielles. Son objectif est plutôt de compléter leur travail en partageant mes expériences personnelles et les projets que j’ai réalisés.\nAu-delà de son utilité pour les étudiants, ce site constitue également une ressource précieuse pour les professionnels souhaitant revisiter certains concepts clés en gestion des risques ou même en statistiques. Il propose des exemples concrets et des pistes de réflexion adaptées aux problématiques actuelles, afin d’aider chacun à mieux appréhender ses projets et à relever ses propres défis académiques et professionnels.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "autres/finance_durable.html",
    "href": "autres/finance_durable.html",
    "title": "Investissement responsable",
    "section": "",
    "text": "Le cadre de la finance durable\nLa finance durable vise à concilier rentabilité financière et prise en compte des enjeux environnementaux, sociaux et de gouvernance (ESG). Elle repose sur trois grands objectifs :\n\nRéorienter les flux de capitaux vers des investissements responsables.\nGérer les risques financiers liés aux critères ESG (changement climatique, violations des droits humains, mauvaise gouvernance).\nFavoriser la transparence et une vision à long terme grâce aux nouvelles régulations.\n\nPour structurer cette transition, plusieurs outils ont été mis en place :\n\nLes critères ESG, qui permettent d’évaluer la durabilité d’un investissement.\nLa Taxonomie Européenne, qui définit quelles activités économiques sont réellement durables. Il a 6 objectifs principaux.\nLes labels ISR (pour des investissements prenant en compte les critères ESG), Greenfin (pour des projects plus verts, excluant les énergies fossiles) et Finansol (une finance solidaire pour prendre en compte l’aspect «social»), qui garantissent des investissements alignés avec la finance responsable.\nLes obligations réglementaires (SFDR - Règlement, CSRD - Directive), qui imposent un reporting extra-financier aux entreprises et investisseurs.\n\nCependant, des défis subsistent, comme le greenwashing, le manque d’uniformisation des notations ESG et la crainte d’un impact négatif sur la performance financière. Malgré cela, la finance durable est devenue un levier incontournable pour assurer une économie plus résiliante et éthique. La finance durable n’est plus une option mais une nécessité pour répondre aux défis climatiques et sociaux tout en maintenant la stabilité du système financier mondial.\n\n\nLa finance durable appliquée à l’investissement\nLa finance durable ne se limite pas à des principes théoriques ; elle se traduit par des stratégies d’investissement concrètes visant à intégrer les critères ESG dans la gestion des actifs financiers. Pour cela, plusieurs approches existent.\n\nLes stratégies d’investissement responsable\n\nL’Exclusion : Éliminer certains secteurs ou entreprises jugés non responsables (ex : tabac, énergies fossiles, armement).\nLe Best-in-Class : Sélectionner les entreprises ayant les meilleures pratiques ESG dans chaque secteur, sans exclure de domaines spécifiques.\nL’Engagement actionnarial : Influer sur les entreprises en tant qu’actionnaire via le vote en assemblée générale et le dialogue.\nL’Impact Investing : Financer directement des entreprises ou projets ayant un impact environnemental ou social positif mesurable. Les stratégies les plus actives et efficaces sont l’Engagement Actionnarial et l’Impact Investing, car elles permettent d’influencer directement les pratiques ESG et de créer un impact mesurable sur l’économie.\n\n\n\nL’importance des données ESG\nL’investissement responsable repose sur des données ESG fiables pour évaluer la performance extra-financière des entreprises. Cependant, plusieurs limites persistent :\n\nManque de standardisation : Chaque fournisseur (MSCI, Sustainalytics, Moody’s ESG) utilise des méthodologies différentes, rendant les comparaisons complexes.\nRisque de biais et greenwashing : Certaines entreprises embellissent leurs rapports ESG pour obtenir de meilleures notes.\nAbsence de transparence : Les investisseurs doivent analyser minutieusement les sources de données avant de prendre des décisions.\n\nDes solutions existent pour améliorer la fiabilité des notations ESG :\n\nHarmonisation des standards via des régulations comme la directive CSRD.\nAudit indépendant des données ESG pour éviter les manipulations.\nUtilisation de l’IA et du Big Data pour analyser des sources plus diversifiées et détecter les incohérences.\n\n\n\n\n\n\n\nNote\n\n\n\nL’intégration de l’ESG est une approche dynamique et évolutive, qui dépend du niveau d’engagement souhaité par l’investisseur.L’Engagement actionnarial et l’Impact Investing sont les stratégies les plus efficaces, car elles permettent d’influencer l’économie de manière proactive. La fiabilité des données ESG doit encore être renforcée pour garantir une transparence totale dans la finance durable.\n\n\n\n\n\nPrise en compte du risque climatique dans la gestion d’actifs\nLe changement climatique est devenu un facteur clé dans la gestion d’actifs. Les investisseurs doivent intégrer ces risques pour protéger leurs portefeuilles, se conformer aux régulations et capter les opportunités de la transition énergétique.\n\nImpact du risque climatique sur les actifs financiers\nDeux types de risques à prendre en compte :\n\nRisque physique → Catastrophes naturelles (ouragans, inondations, sécheresses) qui endommagent les infrastructures et réduisent la valeur des actifs.\nRisque de transition → Régulations environnementales et évolutions du marché qui dévalorisent les industries polluantes (pétrole, charbon, transport intensif, etc.). Exemple : L’immobilier en zone inondable peut perdre de la valeur, et les entreprises pétrolières risquent de voir leurs coûts augmenter avec la taxe carbone.\n\n\n\nMesure et gestion des risques climatiques en finance\nIndicateurs utilisés :\n\nScore ESG → Évaluation des critères environnementaux, sociaux et de gouvernance.\nÉmissions de GES (Scope 1, 2, 3) → Mesure l’empreinte carbone d’une entreprise ou d’un portefeuille.\nAlignement 2°C → Vérifie si un investissement est compatible avec les objectifs climatiques de l’Accord de Paris.\nStress-tests climatiques → Simulent l’impact des scénarios climatiques sur les portefeuilles d’investissement.\n\n\n\nOutils de gestion des risques\n\nDiversification des actifs → Réduire l’exposition aux secteurs vulnérables au climat.\nExclusion des industries polluantes → Éviter les investissements dans le charbon et le pétrole.\nInvestissement dans des actifs verts (Green Bonds, infrastructures bas carbone) → Financer la transition énergétique.\nEngagement actionnarial → Influer sur les entreprises en votant en assemblée générale pour exiger des stratégies bas carbone.\n\n\n\nRaisons pour intégrer les scénarios climatiques en gestion de portefeuille ?\n\nÉviter les pertes financières → Anticiper l’impact du climat sur les entreprises et secteurs sensibles.\nSe conformer aux régulations → Respecter les normes SFDR, CSRD et Taxonomie Européenne.\nCapter les opportunités d’investissement → Financer des entreprises et projets alignés avec la transition énergétique.\n\nExemple : Un investisseur qui anticipe les réglementations sur les énergies fossiles pourra transférer ses capitaux vers les énergies renouvelables, évitant ainsi des pertes et profitant de la croissance du secteur.\n\n\n\n\n\n\nNote\n\n\n\nLe risque climatique est un enjeu central en gestion d’actifs. Il affecte la valorisation des entreprises, les régulations financières et les décisions d’investissement. Les investisseurs doivent intégrer ces risques dans leur gestion de portefeuille, en utilisant des indicateurs ESG, des stress-tests climatiques et des stratégies de diversification. Ne pas prendre en compte ces scénarios expose à des actifs dévalorisés et à des pertes financières à long terme.\n\n\n\n\n\n\n\n\nTake away\n\n\n\nL’investissement responsable intègre les critères ESG (Environnement, Social, Gouvernance) dans la gestion d’actifs afin de concilier performance financière et impact durable. Il repose sur différentes stratégies : exclusion des secteurs controversés, Best-in-Class, Engagement actionnarial et Impact Investing. Les régulations, comme la Taxonomie Européenne, la SFDR et la CSRD, imposent plus de transparence aux entreprises et investisseurs pour lutter contre le greenwashing et favoriser la transition vers une économie bas carbone. Le risque climatique, divisé en risques physiques (catastrophes naturelles) et risques de transition (régulations et évolutions de marché), influence fortement la valorisation des actifs. Les outils comme l’alignement 2°C et les stress-tests climatiques aident à évaluer l’exposition des portefeuilles. Enfin, les Green Bonds et autres financements durables permettent de soutenir la transition énergétique et d’orienter les capitaux vers des projets à fort impact environnemental positif. L’investissement responsable est donc un levier clé pour une finance plus durable et résiliente.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "autres/tesst.html#rs",
    "href": "autres/tesst.html#rs",
    "title": "CHAT",
    "section": "rs",
    "text": "rs"
  },
  {
    "objectID": "autres/tesst.html#m",
    "href": "autres/tesst.html#m",
    "title": "CHAT",
    "section": "m",
    "text": "m"
  },
  {
    "objectID": "autres/tesst.html#quarto",
    "href": "autres/tesst.html#quarto",
    "title": "CHAT",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "autres/tesst.html#bullets",
    "href": "autres/tesst.html#bullets",
    "title": "CHAT",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "autres/tesst.html#code",
    "href": "autres/tesst.html#code",
    "title": "CHAT",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Note\n\n\n\nCompte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire.\n\n\n\n\n\n\nConstruction du bilan d’entreprise\nReglementation prudentielle TO REWRITE\n\n\n\n\n\n\n\nDéfinition du risque financier\nValue-at-risk (VaR) :\n\nDéfinition de la VaR\nImplémentation de la VaR sur python\nTP1 : Méthodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\nTP2 : Méthodes basées sur la théorie des valeurs extrêmes\nTP3 : Méthodes d’une calcul d’une VaR dynamique (basé sur GARCH)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le modèle de black-scholes\nCalibration du modèle à volatilité stochastique de Taylor - Filtre de Kalman\nCalibration du modèle à volatilité stochastique de Taylor - Filtre particulaire bootstrap\nModèle de Heston :\n\nEstimation de la volatilité avec le filtre boostrap\nComparaison du filtre bootstrap au filtre particulaire auxiliaire (APF) TO DO\n\nCourbes de taux (Construction d’une courbe ZC, Modèle Hull -White)\n\n\n\n\n\n\n\nGestion de risques d’un portefeuille d’actifs\nProfil d’écoulement de portefeuille\nPricing d’options vanilles TO DO\nPricing de taux, swap, d’obligations TO DO\nTracking error TO DO\nConstruction de portefeuille markowitz TO DO\n\n\n\n\n\n\n\nInvestissement socialement responsable\nFinance durable\ntest\nCorrélation de spearman vs Corrélation de pearson\n\n\n\n\n\n\nProjets\n\nProjet de séries temporelles\nProjet de scoring\nProjet de théorie de valeurs extrêmes 1\nProjet de théorie de valeurs extrêmes 2\n\nApplications déployées\n\nStock price prediction : CAC40\nAsset pricing and management"
  },
  {
    "objectID": "index_gdr.html#projets-et-applications-déployées",
    "href": "index_gdr.html#projets-et-applications-déployées",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Projets\n\nProjet de séries temporelles\nProjet de scoring\nProjet de théorie de valeurs extrêmes 1\nProjet de théorie de valeurs extrêmes 2\n\nApplications déployées\n\nStock price prediction : CAC40\nAsset pricing and management"
  }
]