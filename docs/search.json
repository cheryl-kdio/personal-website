[
  {
    "objectID": "index_stat.html",
    "href": "index_stat.html",
    "title": "Statistiques",
    "section": "",
    "text": "Note\n\n\n\nLes contenus partag√©s dans cette section d√©di√©e √† la 2·µâ ann√©e (2A) mettront principalement en avant les enseignements sp√©cifiques de ce niveau. Je m‚Äôefforcerai de les rendre aussi pertinents et enrichissants que possible, en esp√©rant qu‚Äôils constituent un rep√®re utile pour celles et ceux qui suivront un parcours similaire.\n\n\n\n\n\nR√©sum√© de la th√©orie des valeurs extr√™mes\n\n\n\n\n\nR√©sum√© des m√©thodes de micro√©conom√©trie appliqu√©e enseign√©es √† la promo 2024-2025 ENSAI\n\n\n\n\n\nR√©gression lin√©aire\nKernel Trick & SVM\nGradient Boosting\nFeature Selection\nR√©gression Ridge & Lasso\nCART & Random Forest"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)\n\n\n33 min\n\n\n\n\n\n\nCheryl Kouadio, Sous la supervision de : Fai√ßal HIHI\n\n\nMar 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProjet de gestion de risques multiples\n\n\n29 min\n\n\n\n\n\n\nCheryl Kouadio, Mariyam Ouyassin\n\n\nMar 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMod√®les de courbe de taux\n\n\n43 min\n\n\n\n\n\n\nCheryl Kouadio, Mariyam Ouyassin\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\n\n\n37 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\n\n\n17 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\n\n\n20 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAPF filter\n\n\n25 min\n\n\n\n\n\n\nCheryl Kouadio, Safa Bouzayene, Mariyam Ouyassin\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le mod√®le d‚ÄôHeston\n\n\n9 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAsset management : risque de march√©\n\n\n11 min\n\n\n\n\n\n\nCheryl Kouadio, Sous la supervision de : Fai√ßal HIHI\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAsset management : Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)\n\n\n28 min\n\n\n\n\n\n\nCheryl Kouadio, Sous la supervision de : Fai√ßal HIHI\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire\n\n\n15 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman\n\n\n8 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le Black-Scholes\n\n\n8 min\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFeatures selection\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\n\n7 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Trick and SVM\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRidge regression vs.¬†Lasso regression\n\n\n12 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nSep 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa r√©glementation prudentielle\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu‚Äôest ce que c‚Äôest ?\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa r√©gression lin√©aire\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise\n\n\n27 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "autres/finance_durable.html",
    "href": "autres/finance_durable.html",
    "title": "Investissement responsable",
    "section": "",
    "text": "Le cadre de la finance durable\nLa finance durable vise √† concilier rentabilit√© financi√®re et prise en compte des enjeux environnementaux, sociaux et de gouvernance (ESG). Elle repose sur trois grands objectifs :\n\nR√©orienter les flux de capitaux vers des investissements responsables.\nG√©rer les risques financiers li√©s aux crit√®res ESG (changement climatique, violations des droits humains, mauvaise gouvernance).\nFavoriser la transparence et une vision √† long terme gr√¢ce aux nouvelles r√©gulations.\n\nPour structurer cette transition, plusieurs outils ont √©t√© mis en place :\n\nLes crit√®res ESG, qui permettent d‚Äô√©valuer la durabilit√© d‚Äôun investissement.\nLa Taxonomie Europ√©enne, qui d√©finit quelles activit√©s √©conomiques sont r√©ellement durables. Il a 6 objectifs principaux.\nLes labels ISR (pour des investissements prenant en compte les crit√®res ESG), Greenfin (pour des projects plus verts, excluant les √©nergies fossiles) et Finansol (une finance solidaire pour prendre en compte l‚Äôaspect ¬´social¬ª), qui garantissent des investissements align√©s avec la finance responsable.\nLes obligations r√©glementaires (SFDR - R√®glement, CSRD - Directive), qui imposent un reporting extra-financier aux entreprises et investisseurs.\n\nCependant, des d√©fis subsistent, comme le greenwashing, le manque d‚Äôuniformisation des notations ESG et la crainte d‚Äôun impact n√©gatif sur la performance financi√®re. Malgr√© cela, la finance durable est devenue un levier incontournable pour assurer une √©conomie plus r√©siliante et √©thique. La finance durable n‚Äôest plus une option mais une n√©cessit√© pour r√©pondre aux d√©fis climatiques et sociaux tout en maintenant la stabilit√© du syst√®me financier mondial.\n\n\nLa finance durable appliqu√©e √† l‚Äôinvestissement\nLa finance durable ne se limite pas √† des principes th√©oriques ; elle se traduit par des strat√©gies d‚Äôinvestissement concr√®tes visant √† int√©grer les crit√®res ESG dans la gestion des actifs financiers. Pour cela, plusieurs approches existent.\n\nLes strat√©gies d‚Äôinvestissement responsable\n\nL‚ÄôExclusion : √âliminer certains secteurs ou entreprises jug√©s non responsables (ex : tabac, √©nergies fossiles, armement).\nLe Best-in-Class : S√©lectionner les entreprises ayant les meilleures pratiques ESG dans chaque secteur, sans exclure de domaines sp√©cifiques.\nL‚ÄôEngagement actionnarial : Influer sur les entreprises en tant qu‚Äôactionnaire via le vote en assembl√©e g√©n√©rale et le dialogue.\nL‚ÄôImpact Investing : Financer directement des entreprises ou projets ayant un impact environnemental ou social positif mesurable. Les strat√©gies les plus actives et efficaces sont l‚ÄôEngagement Actionnarial et l‚ÄôImpact Investing, car elles permettent d‚Äôinfluencer directement les pratiques ESG et de cr√©er un impact mesurable sur l‚Äô√©conomie.\n\n\n\nL‚Äôimportance des donn√©es ESG\nL‚Äôinvestissement responsable repose sur des donn√©es ESG fiables pour √©valuer la performance extra-financi√®re des entreprises. Cependant, plusieurs limites persistent :\n\nManque de standardisation : Chaque fournisseur (MSCI, Sustainalytics, Moody‚Äôs ESG) utilise des m√©thodologies diff√©rentes, rendant les comparaisons complexes.\nRisque de biais et greenwashing : Certaines entreprises embellissent leurs rapports ESG pour obtenir de meilleures notes.\nAbsence de transparence : Les investisseurs doivent analyser minutieusement les sources de donn√©es avant de prendre des d√©cisions.\n\nDes solutions existent pour am√©liorer la fiabilit√© des notations ESG :\n\nHarmonisation des standards via des r√©gulations comme la directive CSRD.\nAudit ind√©pendant des donn√©es ESG pour √©viter les manipulations.\nUtilisation de l‚ÄôIA et du Big Data pour analyser des sources plus diversifi√©es et d√©tecter les incoh√©rences.\n\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôint√©gration de l‚ÄôESG est une approche dynamique et √©volutive, qui d√©pend du niveau d‚Äôengagement souhait√© par l‚Äôinvestisseur.L‚ÄôEngagement actionnarial et l‚ÄôImpact Investing sont les strat√©gies les plus efficaces, car elles permettent d‚Äôinfluencer l‚Äô√©conomie de mani√®re proactive. La fiabilit√© des donn√©es ESG doit encore √™tre renforc√©e pour garantir une transparence totale dans la finance durable.\n\n\n\n\n\nPrise en compte du risque climatique dans la gestion d‚Äôactifs\nLe changement climatique est devenu un facteur cl√© dans la gestion d‚Äôactifs. Les investisseurs doivent int√©grer ces risques pour prot√©ger leurs portefeuilles, se conformer aux r√©gulations et capter les opportunit√©s de la transition √©nerg√©tique.\n\nImpact du risque climatique sur les actifs financiers\nDeux types de risques √† prendre en compte :\n\nRisque physique ‚Üí Catastrophes naturelles (ouragans, inondations, s√©cheresses) qui endommagent les infrastructures et r√©duisent la valeur des actifs.\nRisque de transition ‚Üí R√©gulations environnementales et √©volutions du march√© qui d√©valorisent les industries polluantes (p√©trole, charbon, transport intensif, etc.). Exemple : L‚Äôimmobilier en zone inondable peut perdre de la valeur, et les entreprises p√©troli√®res risquent de voir leurs co√ªts augmenter avec la taxe carbone.\n\n\n\nMesure et gestion des risques climatiques en finance\nIndicateurs utilis√©s :\n\nScore ESG ‚Üí √âvaluation des crit√®res environnementaux, sociaux et de gouvernance.\n√âmissions de GES (Scope 1, 2, 3) ‚Üí Mesure l‚Äôempreinte carbone d‚Äôune entreprise ou d‚Äôun portefeuille.\nAlignement 2¬∞C ‚Üí V√©rifie si un investissement est compatible avec les objectifs climatiques de l‚ÄôAccord de Paris.\nStress-tests climatiques ‚Üí Simulent l‚Äôimpact des sc√©narios climatiques sur les portefeuilles d‚Äôinvestissement.\n\n\n\nOutils de gestion des risques\n\nDiversification des actifs ‚Üí R√©duire l‚Äôexposition aux secteurs vuln√©rables au climat.\nExclusion des industries polluantes ‚Üí √âviter les investissements dans le charbon et le p√©trole.\nInvestissement dans des actifs verts (Green Bonds, infrastructures bas carbone) ‚Üí Financer la transition √©nerg√©tique.\nEngagement actionnarial ‚Üí Influer sur les entreprises en votant en assembl√©e g√©n√©rale pour exiger des strat√©gies bas carbone.\n\n\n\nRaisons pour int√©grer les sc√©narios climatiques en gestion de portefeuille ?\n\n√âviter les pertes financi√®res ‚Üí Anticiper l‚Äôimpact du climat sur les entreprises et secteurs sensibles.\nSe conformer aux r√©gulations ‚Üí Respecter les normes SFDR, CSRD et Taxonomie Europ√©enne.\nCapter les opportunit√©s d‚Äôinvestissement ‚Üí Financer des entreprises et projets align√©s avec la transition √©nerg√©tique.\n\nExemple : Un investisseur qui anticipe les r√©glementations sur les √©nergies fossiles pourra transf√©rer ses capitaux vers les √©nergies renouvelables, √©vitant ainsi des pertes et profitant de la croissance du secteur.\n\n\n\n\n\n\nNote\n\n\n\nLe risque climatique est un enjeu central en gestion d‚Äôactifs. Il affecte la valorisation des entreprises, les r√©gulations financi√®res et les d√©cisions d‚Äôinvestissement. Les investisseurs doivent int√©grer ces risques dans leur gestion de portefeuille, en utilisant des indicateurs ESG, des stress-tests climatiques et des strat√©gies de diversification. Ne pas prendre en compte ces sc√©narios expose √† des actifs d√©valoris√©s et √† des pertes financi√®res √† long terme.\n\n\n\n\n\n\n\n\nTake away\n\n\n\nL‚Äôinvestissement responsable int√®gre les crit√®res ESG (Environnement, Social, Gouvernance) dans la gestion d‚Äôactifs afin de concilier performance financi√®re et impact durable. Il repose sur diff√©rentes strat√©gies : exclusion des secteurs controvers√©s, Best-in-Class, Engagement actionnarial et Impact Investing. Les r√©gulations, comme la Taxonomie Europ√©enne, la SFDR et la CSRD, imposent plus de transparence aux entreprises et investisseurs pour lutter contre le greenwashing et favoriser la transition vers une √©conomie bas carbone. Le risque climatique, divis√© en risques physiques (catastrophes naturelles) et risques de transition (r√©gulations et √©volutions de march√©), influence fortement la valorisation des actifs. Les outils comme l‚Äôalignement 2¬∞C et les stress-tests climatiques aident √† √©valuer l‚Äôexposition des portefeuilles. Enfin, les Green Bonds et autres financements durables permettent de soutenir la transition √©nerg√©tique et d‚Äôorienter les capitaux vers des projets √† fort impact environnemental positif. L‚Äôinvestissement responsable est donc un levier cl√© pour une finance plus durable et r√©siliente.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Universit√© Paris Cit√© ‚Äì Paris\nMaster 2 ‚Äì Mod√©lisation Al√©atoire (M2MO), Ex-DEA Laure Elie | Sept.¬†2025 ‚Äì Sept.¬†2026\nSp√©cialisation : Finance quantitative, probabilit√©s avanc√©es, statistiques, et mod√©lisation stochastique\nENSAI ‚Äì √âcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation ‚Äì Rennes\nDipl√¥me d‚Äôing√©nieur en statistiques (grade de master) | Sept.¬†2022 ‚Äì Sept.¬†2025\nSp√©cialisation : Gestion des risques et ing√©nierie financi√®re\n\n\n\nSoci√©t√© G√©n√©rale ‚Äì RISQ/MDL/MOD\nStagiaire analyste quantitatif ‚Äì IFRS 9 | Avril 2025 - Aout 2025\nINSERM\nStagiaire recherche ‚Äì Mixture cure models | Mai 2024 - Aout 2024\n\n\n\n\n\n\nNote\n\n\n\nCe site web, con√ßu et g√©n√©r√© avec Quarto, s‚Äôadresse principalement aux √©tudiants de l‚ÄôENSAI et du M2MO et a pour vocation d‚Äôoffrir un soutien √† ceux qui, comme moi, ont √©t√© confront√©s √† des d√©fis acad√©miques au cours de leur formation. Il ne pr√©tend en aucun cas se substituer √† l‚Äôenseignement dispens√© par nos professeurs, dont la rigueur et l‚Äôexpertise sont essentielles. Son objectif est plut√¥t de compl√©ter leur travail en partageant mes exp√©riences personnelles et les projets que j‚Äôai r√©alis√©s."
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html",
    "href": "3A/value-at-risk/var_evt.html",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap. Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, on peut utiliser des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold.\nShow the code\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n\n/Users/cherylkouadio/Documents/Repositories/personal-website/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\nShow the code\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\nShow the code\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test"
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "href": "3A/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes",
    "text": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes\n\nI.1. VaR TVE : Approche Maxima par bloc\nL‚Äôapproche des Block Maxima (BM) est une m√©thode mod√©lise les maxima des rendements sur des blocs de taille fixe \\(s\\) en utilisant la distribution GEV. Le seuil de confiance \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nNous allons dans ce projet une taille de blocs \\(s = 21\\) jours ouvr√©s comme ce qui souvent utilis√© en pratique. De ce fait, nous parvenons √† construire 239 blocs de taille 21 et un bloc de taille De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ _{}$ est :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV (\\(\\hat \\mu, \\hat \\sigma, \\hat \\xi\\)) estim√©e.\n\n\nI.1.1. Construction de l‚Äô√©chantillon de maxima sur data_train\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extr√™mes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n\n\n\n\n\n\n\n\n\nPour avoir une id√©e de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour \\(s=21\\), nous utilisons un Gumbel plot qui est un outil graphique pour juger de l‚Äôhypoth√®se \\(\\xi=0\\), i.e.¬†la distribution GEV se r√©duit √† la distribution de Gumbel.\nPour le construire, nous devons suivre les √©tapes suivantes :\n\ncalculer l‚Äôabscisse avec la s√©rie des maximas ordon√©es \\(R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}\\).\ncalculer l‚Äôordonn√©e de la mani√®re suivante :\n\n\\[\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n\\]\nLorsque la distribution adapt√©e est celle de Gumbel alors le Gumbel plot est lin√©aire. Dans notre cas, nous constatons une courbure ce qui nous pousse √† conclure qu‚Äôune distribution Gumbel n‚Äôest pas adapt√©e dans la mod√©lisation des maxima des pertes de rendements du CAC 40. Une distribution Fr√©chet ou de Weibull serait plus adapt√©e.\n\n\nShow the code\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nI.1.2. Estimation des param√®tres de la loi de GEV\nEn estimant les param√®tres de la loi GEV, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = -0.15, \\mu=0.02, \\sigma=0.01\\). Nous constatons par ailleurs que le param√®tre de forme \\(\\xi\\) est n√©gatif ce qui est coh√©rent avec notre observation pr√©c√©dente.\n\n\nShow the code\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n\n\n==================================================\nParam√®tres estim√©s de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\nPour accorder plus de poids √† cette observation, nous avons calcul√© un intervalle de confiance profil√© √† 95% pour le param√®tre de forme \\(\\xi\\). Pour ce faire, nous avons suivi les √©tapes suivantes : 1. Estimation des param√®tres par maximum de vraisemblance : Nous avons estim√© \\(\\hat{\\xi}\\), \\(\\hat{\\mu}\\) et \\(\\hat{\\sigma}\\) en maximisant la log-vraisemblance de la loi GEV.\n\nConstruction du profil de vraisemblance : Nous avons fix√© \\(\\xi\\) √† diff√©rentes valeurs autour de \\(\\hat{\\xi}\\) et, pour chacune, r√©estim√© \\(\\mu\\) et \\(\\sigma\\) afin d‚Äôobtenir une log-vraisemblance profil√©e.\nSeuil bas√© sur le test du rapport de vraisemblance : Le seuil critique est d√©termin√© par la statistique $ ^2(1) $ :\n\\[\n\\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n\\]\nD√©termination des bornes de l‚ÄôIC : L‚Äôintervalle est form√© par les valeurs de \\(\\xi\\) pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l‚Äôincertitude en √©vitant les approximations asymptotiques classiques. La mod√©lisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adapt√©e.\nNous obtenons ainsi un intervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\). Comme 0 n‚Äôappartient pas √† cet intervalle, nous pouvons rejeter l‚Äôhypoth√®se \\(\\xi=0\\). De ce fait, la distribution de Weibull est plus adapt√©e pour mod√©liser les maxima des pertes de rendements du CAC 40 car \\(\\xi\\) est n√©gatif.\n\n\nShow the code\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance n√©gative de la distribution GEV\n    en fixant le param√®tre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale &lt;= 0:  # Contrainte pour √©viter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profil√©s pour le param√®tre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estim√©e\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # R√©optimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' √† la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# D√©terminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood &gt;= chi2_threshold)[0]\nif len(valid_points) &gt; 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profil√© pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de d√©terminer des IC profil√©s avec les param√®tres actuels.\")\n\n# Trac√© du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n\n\nIC profil√© pour shape: [-0.287, -0.042]\n\n\n\n\n\n\n\n\n\n\na. Validation ex-ante\nOn remarque la loi GEV estim√©e par une weibull semble coller √† la distribution des rendements extr√™mes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles th√©oriques de la GEV-Weibull et empiriques sembelnt align√©s sauf pour les quantiles √©l√©v√©s o√π l‚Äôon constate un d√©crochage.\n\n\nShow the code\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV\")\nplt.xlabel('Quantiles th√©oriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nb. Calcul de la VaR TVE par MB\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV\\((\\hat \\mu, \\hat \\sigma, \\hat \\xi)\\) estim√©e, et \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nPour convertir la VaR √† horizon 1jour en VaR √† horizon T jours, la m√©thode de scaling soul√®ve quelques questions, car elle repose essentiellement sur la normalit√© et l‚Äôind√©pendance des rendements ce qui n‚Äôest pas le cas en pratique. De ce fait, nous utiliserons la m√©thode alternative reposant sur la th√©orie des valeurs extr√™mes.\ny revenir\n\n\nShow the code\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n\n\n\n\n\nI.1.3. Estimation des param√®tres de la loi de EV\nBien que l‚Äôintervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\) ne contienne pas 0, nous avons tout de m√™me estim√© les param√®tres de la loi EV pour comparer les r√©sultats avec ceux de la loi GEV. En estimant tout de m√™me les param√®tres de la loi EV, nous obtenons les param√®tres suivants : \\(\\mu=0.02, \\sigma=0.01, \\xi=0\\).\nNous constatons que la loi EV ne semble pas mal s‚Äôadapter √† la distribution des rendements extr√™mes du CAC 40.\n\n\nShow the code\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n\n\n==================================================\nParam√®tres estim√©s de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\n\n\nShow the code\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GEV ajust√©e\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\n\n# Densit√© Gumbel ajust√©e\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajust√©e')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV Weibull\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi Gumbel\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\n\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n\n\nDe plus, les r√©sultats en terme de VaR sont tr√®s proches entre les deux mod√®les."
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "href": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes",
    "section": "I.2. VaR TVE : Approche Peak over threshold",
    "text": "I.2. VaR TVE : Approche Peak over threshold\n\nI.2.1. Choix du seuil u\nCette m√©thode est bas√©e sur la mod√©lisation de la distribution des exc√®s au-dessus d‚Äôun seuil √©lev√© de log-rendement n√©gatif (\\(u\\)), seuil d√©termin√© de mani√®re subjective √† partir de l‚Äôanalyse du mean residual life plot, en ajustant une distribution de Pareto g√©n√©ralis√©e (GPD). Dans le mean residual life plot, si les exc√®s au-del√† de ùíñ suivent une loi GPD, alors le mean-excess plot a un comportement lin√©aire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est lin√©aire. Nous ne privil√©gions pas les seuils \\(u\\) √©lev√©s puisque la moyenne est faite sur peu d‚Äôobservations.\nNous allons choisir un seuil \\(u = 0.03\\) pour lequel le mean residual life plot est lin√©aire. Nous allons ensuite ajuster une distribution GPD pour les exc√®s au-dessus de ce seuil.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Param√®tres :\n    - data : array-like, donn√©es d'entr√©e.\n    - tlim : tuple (min, max), limites des seuils (si None, calcul√© automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils √† consid√©rer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les donn√©es\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn &lt;= 5:\n        raise ValueError(\"Les donn√©es contiennent trop peu de valeurs valides.\")\n\n    # D√©finition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # √âvite les 4 plus grandes valeurs\n\n    if np.all(data &lt;= tlim[1]):\n        raise ValueError(\"La borne sup√©rieure du seuil est trop √©lev√©e.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data &lt;= tlim[0]), np.mean(data &lt;= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les r√©sultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data &gt; u] - u  # Exc√®s au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Trac√© du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile √† 5%\n\n\n\n\n\n\n\n\n\n\n\nI.2.2. Estimation des param√®tres de la loi GPD\nEn estimant les param√®tres de la loi GPD, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01\\). De ce fait, la distribution de Pareto g√©n√©ralis√©e est adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\).\n\n\nShow the code\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n\n\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n\n\n\n\nI.2.3. Validation ex-ante\nEn comparant la distribution GPD estim√©e et la distribution empirique des exc√®s, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estim√© indique que les quantiles th√©oriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observ√©s dans notre distribution des exc√®s. Nous concluons que la distribution GPD n‚Äôest pas adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\). Cela peut √™tre d√ª √† un mauvais choix du seuil \\(u\\), une analyse plus aprofondie aurait √©t√© n√©cessaire pour choisir un seuil plus adapt√©.\n\n\nShow the code\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Donn√©es observ√©es des exc√®s')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GPD ajust√©e\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajust√©e')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une mod√©lisation par loi GPD\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nI.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance \\({\\alpha}\\) est alors obtenue par la formule :\n\\[\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n\\]\no√π \\(\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})\\) est la fonction de r√©partition de la GPD(\\(\\hat{\\sigma},\\hat{\\xi}\\)) estim√©e, \\(\\alpha_{\\text{POT}}\\) est le quantile ajust√©, n√©cessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\nComme on ne se concentre que sur l‚Äô√©chantillon des exc√®s dans cette mod√©lisation, l‚Äôestimation de la VaR √† partir de la GPD ne doit pas se faire au niveau \\(\\alpha\\), mais √† un niveau ajust√© \\(\\alpha_{\\text{POT}}\\) d√©fini par la relation suivante :\n\\[\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n\\]\no√π \\(n\\) repr√©sente le nombre total d‚Äôobservations, \\(N_u\\) correspond au nombre d‚Äôexc√®s au-del√† du seuil \\(u\\),\n\n\nShow the code\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html",
    "href": "3A/value-at-risk/var_classiques.html",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "",
    "text": "Ce TP est fait dans le but de mod√©liser la Value at Risk qui est une mesure de risque financier. La Value at Risk (VaR) est une mesure essentielle du risque de march√© qui estime la perte potentielle maximale d‚Äôun portefeuille sur un horizon h donn√©, avec un certain niveau de confiance \\(\\alpha\\). La VaR est utilis√©e par les institutions financi√®res et les gestionnaires de risques pour √©valuer l‚Äôexposition aux pertes extr√™mes et ajuster leurs strat√©gies d‚Äôinvestissement. Elle est d√©finie comme suit :\n\\[VaR_{\\alpha}(h) = inf \\{ l \\in \\mathbb{R} | P(PnL \\geq -l) \\geq 1 - \\alpha \\}\\]\nLe PnL repr√©sente le profit and loss, c‚Äôest-√†-dire la variation de la valeur du portefeuille. Dans notre cas, nous utilserons les rendements logarithmiques des actifs financiers, qui est stationnaire, pour calculer la VaR. Les rendements logarithmiques sont calcul√©s comme suit :\n\\[r_t = log(\\frac{P_t}{P_{t-1}}) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}.\\]\nCette mesure est pr√©f√©r√©e aux rendements simples car sa d√©composition en termes additifs permet de mieux mod√©liser les variations de prix des actifs financiers. De plus, en supposant la distribution identique et ind√©pendante des rendements, on peut facilement d√©terminer sa loi de probabilit√© et calculer la VaR.\nDans ce projet, nous explorerons plusieurs m√©thodes pour calculer la VaR, en tenant compte de la nature des rendements financiers et des hypoth√®ses sous-jacentes :\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tqdm import tqdm\nfrom scipy.stats import bootstrap\n\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\ndata.head()\n\n\n\n\n\n\n\nPrice\nClose\nHigh\nLow\nOpen\nVolume\nlog_return\n\n\nTicker\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n1990-03-02\n1860.0\n1860.0\n1831.0\n1831.0\n0\n0.015168\n\n\n1990-03-05\n1874.0\n1874.0\n1862.0\n1866.0\n0\n0.007499\n\n\n1990-03-06\n1872.0\n1875.0\n1866.0\n1869.0\n0\n-0.001068\n\n\n1990-03-07\n1880.0\n1881.0\n1874.0\n1874.0\n0\n0.004264\n\n\n1990-03-08\n1917.0\n1923.0\n1891.0\n1891.0\n0\n0.019490"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "href": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.1. Statistiques descriptives",
    "text": "II.1. Statistiques descriptives\nNous constatons que dans la p√©riode d‚Äôentrainement est plus volatile (std=1.39%). Cela s‚Äôexplique par le fait que la p√©riode d‚Äôentrainement prend en compte deux crises majeures : la crise des subprimes et la crise du Covid-19. Cela peut √©galement se voir √† travers les clusters de volatilit√© observables √† la suite de ces crises. Dans la p√©riode de test, aucun √©v√®nement majeur n‚Äôest observ√©, avec une plus faible volatilit√© observ√©e (std=0.08%).\nOn s‚Äôattend √† ce que la VaR entrain√©e sur la p√©riode d‚Äôentrainement performe tr√®s bien sur la p√©riode de test, mais on s‚Äôattend √©galement √† ce que la VaR entrain√©e sur la p√©riode de test performe moins bien sur la p√©riode d‚Äôentrainement.\n\nplt.figure(figsize=(10, 4))\nplt.plot(data_train, label='Train', color='grey')\nplt.plot(data_test, label='Test', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Donn√©es d'entrainement et de test\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_train.describe()\n\ncount    3523.000000\nmean        0.000153\nstd         0.013953\nmin        -0.130983\n25%        -0.006099\n50%         0.000580\n75%         0.006855\nmax         0.096169\nName: log_return, dtype: float64\n\n\n\ndata_test.describe()\n\ncount    586.000000\nmean       0.000292\nstd        0.008947\nmin       -0.036484\n25%       -0.004763\n50%        0.000642\n75%        0.005612\nmax        0.041504\nName: log_return, dtype: float64"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "href": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.2. VaR non param√©trique",
    "text": "II.2. VaR non param√©trique\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de scaling. 2. Approche bootstrap : On tire al√©atoirement des √©chantillons de rendements pass√©s avec remise, puis on prend le quantile empirique d‚Äôordre \\(\\alpha\\) pour calculer la VaR de chaque √©chantillon. La VaR finale est la moyenne des VaR obtenues.\n\nII.2.1. Historique\n\n# Objectif : impl√©menter une fonction calculant la VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1- alpha))\n\n\n# Calcul de la VaR historique sur l'√©chantillon d'entrainement pour h=1j et alpha=0.99\nalpha = 0.99\nvar_hist_train = historical_var(data_train, alpha=alpha)\nprint(f\"La VaR historique pour h=1j et alpha=0.99 est : {var_hist_train:.4%}\")\n\nLa VaR historique pour h=1j et alpha=0.99 est : 4.0850%\n\n\nOn constate que la VaR historique pour une horizon de 1 jour et un niveau de confiance de 99% est de -4,09%. De ce fait, la perte maximale que l‚Äôon peut subir avec un niveau de confiance de 99% sur un jour est de 4,09%. Autrement dit, il y a 1 chance sur 100 que la perte soit sup√©rieure √† 4,09%. Cette perte peut se produire 2 √† 3 ans fois en une ann√©e (252 jours de trading).\n\n\nII.2.2. Bootstrap\nPour l‚Äôimpl√©mentation de la VaR bootstrap, nous faisons le choix de faire un tirage de taille n=la taille de la s√©rie des rendements, avec remise. Ce choix est fait pour des raisons de simplicit√©. En ce qui concerne le choix du nombre d‚Äô√©chantillons, nous allons observer l‚Äô√©volution de de l‚Äôestimation de la VaR en fonction du nombre d‚Äô√©chantillons. Nous limiterons √† des √©chantillons compris entre 1000 et 10000, pour des raisons de temps computationnels, en ayant conscience que plus le nombre d‚Äô√©chantillons est grand, plus l‚Äôestimation de la VaR sera pr√©cise.\n\n# Objectif : impl√©menter une fonction calculant la VaR bootstrap et un IC\n\ndef bootstrap_var(data, alpha=0.99, M=1000, seuil=0.05):\n    \"\"\"\n    Calcul de la VaR bootstrap\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance de la VaR\n    n : le nombre de simulations\n    seuil : le seuil de l'intervalle de confiance\n    \"\"\"\n    # set seed\n    np.random.seed(42)\n\n    # Initialisation du vecteur des VaR\n    var = np.zeros(M)\n\n    # Calcul de la VaR bootstrap\n    for i in range(M):\n        sample = np.random.choice(data, size=len(data), replace=True)\n        var[i] = -np.percentile(sample, 100*(1- alpha))\n\n    # Calcul de l'intervalle de confiance\n    lower = np.percentile(var, 100*(1-seuil)/2)\n    upper = np.percentile(var, 100*(seuil + (1-seuil)/2))\n\n    return np.mean(var), lower, upper\n\n\n# Observer la variation de la VaR en fonction de M\nM_values = np.arange(1000, 10000, 10)\nvar_bs_values = []\n\nfor M in tqdm(M_values):\n    var_bs_train, _, _ = bootstrap_var(data_train, alpha=alpha, M=M)\n    var_bs_values.append(var_bs_train)\n\n  0%|          | 0/900 [00:00&lt;?, ?it/s]  0%|          | 2/900 [00:00&lt;01:04, 14.01it/s]  0%|          | 4/900 [00:00&lt;01:04, 13.90it/s]  1%|          | 6/900 [00:00&lt;01:04, 13.86it/s]  1%|          | 8/900 [00:00&lt;01:05, 13.68it/s]  1%|          | 10/900 [00:00&lt;01:05, 13.50it/s]  1%|‚ñè         | 12/900 [00:00&lt;01:06, 13.32it/s]  2%|‚ñè         | 14/900 [00:01&lt;01:07, 13.12it/s]  2%|‚ñè         | 16/900 [00:01&lt;01:08, 12.93it/s]  2%|‚ñè         | 18/900 [00:01&lt;01:09, 12.72it/s]  2%|‚ñè         | 20/900 [00:01&lt;01:10, 12.55it/s]  2%|‚ñè         | 22/900 [00:01&lt;01:10, 12.37it/s]  3%|‚ñé         | 24/900 [00:01&lt;01:11, 12.19it/s]  3%|‚ñé         | 26/900 [00:02&lt;01:12, 12.01it/s]  3%|‚ñé         | 28/900 [00:02&lt;01:13, 11.83it/s]  3%|‚ñé         | 30/900 [00:02&lt;01:14, 11.65it/s]  4%|‚ñé         | 32/900 [00:02&lt;01:15, 11.44it/s]  4%|‚ñç         | 34/900 [00:02&lt;01:16, 11.28it/s]  4%|‚ñç         | 36/900 [00:02&lt;01:17, 11.12it/s]  4%|‚ñç         | 38/900 [00:03&lt;01:18, 10.95it/s]  4%|‚ñç         | 40/900 [00:03&lt;01:20, 10.74it/s]  5%|‚ñç         | 42/900 [00:03&lt;01:20, 10.61it/s]  5%|‚ñç         | 44/900 [00:03&lt;01:21, 10.47it/s]  5%|‚ñå         | 46/900 [00:03&lt;01:23, 10.29it/s]  5%|‚ñå         | 48/900 [00:04&lt;01:23, 10.16it/s]  6%|‚ñå         | 50/900 [00:04&lt;01:24, 10.03it/s]  6%|‚ñå         | 52/900 [00:04&lt;01:25,  9.91it/s]  6%|‚ñå         | 53/900 [00:04&lt;01:26,  9.84it/s]  6%|‚ñå         | 54/900 [00:04&lt;01:26,  9.76it/s]  6%|‚ñå         | 55/900 [00:04&lt;01:27,  9.67it/s]  6%|‚ñå         | 56/900 [00:04&lt;01:28,  9.57it/s]  6%|‚ñã         | 57/900 [00:05&lt;01:28,  9.48it/s]  6%|‚ñã         | 58/900 [00:05&lt;01:29,  9.40it/s]  7%|‚ñã         | 59/900 [00:05&lt;01:30,  9.29it/s]  7%|‚ñã         | 60/900 [00:05&lt;01:31,  9.22it/s]  7%|‚ñã         | 61/900 [00:05&lt;01:31,  9.17it/s]  7%|‚ñã         | 62/900 [00:05&lt;01:32,  9.11it/s]  7%|‚ñã         | 63/900 [00:05&lt;01:32,  9.05it/s]  7%|‚ñã         | 64/900 [00:05&lt;01:32,  8.99it/s]  7%|‚ñã         | 65/900 [00:05&lt;01:33,  8.93it/s]  7%|‚ñã         | 66/900 [00:06&lt;01:34,  8.79it/s]  7%|‚ñã         | 67/900 [00:06&lt;01:35,  8.74it/s]  8%|‚ñä         | 68/900 [00:06&lt;01:35,  8.72it/s]  8%|‚ñä         | 69/900 [00:06&lt;01:35,  8.69it/s]  8%|‚ñä         | 70/900 [00:06&lt;01:35,  8.65it/s]  8%|‚ñä         | 71/900 [00:06&lt;01:36,  8.59it/s]  8%|‚ñä         | 72/900 [00:06&lt;01:36,  8.55it/s]  8%|‚ñä         | 73/900 [00:06&lt;01:37,  8.50it/s]  8%|‚ñä         | 74/900 [00:07&lt;01:37,  8.47it/s]  8%|‚ñä         | 75/900 [00:07&lt;01:37,  8.42it/s]  8%|‚ñä         | 76/900 [00:07&lt;01:38,  8.35it/s]  9%|‚ñä         | 77/900 [00:07&lt;01:38,  8.31it/s]  9%|‚ñä         | 78/900 [00:07&lt;01:39,  8.27it/s]  9%|‚ñâ         | 79/900 [00:07&lt;01:39,  8.23it/s]  9%|‚ñâ         | 80/900 [00:07&lt;01:40,  8.18it/s]  9%|‚ñâ         | 81/900 [00:07&lt;01:40,  8.14it/s]  9%|‚ñâ         | 82/900 [00:07&lt;01:41,  8.10it/s]  9%|‚ñâ         | 83/900 [00:08&lt;01:41,  8.06it/s]  9%|‚ñâ         | 84/900 [00:08&lt;01:41,  8.02it/s]  9%|‚ñâ         | 85/900 [00:08&lt;01:42,  7.95it/s] 10%|‚ñâ         | 86/900 [00:08&lt;01:42,  7.91it/s] 10%|‚ñâ         | 87/900 [00:08&lt;01:43,  7.87it/s] 10%|‚ñâ         | 88/900 [00:08&lt;01:43,  7.83it/s] 10%|‚ñâ         | 89/900 [00:08&lt;01:44,  7.79it/s] 10%|‚ñà         | 90/900 [00:09&lt;01:44,  7.75it/s] 10%|‚ñà         | 91/900 [00:09&lt;01:44,  7.71it/s] 10%|‚ñà         | 92/900 [00:09&lt;01:45,  7.67it/s] 10%|‚ñà         | 93/900 [00:09&lt;01:45,  7.62it/s] 10%|‚ñà         | 94/900 [00:09&lt;01:46,  7.57it/s] 11%|‚ñà         | 95/900 [00:09&lt;01:46,  7.52it/s] 11%|‚ñà         | 96/900 [00:09&lt;01:47,  7.49it/s] 11%|‚ñà         | 97/900 [00:09&lt;01:47,  7.44it/s] 11%|‚ñà         | 98/900 [00:10&lt;01:48,  7.39it/s] 11%|‚ñà         | 99/900 [00:10&lt;01:49,  7.34it/s] 11%|‚ñà         | 100/900 [00:10&lt;01:49,  7.32it/s] 11%|‚ñà         | 101/900 [00:10&lt;01:49,  7.30it/s] 11%|‚ñà‚ñè        | 102/900 [00:10&lt;01:51,  7.17it/s] 11%|‚ñà‚ñè        | 103/900 [00:10&lt;01:51,  7.15it/s] 12%|‚ñà‚ñè        | 104/900 [00:10&lt;01:51,  7.14it/s] 12%|‚ñà‚ñè        | 105/900 [00:11&lt;01:51,  7.10it/s] 12%|‚ñà‚ñè        | 106/900 [00:11&lt;01:52,  7.08it/s] 12%|‚ñà‚ñè        | 107/900 [00:11&lt;01:52,  7.03it/s] 12%|‚ñà‚ñè        | 108/900 [00:11&lt;01:53,  7.01it/s] 12%|‚ñà‚ñè        | 109/900 [00:11&lt;01:53,  6.99it/s] 12%|‚ñà‚ñè        | 110/900 [00:11&lt;01:53,  6.96it/s] 12%|‚ñà‚ñè        | 111/900 [00:11&lt;01:53,  6.94it/s] 12%|‚ñà‚ñè        | 112/900 [00:12&lt;01:54,  6.89it/s] 13%|‚ñà‚ñé        | 113/900 [00:12&lt;01:54,  6.86it/s] 13%|‚ñà‚ñé        | 114/900 [00:12&lt;01:54,  6.84it/s] 13%|‚ñà‚ñé        | 115/900 [00:12&lt;01:55,  6.81it/s] 13%|‚ñà‚ñé        | 116/900 [00:12&lt;01:55,  6.78it/s] 13%|‚ñà‚ñé        | 117/900 [00:12&lt;01:55,  6.75it/s] 13%|‚ñà‚ñé        | 118/900 [00:12&lt;01:56,  6.73it/s] 13%|‚ñà‚ñé        | 119/900 [00:13&lt;01:56,  6.70it/s] 13%|‚ñà‚ñé        | 120/900 [00:13&lt;01:57,  6.66it/s] 13%|‚ñà‚ñé        | 121/900 [00:13&lt;01:57,  6.64it/s] 14%|‚ñà‚ñé        | 122/900 [00:13&lt;01:57,  6.61it/s] 14%|‚ñà‚ñé        | 123/900 [00:13&lt;01:58,  6.58it/s] 14%|‚ñà‚ñç        | 124/900 [00:13&lt;01:58,  6.53it/s] 14%|‚ñà‚ñç        | 125/900 [00:14&lt;01:59,  6.51it/s] 14%|‚ñà‚ñç        | 126/900 [00:14&lt;01:59,  6.48it/s] 14%|‚ñà‚ñç        | 127/900 [00:14&lt;01:59,  6.46it/s] 14%|‚ñà‚ñç        | 128/900 [00:14&lt;02:00,  6.43it/s] 14%|‚ñà‚ñç        | 129/900 [00:14&lt;02:00,  6.38it/s] 14%|‚ñà‚ñç        | 130/900 [00:14&lt;02:01,  6.36it/s] 15%|‚ñà‚ñç        | 131/900 [00:15&lt;02:01,  6.34it/s] 15%|‚ñà‚ñç        | 132/900 [00:15&lt;02:01,  6.31it/s] 15%|‚ñà‚ñç        | 133/900 [00:15&lt;02:02,  6.29it/s] 15%|‚ñà‚ñç        | 134/900 [00:15&lt;02:02,  6.26it/s] 15%|‚ñà‚ñå        | 135/900 [00:15&lt;02:02,  6.23it/s] 15%|‚ñà‚ñå        | 136/900 [00:15&lt;02:03,  6.20it/s] 15%|‚ñà‚ñå        | 137/900 [00:15&lt;02:03,  6.17it/s] 15%|‚ñà‚ñå        | 138/900 [00:16&lt;02:04,  6.13it/s] 15%|‚ñà‚ñå        | 139/900 [00:16&lt;02:04,  6.11it/s] 16%|‚ñà‚ñå        | 140/900 [00:16&lt;02:04,  6.09it/s] 16%|‚ñà‚ñå        | 141/900 [00:16&lt;02:04,  6.08it/s] 16%|‚ñà‚ñå        | 142/900 [00:16&lt;02:05,  6.05it/s] 16%|‚ñà‚ñå        | 143/900 [00:16&lt;02:05,  6.03it/s] 16%|‚ñà‚ñå        | 144/900 [00:17&lt;02:05,  6.00it/s] 16%|‚ñà‚ñå        | 145/900 [00:17&lt;02:06,  5.98it/s] 16%|‚ñà‚ñå        | 146/900 [00:17&lt;02:06,  5.95it/s] 16%|‚ñà‚ñã        | 147/900 [00:17&lt;02:06,  5.93it/s] 16%|‚ñà‚ñã        | 148/900 [00:17&lt;02:07,  5.91it/s] 17%|‚ñà‚ñã        | 149/900 [00:17&lt;02:07,  5.88it/s] 17%|‚ñà‚ñã        | 150/900 [00:18&lt;02:07,  5.86it/s] 17%|‚ñà‚ñã        | 151/900 [00:18&lt;02:08,  5.84it/s] 17%|‚ñà‚ñã        | 152/900 [00:18&lt;02:08,  5.82it/s] 17%|‚ñà‚ñã        | 153/900 [00:18&lt;02:08,  5.80it/s] 17%|‚ñà‚ñã        | 154/900 [00:18&lt;02:09,  5.78it/s] 17%|‚ñà‚ñã        | 155/900 [00:19&lt;02:09,  5.75it/s] 17%|‚ñà‚ñã        | 156/900 [00:19&lt;02:09,  5.73it/s] 17%|‚ñà‚ñã        | 157/900 [00:19&lt;02:10,  5.71it/s] 18%|‚ñà‚ñä        | 158/900 [00:19&lt;02:10,  5.68it/s] 18%|‚ñà‚ñä        | 159/900 [00:19&lt;02:10,  5.66it/s] 18%|‚ñà‚ñä        | 160/900 [00:19&lt;02:11,  5.62it/s] 18%|‚ñà‚ñä        | 161/900 [00:20&lt;02:12,  5.59it/s] 18%|‚ñà‚ñä        | 162/900 [00:20&lt;02:12,  5.57it/s] 18%|‚ñà‚ñä        | 163/900 [00:20&lt;02:13,  5.54it/s] 18%|‚ñà‚ñä        | 164/900 [00:20&lt;02:13,  5.52it/s] 18%|‚ñà‚ñä        | 165/900 [00:20&lt;02:13,  5.50it/s] 18%|‚ñà‚ñä        | 166/900 [00:21&lt;02:13,  5.49it/s] 19%|‚ñà‚ñä        | 167/900 [00:21&lt;02:15,  5.42it/s] 19%|‚ñà‚ñä        | 168/900 [00:21&lt;02:15,  5.41it/s] 19%|‚ñà‚ñâ        | 169/900 [00:21&lt;02:15,  5.40it/s] 19%|‚ñà‚ñâ        | 170/900 [00:21&lt;02:15,  5.38it/s] 19%|‚ñà‚ñâ        | 171/900 [00:21&lt;02:15,  5.37it/s] 19%|‚ñà‚ñâ        | 172/900 [00:22&lt;02:16,  5.34it/s] 19%|‚ñà‚ñâ        | 173/900 [00:22&lt;02:16,  5.33it/s] 19%|‚ñà‚ñâ        | 174/900 [00:22&lt;02:16,  5.31it/s] 19%|‚ñà‚ñâ        | 175/900 [00:22&lt;02:16,  5.29it/s] 20%|‚ñà‚ñâ        | 176/900 [00:22&lt;02:16,  5.28it/s] 20%|‚ñà‚ñâ        | 177/900 [00:23&lt;02:17,  5.27it/s] 20%|‚ñà‚ñâ        | 178/900 [00:23&lt;02:17,  5.26it/s] 20%|‚ñà‚ñâ        | 179/900 [00:23&lt;02:17,  5.24it/s] 20%|‚ñà‚ñà        | 180/900 [00:23&lt;02:17,  5.22it/s] 20%|‚ñà‚ñà        | 181/900 [00:23&lt;02:18,  5.20it/s] 20%|‚ñà‚ñà        | 182/900 [00:24&lt;02:18,  5.19it/s] 20%|‚ñà‚ñà        | 183/900 [00:24&lt;02:18,  5.17it/s] 20%|‚ñà‚ñà        | 184/900 [00:24&lt;02:18,  5.15it/s] 21%|‚ñà‚ñà        | 185/900 [00:24&lt;02:19,  5.14it/s] 21%|‚ñà‚ñà        | 186/900 [00:24&lt;02:19,  5.12it/s] 21%|‚ñà‚ñà        | 187/900 [00:25&lt;02:19,  5.09it/s] 21%|‚ñà‚ñà        | 188/900 [00:25&lt;02:20,  5.06it/s] 21%|‚ñà‚ñà        | 189/900 [00:25&lt;02:20,  5.05it/s] 21%|‚ñà‚ñà        | 190/900 [00:25&lt;02:20,  5.04it/s] 21%|‚ñà‚ñà        | 191/900 [00:25&lt;02:21,  5.02it/s] 21%|‚ñà‚ñà‚ñè       | 192/900 [00:26&lt;02:21,  5.00it/s] 21%|‚ñà‚ñà‚ñè       | 193/900 [00:26&lt;02:21,  4.99it/s] 22%|‚ñà‚ñà‚ñè       | 194/900 [00:26&lt;02:22,  4.97it/s] 22%|‚ñà‚ñà‚ñè       | 195/900 [00:26&lt;02:23,  4.92it/s] 22%|‚ñà‚ñà‚ñè       | 196/900 [00:26&lt;02:24,  4.87it/s] 22%|‚ñà‚ñà‚ñè       | 197/900 [00:27&lt;02:24,  4.87it/s] 22%|‚ñà‚ñà‚ñè       | 198/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 199/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 200/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 201/900 [00:27&lt;02:25,  4.79it/s] 22%|‚ñà‚ñà‚ñè       | 202/900 [00:28&lt;02:25,  4.79it/s] 23%|‚ñà‚ñà‚ñé       | 203/900 [00:28&lt;02:26,  4.77it/s] 23%|‚ñà‚ñà‚ñé       | 204/900 [00:28&lt;02:25,  4.77it/s] 23%|‚ñà‚ñà‚ñé       | 205/900 [00:28&lt;02:26,  4.75it/s] 23%|‚ñà‚ñà‚ñé       | 206/900 [00:28&lt;02:26,  4.73it/s] 23%|‚ñà‚ñà‚ñé       | 207/900 [00:29&lt;02:26,  4.73it/s] 23%|‚ñà‚ñà‚ñé       | 208/900 [00:29&lt;02:26,  4.72it/s] 23%|‚ñà‚ñà‚ñé       | 209/900 [00:29&lt;02:26,  4.72it/s] 23%|‚ñà‚ñà‚ñé       | 210/900 [00:29&lt;02:26,  4.71it/s] 23%|‚ñà‚ñà‚ñé       | 211/900 [00:30&lt;02:26,  4.69it/s] 24%|‚ñà‚ñà‚ñé       | 212/900 [00:30&lt;02:27,  4.67it/s] 24%|‚ñà‚ñà‚ñé       | 213/900 [00:30&lt;02:27,  4.66it/s] 24%|‚ñà‚ñà‚ñç       | 214/900 [00:30&lt;02:27,  4.65it/s] 24%|‚ñà‚ñà‚ñç       | 215/900 [00:30&lt;02:27,  4.63it/s] 24%|‚ñà‚ñà‚ñç       | 216/900 [00:31&lt;02:28,  4.62it/s] 24%|‚ñà‚ñà‚ñç       | 217/900 [00:31&lt;02:28,  4.60it/s] 24%|‚ñà‚ñà‚ñç       | 218/900 [00:31&lt;02:28,  4.59it/s] 24%|‚ñà‚ñà‚ñç       | 219/900 [00:31&lt;02:28,  4.57it/s] 24%|‚ñà‚ñà‚ñç       | 220/900 [00:31&lt;02:29,  4.56it/s] 25%|‚ñà‚ñà‚ñç       | 221/900 [00:32&lt;02:29,  4.55it/s] 25%|‚ñà‚ñà‚ñç       | 222/900 [00:32&lt;02:29,  4.52it/s] 25%|‚ñà‚ñà‚ñç       | 223/900 [00:32&lt;02:29,  4.52it/s] 25%|‚ñà‚ñà‚ñç       | 224/900 [00:32&lt;02:30,  4.49it/s] 25%|‚ñà‚ñà‚ñå       | 225/900 [00:33&lt;02:30,  4.48it/s] 25%|‚ñà‚ñà‚ñå       | 226/900 [00:33&lt;02:30,  4.47it/s] 25%|‚ñà‚ñà‚ñå       | 227/900 [00:33&lt;02:30,  4.47it/s] 25%|‚ñà‚ñà‚ñå       | 228/900 [00:33&lt;02:30,  4.45it/s] 25%|‚ñà‚ñà‚ñå       | 229/900 [00:34&lt;02:31,  4.44it/s] 26%|‚ñà‚ñà‚ñå       | 230/900 [00:34&lt;02:31,  4.43it/s] 26%|‚ñà‚ñà‚ñå       | 231/900 [00:34&lt;02:31,  4.42it/s] 26%|‚ñà‚ñà‚ñå       | 232/900 [00:34&lt;02:32,  4.39it/s] 26%|‚ñà‚ñà‚ñå       | 233/900 [00:34&lt;02:32,  4.38it/s] 26%|‚ñà‚ñà‚ñå       | 234/900 [00:35&lt;02:32,  4.37it/s] 26%|‚ñà‚ñà‚ñå       | 235/900 [00:35&lt;02:32,  4.36it/s] 26%|‚ñà‚ñà‚ñå       | 236/900 [00:35&lt;02:33,  4.33it/s] 26%|‚ñà‚ñà‚ñã       | 237/900 [00:35&lt;02:33,  4.33it/s] 26%|‚ñà‚ñà‚ñã       | 238/900 [00:36&lt;02:33,  4.32it/s] 27%|‚ñà‚ñà‚ñã       | 239/900 [00:36&lt;02:33,  4.30it/s] 27%|‚ñà‚ñà‚ñã       | 240/900 [00:36&lt;02:33,  4.29it/s] 27%|‚ñà‚ñà‚ñã       | 241/900 [00:36&lt;02:33,  4.28it/s] 27%|‚ñà‚ñà‚ñã       | 242/900 [00:37&lt;02:33,  4.27it/s] 27%|‚ñà‚ñà‚ñã       | 243/900 [00:37&lt;02:34,  4.26it/s] 27%|‚ñà‚ñà‚ñã       | 244/900 [00:37&lt;02:34,  4.25it/s] 27%|‚ñà‚ñà‚ñã       | 245/900 [00:37&lt;02:34,  4.23it/s] 27%|‚ñà‚ñà‚ñã       | 246/900 [00:37&lt;02:35,  4.22it/s] 27%|‚ñà‚ñà‚ñã       | 247/900 [00:38&lt;02:35,  4.21it/s] 28%|‚ñà‚ñà‚ñä       | 248/900 [00:38&lt;02:35,  4.20it/s] 28%|‚ñà‚ñà‚ñä       | 249/900 [00:38&lt;02:35,  4.19it/s] 28%|‚ñà‚ñà‚ñä       | 250/900 [00:38&lt;02:35,  4.18it/s] 28%|‚ñà‚ñà‚ñä       | 251/900 [00:39&lt;02:36,  4.15it/s] 28%|‚ñà‚ñà‚ñä       | 252/900 [00:39&lt;02:36,  4.15it/s] 28%|‚ñà‚ñà‚ñä       | 253/900 [00:39&lt;02:36,  4.14it/s] 28%|‚ñà‚ñà‚ñä       | 254/900 [00:39&lt;02:36,  4.13it/s] 28%|‚ñà‚ñà‚ñä       | 255/900 [00:40&lt;02:37,  4.11it/s] 28%|‚ñà‚ñà‚ñä       | 256/900 [00:40&lt;02:37,  4.09it/s] 29%|‚ñà‚ñà‚ñä       | 257/900 [00:40&lt;02:38,  4.07it/s] 29%|‚ñà‚ñà‚ñä       | 258/900 [00:40&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 259/900 [00:41&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 260/900 [00:41&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 261/900 [00:41&lt;02:38,  4.04it/s] 29%|‚ñà‚ñà‚ñâ       | 262/900 [00:41&lt;02:38,  4.04it/s] 29%|‚ñà‚ñà‚ñâ       | 263/900 [00:42&lt;02:38,  4.03it/s] 29%|‚ñà‚ñà‚ñâ       | 264/900 [00:42&lt;02:39,  4.00it/s] 29%|‚ñà‚ñà‚ñâ       | 265/900 [00:42&lt;02:38,  4.00it/s] 30%|‚ñà‚ñà‚ñâ       | 266/900 [00:42&lt;02:39,  3.97it/s] 30%|‚ñà‚ñà‚ñâ       | 267/900 [00:43&lt;02:39,  3.97it/s] 30%|‚ñà‚ñà‚ñâ       | 268/900 [00:43&lt;02:39,  3.96it/s] 30%|‚ñà‚ñà‚ñâ       | 269/900 [00:43&lt;02:39,  3.95it/s] 30%|‚ñà‚ñà‚ñà       | 270/900 [00:43&lt;02:39,  3.95it/s] 30%|‚ñà‚ñà‚ñà       | 271/900 [00:44&lt;02:39,  3.94it/s] 30%|‚ñà‚ñà‚ñà       | 272/900 [00:44&lt;02:39,  3.93it/s] 30%|‚ñà‚ñà‚ñà       | 273/900 [00:44&lt;02:40,  3.91it/s] 30%|‚ñà‚ñà‚ñà       | 274/900 [00:44&lt;02:41,  3.89it/s] 31%|‚ñà‚ñà‚ñà       | 275/900 [00:45&lt;02:40,  3.89it/s] 31%|‚ñà‚ñà‚ñà       | 276/900 [00:45&lt;02:40,  3.88it/s] 31%|‚ñà‚ñà‚ñà       | 277/900 [00:45&lt;02:41,  3.87it/s] 31%|‚ñà‚ñà‚ñà       | 278/900 [00:45&lt;02:40,  3.86it/s] 31%|‚ñà‚ñà‚ñà       | 279/900 [00:46&lt;02:41,  3.85it/s] 31%|‚ñà‚ñà‚ñà       | 280/900 [00:46&lt;02:41,  3.83it/s] 31%|‚ñà‚ñà‚ñà       | 281/900 [00:46&lt;02:41,  3.82it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 282/900 [00:47&lt;02:41,  3.82it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 283/900 [00:47&lt;02:41,  3.81it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 284/900 [00:47&lt;02:42,  3.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 285/900 [00:47&lt;02:42,  3.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 286/900 [00:48&lt;02:42,  3.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 287/900 [00:48&lt;02:42,  3.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 288/900 [00:48&lt;02:42,  3.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 289/900 [00:48&lt;02:43,  3.74it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 290/900 [00:49&lt;02:43,  3.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 291/900 [00:49&lt;02:45,  3.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 292/900 [00:49&lt;02:45,  3.68it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 293/900 [00:49&lt;02:45,  3.67it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 294/900 [00:50&lt;02:45,  3.67it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 295/900 [00:50&lt;02:45,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 296/900 [00:50&lt;02:45,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 297/900 [00:51&lt;02:44,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 298/900 [00:51&lt;02:44,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 299/900 [00:51&lt;02:44,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 300/900 [00:51&lt;02:44,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 301/900 [00:52&lt;02:45,  3.63it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 302/900 [00:52&lt;02:44,  3.63it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 303/900 [00:52&lt;02:44,  3.62it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 304/900 [00:53&lt;02:45,  3.60it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 305/900 [00:53&lt;02:46,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 306/900 [00:53&lt;02:45,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 307/900 [00:53&lt;02:45,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 308/900 [00:54&lt;02:46,  3.56it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 309/900 [00:54&lt;02:46,  3.55it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 310/900 [00:54&lt;02:46,  3.55it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 311/900 [00:55&lt;02:46,  3.54it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 312/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 313/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 314/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 315/900 [00:56&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 316/900 [00:56&lt;02:46,  3.51it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 317/900 [00:56&lt;02:46,  3.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 318/900 [00:57&lt;02:46,  3.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 319/900 [00:57&lt;02:46,  3.49it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 320/900 [00:57&lt;02:46,  3.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 321/900 [00:57&lt;02:46,  3.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 322/900 [00:58&lt;02:46,  3.47it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 323/900 [00:58&lt;02:46,  3.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 324/900 [00:58&lt;02:46,  3.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 325/900 [00:59&lt;02:46,  3.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 326/900 [00:59&lt;02:46,  3.44it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 327/900 [00:59&lt;02:47,  3.41it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 328/900 [00:59&lt;02:47,  3.41it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 329/900 [01:00&lt;02:47,  3.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 330/900 [01:00&lt;02:47,  3.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 331/900 [01:00&lt;02:47,  3.39it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 332/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 333/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 334/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 335/900 [01:01&lt;02:48,  3.36it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 336/900 [01:02&lt;02:48,  3.36it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 337/900 [01:02&lt;02:48,  3.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 338/900 [01:02&lt;02:49,  3.33it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 339/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 340/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 341/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 342/900 [01:04&lt;02:48,  3.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 343/900 [01:04&lt;02:48,  3.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 344/900 [01:04&lt;02:48,  3.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 345/900 [01:05&lt;02:48,  3.29it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 346/900 [01:05&lt;02:48,  3.29it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 347/900 [01:05&lt;02:48,  3.28it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 348/900 [01:05&lt;02:48,  3.27it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 349/900 [01:06&lt;02:48,  3.26it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 350/900 [01:06&lt;02:49,  3.25it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 351/900 [01:06&lt;02:49,  3.25it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 352/900 [01:07&lt;02:49,  3.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 353/900 [01:07&lt;02:49,  3.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 354/900 [01:07&lt;02:52,  3.16it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 355/900 [01:08&lt;02:51,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 356/900 [01:08&lt;02:51,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 357/900 [01:08&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 358/900 [01:09&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 359/900 [01:09&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 360/900 [01:09&lt;02:50,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 361/900 [01:10&lt;02:50,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 362/900 [01:10&lt;02:50,  3.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 363/900 [01:10&lt;02:50,  3.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 364/900 [01:10&lt;02:50,  3.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 365/900 [01:11&lt;02:50,  3.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 366/900 [01:11&lt;02:50,  3.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 367/900 [01:11&lt;02:50,  3.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 368/900 [01:12&lt;02:50,  3.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 369/900 [01:12&lt;02:50,  3.11it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 370/900 [01:12&lt;02:50,  3.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 371/900 [01:13&lt;02:50,  3.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 372/900 [01:13&lt;02:51,  3.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 373/900 [01:13&lt;02:51,  3.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 374/900 [01:14&lt;02:51,  3.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 375/900 [01:14&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 376/900 [01:14&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 377/900 [01:15&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 378/900 [01:15&lt;02:51,  3.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 379/900 [01:15&lt;02:51,  3.04it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 380/900 [01:16&lt;02:51,  3.03it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 381/900 [01:16&lt;02:52,  3.02it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 382/900 [01:16&lt;02:51,  3.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 383/900 [01:17&lt;02:51,  3.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 384/900 [01:17&lt;02:51,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 385/900 [01:17&lt;02:51,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 386/900 [01:18&lt;02:50,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 387/900 [01:18&lt;02:50,  3.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 388/900 [01:18&lt;02:50,  3.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 389/900 [01:19&lt;02:51,  2.99it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 390/900 [01:19&lt;02:50,  2.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 391/900 [01:19&lt;02:51,  2.97it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 392/900 [01:20&lt;02:51,  2.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 393/900 [01:20&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 394/900 [01:20&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 395/900 [01:21&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 396/900 [01:21&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 397/900 [01:21&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 398/900 [01:22&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 399/900 [01:22&lt;02:51,  2.92it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 400/900 [01:22&lt;02:52,  2.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 401/900 [01:23&lt;02:51,  2.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 402/900 [01:23&lt;02:51,  2.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 403/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 404/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 405/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 406/900 [01:25&lt;02:51,  2.88it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 407/900 [01:25&lt;02:55,  2.81it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 408/900 [01:25&lt;03:01,  2.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 409/900 [01:26&lt;02:58,  2.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 410/900 [01:26&lt;02:55,  2.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 411/900 [01:26&lt;02:56,  2.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 412/900 [01:27&lt;02:57,  2.75it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 413/900 [01:27&lt;02:55,  2.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 414/900 [01:27&lt;02:53,  2.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 415/900 [01:28&lt;02:53,  2.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 416/900 [01:28&lt;02:52,  2.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 417/900 [01:29&lt;02:51,  2.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 418/900 [01:29&lt;02:51,  2.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 419/900 [01:29&lt;02:51,  2.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 420/900 [01:30&lt;02:51,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 421/900 [01:30&lt;02:50,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 422/900 [01:30&lt;02:50,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 423/900 [01:31&lt;02:51,  2.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 424/900 [01:31&lt;02:51,  2.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 425/900 [01:31&lt;02:51,  2.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 426/900 [01:32&lt;02:51,  2.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 427/900 [01:32&lt;02:51,  2.76it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 428/900 [01:32&lt;02:51,  2.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 429/900 [01:33&lt;02:51,  2.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 430/900 [01:33&lt;02:55,  2.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 431/900 [01:34&lt;02:54,  2.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 432/900 [01:34&lt;02:52,  2.71it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 433/900 [01:34&lt;02:51,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 434/900 [01:35&lt;02:51,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 435/900 [01:35&lt;02:50,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 436/900 [01:35&lt;02:50,  2.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 437/900 [01:36&lt;02:51,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 438/900 [01:36&lt;02:50,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 439/900 [01:37&lt;02:50,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 440/900 [01:37&lt;02:50,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 441/900 [01:37&lt;02:49,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 442/900 [01:38&lt;02:49,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 443/900 [01:38&lt;02:49,  2.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 444/900 [01:38&lt;02:49,  2.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 445/900 [01:39&lt;02:49,  2.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 446/900 [01:39&lt;02:49,  2.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 447/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 448/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 449/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 450/900 [01:41&lt;02:49,  2.65it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 451/900 [01:41&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 452/900 [01:41&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 453/900 [01:42&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 454/900 [01:42&lt;02:49,  2.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 455/900 [01:43&lt;02:49,  2.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 456/900 [01:43&lt;02:49,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 457/900 [01:43&lt;02:49,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 458/900 [01:44&lt;02:48,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 459/900 [01:44&lt;02:48,  2.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 460/900 [01:45&lt;02:48,  2.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 461/900 [01:45&lt;02:48,  2.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 462/900 [01:45&lt;02:48,  2.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 463/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 464/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 465/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 466/900 [01:47&lt;02:48,  2.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 467/900 [01:47&lt;02:47,  2.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 468/900 [01:48&lt;02:47,  2.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 469/900 [01:48&lt;02:47,  2.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 470/900 [01:48&lt;02:47,  2.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 471/900 [01:49&lt;02:47,  2.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 472/900 [01:49&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 473/900 [01:50&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 474/900 [01:50&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 475/900 [01:50&lt;02:47,  2.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 476/900 [01:51&lt;02:47,  2.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 477/900 [01:51&lt;02:47,  2.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 478/900 [01:52&lt;02:46,  2.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 479/900 [01:52&lt;02:47,  2.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 480/900 [01:52&lt;02:47,  2.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 481/900 [01:53&lt;02:46,  2.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 482/900 [01:53&lt;02:46,  2.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 483/900 [01:54&lt;02:46,  2.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 484/900 [01:54&lt;02:46,  2.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 485/900 [01:54&lt;02:47,  2.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 486/900 [01:55&lt;02:46,  2.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 487/900 [01:55&lt;02:46,  2.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 488/900 [01:56&lt;02:47,  2.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 489/900 [01:56&lt;02:46,  2.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 490/900 [01:56&lt;02:46,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 491/900 [01:57&lt;02:45,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 492/900 [01:57&lt;02:45,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 493/900 [01:58&lt;02:45,  2.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 494/900 [01:58&lt;02:45,  2.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 495/900 [01:58&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 496/900 [01:59&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 497/900 [01:59&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 498/900 [02:00&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 499/900 [02:00&lt;02:45,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 500/900 [02:01&lt;02:45,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 501/900 [02:01&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 502/900 [02:01&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 503/900 [02:02&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 504/900 [02:02&lt;02:44,  2.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 505/900 [02:03&lt;02:43,  2.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 506/900 [02:03&lt;02:44,  2.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 507/900 [02:03&lt;02:44,  2.39it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 508/900 [02:04&lt;02:44,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 509/900 [02:04&lt;02:43,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 510/900 [02:05&lt;02:43,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 511/900 [02:05&lt;02:42,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 512/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 513/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 514/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 515/900 [02:07&lt;02:42,  2.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 516/900 [02:07&lt;02:41,  2.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 517/900 [02:08&lt;02:41,  2.37it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 518/900 [02:08&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 519/900 [02:08&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 520/900 [02:09&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 521/900 [02:09&lt;02:41,  2.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 522/900 [02:10&lt;02:41,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 523/900 [02:10&lt;02:41,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 524/900 [02:11&lt;02:40,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 525/900 [02:11&lt;02:40,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 526/900 [02:11&lt;02:40,  2.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 527/900 [02:12&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 528/900 [02:12&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 529/900 [02:13&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 530/900 [02:13&lt;02:39,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 531/900 [02:14&lt;02:40,  2.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 532/900 [02:14&lt;02:39,  2.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 533/900 [02:15&lt;02:40,  2.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 534/900 [02:15&lt;02:39,  2.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 535/900 [02:15&lt;02:39,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 536/900 [02:16&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 537/900 [02:16&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 538/900 [02:17&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 539/900 [02:17&lt;02:38,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 540/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 541/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 542/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 543/900 [02:19&lt;02:37,  2.27it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 544/900 [02:19&lt;02:36,  2.27it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 545/900 [02:20&lt;02:36,  2.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 546/900 [02:20&lt;02:37,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 547/900 [02:21&lt;02:36,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 548/900 [02:21&lt;02:36,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 549/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 550/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 551/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 552/900 [02:23&lt;02:35,  2.24it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 553/900 [02:23&lt;02:35,  2.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 554/900 [02:24&lt;02:35,  2.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 555/900 [02:24&lt;02:35,  2.22it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 556/900 [02:25&lt;02:35,  2.21it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 557/900 [02:25&lt;02:41,  2.12it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 558/900 [02:26&lt;02:39,  2.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 559/900 [02:26&lt;02:37,  2.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 560/900 [02:27&lt;02:39,  2.13it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 561/900 [02:27&lt;02:37,  2.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 562/900 [02:28&lt;02:35,  2.17it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 563/900 [02:28&lt;02:34,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 564/900 [02:28&lt;02:33,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 565/900 [02:29&lt;02:33,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 566/900 [02:29&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 567/900 [02:30&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 568/900 [02:30&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 569/900 [02:31&lt;02:31,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 570/900 [02:31&lt;02:31,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 571/900 [02:32&lt;02:31,  2.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 572/900 [02:32&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 573/900 [02:33&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 574/900 [02:33&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 575/900 [02:34&lt;02:30,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 576/900 [02:34&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 577/900 [02:34&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 578/900 [02:35&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 579/900 [02:35&lt;02:29,  2.15it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 580/900 [02:36&lt;02:29,  2.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 581/900 [02:36&lt;02:29,  2.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 582/900 [02:37&lt;02:29,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 583/900 [02:37&lt;02:28,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 584/900 [02:38&lt;02:28,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 585/900 [02:38&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 586/900 [02:39&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 587/900 [02:39&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 588/900 [02:40&lt;02:26,  2.12it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 589/900 [02:40&lt;02:26,  2.12it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 590/900 [02:41&lt;02:26,  2.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 591/900 [02:41&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 592/900 [02:42&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 593/900 [02:42&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 594/900 [02:42&lt;02:25,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 595/900 [02:43&lt;02:25,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 596/900 [02:43&lt;02:25,  2.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 597/900 [02:44&lt;02:25,  2.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 598/900 [02:44&lt;02:25,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 599/900 [02:45&lt;02:24,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 600/900 [02:45&lt;02:24,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 601/900 [02:46&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 602/900 [02:46&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 603/900 [02:47&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 604/900 [02:47&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 605/900 [02:48&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 606/900 [02:48&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 607/900 [02:49&lt;02:22,  2.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 608/900 [02:49&lt;02:21,  2.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 609/900 [02:50&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 610/900 [02:50&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 611/900 [02:51&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 612/900 [02:51&lt;02:20,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 613/900 [02:52&lt;02:20,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 614/900 [02:52&lt;02:20,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 615/900 [02:53&lt;02:19,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 616/900 [02:53&lt;02:19,  2.04it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 617/900 [02:54&lt;02:19,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 618/900 [02:54&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 619/900 [02:55&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 620/900 [02:55&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 621/900 [02:56&lt;02:18,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 622/900 [02:56&lt;02:17,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 623/900 [02:57&lt;02:17,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 624/900 [02:57&lt;02:16,  2.01it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 625/900 [02:58&lt;02:16,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 626/900 [02:58&lt;02:16,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 627/900 [02:59&lt;02:15,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 628/900 [02:59&lt;02:15,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 629/900 [03:00&lt;02:15,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 630/900 [03:00&lt;02:15,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 631/900 [03:01&lt;02:14,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 632/900 [03:01&lt;02:14,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 633/900 [03:02&lt;02:13,  1.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 634/900 [03:02&lt;02:13,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 635/900 [03:03&lt;02:13,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 636/900 [03:03&lt;02:12,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 637/900 [03:04&lt;02:12,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 638/900 [03:04&lt;02:12,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 639/900 [03:05&lt;02:11,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 640/900 [03:05&lt;02:11,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 641/900 [03:06&lt;02:11,  1.97it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 642/900 [03:06&lt;02:11,  1.96it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 643/900 [03:07&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 644/900 [03:07&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 645/900 [03:08&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 646/900 [03:08&lt;02:10,  1.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 647/900 [03:09&lt;02:09,  1.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 648/900 [03:09&lt;02:09,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 649/900 [03:10&lt;02:09,  1.93it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 650/900 [03:10&lt;02:09,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 651/900 [03:11&lt;02:08,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 652/900 [03:11&lt;02:08,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 653/900 [03:12&lt;02:07,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 654/900 [03:12&lt;02:07,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 655/900 [03:13&lt;02:09,  1.89it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 656/900 [03:13&lt;02:08,  1.90it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 657/900 [03:14&lt;02:07,  1.90it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 658/900 [03:15&lt;02:06,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 659/900 [03:15&lt;02:06,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 660/900 [03:16&lt;02:05,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 661/900 [03:16&lt;02:04,  1.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 662/900 [03:17&lt;02:04,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 663/900 [03:17&lt;02:03,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 664/900 [03:18&lt;02:03,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 665/900 [03:18&lt;02:04,  1.89it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 666/900 [03:19&lt;02:08,  1.82it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 667/900 [03:19&lt;02:06,  1.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 668/900 [03:20&lt;02:06,  1.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 669/900 [03:20&lt;02:05,  1.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 670/900 [03:21&lt;02:04,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 671/900 [03:21&lt;02:03,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 672/900 [03:22&lt;02:03,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 673/900 [03:23&lt;02:02,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 674/900 [03:23&lt;02:01,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 675/900 [03:24&lt;02:00,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 676/900 [03:24&lt;02:01,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 677/900 [03:25&lt;02:01,  1.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 678/900 [03:25&lt;02:00,  1.83it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 679/900 [03:26&lt;02:01,  1.82it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 680/900 [03:26&lt;02:00,  1.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 681/900 [03:27&lt;01:59,  1.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 682/900 [03:28&lt;02:05,  1.73it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 683/900 [03:28&lt;02:03,  1.75it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 684/900 [03:29&lt;02:02,  1.76it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 685/900 [03:29&lt;02:01,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 686/900 [03:30&lt;02:01,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 687/900 [03:30&lt;02:00,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 688/900 [03:31&lt;02:01,  1.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 689/900 [03:32&lt;02:00,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 690/900 [03:32&lt;01:59,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 691/900 [03:33&lt;01:58,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 692/900 [03:33&lt;01:57,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 693/900 [03:34&lt;01:57,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 694/900 [03:34&lt;01:56,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 695/900 [03:35&lt;01:57,  1.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 696/900 [03:35&lt;01:55,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 697/900 [03:36&lt;01:54,  1.78it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 698/900 [03:37&lt;01:52,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 699/900 [03:37&lt;01:51,  1.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/900 [03:38&lt;01:51,  1.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 701/900 [03:38&lt;01:50,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 702/900 [03:39&lt;01:50,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 703/900 [03:39&lt;01:49,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 704/900 [03:40&lt;01:49,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 705/900 [03:40&lt;01:48,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 706/900 [03:41&lt;01:48,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 707/900 [03:42&lt;01:47,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 708/900 [03:42&lt;01:47,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 709/900 [03:43&lt;01:46,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 710/900 [03:43&lt;01:46,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 711/900 [03:44&lt;01:46,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 712/900 [03:44&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 713/900 [03:45&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 714/900 [03:46&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 715/900 [03:46&lt;01:44,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 716/900 [03:47&lt;01:45,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 717/900 [03:47&lt;01:44,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 718/900 [03:48&lt;01:43,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 719/900 [03:48&lt;01:42,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 720/900 [03:49&lt;01:42,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 721/900 [03:50&lt;01:41,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 722/900 [03:50&lt;01:41,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 723/900 [03:51&lt;01:40,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 724/900 [03:51&lt;01:40,  1.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 725/900 [03:52&lt;01:39,  1.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 726/900 [03:52&lt;01:39,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 727/900 [03:53&lt;01:38,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 728/900 [03:54&lt;01:38,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 729/900 [03:54&lt;01:37,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 730/900 [03:55&lt;01:37,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 731/900 [03:55&lt;01:39,  1.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 732/900 [03:56&lt;01:39,  1.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 733/900 [03:57&lt;01:38,  1.69it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 734/900 [03:57&lt;01:39,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 735/900 [03:58&lt;01:37,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 736/900 [03:58&lt;01:37,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 737/900 [03:59&lt;01:37,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 738/900 [04:00&lt;01:36,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 739/900 [04:00&lt;01:36,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 740/900 [04:01&lt;01:35,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 741/900 [04:01&lt;01:35,  1.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 742/900 [04:02&lt;01:34,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 743/900 [04:02&lt;01:33,  1.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 744/900 [04:03&lt;01:33,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 745/900 [04:04&lt;01:32,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 746/900 [04:04&lt;01:32,  1.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 747/900 [04:05&lt;01:32,  1.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 748/900 [04:06&lt;01:32,  1.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 749/900 [04:06&lt;01:32,  1.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 750/900 [04:07&lt;01:30,  1.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 751/900 [04:07&lt;01:29,  1.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 752/900 [04:08&lt;01:29,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 753/900 [04:09&lt;01:28,  1.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 754/900 [04:09&lt;01:28,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 755/900 [04:10&lt;01:27,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 756/900 [04:10&lt;01:26,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 757/900 [04:11&lt;01:25,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 758/900 [04:12&lt;01:25,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 759/900 [04:12&lt;01:25,  1.64it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 760/900 [04:13&lt;01:25,  1.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 761/900 [04:13&lt;01:24,  1.65it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 762/900 [04:14&lt;01:23,  1.65it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 763/900 [04:15&lt;01:22,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 764/900 [04:15&lt;01:21,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 765/900 [04:16&lt;01:21,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 766/900 [04:16&lt;01:20,  1.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 767/900 [04:17&lt;01:19,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 768/900 [04:18&lt;01:19,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 769/900 [04:18&lt;01:19,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 770/900 [04:19&lt;01:18,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 771/900 [04:19&lt;01:18,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 772/900 [04:20&lt;01:17,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 773/900 [04:21&lt;01:17,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 774/900 [04:21&lt;01:16,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 775/900 [04:22&lt;01:15,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 776/900 [04:22&lt;01:15,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 777/900 [04:23&lt;01:14,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 778/900 [04:24&lt;01:14,  1.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 779/900 [04:24&lt;01:14,  1.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 780/900 [04:25&lt;01:19,  1.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 781/900 [04:26&lt;01:23,  1.42it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 782/900 [04:27&lt;01:23,  1.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 783/900 [04:27&lt;01:20,  1.45it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 784/900 [04:28&lt;01:17,  1.49it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 785/900 [04:29&lt;01:15,  1.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 786/900 [04:29&lt;01:13,  1.54it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 787/900 [04:30&lt;01:14,  1.51it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 788/900 [04:30&lt;01:13,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 789/900 [04:31&lt;01:11,  1.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 790/900 [04:32&lt;01:11,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 791/900 [04:32&lt;01:12,  1.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 792/900 [04:33&lt;01:10,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 793/900 [04:34&lt;01:09,  1.54it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 794/900 [04:34&lt;01:07,  1.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 795/900 [04:35&lt;01:06,  1.58it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 796/900 [04:36&lt;01:10,  1.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 797/900 [04:37&lt;01:15,  1.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 798/900 [04:37&lt;01:11,  1.43it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 799/900 [04:38&lt;01:08,  1.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 800/900 [04:38&lt;01:05,  1.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 801/900 [04:39&lt;01:04,  1.55it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 802/900 [04:40&lt;01:02,  1.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 803/900 [04:40&lt;01:01,  1.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 804/900 [04:41&lt;01:00,  1.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 805/900 [04:42&lt;01:00,  1.58it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 806/900 [04:42&lt;00:59,  1.59it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 807/900 [04:43&lt;00:58,  1.59it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 808/900 [04:44&lt;00:59,  1.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 809/900 [04:44&lt;00:59,  1.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 810/900 [04:45&lt;00:58,  1.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 811/900 [04:45&lt;00:57,  1.55it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 812/900 [04:46&lt;00:56,  1.55it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 813/900 [04:47&lt;00:55,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 814/900 [04:47&lt;00:54,  1.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 815/900 [04:48&lt;00:54,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 816/900 [04:49&lt;00:54,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 817/900 [04:49&lt;00:53,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 818/900 [04:50&lt;00:52,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 819/900 [04:51&lt;00:52,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 820/900 [04:51&lt;00:51,  1.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 821/900 [04:52&lt;00:50,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 822/900 [04:53&lt;00:50,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 823/900 [04:53&lt;00:49,  1.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 824/900 [04:54&lt;00:48,  1.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 825/900 [04:54&lt;00:48,  1.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 826/900 [04:55&lt;00:47,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 827/900 [04:56&lt;00:47,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 828/900 [04:56&lt;00:47,  1.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 829/900 [04:57&lt;00:46,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 830/900 [04:58&lt;00:45,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 831/900 [04:58&lt;00:44,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 832/900 [04:59&lt;00:44,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 833/900 [05:00&lt;00:43,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 834/900 [05:00&lt;00:44,  1.49it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 835/900 [05:01&lt;00:43,  1.50it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 836/900 [05:02&lt;00:42,  1.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 837/900 [05:02&lt;00:41,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 838/900 [05:03&lt;00:40,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 839/900 [05:04&lt;00:39,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 840/900 [05:04&lt;00:38,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 841/900 [05:05&lt;00:38,  1.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 842/900 [05:06&lt;00:37,  1.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 843/900 [05:06&lt;00:37,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 844/900 [05:07&lt;00:36,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 845/900 [05:08&lt;00:35,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 846/900 [05:08&lt;00:35,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 847/900 [05:09&lt;00:34,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 848/900 [05:10&lt;00:34,  1.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 849/900 [05:10&lt;00:33,  1.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 850/900 [05:11&lt;00:32,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 851/900 [05:12&lt;00:32,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 852/900 [05:12&lt;00:31,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 853/900 [05:13&lt;00:31,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 854/900 [05:14&lt;00:30,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 855/900 [05:14&lt;00:29,  1.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 856/900 [05:15&lt;00:29,  1.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 857/900 [05:16&lt;00:28,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 858/900 [05:16&lt;00:28,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 859/900 [05:17&lt;00:27,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 860/900 [05:18&lt;00:26,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 861/900 [05:18&lt;00:25,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 862/900 [05:19&lt;00:25,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 863/900 [05:20&lt;00:24,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 864/900 [05:20&lt;00:24,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 865/900 [05:21&lt;00:23,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 866/900 [05:22&lt;00:22,  1.48it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 867/900 [05:22&lt;00:23,  1.43it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 868/900 [05:23&lt;00:22,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 869/900 [05:24&lt;00:21,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 870/900 [05:24&lt;00:20,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 871/900 [05:25&lt;00:20,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 872/900 [05:26&lt;00:19,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 873/900 [05:26&lt;00:18,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 874/900 [05:27&lt;00:18,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 875/900 [05:28&lt;00:17,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 876/900 [05:29&lt;00:16,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 877/900 [05:29&lt;00:15,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 878/900 [05:30&lt;00:15,  1.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 879/900 [05:31&lt;00:14,  1.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 880/900 [05:31&lt;00:13,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 881/900 [05:32&lt;00:13,  1.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 882/900 [05:33&lt;00:12,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 883/900 [05:33&lt;00:11,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 884/900 [05:34&lt;00:11,  1.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 885/900 [05:35&lt;00:10,  1.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 886/900 [05:36&lt;00:09,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 887/900 [05:36&lt;00:09,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 888/900 [05:37&lt;00:08,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 889/900 [05:38&lt;00:07,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 890/900 [05:38&lt;00:06,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 891/900 [05:39&lt;00:06,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 892/900 [05:40&lt;00:05,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 893/900 [05:40&lt;00:04,  1.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 894/900 [05:41&lt;00:04,  1.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 895/900 [05:42&lt;00:03,  1.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 896/900 [05:42&lt;00:02,  1.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 897/900 [05:43&lt;00:02,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 898/900 [05:44&lt;00:01,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 899/900 [05:45&lt;00:00,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:45&lt;00:00,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:45&lt;00:00,  2.60it/s]\n\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(M_values, var_bs_values, marker='o', color='red')\nplt.xlabel('Nombre de simulations')\nplt.ylabel('VaR bootstrap')\nplt.title(\"Variation de la VaR bootstrap en fonction du nombre de simulations\")\n\nText(0.5, 1.0, 'Variation de la VaR bootstrap en fonction du nombre de simulations')\n\n\n\n\n\n\n\n\n\nPour la taille de l‚Äôechantillon bootstrap, nous allons prendre M=8000 √©tant donn√© que la courbe semble se stabiliser √† partir de cette valeur. Avec ce choix, la VaR estim√© est de 4.11%% avec un intervalle de confiance √† 5% de [4.05%, 4.09%]. De plus, en ce qui concerne la VaR historique, nous constatons que l‚Äôestimation est contenu dans l‚Äôintervalle de confiance.\n\nM=8000\nvar_bs_train, lower_ic,upper_ic = bootstrap_var(data_train, alpha=alpha, M=M)\nprint(f\"La VaR bootstrap pour h=1j et alpha=0.99 est : {var_bs_train:.4%}\")\nprint(f\"L'intervalle de confiance est : [{lower_ic:.4%}, {upper_ic:.4%}]\")\n\nLa VaR bootstrap pour h=1j et alpha=0.99 est : 4.1065%\nL'intervalle de confiance est : [4.0536%, 4.0939%]\n\n\n\n\nII.2.3. Backtest\nPour l‚Äôexercice de backtest, il s‚Äôagit de : 1. D√©terminer si la proportion \\(p\\) de violations de la VaR est coh√©rente avec le niveau de confiance, i.e.¬†√©gale √† \\(1-\\alpha\\). Cela permet de v√©rifier si la mesure de risque est bien calibr√©e. Pour cela, nous pouvons avoir recours √† un test de proportion ou un test de ratio de vraisemblance.\n**Unconditional coverage test** :\nSoit I la variable indicatrice de violation de la VaR, i.e. $I=1$ si la perte est sup√©rieure √† la VaR, et $I=0$ sinon. La proportion de violations de la VaR est donn√©e par :\n\n$$p = \\frac{1}{n} \\sum_{i=1}^{n} I_i = \\frac{Z}{n}$$\n\nSous H0, i.e. p=1-$\\alpha$, Z $\\sim$ Binomiale(n, 1-$\\alpha$). En supposant que n est suffisamment grand, on peut approximer Z par une loi normale. Ainsi donc :\n$$\\frac{Z - n (1-\\alpha)}{\\sqrt{\\alpha (1-\\alpha) n}} \\sim \\mathcal{N}(n(1-\\alpha), n\\alpha(1-\\alpha))$$\n\nSous cette hypoth√®se asymptotique, on peut calculer la statistique du ratio de vraisemblance suivant :\n$$LR = -2 ln \\left( \\frac{L(H1)}{L(H0)} \\right) =-2 ln \\left( 1- (1-\\alpha))^{n-e}(1-\\alpha)^e \\right) + 2 ln \\left( (1-\\frac{e}{n})^{n-e} (\\frac{e}{n})^e  \\right)  \\sim \\chi^2(1)$$\n\no√π e est le nombre de violations de la VaR. On rejette H0 si LR &gt; $\\chi^2(1-\\alpha)$.\n\n**Test de proportion** :\n$$\nH_0 : p = p_0 = 1-\\alpha \\\\ H_1 : p &gt; 1-\\alpha\n$$\nOn peut √©galement utiliser un test binomial pour tester si la proportion de violations de la VaR est √©gale √† $1-\\alpha$. On peut calculer la statistique du test suivant :\n$$Z = \\frac{p - p_0}{\\sqrt{p_0 (1-p_0) / n}} \\sim \\mathcal{N}(0,1)$$\n\nOn rejette H0 si Z &gt; $p_0+ \\phi^{-1}(1-\\alpha) \\sqrt{p_0 (1-p_0)/n}$, o√π $\\phi$ est la quantile de la loi normale standard.\n\nD√©terminer si, lorsqu‚Äôil y en a, les violations de VaR √† deux diff√©rents jours sont ind√©pendantes. Cela permet si la mesure de risque est capable de r√©agir aux chocs de march√© affectant la volatilit√© des rendements. Pour cela, nous utilisons un conditional coverage test.\nConditional coverage test : y revenir\n\n\nimport scipy.stats as stats\n\n# Objectif : impl√©menter une fonction calculant le nombre d'exception sur l'√©chantillon test\ndef exceptions(data, var):\n    \"\"\"\n    Calcul du nombre d'exception\n    data : les rendements logarithmiques\n    var : la VaR\n    \"\"\"\n    return np.sum(data &lt; -var)\n\n\n# Objectif : test de proportion binomiale\n\ndef binomial_test(n, p, p0 = 0.01, alpha=0.05):\n    \"\"\"\n    Test de proportion binomiale\n    H0 : p = p0\n    H1 : p &gt; p0\n    n : le nombre d'essais\n    p : la proportion\n    alpha : le niveau de confiance\n    \"\"\"\n\n    z = (p - p0) / np.sqrt(p0 * (1 - p0) / n)\n    #reject_zone = p0 + stats.norm.ppf(1 - alpha) * np.sqrt(p0 * (1 - p0) / n)\n    p_value = 1 - stats.norm.cdf(z)\n    reject = p_value &lt; alpha\n\n    # Calcul des IC\n    lower = p - stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n    upper = p + stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n\n    return p_value, reject, lower, upper\n\n# Unconditionnal coverage test ==&gt; to do.\n\n\n# Backtest de la VaR historique\n\nexceptions_test = exceptions(data_test, var_hist_train)\nprint(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {exceptions_test}\")\n\nprint(\"=\"*80)\nn = len(data_test)\np = exceptions_test / n\np_value, reject, lower,upper = binomial_test(n, p)\nprint(f\"H0 : le nombre d'exceptions est inf√©rieur ou √©gale √† {1-alpha:.2%}\")\nprint(f\"IC : [{lower:.2%},{upper:.2%}]\")\nprint(f\"La p-value du test de proportion binomiale est : {p_value:.4f}\")\nprint(f\"Rejet de l'hypoth√®se nulle : {reject}\")\nprint(\"=\"*80)\n\nLe nombre d'exceptions sur l'√©chantillon de test est : 0\n================================================================================\nH0 : le nombre d'exceptions est inf√©rieur ou √©gale √† 1.00%\nIC : [0.00%,0.00%]\nLa p-value du test de proportion binomiale est : 0.9925\nRejet de l'hypoth√®se nulle : False\n================================================================================"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "href": "3A/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.3. VaR param√©trique",
    "text": "II.3. VaR param√©trique\n\nII.3.1. Validation ex-ante\nVisuellement, les donn√©es ne semblent pas suivre une loi normale. En effet, les quantiles th√©oriques d‚Äôune loi normale ne collent pas avec les quantiles empiriques des rendements. Cela peut √™tre d√ª √† la pr√©sence de queues √©paisses observables sur l‚Äôestimation de la densit√© des rendements sur l‚Äô√©chantillon d‚Äôapprentissage, de pics, de clusters de volatilit√© que nous avons observ√©es plus haut. De plus, le skewness est n√©gatif ce qui indique une asym√©trie n√©gative des rendements. Enfin, le kurtosis est sup√©rieur √† 3, ce qui indique une distribution leptokurtique des rendements.\nNous allons tout de m√™me impl√©menter une VaR gaussienne pour voir comment elle se comporte dans le backtest.\n\n# Test visuel d'ad√©quation de la loi normale\n\n# Cr√©er un Q-Q plot\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(data_train, dist=\"norm\", plot=ax)\n\n# Personnalisation du graphique\nax.set_title(\"Q-Q Plot (Normal Distribution)\")\nax.set_xlabel(\"Theoretical Quantiles\")\nax.set_ylabel(\"Sample Quantiles\")\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\n# Densit√© de l'echantillon train et l'√©chantillon de test\n\nplt.figure(figsize=(10, 4))\ndata_train.plot(kind='kde', label='Train', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Densit√© de l'√©chantillon d'entrainement\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Skewness et kurtosis\nprint(\"=\"*80)\nprint(\"Skewness de l'√©chantillon d'entrainement : \", data_train.skew())\nprint(\"Kurtosis de l'√©chantillon d'entrainement : \", data_train.kurt())\nprint(\"=\"*80)\n\n================================================================================\nSkewness de l'√©chantillon d'entrainement :  -0.2981820421484688\nKurtosis de l'√©chantillon d'entrainement :  7.353960005618779\n================================================================================\n\n\n\nfrom scipy.stats import kstest\n\n# Test de Kolmogorov-Smirnov\nks_stat, ks_p_value = kstest(data_train, 'norm')\nprint(\"=\"*80)\nprint(\"H0 : Les donn√©es suivent une loi normale\")\nprint(f\"Statistique de test : {ks_stat:.4f}\")\nprint(f\"P-value : {ks_p_value:.4f}\")\nprint(\"=\"*80)\n\n================================================================================\nH0 : Les donn√©es suivent une loi normale\nStatistique de test : 0.4775\nP-value : 0.0000\n================================================================================\n\n\n\n\nII.3.2. Impl√©mentation de la VaR\n\na. M√©thode scaling\n\n# Objectif : √©crire une fonction qui calcule la VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n\nvar_gauss_train = gaussian_var(data_train, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=1j et alpha={alpha} est : {var_gauss_train:.4%}\")\nprint(f\"La VaR historique pour h=10j et alpha={alpha} est : {np.sqrt(10)*var_gauss_train:.4%}\")\n\nLa VaR gaussienne pour h=1j et alpha=0.99 est : 3.2302%\nLa VaR historique pour h=10j et alpha=0.99 est : 10.2148%\n\n\n\n\nb. M√©thode de diffusion d‚Äôun actif\nPour calculer la VaR gaussienne √† 10 jours par m√©thode de diffusion d‚Äôun actif, nous allons suivre les √©tapes suivantes :\nL‚Äô√©volution du prix d‚Äôun actif suit un processus de type mouvement brownien g√©om√©trique : \\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t &lt;=&gt; S_t = S_{t-1} e^{(\\mu - \\frac{1}{2} \\sigma^2) + \\sigma W_t}\n\\] o√π $ S_t$ est le prix de l‚Äôactif √† l‚Äôinstant$ t$, $ $ est le rendement moyen estim√© (drift), $ $ est la volatilit√© du rendement, $ dW_t$ est un mouvement brownien standard.\nOn peut de ce fait calculer plusieurs trajectoires de rendements de \\(S_0\\) et \\(S_{10}\\), puis calculer la VaR √† partir de la s√©rie des rendements $ r_{10j} = (S_{10} / S_{0}) $ obtenus avec ces trajectoires.\nEn utilisant la m√©thode de scaling et la m√©thode de diffusion, nous obtenons sensiblement la m√™me VaR.\n\nimport numpy as np\n\nmu = np.mean(data_train)\nsigma = np.std(data_train)\n\nreturn_sim = []\nT = 10\nM = 1000\nS0 = train['Close'].iloc[-1]\n\nfor i in range(M):\n    S = np.zeros(T+1)\n    S[0] = S0\n    for t in range(1,T+1):\n        Wt = np.random.normal()\n        S[t] = S[t-1] * np.exp((mu - 0.5 * sigma**2) + sigma * Wt)\n\n    return_sim.append(np.log(S[T]/S0))\n\nvar_gauss_diffusion = gaussian_var(return_sim, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=10j et alpha={alpha} est : {var_gauss_diffusion:.4%}\")\n\nLa VaR gaussienne pour h=10j et alpha=0.99 est : 10.0053%\n\n\n\n\nc.¬†M√©thode EWMA\nLa VaR EWMA est une m√©thode qui permet de calculer la VaR en surpond√©rant les rendements les plus r√©cents. Cela permet de donner plus de poids aux rendements les plus r√©cents, et donc de mieux capturer les changements de volatilit√©. La VaR EWMA est donn√©e par la formule suivante :\n\\[\nVaR_{t+1} = \\mu + \\sigma \\times z_{\\alpha}\n\\]\n\n# VaR la m√©thode EWMA (Exponential Weighting Moving Average)\n\ndef gaussian_var_ewma(data, alpha, lambda_=0.94):\n    \"\"\"\n    Calcul de la VaR gaussienne EWMA\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    lambda_ : le param√®tre de lissage\n    \"\"\"\n\n    weights = np.array([(1-lambda_)*(lambda_**i) for i in range(len(data))])\n    weights = weights / np.sum(weights)\n\n    mu = np.sum(data[::-1] * weights)\n    sigma = np.sqrt(np.sum(weights * (data[::-1] - mu)**2))\n\n    return -(mu + sigma * stats.norm.ppf(1 - alpha)), mu, sigma\n\n# y revenir\n\n\nlambdas = [0.9, 0.95, 0.99]\nalpha = 0.99\nimport scipy.stats as stats\nfor l in lambdas:\n    print(\"=\"*80)\n    print(\"Lambda : \", l)\n    print(\"-\"*15)\n    var_gauss_emwa, mu, sigma = gaussian_var_ewma(data_train, alpha=alpha, lambda_=l)\n    print(f\"La VaR gaussienne EWMA pour h=1j et alpha={alpha} est : {var_gauss_emwa:.4%}\")\n    print(f\"La moyenne est : {mu:.4%}\")\n    print(f\"L'√©cart-type est : {sigma:.4%}\")\n\n    n_exceptions = exceptions(data_test, var_gauss_emwa)\n    print(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {n_exceptions}\")\n\n================================================================================\nLambda :  0.9\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.3751%\nLa moyenne est : 0.1898%\nL'√©cart-type est : 1.1025%\nLe nombre d'exceptions sur l'√©chantillon de test est : 5\n================================================================================\nLambda :  0.95\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.8969%\nLa moyenne est : 0.0735%\nL'√©cart-type est : 1.2768%\nLe nombre d'exceptions sur l'√©chantillon de test est : 4\n================================================================================\nLambda :  0.99\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 3.3511%\nLa moyenne est : -0.0321%\nL'√©cart-type est : 1.4267%\nLe nombre d'exceptions sur l'√©chantillon de test est : 1\n\n\nAvec la m√©thode EWMA, nous observons une que la VaR diminue plus \\(\\lambda\\) augmente. Cela est d√ª au fait que plus \\(\\lambda\\) est grand, plus les rendements les plus r√©cents sont surpond√©r√©s, et donc la volatilit√© est plus faible, en raison de la fin de la p√©riode d‚Äôapprentissage."
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "href": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.4. VaR skew-student",
    "text": "II.4. VaR skew-student\n\nII.4.1. Validation ex-ante\n\n# Ecrire une fonction permettant d‚Äôestimer les param√®tres d‚Äôune loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n\n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n\n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(data_train)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0],\n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.0023251952411471218\nSigma :  0.008823931435233275\nGamma :  -0.23188477391476636\nNu :  2.9618199934116825\n================================================================================\n\n\n\n# Superposition de la densit√© th√©orique et des donn√©es\n\nx_values = np.linspace(min(data_train), max(data_train), 1000)\n\ntheoretical_density = skew_student_pdf(x_values, **params_sstd)\nplt.figure(figsize = (10,4))\nplt.hist(data_train, bins=30, density=True, alpha=0.5, label='Donn√©es empiriques')\nplt.plot(x_values, theoretical_density, label='Densit√© Skew Student', color='red')\n# Densit√© normale\nplt.plot(x_values, stats.norm.pdf(x_values, np.mean(data_train), np.std(data_train)), label='Densit√© normale', color='blue')\nplt.xlabel('Rendements logarithmiques')\nplt.ylabel('Densit√©')\nplt.title(\"Comparaison entre les donn√©es et la densit√© th√©orique d'une loi de Skew Student\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densit√© th√©orique de la skew-student semble bien s‚Äôajuster aux donn√©es. La loi skew-student de param√®tres (\\(\\mu = 0.002, \\sigma = 0.009, \\gamma = -0.23, \\nu = 2.96\\)). Le \\(\\mu\\) est le rendement moyen, \\(\\sigma\\) est l‚Äô√©cart-type, \\(\\gamma\\) est le coefficient d‚Äôasym√©trie et \\(\\nu\\) est le degr√© de libert√©. Le skewness est n√©gatif, ce qui indique une asym√©trie n√©gative des rendements, comme ce qu‚Äôon a observ√© plus haut.\nNous allons appuyer cette validation en utilisant le QQ-plot. La fonction quantile d‚Äôune loi skew-student n‚Äôest pas analytique. Pour ce faire, nous allons construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l‚Äôinverse de cette fonction de repartition. Nous allons ensuite comparer les quantiles th√©oriques de la skew-student avec les quantiles empiriques des rendements.\nEn observant le QQ-plot, on constate que les quantiles th√©oriques de la skew-student collent bien avec les quantiles empiriques des rendements. Cela confirme que la skew-student est une bonne approximation de la distribution des rendements. Pour une validation plus rigoureuse, on peut utiliser un test de Kolmogorov-Smirnov pour tester si les rendements suivent une loi skew-student.\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\nquantiles_empiriques = np.quantile(data_train, niveaux_quantiles)\nquantiles_theoriques = [skew_student_quantile(alpha,**params_sstd) for alpha in tqdm(niveaux_quantiles)]\n\n  0%|          | 0/999 [00:00&lt;?, ?it/s]  0%|          | 1/999 [00:00&lt;05:35,  2.97it/s]  0%|          | 2/999 [00:00&lt;05:26,  3.05it/s]  0%|          | 3/999 [00:00&lt;05:30,  3.02it/s]  0%|          | 4/999 [00:01&lt;06:18,  2.63it/s]  1%|          | 5/999 [00:01&lt;06:32,  2.53it/s]  1%|          | 6/999 [00:02&lt;06:37,  2.50it/s]  1%|          | 7/999 [00:02&lt;06:34,  2.52it/s]  1%|          | 8/999 [00:03&lt;06:49,  2.42it/s]  1%|          | 9/999 [00:03&lt;06:48,  2.43it/s]  1%|          | 10/999 [00:03&lt;06:58,  2.36it/s]  1%|          | 11/999 [00:04&lt;06:43,  2.45it/s]  1%|          | 12/999 [00:04&lt;06:49,  2.41it/s]  1%|‚ñè         | 13/999 [00:05&lt;06:58,  2.36it/s]  1%|‚ñè         | 14/999 [00:05&lt;07:08,  2.30it/s]  2%|‚ñè         | 15/999 [00:06&lt;06:57,  2.36it/s]  2%|‚ñè         | 16/999 [00:06&lt;06:59,  2.34it/s]  2%|‚ñè         | 17/999 [00:06&lt;06:54,  2.37it/s]  2%|‚ñè         | 18/999 [00:07&lt;06:56,  2.36it/s]  2%|‚ñè         | 19/999 [00:07&lt;06:58,  2.34it/s]  2%|‚ñè         | 20/999 [00:08&lt;06:46,  2.41it/s]  2%|‚ñè         | 21/999 [00:08&lt;06:44,  2.42it/s]  2%|‚ñè         | 22/999 [00:09&lt;06:54,  2.36it/s]  2%|‚ñè         | 23/999 [00:09&lt;06:52,  2.37it/s]  2%|‚ñè         | 24/999 [00:09&lt;06:45,  2.41it/s]  3%|‚ñé         | 25/999 [00:10&lt;06:50,  2.37it/s]  3%|‚ñé         | 26/999 [00:10&lt;07:13,  2.25it/s]  3%|‚ñé         | 27/999 [00:11&lt;07:05,  2.29it/s]  3%|‚ñé         | 28/999 [00:11&lt;06:59,  2.31it/s]  3%|‚ñé         | 29/999 [00:12&lt;07:03,  2.29it/s]  3%|‚ñé         | 30/999 [00:12&lt;06:59,  2.31it/s]  3%|‚ñé         | 31/999 [00:12&lt;06:58,  2.32it/s]  3%|‚ñé         | 32/999 [00:13&lt;07:06,  2.26it/s]  3%|‚ñé         | 33/999 [00:13&lt;07:24,  2.17it/s]  3%|‚ñé         | 34/999 [00:14&lt;07:34,  2.12it/s]  4%|‚ñé         | 35/999 [00:14&lt;07:34,  2.12it/s]  4%|‚ñé         | 36/999 [00:15&lt;07:39,  2.10it/s]  4%|‚ñé         | 37/999 [00:15&lt;07:42,  2.08it/s]  4%|‚ñç         | 38/999 [00:16&lt;07:58,  2.01it/s]  4%|‚ñç         | 39/999 [00:16&lt;08:08,  1.96it/s]  4%|‚ñç         | 40/999 [00:17&lt;07:59,  2.00it/s]  4%|‚ñç         | 41/999 [00:17&lt;07:53,  2.02it/s]  4%|‚ñç         | 42/999 [00:18&lt;07:53,  2.02it/s]  4%|‚ñç         | 43/999 [00:18&lt;07:49,  2.04it/s]  4%|‚ñç         | 44/999 [00:19&lt;07:44,  2.06it/s]  5%|‚ñç         | 45/999 [00:19&lt;07:43,  2.06it/s]  5%|‚ñç         | 46/999 [00:20&lt;07:46,  2.04it/s]  5%|‚ñç         | 47/999 [00:20&lt;07:40,  2.07it/s]  5%|‚ñç         | 48/999 [00:21&lt;07:50,  2.02it/s]  5%|‚ñç         | 49/999 [00:21&lt;08:00,  1.98it/s]  5%|‚ñå         | 50/999 [00:22&lt;07:49,  2.02it/s]  5%|‚ñå         | 51/999 [00:22&lt;07:48,  2.02it/s]  5%|‚ñå         | 52/999 [00:23&lt;08:03,  1.96it/s]  5%|‚ñå         | 53/999 [00:23&lt;08:03,  1.96it/s]  5%|‚ñå         | 54/999 [00:24&lt;07:50,  2.01it/s]  6%|‚ñå         | 55/999 [00:24&lt;07:59,  1.97it/s]  6%|‚ñå         | 56/999 [00:25&lt;07:53,  1.99it/s]  6%|‚ñå         | 57/999 [00:25&lt;07:58,  1.97it/s]  6%|‚ñå         | 58/999 [00:26&lt;08:02,  1.95it/s]  6%|‚ñå         | 59/999 [00:26&lt;07:43,  2.03it/s]  6%|‚ñå         | 60/999 [00:27&lt;07:45,  2.02it/s]  6%|‚ñå         | 61/999 [00:27&lt;08:01,  1.95it/s]  6%|‚ñå         | 62/999 [00:28&lt;08:07,  1.92it/s]  6%|‚ñã         | 63/999 [00:28&lt;08:08,  1.92it/s]  6%|‚ñã         | 64/999 [00:29&lt;08:07,  1.92it/s]  7%|‚ñã         | 65/999 [00:29&lt;07:53,  1.97it/s]  7%|‚ñã         | 66/999 [00:30&lt;07:41,  2.02it/s]  7%|‚ñã         | 67/999 [00:30&lt;07:55,  1.96it/s]  7%|‚ñã         | 68/999 [00:31&lt;07:41,  2.02it/s]  7%|‚ñã         | 69/999 [00:31&lt;07:47,  1.99it/s]  7%|‚ñã         | 70/999 [00:32&lt;07:38,  2.03it/s]  7%|‚ñã         | 71/999 [00:32&lt;07:47,  1.99it/s]  7%|‚ñã         | 72/999 [00:33&lt;07:53,  1.96it/s]  7%|‚ñã         | 73/999 [00:34&lt;07:58,  1.94it/s]  7%|‚ñã         | 74/999 [00:34&lt;07:51,  1.96it/s]  8%|‚ñä         | 75/999 [00:35&lt;07:59,  1.93it/s]  8%|‚ñä         | 76/999 [00:35&lt;08:03,  1.91it/s]  8%|‚ñä         | 77/999 [00:36&lt;07:51,  1.95it/s]  8%|‚ñä         | 78/999 [00:36&lt;08:02,  1.91it/s]  8%|‚ñä         | 79/999 [00:37&lt;08:06,  1.89it/s]  8%|‚ñä         | 80/999 [00:37&lt;08:01,  1.91it/s]  8%|‚ñä         | 81/999 [00:38&lt;07:47,  1.96it/s]  8%|‚ñä         | 82/999 [00:38&lt;07:46,  1.97it/s]  8%|‚ñä         | 83/999 [00:39&lt;07:37,  2.00it/s]  8%|‚ñä         | 84/999 [00:39&lt;07:39,  1.99it/s]  9%|‚ñä         | 85/999 [00:40&lt;07:41,  1.98it/s]  9%|‚ñä         | 86/999 [00:40&lt;07:39,  1.99it/s]  9%|‚ñä         | 87/999 [00:41&lt;07:42,  1.97it/s]  9%|‚ñâ         | 88/999 [00:41&lt;07:31,  2.02it/s]  9%|‚ñâ         | 89/999 [00:42&lt;07:26,  2.04it/s]  9%|‚ñâ         | 90/999 [00:42&lt;07:11,  2.11it/s]  9%|‚ñâ         | 91/999 [00:43&lt;07:16,  2.08it/s]  9%|‚ñâ         | 92/999 [00:43&lt;07:15,  2.08it/s]  9%|‚ñâ         | 93/999 [00:43&lt;06:51,  2.20it/s]  9%|‚ñâ         | 94/999 [00:44&lt;07:07,  2.11it/s] 10%|‚ñâ         | 95/999 [00:44&lt;07:23,  2.04it/s] 10%|‚ñâ         | 96/999 [00:45&lt;07:25,  2.03it/s] 10%|‚ñâ         | 97/999 [00:45&lt;07:35,  1.98it/s] 10%|‚ñâ         | 98/999 [00:46&lt;07:38,  1.96it/s] 10%|‚ñâ         | 99/999 [00:47&lt;07:37,  1.97it/s] 10%|‚ñà         | 100/999 [00:47&lt;07:47,  1.92it/s] 10%|‚ñà         | 101/999 [00:48&lt;07:42,  1.94it/s] 10%|‚ñà         | 102/999 [00:48&lt;07:54,  1.89it/s] 10%|‚ñà         | 103/999 [00:49&lt;07:43,  1.93it/s] 10%|‚ñà         | 104/999 [00:49&lt;07:34,  1.97it/s] 11%|‚ñà         | 105/999 [00:50&lt;07:23,  2.01it/s] 11%|‚ñà         | 106/999 [00:50&lt;07:29,  1.99it/s] 11%|‚ñà         | 107/999 [00:51&lt;07:27,  1.99it/s] 11%|‚ñà         | 108/999 [00:51&lt;07:42,  1.92it/s] 11%|‚ñà         | 109/999 [00:52&lt;07:42,  1.92it/s] 11%|‚ñà         | 110/999 [00:52&lt;07:36,  1.95it/s] 11%|‚ñà         | 111/999 [00:53&lt;07:37,  1.94it/s] 11%|‚ñà         | 112/999 [00:53&lt;07:46,  1.90it/s] 11%|‚ñà‚ñè        | 113/999 [00:54&lt;07:52,  1.88it/s] 11%|‚ñà‚ñè        | 114/999 [00:54&lt;07:46,  1.90it/s] 12%|‚ñà‚ñè        | 115/999 [00:55&lt;07:46,  1.90it/s] 12%|‚ñà‚ñè        | 116/999 [00:55&lt;07:40,  1.92it/s] 12%|‚ñà‚ñè        | 117/999 [00:56&lt;07:42,  1.91it/s] 12%|‚ñà‚ñè        | 118/999 [00:56&lt;07:43,  1.90it/s] 12%|‚ñà‚ñè        | 119/999 [00:57&lt;07:48,  1.88it/s] 12%|‚ñà‚ñè        | 120/999 [00:57&lt;07:39,  1.91it/s] 12%|‚ñà‚ñè        | 121/999 [00:58&lt;07:43,  1.90it/s] 12%|‚ñà‚ñè        | 122/999 [00:59&lt;08:00,  1.83it/s] 12%|‚ñà‚ñè        | 123/999 [00:59&lt;07:57,  1.83it/s] 12%|‚ñà‚ñè        | 124/999 [01:00&lt;08:07,  1.80it/s] 13%|‚ñà‚ñé        | 125/999 [01:00&lt;08:03,  1.81it/s] 13%|‚ñà‚ñé        | 126/999 [01:01&lt;08:09,  1.78it/s] 13%|‚ñà‚ñé        | 127/999 [01:01&lt;08:04,  1.80it/s] 13%|‚ñà‚ñé        | 128/999 [01:02&lt;07:53,  1.84it/s] 13%|‚ñà‚ñé        | 129/999 [01:02&lt;07:47,  1.86it/s] 13%|‚ñà‚ñé        | 130/999 [01:03&lt;07:50,  1.85it/s] 13%|‚ñà‚ñé        | 131/999 [01:04&lt;07:48,  1.85it/s] 13%|‚ñà‚ñé        | 132/999 [01:04&lt;07:37,  1.89it/s] 13%|‚ñà‚ñé        | 133/999 [01:04&lt;07:29,  1.93it/s] 13%|‚ñà‚ñé        | 134/999 [01:05&lt;07:19,  1.97it/s] 14%|‚ñà‚ñé        | 135/999 [01:06&lt;07:23,  1.95it/s] 14%|‚ñà‚ñé        | 136/999 [01:06&lt;07:29,  1.92it/s] 14%|‚ñà‚ñé        | 137/999 [01:07&lt;07:33,  1.90it/s] 14%|‚ñà‚ñç        | 138/999 [01:07&lt;07:24,  1.94it/s] 14%|‚ñà‚ñç        | 139/999 [01:08&lt;07:39,  1.87it/s] 14%|‚ñà‚ñç        | 140/999 [01:08&lt;07:44,  1.85it/s] 14%|‚ñà‚ñç        | 141/999 [01:09&lt;07:38,  1.87it/s] 14%|‚ñà‚ñç        | 142/999 [01:09&lt;07:29,  1.91it/s] 14%|‚ñà‚ñç        | 143/999 [01:10&lt;07:19,  1.95it/s] 14%|‚ñà‚ñç        | 144/999 [01:10&lt;07:12,  1.98it/s] 15%|‚ñà‚ñç        | 145/999 [01:11&lt;07:07,  2.00it/s] 15%|‚ñà‚ñç        | 146/999 [01:11&lt;07:05,  2.00it/s] 15%|‚ñà‚ñç        | 147/999 [01:12&lt;07:16,  1.95it/s] 15%|‚ñà‚ñç        | 148/999 [01:12&lt;07:45,  1.83it/s] 15%|‚ñà‚ñç        | 149/999 [01:13&lt;07:45,  1.83it/s] 15%|‚ñà‚ñå        | 150/999 [01:14&lt;07:54,  1.79it/s] 15%|‚ñà‚ñå        | 151/999 [01:14&lt;07:58,  1.77it/s] 15%|‚ñà‚ñå        | 152/999 [01:15&lt;07:43,  1.83it/s] 15%|‚ñà‚ñå        | 153/999 [01:15&lt;07:46,  1.81it/s] 15%|‚ñà‚ñå        | 154/999 [01:16&lt;07:46,  1.81it/s] 16%|‚ñà‚ñå        | 155/999 [01:16&lt;07:54,  1.78it/s] 16%|‚ñà‚ñå        | 156/999 [01:17&lt;08:07,  1.73it/s] 16%|‚ñà‚ñå        | 157/999 [01:18&lt;08:14,  1.70it/s] 16%|‚ñà‚ñå        | 158/999 [01:18&lt;07:59,  1.75it/s] 16%|‚ñà‚ñå        | 159/999 [01:19&lt;07:58,  1.75it/s] 16%|‚ñà‚ñå        | 160/999 [01:19&lt;08:07,  1.72it/s] 16%|‚ñà‚ñå        | 161/999 [01:20&lt;08:05,  1.73it/s] 16%|‚ñà‚ñå        | 162/999 [01:20&lt;08:09,  1.71it/s] 16%|‚ñà‚ñã        | 163/999 [01:21&lt;08:13,  1.69it/s] 16%|‚ñà‚ñã        | 164/999 [01:22&lt;08:23,  1.66it/s] 17%|‚ñà‚ñã        | 165/999 [01:22&lt;08:22,  1.66it/s] 17%|‚ñà‚ñã        | 166/999 [01:23&lt;08:20,  1.67it/s] 17%|‚ñà‚ñã        | 167/999 [01:23&lt;08:19,  1.66it/s] 17%|‚ñà‚ñã        | 168/999 [01:24&lt;08:19,  1.66it/s] 17%|‚ñà‚ñã        | 169/999 [01:25&lt;08:08,  1.70it/s] 17%|‚ñà‚ñã        | 170/999 [01:25&lt;08:12,  1.68it/s] 17%|‚ñà‚ñã        | 171/999 [01:26&lt;08:05,  1.70it/s] 17%|‚ñà‚ñã        | 172/999 [01:26&lt;08:20,  1.65it/s] 17%|‚ñà‚ñã        | 173/999 [01:27&lt;08:15,  1.67it/s] 17%|‚ñà‚ñã        | 174/999 [01:28&lt;08:06,  1.69it/s] 18%|‚ñà‚ñä        | 175/999 [01:28&lt;08:11,  1.68it/s] 18%|‚ñà‚ñä        | 176/999 [01:29&lt;08:13,  1.67it/s] 18%|‚ñà‚ñä        | 177/999 [01:29&lt;08:23,  1.63it/s] 18%|‚ñà‚ñä        | 178/999 [01:30&lt;08:15,  1.66it/s] 18%|‚ñà‚ñä        | 179/999 [01:31&lt;08:13,  1.66it/s] 18%|‚ñà‚ñä        | 180/999 [01:31&lt;08:03,  1.69it/s] 18%|‚ñà‚ñä        | 181/999 [01:32&lt;08:05,  1.68it/s] 18%|‚ñà‚ñä        | 182/999 [01:32&lt;08:02,  1.69it/s] 18%|‚ñà‚ñä        | 183/999 [01:33&lt;08:23,  1.62it/s] 18%|‚ñà‚ñä        | 184/999 [01:34&lt;08:12,  1.66it/s] 19%|‚ñà‚ñä        | 185/999 [01:34&lt;08:20,  1.63it/s] 19%|‚ñà‚ñä        | 186/999 [01:35&lt;08:34,  1.58it/s] 19%|‚ñà‚ñä        | 187/999 [01:36&lt;08:31,  1.59it/s] 19%|‚ñà‚ñâ        | 188/999 [01:36&lt;08:38,  1.56it/s] 19%|‚ñà‚ñâ        | 189/999 [01:37&lt;08:30,  1.59it/s] 19%|‚ñà‚ñâ        | 190/999 [01:37&lt;08:26,  1.60it/s] 19%|‚ñà‚ñâ        | 191/999 [01:38&lt;08:10,  1.65it/s] 19%|‚ñà‚ñâ        | 192/999 [01:39&lt;08:18,  1.62it/s] 19%|‚ñà‚ñâ        | 193/999 [01:39&lt;08:03,  1.67it/s] 19%|‚ñà‚ñâ        | 194/999 [01:40&lt;07:45,  1.73it/s] 20%|‚ñà‚ñâ        | 195/999 [01:40&lt;07:36,  1.76it/s] 20%|‚ñà‚ñâ        | 196/999 [01:41&lt;07:40,  1.74it/s] 20%|‚ñà‚ñâ        | 197/999 [01:41&lt;07:50,  1.70it/s] 20%|‚ñà‚ñâ        | 198/999 [01:42&lt;07:42,  1.73it/s] 20%|‚ñà‚ñâ        | 199/999 [01:43&lt;07:36,  1.75it/s] 20%|‚ñà‚ñà        | 200/999 [01:43&lt;07:51,  1.69it/s] 20%|‚ñà‚ñà        | 201/999 [01:44&lt;07:43,  1.72it/s] 20%|‚ñà‚ñà        | 202/999 [01:44&lt;07:41,  1.73it/s] 20%|‚ñà‚ñà        | 203/999 [01:45&lt;07:36,  1.74it/s] 20%|‚ñà‚ñà        | 204/999 [01:45&lt;07:13,  1.83it/s] 21%|‚ñà‚ñà        | 205/999 [01:46&lt;07:06,  1.86it/s] 21%|‚ñà‚ñà        | 206/999 [01:46&lt;07:08,  1.85it/s] 21%|‚ñà‚ñà        | 207/999 [01:47&lt;07:24,  1.78it/s] 21%|‚ñà‚ñà        | 208/999 [01:48&lt;07:26,  1.77it/s] 21%|‚ñà‚ñà        | 209/999 [01:48&lt;07:48,  1.69it/s] 21%|‚ñà‚ñà        | 210/999 [01:49&lt;07:58,  1.65it/s] 21%|‚ñà‚ñà        | 211/999 [01:50&lt;08:06,  1.62it/s] 21%|‚ñà‚ñà        | 212/999 [01:50&lt;08:02,  1.63it/s] 21%|‚ñà‚ñà‚ñè       | 213/999 [01:51&lt;08:08,  1.61it/s] 21%|‚ñà‚ñà‚ñè       | 214/999 [01:51&lt;07:57,  1.64it/s] 22%|‚ñà‚ñà‚ñè       | 215/999 [01:52&lt;07:56,  1.65it/s] 22%|‚ñà‚ñà‚ñè       | 216/999 [01:53&lt;07:50,  1.67it/s] 22%|‚ñà‚ñà‚ñè       | 217/999 [01:53&lt;07:39,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 218/999 [01:54&lt;07:40,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 219/999 [01:54&lt;07:33,  1.72it/s] 22%|‚ñà‚ñà‚ñè       | 220/999 [01:55&lt;07:37,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 221/999 [01:56&lt;07:41,  1.69it/s] 22%|‚ñà‚ñà‚ñè       | 222/999 [01:56&lt;07:31,  1.72it/s] 22%|‚ñà‚ñà‚ñè       | 223/999 [01:57&lt;07:21,  1.76it/s] 22%|‚ñà‚ñà‚ñè       | 224/999 [01:57&lt;07:26,  1.73it/s] 23%|‚ñà‚ñà‚ñé       | 225/999 [01:58&lt;07:42,  1.67it/s] 23%|‚ñà‚ñà‚ñé       | 226/999 [01:58&lt;07:40,  1.68it/s] 23%|‚ñà‚ñà‚ñé       | 227/999 [01:59&lt;07:26,  1.73it/s] 23%|‚ñà‚ñà‚ñé       | 228/999 [02:00&lt;07:27,  1.72it/s] 23%|‚ñà‚ñà‚ñé       | 229/999 [02:00&lt;07:17,  1.76it/s] 23%|‚ñà‚ñà‚ñé       | 230/999 [02:01&lt;07:30,  1.71it/s] 23%|‚ñà‚ñà‚ñé       | 231/999 [02:01&lt;07:26,  1.72it/s] 23%|‚ñà‚ñà‚ñé       | 232/999 [02:02&lt;07:28,  1.71it/s] 23%|‚ñà‚ñà‚ñé       | 233/999 [02:02&lt;07:33,  1.69it/s] 23%|‚ñà‚ñà‚ñé       | 234/999 [02:03&lt;07:29,  1.70it/s] 24%|‚ñà‚ñà‚ñé       | 235/999 [02:04&lt;07:28,  1.70it/s] 24%|‚ñà‚ñà‚ñé       | 236/999 [02:04&lt;07:32,  1.69it/s] 24%|‚ñà‚ñà‚ñé       | 237/999 [02:05&lt;07:37,  1.67it/s] 24%|‚ñà‚ñà‚ñç       | 238/999 [02:06&lt;07:55,  1.60it/s] 24%|‚ñà‚ñà‚ñç       | 239/999 [02:06&lt;08:02,  1.57it/s] 24%|‚ñà‚ñà‚ñç       | 240/999 [02:07&lt;08:10,  1.55it/s] 24%|‚ñà‚ñà‚ñç       | 241/999 [02:08&lt;08:06,  1.56it/s] 24%|‚ñà‚ñà‚ñç       | 242/999 [02:08&lt;08:12,  1.54it/s] 24%|‚ñà‚ñà‚ñç       | 243/999 [02:09&lt;08:23,  1.50it/s] 24%|‚ñà‚ñà‚ñç       | 244/999 [02:10&lt;08:25,  1.49it/s] 25%|‚ñà‚ñà‚ñç       | 245/999 [02:10&lt;08:22,  1.50it/s] 25%|‚ñà‚ñà‚ñç       | 246/999 [02:11&lt;08:28,  1.48it/s] 25%|‚ñà‚ñà‚ñç       | 247/999 [02:12&lt;08:23,  1.49it/s] 25%|‚ñà‚ñà‚ñç       | 248/999 [02:12&lt;08:31,  1.47it/s] 25%|‚ñà‚ñà‚ñç       | 249/999 [02:13&lt;08:32,  1.46it/s] 25%|‚ñà‚ñà‚ñå       | 250/999 [02:14&lt;08:32,  1.46it/s] 25%|‚ñà‚ñà‚ñå       | 251/999 [02:14&lt;08:28,  1.47it/s] 25%|‚ñà‚ñà‚ñå       | 252/999 [02:15&lt;08:24,  1.48it/s] 25%|‚ñà‚ñà‚ñå       | 253/999 [02:16&lt;08:32,  1.45it/s] 25%|‚ñà‚ñà‚ñå       | 254/999 [02:16&lt;08:45,  1.42it/s] 26%|‚ñà‚ñà‚ñå       | 255/999 [02:17&lt;08:30,  1.46it/s] 26%|‚ñà‚ñà‚ñå       | 256/999 [02:18&lt;08:19,  1.49it/s] 26%|‚ñà‚ñà‚ñå       | 257/999 [02:18&lt;08:08,  1.52it/s] 26%|‚ñà‚ñà‚ñå       | 258/999 [02:19&lt;08:10,  1.51it/s] 26%|‚ñà‚ñà‚ñå       | 259/999 [02:20&lt;07:59,  1.54it/s] 26%|‚ñà‚ñà‚ñå       | 260/999 [02:20&lt;07:56,  1.55it/s] 26%|‚ñà‚ñà‚ñå       | 261/999 [02:21&lt;07:49,  1.57it/s] 26%|‚ñà‚ñà‚ñå       | 262/999 [02:21&lt;07:32,  1.63it/s] 26%|‚ñà‚ñà‚ñã       | 263/999 [02:22&lt;07:30,  1.63it/s] 26%|‚ñà‚ñà‚ñã       | 264/999 [02:23&lt;07:42,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 265/999 [02:23&lt;07:42,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 266/999 [02:24&lt;07:35,  1.61it/s] 27%|‚ñà‚ñà‚ñã       | 267/999 [02:25&lt;07:33,  1.61it/s] 27%|‚ñà‚ñà‚ñã       | 268/999 [02:25&lt;07:45,  1.57it/s] 27%|‚ñà‚ñà‚ñã       | 269/999 [02:26&lt;07:42,  1.58it/s] 27%|‚ñà‚ñà‚ñã       | 270/999 [02:27&lt;07:48,  1.56it/s] 27%|‚ñà‚ñà‚ñã       | 271/999 [02:27&lt;07:38,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 272/999 [02:28&lt;07:45,  1.56it/s] 27%|‚ñà‚ñà‚ñã       | 273/999 [02:28&lt;07:49,  1.55it/s] 27%|‚ñà‚ñà‚ñã       | 274/999 [02:29&lt;07:44,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 275/999 [02:30&lt;07:48,  1.55it/s] 28%|‚ñà‚ñà‚ñä       | 276/999 [02:30&lt;07:48,  1.54it/s] 28%|‚ñà‚ñà‚ñä       | 277/999 [02:31&lt;07:40,  1.57it/s] 28%|‚ñà‚ñà‚ñä       | 278/999 [02:32&lt;07:41,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 279/999 [02:32&lt;07:41,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 280/999 [02:33&lt;07:54,  1.52it/s] 28%|‚ñà‚ñà‚ñä       | 281/999 [02:34&lt;07:56,  1.51it/s] 28%|‚ñà‚ñà‚ñä       | 282/999 [02:34&lt;08:04,  1.48it/s] 28%|‚ñà‚ñà‚ñä       | 283/999 [02:35&lt;08:11,  1.46it/s] 28%|‚ñà‚ñà‚ñä       | 284/999 [02:36&lt;08:23,  1.42it/s] 29%|‚ñà‚ñà‚ñä       | 285/999 [02:37&lt;08:33,  1.39it/s] 29%|‚ñà‚ñà‚ñä       | 286/999 [02:37&lt;08:23,  1.42it/s] 29%|‚ñà‚ñà‚ñä       | 287/999 [02:38&lt;08:30,  1.39it/s] 29%|‚ñà‚ñà‚ñâ       | 288/999 [02:39&lt;08:16,  1.43it/s] 29%|‚ñà‚ñà‚ñâ       | 289/999 [02:39&lt;08:29,  1.39it/s] 29%|‚ñà‚ñà‚ñâ       | 290/999 [02:40&lt;08:27,  1.40it/s] 29%|‚ñà‚ñà‚ñâ       | 291/999 [02:41&lt;08:21,  1.41it/s] 29%|‚ñà‚ñà‚ñâ       | 292/999 [02:42&lt;08:17,  1.42it/s] 29%|‚ñà‚ñà‚ñâ       | 293/999 [02:42&lt;08:11,  1.44it/s] 29%|‚ñà‚ñà‚ñâ       | 294/999 [02:43&lt;08:09,  1.44it/s] 30%|‚ñà‚ñà‚ñâ       | 295/999 [02:44&lt;08:12,  1.43it/s] 30%|‚ñà‚ñà‚ñâ       | 296/999 [02:44&lt;08:21,  1.40it/s] 30%|‚ñà‚ñà‚ñâ       | 297/999 [02:45&lt;08:23,  1.39it/s] 30%|‚ñà‚ñà‚ñâ       | 298/999 [02:46&lt;08:34,  1.36it/s] 30%|‚ñà‚ñà‚ñâ       | 299/999 [02:47&lt;08:17,  1.41it/s] 30%|‚ñà‚ñà‚ñà       | 300/999 [02:47&lt;08:17,  1.41it/s] 30%|‚ñà‚ñà‚ñà       | 301/999 [02:48&lt;08:22,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 302/999 [02:49&lt;08:19,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 303/999 [02:49&lt;08:19,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 304/999 [02:50&lt;08:16,  1.40it/s] 31%|‚ñà‚ñà‚ñà       | 305/999 [02:51&lt;08:14,  1.40it/s] 31%|‚ñà‚ñà‚ñà       | 306/999 [02:52&lt;08:04,  1.43it/s] 31%|‚ñà‚ñà‚ñà       | 307/999 [02:52&lt;07:59,  1.44it/s] 31%|‚ñà‚ñà‚ñà       | 308/999 [02:53&lt;07:49,  1.47it/s] 31%|‚ñà‚ñà‚ñà       | 309/999 [02:54&lt;07:55,  1.45it/s] 31%|‚ñà‚ñà‚ñà       | 310/999 [02:54&lt;08:00,  1.43it/s] 31%|‚ñà‚ñà‚ñà       | 311/999 [02:55&lt;08:03,  1.42it/s] 31%|‚ñà‚ñà‚ñà       | 312/999 [02:56&lt;08:11,  1.40it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 313/999 [02:56&lt;08:11,  1.40it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 314/999 [02:57&lt;08:04,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 315/999 [02:58&lt;08:06,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 316/999 [02:59&lt;08:05,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 317/999 [02:59&lt;08:03,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 318/999 [03:00&lt;07:56,  1.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 319/999 [03:01&lt;08:07,  1.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 320/999 [03:01&lt;08:11,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 321/999 [03:02&lt;08:24,  1.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 322/999 [03:03&lt;08:10,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 323/999 [03:04&lt;08:09,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 324/999 [03:04&lt;08:10,  1.38it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 325/999 [03:05&lt;07:55,  1.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 326/999 [03:06&lt;07:52,  1.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 327/999 [03:06&lt;08:04,  1.39it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 328/999 [03:07&lt;07:56,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 329/999 [03:08&lt;07:56,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 330/999 [03:09&lt;07:52,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 331/999 [03:09&lt;08:01,  1.39it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 332/999 [03:10&lt;07:52,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 333/999 [03:11&lt;07:51,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 334/999 [03:11&lt;07:47,  1.42it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 335/999 [03:12&lt;07:43,  1.43it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 336/999 [03:13&lt;07:50,  1.41it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 337/999 [03:14&lt;07:47,  1.41it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 338/999 [03:14&lt;07:55,  1.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 339/999 [03:15&lt;07:43,  1.42it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 340/999 [03:16&lt;07:35,  1.45it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 341/999 [03:16&lt;07:41,  1.43it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 342/999 [03:17&lt;07:35,  1.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 343/999 [03:18&lt;07:34,  1.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 344/999 [03:18&lt;07:38,  1.43it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 345/999 [03:19&lt;07:31,  1.45it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 346/999 [03:20&lt;07:34,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 347/999 [03:21&lt;07:36,  1.43it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 348/999 [03:21&lt;07:40,  1.41it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 349/999 [03:22&lt;07:31,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 350/999 [03:23&lt;07:31,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 351/999 [03:23&lt;07:37,  1.42it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 352/999 [03:24&lt;07:38,  1.41it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 353/999 [03:25&lt;07:42,  1.40it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 354/999 [03:25&lt;07:40,  1.40it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 355/999 [03:26&lt;07:32,  1.42it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 356/999 [03:27&lt;07:26,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 357/999 [03:28&lt;07:23,  1.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 358/999 [03:28&lt;07:19,  1.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 359/999 [03:29&lt;07:27,  1.43it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 360/999 [03:30&lt;07:22,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 361/999 [03:30&lt;07:36,  1.40it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 362/999 [03:31&lt;07:30,  1.42it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 363/999 [03:32&lt;07:20,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 364/999 [03:32&lt;07:15,  1.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 365/999 [03:33&lt;07:17,  1.45it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 366/999 [03:34&lt;07:04,  1.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 367/999 [03:34&lt;07:07,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 368/999 [03:35&lt;07:07,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 369/999 [03:36&lt;07:08,  1.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 370/999 [03:36&lt;07:06,  1.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 371/999 [03:37&lt;07:04,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 372/999 [03:38&lt;06:59,  1.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 373/999 [03:38&lt;06:56,  1.50it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 374/999 [03:39&lt;06:55,  1.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 375/999 [03:40&lt;06:49,  1.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 376/999 [03:40&lt;06:48,  1.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 377/999 [03:41&lt;06:48,  1.52it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 378/999 [03:42&lt;06:52,  1.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 379/999 [03:42&lt;06:50,  1.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 380/999 [03:43&lt;06:59,  1.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 381/999 [03:44&lt;07:05,  1.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 382/999 [03:44&lt;06:53,  1.49it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 383/999 [03:45&lt;06:50,  1.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 384/999 [03:46&lt;07:00,  1.46it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 385/999 [03:46&lt;07:01,  1.46it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 386/999 [03:47&lt;07:01,  1.45it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 387/999 [03:48&lt;06:45,  1.51it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 388/999 [03:48&lt;06:52,  1.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 389/999 [03:49&lt;06:51,  1.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 390/999 [03:50&lt;06:50,  1.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 391/999 [03:51&lt;06:52,  1.47it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 392/999 [03:51&lt;06:58,  1.45it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 393/999 [03:52&lt;07:01,  1.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 394/999 [03:53&lt;06:57,  1.45it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 395/999 [03:53&lt;06:57,  1.45it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 396/999 [03:54&lt;07:02,  1.43it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 397/999 [03:55&lt;07:04,  1.42it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 398/999 [03:56&lt;07:12,  1.39it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 399/999 [03:56&lt;07:19,  1.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 400/999 [03:57&lt;07:20,  1.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 401/999 [03:58&lt;07:17,  1.37it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 402/999 [03:58&lt;07:11,  1.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 403/999 [03:59&lt;07:43,  1.29it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 404/999 [04:00&lt;07:26,  1.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 405/999 [04:01&lt;07:14,  1.37it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 406/999 [04:01&lt;07:01,  1.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 407/999 [04:02&lt;06:55,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 408/999 [04:03&lt;06:52,  1.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 409/999 [04:03&lt;06:56,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 410/999 [04:04&lt;06:53,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 411/999 [04:05&lt;07:00,  1.40it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 412/999 [04:06&lt;06:48,  1.44it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 413/999 [04:06&lt;06:42,  1.46it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 414/999 [04:07&lt;06:42,  1.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 415/999 [04:08&lt;06:44,  1.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/999 [04:08&lt;06:53,  1.41it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/999 [04:09&lt;06:59,  1.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 418/999 [04:10&lt;06:55,  1.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 419/999 [04:11&lt;07:03,  1.37it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/999 [04:11&lt;07:06,  1.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 421/999 [04:12&lt;07:06,  1.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 422/999 [04:13&lt;07:09,  1.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/999 [04:14&lt;07:09,  1.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/999 [04:14&lt;07:06,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/999 [04:15&lt;07:09,  1.34it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 426/999 [04:16&lt;07:03,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/999 [04:17&lt;07:12,  1.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 428/999 [04:17&lt;07:04,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/999 [04:18&lt;07:16,  1.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 430/999 [04:19&lt;07:02,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 431/999 [04:19&lt;06:45,  1.40it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/999 [04:20&lt;06:35,  1.43it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433/999 [04:21&lt;06:32,  1.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 434/999 [04:22&lt;06:41,  1.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 435/999 [04:22&lt;06:37,  1.42it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 436/999 [04:23&lt;06:44,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/999 [04:24&lt;06:49,  1.37it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/999 [04:24&lt;06:43,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 439/999 [04:25&lt;06:40,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/999 [04:26&lt;06:45,  1.38it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 441/999 [04:27&lt;06:39,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 442/999 [04:27&lt;06:38,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 443/999 [04:28&lt;06:38,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/999 [04:29&lt;06:23,  1.45it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445/999 [04:29&lt;06:29,  1.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/999 [04:30&lt;06:32,  1.41it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 447/999 [04:31&lt;06:33,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/999 [04:32&lt;06:23,  1.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/999 [04:32&lt;06:25,  1.43it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 450/999 [04:33&lt;06:32,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/999 [04:34&lt;06:32,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/999 [04:34&lt;06:43,  1.36it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/999 [04:35&lt;06:46,  1.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/999 [04:36&lt;06:47,  1.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/999 [04:37&lt;06:40,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 456/999 [04:37&lt;06:35,  1.37it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 457/999 [04:38&lt;06:31,  1.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/999 [04:39&lt;06:36,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 459/999 [04:40&lt;06:41,  1.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/999 [04:40&lt;06:39,  1.35it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/999 [04:41&lt;06:49,  1.31it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 462/999 [04:42&lt;06:35,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 463/999 [04:43&lt;06:32,  1.37it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/999 [04:43&lt;06:36,  1.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/999 [04:44&lt;06:37,  1.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/999 [04:45&lt;06:40,  1.33it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467/999 [04:46&lt;06:27,  1.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/999 [04:46&lt;06:21,  1.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/999 [04:47&lt;06:15,  1.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/999 [04:48&lt;06:20,  1.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/999 [04:48&lt;06:26,  1.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/999 [04:49&lt;06:31,  1.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/999 [04:50&lt;06:36,  1.33it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 474/999 [04:51&lt;06:21,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475/999 [04:51&lt;06:16,  1.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/999 [04:52&lt;06:09,  1.41it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/999 [04:53&lt;06:06,  1.42it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/999 [04:53&lt;06:12,  1.40it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 479/999 [04:54&lt;06:15,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/999 [04:55&lt;06:16,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481/999 [04:56&lt;06:26,  1.34it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/999 [04:56&lt;06:23,  1.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 483/999 [04:57&lt;06:24,  1.34it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 484/999 [04:58&lt;06:09,  1.39it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 485/999 [04:59&lt;06:07,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/999 [04:59&lt;05:59,  1.43it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/999 [05:00&lt;06:05,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/999 [05:01&lt;06:04,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 489/999 [05:01&lt;06:01,  1.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/999 [05:02&lt;05:51,  1.45it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491/999 [05:03&lt;05:44,  1.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 492/999 [05:03&lt;05:40,  1.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/999 [05:04&lt;05:46,  1.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 494/999 [05:05&lt;05:52,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 495/999 [05:05&lt;05:51,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/999 [05:06&lt;05:56,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/999 [05:07&lt;06:00,  1.39it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 498/999 [05:08&lt;06:16,  1.33it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/999 [05:08&lt;05:53,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/999 [05:09&lt;05:48,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/999 [05:10&lt;05:37,  1.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 502/999 [05:10&lt;05:51,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/999 [05:11&lt;05:42,  1.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/999 [05:12&lt;05:31,  1.49it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/999 [05:12&lt;05:29,  1.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 506/999 [05:13&lt;05:35,  1.47it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/999 [05:14&lt;05:38,  1.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/999 [05:14&lt;05:25,  1.51it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 509/999 [05:15&lt;05:22,  1.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/999 [05:16&lt;05:30,  1.48it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 511/999 [05:16&lt;05:20,  1.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 512/999 [05:17&lt;05:14,  1.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 513/999 [05:18&lt;05:11,  1.56it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 514/999 [05:18&lt;05:12,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 515/999 [05:19&lt;05:17,  1.53it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/999 [05:20&lt;05:13,  1.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/999 [05:20&lt;05:13,  1.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 518/999 [05:21&lt;05:13,  1.53it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 519/999 [05:22&lt;05:10,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/999 [05:22&lt;05:05,  1.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/999 [05:23&lt;05:07,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 522/999 [05:23&lt;05:06,  1.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 523/999 [05:24&lt;05:00,  1.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/999 [05:25&lt;05:06,  1.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/999 [05:26&lt;05:21,  1.47it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 526/999 [05:26&lt;05:22,  1.46it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 527/999 [05:27&lt;05:15,  1.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/999 [05:28&lt;05:16,  1.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/999 [05:28&lt;05:13,  1.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 530/999 [05:29&lt;05:24,  1.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/999 [05:30&lt;05:24,  1.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532/999 [05:30&lt;05:26,  1.43it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/999 [05:31&lt;05:31,  1.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 534/999 [05:32&lt;05:26,  1.43it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 535/999 [05:32&lt;05:19,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 536/999 [05:33&lt;05:18,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 537/999 [05:34&lt;05:17,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/999 [05:34&lt;05:11,  1.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 539/999 [05:35&lt;05:06,  1.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/999 [05:36&lt;05:01,  1.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/999 [05:36&lt;05:00,  1.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 542/999 [05:37&lt;04:57,  1.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 543/999 [05:38&lt;05:07,  1.49it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/999 [05:38&lt;05:09,  1.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/999 [05:39&lt;05:12,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 546/999 [05:40&lt;05:10,  1.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/999 [05:41&lt;05:12,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/999 [05:41&lt;05:08,  1.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/999 [05:42&lt;05:02,  1.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 550/999 [05:43&lt;05:01,  1.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/999 [05:43&lt;05:08,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/999 [05:44&lt;05:02,  1.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 553/999 [05:45&lt;05:02,  1.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 554/999 [05:45&lt;05:05,  1.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/999 [05:46&lt;05:07,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/999 [05:47&lt;05:06,  1.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/999 [05:47&lt;05:08,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 558/999 [05:48&lt;05:07,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/999 [05:49&lt;05:09,  1.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/999 [05:49&lt;05:07,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/999 [05:50&lt;05:03,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 562/999 [05:51&lt;05:02,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563/999 [05:52&lt;05:11,  1.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/999 [05:52&lt;05:05,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 565/999 [05:53&lt;05:06,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/999 [05:54&lt;05:00,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/999 [05:54&lt;04:57,  1.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/999 [05:55&lt;04:59,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/999 [05:56&lt;05:04,  1.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 570/999 [05:57&lt;05:03,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 571/999 [05:57&lt;05:02,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 572/999 [05:58&lt;04:56,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 573/999 [05:59&lt;04:53,  1.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/999 [05:59&lt;04:57,  1.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 575/999 [06:00&lt;04:49,  1.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/999 [06:01&lt;04:43,  1.49it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 577/999 [06:01&lt;04:46,  1.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/999 [06:02&lt;04:43,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579/999 [06:03&lt;04:39,  1.50it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/999 [06:03&lt;04:36,  1.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 581/999 [06:04&lt;04:45,  1.46it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/999 [06:05&lt;04:41,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583/999 [06:05&lt;04:41,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 584/999 [06:06&lt;04:44,  1.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/999 [06:07&lt;04:45,  1.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/999 [06:07&lt;04:51,  1.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 587/999 [06:08&lt;04:53,  1.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/999 [06:09&lt;04:54,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/999 [06:10&lt;04:49,  1.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/999 [06:10&lt;04:51,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/999 [06:11&lt;04:55,  1.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/999 [06:12&lt;04:51,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 593/999 [06:12&lt;04:49,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/999 [06:13&lt;05:00,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/999 [06:14&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 596/999 [06:15&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 597/999 [06:16&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/999 [06:16&lt;04:57,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/999 [06:17&lt;04:59,  1.34it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/999 [06:18&lt;04:55,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/999 [06:18&lt;04:50,  1.37it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/999 [06:19&lt;04:46,  1.39it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 603/999 [06:20&lt;04:40,  1.41it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/999 [06:21&lt;04:50,  1.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/999 [06:21&lt;04:47,  1.37it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/999 [06:22&lt;04:52,  1.34it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/999 [06:23&lt;04:44,  1.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/999 [06:23&lt;04:36,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/999 [06:24&lt;04:38,  1.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/999 [06:25&lt;04:36,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/999 [06:26&lt;04:35,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 612/999 [06:26&lt;04:37,  1.39it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 613/999 [06:27&lt;04:31,  1.42it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/999 [06:28&lt;04:26,  1.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 615/999 [06:28&lt;04:28,  1.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 616/999 [06:29&lt;04:37,  1.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 617/999 [06:30&lt;04:31,  1.40it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 618/999 [06:31&lt;04:27,  1.42it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/999 [06:31&lt;04:21,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/999 [06:32&lt;04:20,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 621/999 [06:33&lt;04:18,  1.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 622/999 [06:33&lt;04:19,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 623/999 [06:34&lt;04:23,  1.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/999 [06:35&lt;04:14,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/999 [06:35&lt;04:14,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/999 [06:36&lt;04:12,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/999 [06:37&lt;04:05,  1.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/999 [06:37&lt;04:09,  1.49it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 629/999 [06:38&lt;04:09,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/999 [06:39&lt;04:11,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/999 [06:39&lt;04:08,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/999 [06:40&lt;04:08,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/999 [06:41&lt;04:04,  1.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 634/999 [06:41&lt;04:06,  1.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/999 [06:42&lt;04:04,  1.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/999 [06:43&lt;04:03,  1.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 637/999 [06:43&lt;03:59,  1.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 638/999 [06:44&lt;04:04,  1.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 639/999 [06:45&lt;04:04,  1.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/999 [06:45&lt;04:05,  1.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 641/999 [06:46&lt;04:05,  1.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 642/999 [06:47&lt;04:06,  1.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/999 [06:48&lt;04:05,  1.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 644/999 [06:48&lt;04:04,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/999 [06:49&lt;03:57,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 646/999 [06:50&lt;04:02,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 647/999 [06:50&lt;04:03,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 648/999 [06:51&lt;03:55,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 649/999 [06:52&lt;03:51,  1.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/999 [06:52&lt;03:46,  1.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/999 [06:53&lt;03:45,  1.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 652/999 [06:53&lt;03:51,  1.50it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 653/999 [06:54&lt;03:52,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/999 [06:55&lt;03:43,  1.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/999 [06:55&lt;03:38,  1.58it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/999 [06:56&lt;03:47,  1.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 657/999 [06:57&lt;03:49,  1.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 658/999 [06:57&lt;03:42,  1.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 659/999 [06:58&lt;03:42,  1.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/999 [06:59&lt;03:44,  1.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 661/999 [06:59&lt;03:42,  1.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 662/999 [07:00&lt;03:45,  1.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/999 [07:01&lt;03:48,  1.47it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/999 [07:01&lt;03:49,  1.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 665/999 [07:02&lt;03:45,  1.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/999 [07:03&lt;03:41,  1.50it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/999 [07:03&lt;03:36,  1.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/999 [07:04&lt;03:39,  1.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 669/999 [07:05&lt;03:43,  1.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/999 [07:05&lt;03:43,  1.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 671/999 [07:06&lt;03:36,  1.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/999 [07:07&lt;03:31,  1.54it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 673/999 [07:07&lt;03:32,  1.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/999 [07:08&lt;03:27,  1.57it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/999 [07:09&lt;03:29,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/999 [07:09&lt;03:28,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 677/999 [07:10&lt;03:27,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 678/999 [07:11&lt;03:28,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/999 [07:11&lt;03:27,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/999 [07:12&lt;03:27,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 681/999 [07:13&lt;03:24,  1.56it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/999 [07:13&lt;03:19,  1.59it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 683/999 [07:14&lt;03:24,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 684/999 [07:14&lt;03:27,  1.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/999 [07:15&lt;03:28,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/999 [07:16&lt;03:28,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 687/999 [07:17&lt;03:32,  1.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/999 [07:17&lt;03:32,  1.46it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 689/999 [07:18&lt;03:31,  1.46it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/999 [07:19&lt;03:28,  1.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691/999 [07:19&lt;03:26,  1.49it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 692/999 [07:20&lt;03:23,  1.51it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 693/999 [07:21&lt;03:24,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/999 [07:21&lt;03:31,  1.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 695/999 [07:22&lt;03:32,  1.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/999 [07:23&lt;03:38,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/999 [07:24&lt;03:38,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 698/999 [07:24&lt;03:38,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/999 [07:25&lt;03:37,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/999 [07:26&lt;03:34,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/999 [07:26&lt;03:33,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702/999 [07:27&lt;03:41,  1.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/999 [07:28&lt;03:39,  1.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/999 [07:29&lt;03:34,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 705/999 [07:29&lt;03:42,  1.32it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/999 [07:30&lt;03:38,  1.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/999 [07:31&lt;03:32,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 708/999 [07:32&lt;03:32,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/999 [07:32&lt;03:30,  1.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/999 [07:33&lt;03:34,  1.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 711/999 [07:34&lt;03:32,  1.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 712/999 [07:35&lt;03:31,  1.36it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 713/999 [07:35&lt;03:25,  1.39it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 714/999 [07:36&lt;03:29,  1.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 715/999 [07:37&lt;03:28,  1.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/999 [07:37&lt;03:27,  1.37it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/999 [07:38&lt;03:33,  1.32it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 718/999 [07:39&lt;03:31,  1.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/999 [07:40&lt;03:33,  1.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/999 [07:41&lt;03:38,  1.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/999 [07:41&lt;03:41,  1.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/999 [07:42&lt;03:38,  1.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 723/999 [07:43&lt;03:34,  1.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 724/999 [07:44&lt;03:31,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/999 [07:45&lt;03:32,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/999 [07:45&lt;03:30,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 727/999 [07:46&lt;03:27,  1.31it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/999 [07:47&lt;03:28,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 729/999 [07:48&lt;03:26,  1.31it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 730/999 [07:48&lt;03:32,  1.26it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/999 [07:49&lt;03:32,  1.26it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/999 [07:50&lt;03:27,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/999 [07:51&lt;03:24,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 734/999 [07:51&lt;03:18,  1.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 735/999 [07:52&lt;03:21,  1.31it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/999 [07:53&lt;03:28,  1.26it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 737/999 [07:54&lt;03:25,  1.27it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 738/999 [07:55&lt;03:38,  1.19it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739/999 [07:56&lt;03:44,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/999 [07:57&lt;03:45,  1.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 741/999 [07:57&lt;03:42,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 742/999 [07:58&lt;03:41,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 743/999 [07:59&lt;03:37,  1.18it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/999 [08:00&lt;03:40,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/999 [08:01&lt;03:37,  1.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/999 [08:02&lt;03:36,  1.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 747/999 [08:03&lt;03:33,  1.18it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/999 [08:03&lt;03:37,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 749/999 [08:04&lt;03:36,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750/999 [08:05&lt;03:38,  1.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/999 [08:06&lt;03:38,  1.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/999 [08:07&lt;03:33,  1.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 753/999 [08:08&lt;03:31,  1.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/999 [08:09&lt;03:31,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/999 [08:10&lt;03:34,  1.14it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 756/999 [08:10&lt;03:31,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/999 [08:11&lt;03:31,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 758/999 [08:12&lt;03:28,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 759/999 [08:13&lt;03:27,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 760/999 [08:14&lt;03:26,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/999 [08:15&lt;03:28,  1.14it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 762/999 [08:16&lt;03:25,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 763/999 [08:17&lt;03:29,  1.12it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/999 [08:17&lt;03:27,  1.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/999 [08:18&lt;03:21,  1.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/999 [08:19&lt;03:20,  1.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/999 [08:20&lt;03:16,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/999 [08:21&lt;03:15,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/999 [08:22&lt;03:20,  1.15it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 770/999 [08:23&lt;03:14,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/999 [08:23&lt;03:10,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 772/999 [08:24&lt;03:09,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 773/999 [08:25&lt;03:07,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/999 [08:26&lt;03:05,  1.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/999 [08:27&lt;03:05,  1.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 776/999 [08:27&lt;03:06,  1.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 777/999 [08:28&lt;03:08,  1.17it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/999 [08:29&lt;03:11,  1.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/999 [08:30&lt;03:07,  1.17it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 780/999 [08:31&lt;03:05,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/999 [08:32&lt;03:04,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 782/999 [08:33&lt;03:03,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/999 [08:33&lt;03:03,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/999 [08:34&lt;03:00,  1.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 785/999 [08:35&lt;03:08,  1.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 786/999 [08:36&lt;03:09,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 787/999 [08:37&lt;03:08,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/999 [08:38&lt;03:07,  1.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/999 [08:39&lt;03:09,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 790/999 [08:40&lt;03:07,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 791/999 [08:41&lt;03:04,  1.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 792/999 [08:42&lt;03:04,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 793/999 [08:42&lt;03:05,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 794/999 [08:43&lt;03:05,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/999 [08:44&lt;03:02,  1.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/999 [08:45&lt;02:59,  1.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 797/999 [08:46&lt;02:59,  1.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 798/999 [08:47&lt;03:00,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/999 [08:48&lt;03:02,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/999 [08:49&lt;03:01,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/999 [08:50&lt;02:58,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/999 [08:51&lt;02:59,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/999 [08:52&lt;02:59,  1.09it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/999 [08:52&lt;02:59,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/999 [08:53&lt;02:57,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 806/999 [08:54&lt;02:57,  1.08it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 807/999 [08:55&lt;02:56,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 808/999 [08:56&lt;02:53,  1.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 809/999 [08:57&lt;02:53,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/999 [08:58&lt;02:50,  1.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 811/999 [08:59&lt;02:47,  1.12it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 812/999 [09:00&lt;02:45,  1.13it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 813/999 [09:01&lt;02:47,  1.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/999 [09:02&lt;02:52,  1.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/999 [09:03&lt;02:52,  1.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/999 [09:03&lt;02:49,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/999 [09:04&lt;02:48,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/999 [09:05&lt;02:47,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 819/999 [09:06&lt;02:55,  1.03it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/999 [09:07&lt;02:55,  1.02it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 821/999 [09:08&lt;02:55,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/999 [09:09&lt;02:54,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/999 [09:10&lt;02:54,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 824/999 [09:11&lt;02:54,  1.00it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/999 [09:12&lt;02:56,  1.01s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 826/999 [09:13&lt;02:50,  1.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/999 [09:14&lt;02:43,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/999 [09:15&lt;02:42,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 829/999 [09:16&lt;02:42,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 830/999 [09:17&lt;02:38,  1.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/999 [09:18&lt;02:33,  1.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/999 [09:19&lt;02:33,  1.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 833/999 [09:20&lt;02:25,  1.14it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/999 [09:20&lt;02:26,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/999 [09:21&lt;02:27,  1.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/999 [09:22&lt;02:25,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 837/999 [09:23&lt;02:24,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 838/999 [09:24&lt;02:25,  1.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 839/999 [09:25&lt;02:23,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/999 [09:26&lt;02:20,  1.13it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/999 [09:27&lt;02:16,  1.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/999 [09:28&lt;02:17,  1.14it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/999 [09:28&lt;02:15,  1.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 844/999 [09:29&lt;02:13,  1.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 845/999 [09:30&lt;02:11,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/999 [09:31&lt;02:10,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 847/999 [09:32&lt;02:09,  1.18it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/999 [09:33&lt;02:08,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/999 [09:34&lt;02:10,  1.15it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 850/999 [09:34&lt;02:08,  1.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/999 [09:35&lt;02:09,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 852/999 [09:36&lt;02:08,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 853/999 [09:37&lt;02:08,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 854/999 [09:38&lt;02:04,  1.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/999 [09:39&lt;02:05,  1.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 856/999 [09:40&lt;02:02,  1.17it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 857/999 [09:40&lt;01:58,  1.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 858/999 [09:41&lt;01:58,  1.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 859/999 [09:42&lt;01:59,  1.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/999 [09:43&lt;01:58,  1.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 861/999 [09:44&lt;02:01,  1.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 862/999 [09:45&lt;01:57,  1.17it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/999 [09:46&lt;01:59,  1.13it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 864/999 [09:47&lt;01:58,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/999 [09:47&lt;01:58,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 866/999 [09:48&lt;01:56,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/999 [09:49&lt;01:57,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 868/999 [09:50&lt;01:54,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 869/999 [09:51&lt;01:51,  1.17it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/999 [09:52&lt;01:49,  1.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 871/999 [09:53&lt;01:50,  1.16it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/999 [09:54&lt;01:52,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 873/999 [09:54&lt;01:51,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/999 [09:55&lt;01:51,  1.12it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/999 [09:56&lt;01:52,  1.11it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 876/999 [09:57&lt;01:51,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877/999 [09:58&lt;01:52,  1.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/999 [09:59&lt;01:49,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 879/999 [10:00&lt;01:49,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 880/999 [10:01&lt;01:49,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/999 [10:02&lt;01:47,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 882/999 [10:03&lt;01:46,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/999 [10:04&lt;01:46,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 884/999 [10:04&lt;01:43,  1.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 885/999 [10:05&lt;01:39,  1.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/999 [10:06&lt;01:39,  1.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 887/999 [10:07&lt;01:38,  1.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/999 [10:08&lt;01:41,  1.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 889/999 [10:09&lt;01:40,  1.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 890/999 [10:10&lt;01:37,  1.12it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/999 [10:11&lt;01:34,  1.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 892/999 [10:11&lt;01:31,  1.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 893/999 [10:12&lt;01:30,  1.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/999 [10:13&lt;01:30,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/999 [10:14&lt;01:29,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/999 [10:15&lt;01:28,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 897/999 [10:16&lt;01:27,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/999 [10:17&lt;01:26,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/999 [10:17&lt;01:25,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/999 [10:18&lt;01:24,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/999 [10:19&lt;01:22,  1.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/999 [10:20&lt;01:22,  1.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 903/999 [10:21&lt;01:19,  1.21it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 904/999 [10:22&lt;01:19,  1.19it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/999 [10:23&lt;01:20,  1.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/999 [10:23&lt;01:19,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 907/999 [10:24&lt;01:18,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 908/999 [10:25&lt;01:16,  1.18it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/999 [10:26&lt;01:16,  1.18it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 910/999 [10:27&lt;01:16,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/999 [10:28&lt;01:15,  1.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 912/999 [10:29&lt;01:15,  1.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/999 [10:29&lt;01:14,  1.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 914/999 [10:30&lt;01:13,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 915/999 [10:31&lt;01:13,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/999 [10:32&lt;01:12,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/999 [10:33&lt;01:12,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 918/999 [10:34&lt;01:10,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/999 [10:35&lt;01:09,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 920/999 [10:35&lt;01:06,  1.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 921/999 [10:36&lt;01:05,  1.19it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/999 [10:37&lt;01:03,  1.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/999 [10:38&lt;01:03,  1.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 924/999 [10:39&lt;01:02,  1.20it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 925/999 [10:40&lt;01:02,  1.18it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/999 [10:40&lt;01:01,  1.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/999 [10:41&lt;01:01,  1.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/999 [10:42&lt;01:00,  1.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 929/999 [10:43&lt;01:00,  1.16it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 930/999 [10:44&lt;00:59,  1.15it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 931/999 [10:45&lt;00:59,  1.15it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/999 [10:46&lt;00:58,  1.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 933/999 [10:47&lt;00:58,  1.13it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 934/999 [10:47&lt;00:56,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 935/999 [10:48&lt;00:54,  1.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/999 [10:49&lt;00:54,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 937/999 [10:50&lt;00:55,  1.11it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/999 [10:51&lt;00:53,  1.14it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/999 [10:52&lt;00:53,  1.13it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 940/999 [10:53&lt;00:50,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/999 [10:54&lt;00:50,  1.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/999 [10:54&lt;00:49,  1.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 943/999 [10:55&lt;00:48,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/999 [10:56&lt;00:47,  1.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 945/999 [10:57&lt;00:45,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/999 [10:58&lt;00:45,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/999 [10:59&lt;00:43,  1.20it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 948/999 [10:59&lt;00:41,  1.22it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 949/999 [11:00&lt;00:41,  1.21it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/999 [11:01&lt;00:41,  1.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/999 [11:02&lt;00:41,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 952/999 [11:03&lt;00:40,  1.15it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/999 [11:04&lt;00:40,  1.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/999 [11:05&lt;00:40,  1.12it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 955/999 [11:06&lt;00:39,  1.11it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 956/999 [11:07&lt;00:39,  1.10it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/999 [11:08&lt;00:38,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/999 [11:08&lt;00:37,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 959/999 [11:09&lt;00:37,  1.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 960/999 [11:10&lt;00:35,  1.09it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/999 [11:11&lt;00:35,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 962/999 [11:12&lt;00:33,  1.09it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/999 [11:13&lt;00:33,  1.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964/999 [11:14&lt;00:33,  1.04it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/999 [11:15&lt;00:32,  1.05it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 966/999 [11:16&lt;00:31,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 967/999 [11:17&lt;00:30,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 968/999 [11:18&lt;00:28,  1.07it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/999 [11:19&lt;00:28,  1.05it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 970/999 [11:20&lt;00:27,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 971/999 [11:21&lt;00:26,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/999 [11:22&lt;00:24,  1.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/999 [11:23&lt;00:24,  1.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 974/999 [11:24&lt;00:23,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/999 [11:24&lt;00:22,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 976/999 [11:25&lt;00:21,  1.09it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/999 [11:26&lt;00:20,  1.06it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 978/999 [11:27&lt;00:19,  1.06it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/999 [11:28&lt;00:19,  1.04it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 980/999 [11:29&lt;00:18,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/999 [11:30&lt;00:17,  1.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 982/999 [11:31&lt;00:16,  1.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 983/999 [11:32&lt;00:15,  1.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 984/999 [11:33&lt;00:14,  1.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/999 [11:34&lt;00:13,  1.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/999 [11:35&lt;00:12,  1.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 987/999 [11:36&lt;00:11,  1.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 988/999 [11:37&lt;00:10,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 989/999 [11:38&lt;00:09,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 990/999 [11:39&lt;00:08,  1.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/999 [11:40&lt;00:07,  1.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 992/999 [11:41&lt;00:06,  1.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/999 [11:42&lt;00:05,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/999 [11:43&lt;00:04,  1.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 995/999 [11:44&lt;00:03,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/999 [11:45&lt;00:02,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 997/999 [11:46&lt;00:01,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/999 [11:47&lt;00:00,  1.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [11:47&lt;00:00,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [11:47&lt;00:00,  1.41it/s]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', label='Premi√®re bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles th√©oriques')\nplt.xlabel('Quantiles th√©oriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# # kstest y revenir\n# ks_stat, ks_p_value = kstest(data_train, skew_student_pdf, args=(**params_sstd,))\n\n# print(\"=\"*80)\n# print(\"H0 : Les donn√©es suivent une loi de Skew Student\")\n# print(f\"Statistique de test : {ks_stat:.4f}\")\n# print(f\"P-value : {ks_p_value:.4f}\")\n# print(\"=\"*80)\n# A revoir\n\n\n\nII.4.2. Calcul de la VaR Skew Student\n\n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var_fct(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\nsstd_var = sstd_var_fct(alpha, params_sstd)\nprint(f\"La VaR skew student pour h=1j et alpha={alpha} est : {sstd_var:.4%}\")\n\nLa VaR skew student pour h=1j et alpha=0.99 est : 4.2576%"
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html",
    "href": "3A/value-at-risk/pj_copules.html",
    "title": "Projet de gestion de risques multiples",
    "section": "",
    "text": "Les banques jouent un r√¥le central dans l‚Äô√©conomie nationale et internationale. En effet, elles assurent l‚Äôinterm√©diation entre les agents disposant d‚Äôun exc√©dent de financement et ceux ayant un besoin de financement, facilitant ainsi les transactions et soutenant l‚Äôinvestissement, et donc la croissance √©conomique. Toutefois, cette activit√© les expose √† divers risques majeurs, notamment le risque de cr√©dit, le risque de liquidit√© et le risque de march√©. La quantification de ces risques est essentielle pour permettre aux institutions bancaires de s‚Äôen pr√©munir et de les surveiller efficacement. Dans ce cadre, la Value-at-Risk (VaR) s‚Äôimpose comme une mesure de r√©f√©rence. Elle permet d‚Äô√©valuer la perte potentielle maximale qu‚Äôune institution pourrait subir, avec un certain niveau de confiance, sur un horizon temporel donn√© et pour un portefeuille sp√©cifique.\nDans l‚Äôanalyse de la VaR, un portefeuille est souvent compos√© d‚Äôau moins deux cr√©ances. Il est donc indispensable de prendre en compte les d√©pendances entre les facteurs de risque. Une approche classique consiste √† supposer que le vecteur des risques individuels suit une distribution normale multivari√©e et √† utiliser le coefficient de corr√©lation lin√©aire de Pearson comme mesure de d√©pendance. Cependant, cette hypoth√®se est souvent trop restrictive en finance : les distributions des facteurs de risque ne sont pas n√©cessairement gaussiennes et le coefficient de Pearson ne permet pas toujours de capturer les structures de d√©pendance non lin√©aires. De plus, cette mesure de corr√©lation est pertinente uniquement dans un cadre gaussien, qui repr√©sente rarement les dynamiques financi√®res r√©elles.\nDans ce contexte, la th√©orie des copules constitue un outil statistique puissant permettant de mod√©liser la d√©pendance entre les risques sans se limiter √† l‚Äôhypoth√®se de normalit√©. Une copule est une fonction qui caract√©rise la structure de d√©pendance entre plusieurs variables al√©atoires ind√©pendamment de leurs distributions marginales. En s√©parant la mod√©lisation des distributions marginales et celle de la d√©pendance conjointe, les copules offrent une flexibilit√© accrue pour l‚Äôanalyse du risque et permettent de mieux repr√©senter les interactions entre les actifs financiers.\nL‚Äôobjectif de ce projet est d‚Äô√©valuer le risque de cr√©dit en calculant une CreditVaR avec un niveau de confiance de 99% sur un portefeuille compos√© de deux obligations bancaires. Ces obligations, bien que diff√©rentes en termes de subordination et de risque de recouvrement, appartiennent au m√™me secteur, ce qui accro√Æt le risque global du portefeuille. Il est donc essentiel de mod√©liser ad√©quatement la d√©pendance entre ces actifs pour obtenir une estimation r√©aliste du risque de cr√©dit."
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html#ii.3.-comparaison-des-distributions",
    "href": "3A/value-at-risk/pj_copules.html#ii.3.-comparaison-des-distributions",
    "title": "Projet de gestion de risques multiples",
    "section": "II.3. Comparaison des distributions",
    "text": "II.3. Comparaison des distributions\n\nplt.figure(figsize=(10, 5))\n\nplt.plot(x, y_BNP, label='BNP')\nplt.plot(x, y_SG, label='SG')\nplt.title(\"Densit√©s de probabilit√© des taux de recouvrement\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nBNP senior : L‚Äôobligation senior b√©n√©ficie d‚Äôun taux de recouvrement plus √©lev√© et plus stable, donc moins risqu√©.\nSG junior : L‚Äôobligation junior a une probabilit√© √©lev√©e d‚Äôun recouvrement tr√®s faible, ce qui refl√®te un risque plus important.\n\nCela est coh√©rent avec la cat√©gorisation des obligations. Dans la hi√©rarchie des dettes, une obligation peut √™tre class√©e comme senior ou junior (subordonn√©e) en fonction de la priorit√© de remboursement en cas de faillite de l‚Äô√©metteur. Cette distinction est essentielle pour √©valuer le risque de cr√©dit et le taux de recouvrement attendu. Une obligation class√©e senior est moins risqu√©e qu‚Äôune obligation junior, car elle est rembours√©e en premier en cas de d√©faut de l‚Äô√©metteur, cependant le rendement attendu est moins √©lev√©. Par cons√©quent, les obligations senior ont un taux de recouvrement plus √©lev√© et plus stable que les obligations junior."
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html#iii.1.-analyse-exploratoire-univari√©e-des-donn√©es-actions-de-ces-deux-entreprises.",
    "href": "3A/value-at-risk/pj_copules.html#iii.1.-analyse-exploratoire-univari√©e-des-donn√©es-actions-de-ces-deux-entreprises.",
    "title": "Projet de gestion de risques multiples",
    "section": "III.1. Analyse exploratoire univari√©e des donn√©es actions de ces deux entreprises.",
    "text": "III.1. Analyse exploratoire univari√©e des donn√©es actions de ces deux entreprises.\nEn observant le prix des actions BNP et SG, nous constatons que les actions BNP ont un prix plus √©lev√©s que les actions SG. Cela est coh√©rent avec la capitalisation boursi√®re des deux entreprises. De plus, nous constatons que les rendements de BNP sont semblables √† ceux de SG. Cela est coh√©rent avec le fait que les deux entreprises sont des banques fran√ßaises et sont donc expos√©es aux m√™mes risques macro√©conomiques. N√©anmoins, les actions de BNP pr√©sentent une volatilit√© l√©g√®rement plus √©lev√© que celles de SG.\n\n# read data.txt\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_csv('data_copules.txt', sep=\"\\t\")\ndata.head()\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\n0\n42.36\n55.24\n\n\n1\n42.72\n55.59\n\n\n2\n43.20\n56.45\n\n\n3\n42.67\n55.55\n\n\n4\n41.81\n54.50\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data['BNP'], label='BNP')\nplt.plot(data['SG'], label='SG')\nplt.title(\"Prix des actions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nreturns = pd.DataFrame()\nreturns[\"BNP\"] = data[\"BNP\"].pct_change().dropna()\nreturns[\"SG\"] = data[\"SG\"].pct_change().dropna()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Premier sous-graphique : Rendements BNP\naxes[0].plot(returns[\"BNP\"], color='tab:blue')\naxes[0].set_title(\"Rendements BNP\")\naxes[0].grid(True)\n\n# Deuxi√®me sous-graphique : Rendements SG\naxes[1].plot(returns[\"SG\"], color='tab:orange')\naxes[1].set_title(\"Rendements SG\")\naxes[1].grid(True)\n\n# Ajustement automatique pour √©viter les chevauchements\nplt.tight_layout()\n\n# Affichage\nplt.show()\n\n\n\n\n\n\n\n\n\nreturns.describe()\n\n\n\n\n\n\n\n\nBNP\nSG\n\n\n\n\ncount\n999.000000\n999.000000\n\n\nmean\n-0.000590\n-0.000473\n\n\nstd\n0.024340\n0.020690\n\n\nmin\n-0.116199\n-0.093616\n\n\n25%\n-0.013771\n-0.011760\n\n\n50%\n-0.000358\n-0.000547\n\n\n75%\n0.012605\n0.011286\n\n\nmax\n0.086786\n0.079479"
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html#iii.2.-mod√©lisation-des-distributions-univari√©es-des-facteurs-de-risques",
    "href": "3A/value-at-risk/pj_copules.html#iii.2.-mod√©lisation-des-distributions-univari√©es-des-facteurs-de-risques",
    "title": "Projet de gestion de risques multiples",
    "section": "III.2. Mod√©lisation des distributions univari√©es des facteurs de risques",
    "text": "III.2. Mod√©lisation des distributions univari√©es des facteurs de risques\nDans le cadre des prix des actions, le seul facteur de risque est le rendement. Nous allons donc mod√©liser les rendements des actions de BNP et SG par des lois normales, lois student, skew student, et Normal Inverse Gaussian afin de d√©terminer la loi qui s‚Äôajuste le mieux aux donn√©es.\n\nIII.2.1. Mod√©lisation des rendements de BNP\nEn ce qui concerne les rendements de BNP, nous constatons que les lois de student et normal inverse gaussian sont les plus adapt√©es pour mod√©liser les rendements de BNP. En effet, lorsqu‚Äôon compare les QQ-plot des rendements de BNP avec les lois normales, student, skew student et normal inverse gaussian, on constate que les quantiles empiriques des rendements de BNP sont plus proches des quantiles th√©oriques des lois student et normal inverse gaussian. De plus, les densit√©s ajust√©es semblent √©galement mieux coller aux donn√©es.\nSi l‚Äôon devait choisir une loi pour mod√©liser les rendements de BNP, nous choisirions la loi normal inverse gaussian. En effet, bien qu‚Äôelle soit plus complexe √† mod√©liser, elle permet d‚Äôavoir un p-value, au test de Kolmogorov-Smirnov, plus √©lev√© que la loi student. Cela signifie que la loi normal inverse gaussian est plus adapt√©e pour mod√©liser les rendements de BNP.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de donn√©es pour l'exemple (remplace par tes donn√©es r√©elles)\ndata = returns[\"BNP\"]\n\n# Cr√©ation de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densit√© ajust√©e - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des param√®tres de la distribution de Student pour vos donn√©es.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densit√© ajust√©e - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densit√© ajust√©e - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU = pd.DataFrame(index=returns.index, columns=returns.columns)\n\nU['BNP'] = stats.norminvgauss.cdf(data,*params_nig)\n\n\n\n\n\n\n\n\n\nprint(params_norm)\nprint(params_std)\nprint(params_nig)\n\n(np.float64(-0.0005897932128196069), np.float64(0.024327449407557697))\n(np.float64(4.605552436800593), np.float64(-0.00046153919290846087), np.float64(0.018734588738427177))\n(np.float64(1.114374971099747), np.float64(-0.01637147555477723), np.float64(-0.00021011775496697913), np.float64(0.025828797181227353))\n\n\n\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.051878  0.008908\nStudent                   0.027225  0.441752\nNormal Inverse Gaussian   0.026466  0.477953\n==================================================\n\n\n\n\nIII.2.1. Mod√©lisation des rendements de SG\nComme les rendements de BNP, les rendements de SG sont mieux mod√©lis√©s par les lois student et normal inverse gaussian.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Simulation de donn√©es pour l'exemple (remplace par tes donn√©es r√©elles)\ndata = returns[\"SG\"]\n\n# Cr√©ation de la figure et des axes pour 4 subplots (2 lignes, 2 colonnes)\nfig, axs = plt.subplots(3, 2, figsize=(14, 10))\n\n######################## Loi normale ########################\nparams_norm = stats.norm.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi normale (subplot 0,0)\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[0, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[0, 0].plot(xs, stats.norm.pdf(xs, *params_norm), label='Normal Distribution', color='red')\naxs[0, 0].set_title('Densit√© ajust√©e - loi normale')\naxs[0, 0].legend(loc='upper left')\n\n# Q-Q plot (subplot 0,1)\nstats.probplot(data, dist=\"norm\",sparams=(params_norm), plot=axs[0, 1])\naxs[0, 1].set_title('Q-Q Plot - loi normale')\n\n######################## Loi de student ########################\n\n# Estimation des param√®tres de la distribution de Student pour vos donn√©es.\nparams_std= stats.t.fit(data)\n\n# Histogramme avec densit√© th√©orique de la loi de Student.\nxs = np.linspace(np.min(data), np.max(data), 200)\naxs[1, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[1, 0].plot(xs, stats.t.pdf(xs, *params_std), label='Fitted t-Distribution',color='orange')\naxs[1, 0].set_title('Densit√© ajust√©e - loi de student')\naxs[1, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de Student.\nstats.probplot(data, dist=\"t\", sparams=(params_std), plot=axs[1, 1])\naxs[1, 1].set_title('Q-Q Plot - loi de student')\n\n######################## Loi de Normal Inverse Gaussian ########################\nparams_nig = stats.norminvgauss.fit(data)\n\naxs[2, 0].hist(data, bins=30, density=True, alpha=0.5, label=\"Histogram\")\naxs[2, 0].plot(xs, stats.norminvgauss.pdf(xs, *params_nig), label='Fitted normal inverse gaussian',color='green')\naxs[2, 0].set_title('Densit√© ajust√©e - loi normale inverse gaussienne')\naxs[2, 0].legend(loc='upper left')\n\n# Q-Q plot avec une loi de NIG.\nstats.probplot(data, dist=\"norminvgauss\", sparams=(params_nig), plot=axs[2, 1])\naxs[2, 1].set_title('Q-Q Plot - loi normale inverse gaussienne')\n\nplt.tight_layout()\n\n# Affichage des graphiques\nplt.show()\n\nU['SG'] = stats.norminvgauss.cdf(data,*params_nig)\n\nU = U.to_numpy()\n\n\n\n\n\n\n\n\n\n######## Test de kolmogorov-smirnov ########\n\nks_stat_norm, ks_p_value_norm = stats.kstest(data, 'norm', args=(params_norm))\nks_stat_std, ks_p_value_std = stats.kstest(data, 't', args=(params_std))\nks_stat_nig, ks_p_value_nig = stats.kstest(data, 'norminvgauss', args=(params_nig))\n\nres = pd.DataFrame({\n                \"Statistic\": [ks_stat_norm, ks_stat_std, ks_stat_nig],\n                \"p-value\": [ks_p_value_norm, ks_p_value_std, ks_p_value_nig]\n            }, index=[\"Normal\", \"Student\",\"Normal Inverse Gaussian\"])\n\nprint(\"=\"*50)\nprint(\"Test de Kolmogorov-Smirnov\")\nprint(\"=\"*50)\nprint(res)\nprint(\"=\"*50)\n\n==================================================\nTest de Kolmogorov-Smirnov\n==================================================\n                         Statistic   p-value\nNormal                    0.047345  0.021967\nStudent                   0.025635  0.519206\nNormal Inverse Gaussian   0.022462  0.685866\n=================================================="
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html#iii.3.-etude-de-la-structure-de-d√©pendance",
    "href": "3A/value-at-risk/pj_copules.html#iii.3.-etude-de-la-structure-de-d√©pendance",
    "title": "Projet de gestion de risques multiples",
    "section": "III.3. Etude de la structure de d√©pendance",
    "text": "III.3. Etude de la structure de d√©pendance\nL‚Äô√©valuation de la d√©pendance entre les facteurs de risque sera r√©alis√©e en utilisant des outils graphiques bas√©s sur des crit√®res non param√©triques, tels que les nuages de points, les ajustements lin√©aires et le d√©pendogramme.\nCes m√©thodes, choisies pour leur capacit√© √† traiter des donn√©es sans pr√©supposer une distribution sp√©cifique, offrent une approche flexible et visuelle pour identifier et analyser les relations entre les variables de risque. Par exemple, les nuages de points permettent de visualiser la dispersion et la relation potentielle entre deux variables, tandis que les ajustements lin√©aires cherchent √† mod√©liser la relation par une ligne droite, facilitant ainsi la compr√©hension des tendances g√©n√©rales.\nLe d√©pendogramme, quant √† lui, repr√©sente la structure de d√©pendance sous la forme du nuage de points des marges uniformes extraites de l‚Äô√©chantillon n couples de donn√©es \\(\\left(\\left(x_{1,1} ; x_{2,1}\\right), \\cdots,\\left(x_{1, n} ; x_{2, n}\\right)\\right)\\), i.e.¬†:\n\\[\nu_{i, j}=\\frac{1}{n} \\sum_{k=1}^n 1_{\\left\\{x_{j, k} \\leq x_{j,i}\\right\\}}, \\quad i \\in[1, n], \\quad \\forall j \\in[1,2]\n\\]\nLe d√©pendogramme de l‚Äô√©chantillon est donc la repr√©sentation de n couples \\(\\left(\\left(u_{1,1} ; u_{2,1}\\right), \\cdots,\\left(u_{1, n} ; u_{2, n}\\right)\\right)\\). Il permet d‚Äôobserver le caract√®re plus ou moins simultan√© des r√©alisations issues de l‚Äô√©chantillon.\n\n# Tableax avec tx de pearson, spearman et kendall\n\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\n\npearson = pearsonr(returns[\"BNP\"], returns[\"SG\"])\nspearman = spearmanr(returns[\"BNP\"], returns[\"SG\"])\nkendall = kendalltau(returns[\"BNP\"], returns[\"SG\"])\n\ntableau_correlation = pd.DataFrame({\n    \"Pearson\": pearson,\n    \"Spearman\": spearman,\n    \"Kendall\": kendall\n}, index=[\"Coefficient\", \"p-value\"])\n\ntableau_correlation\n\n\n\n\n\n\n\n\nPearson\nSpearman\nKendall\n\n\n\n\nCoefficient\n8.646588e-01\n8.409565e-01\n6.690959e-01\n\n\np-value\n2.383438e-300\n3.609139e-268\n9.880880e-220\n\n\n\n\n\n\n\nEn analysant la corr√©lation entre les rendements de BNP et SG, nous constatons qu‚Äôil y a une corr√©lation positive significative peu importe le test de corr√©lation effectu√©. La corr√©lation de spearman et de pearson indique qu‚Äôil y a une liaison monotone et lin√©aire forte d‚Äôau moins 84% entre les rendements des deux entreprises. En ce qui concerne le taux de kendall, nous constatons une corr√©lation positive significative de 67% environ. Cela signifie que les rendements des actions de BNP et SG sont positivement corr√©l√©s.\n\n\n\n\n\n\nWarning\n\n\n\nAttention la corr√©lation de pearson n‚Äôest pas une mesure de concordance contrairement au coefficient de spearman. Dans notre cas, il indique une corr√©lation lin√©aire forte.\n\n\n\n# Dependogramme\n# Posons x1, u1 = BNP et x2, u2 = SG\nimport warnings\nfrom scipy.stats import rankdata\n\nwarnings.filterwarnings(\"ignore\")\n\n# 1. Transformation en pseudo-observations\ndef pseudo_observations(X):\n    \"\"\"Transforme les donn√©es en pseudo-observations U dans [0,1].\"\"\"\n    n, d = X.shape\n    U = np.zeros((n, d))\n    for j in range(d):\n        U[:, j] = rankdata(X[:, j]) / (n + 1)  # Pour √©viter les 1 stricts\n    return U\n\nX = returns.to_numpy()\nu_obs = pseudo_observations(X)\n\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1,2,1) # 1 ligne, 2 colonnes, premier graphique\n\n# nuages de points des donn√©es BNP et SG\nplt.scatter(returns[\"BNP\"], returns[\"SG\"], cmap=\"viridis\")\n# Ajout la droite qui s'ajuste aux donn√©es\nplt.plot(np.unique(returns[\"BNP\"]), np.poly1d(np.polyfit(returns[\"BNP\"], returns[\"SG\"], 1))(np.unique(returns[\"BNP\"])), color=\"red\", label=\"Droite d'ajustement\")\nplt.title(\"Nuages de points des rendements\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.grid(False)\n\nplt.subplot(1,2,2)\nplt.scatter(u_obs[:,0], u_obs[:,1])\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\nEn observant le d√©pendogramme, nous constatons la m√™me d√©pendance positive entre les deux entreprises. Cela signifie que si l‚Äôune des entreprises fait d√©faut, l‚Äôautre a plus de chance de faire d√©faut √©galement. Cela est coh√©rent avec la corr√©lation positive observ√©e entre les rendements des actions de BNP et SG.\nDe plus, en observant le d√©pendogramme, il semble avoir des d√©pendances √† gauche et √† droite entre les deux entreprises. Nous allons tout de m√™me tester un √©ventail de copules afin de v√©rifier laquelle des copules est la plus adapt√©e √† notre cas : - Copules elliptiques : gaussienne, Student. - Copules archim√©diennes : Clayton, Gumbel, Frank.\nIl faudrait au pr√©alable estimer les param√®tres des copules archim√©diennes et elliptiques pour d√©terminer laquelle des copules est la plus adapt√©e √† notre cas. Pr√©cedemment, nous avons estim√© les param√®tres des lois marginales des rendements de BNP et SG. Nous allons utiliser ces param√®tres pour estimer les param√®tres des copules. Nous allons utiliser ces param√®tres pour la mod√©lisation des diff√©rentes copules.\n\nfrom scipy.optimize import minimize\nfrom statsmodels.distributions.copula.api import (\n    GaussianCopula, StudentTCopula, ClaytonCopula, GumbelCopula, FrankCopula\n)\n\ndef get_copula(copula_type, params):\n    if copula_type == \"gaussian\":\n        rho = params[0]\n        return GaussianCopula(corr=np.array([[1, rho], [rho, 1]]))\n    elif copula_type == \"student\":\n        rho, nu = params\n        return StudentTCopula(corr=np.array([[1, rho], [rho, 1]]), df=nu)\n    elif copula_type == \"clayton\":\n        theta = params[0]\n        return ClaytonCopula(theta=theta)\n    elif copula_type == \"gumbel\":\n        theta = params[0]\n        return GumbelCopula(theta=theta)\n    elif copula_type == \"frank\":\n        theta = params[0]\n        return FrankCopula(theta=theta)\n    else:\n        raise ValueError(f\"Copula type {copula_type} not supported\")\n\n\n# Log-vraisemblance n√©gative pour estimation MLE\ndef negative_log_likelihood(params, U, copula_type):\n    copula = get_copula(copula_type, params)\n    log_likelihood = copula.logpdf(U)\n    return -np.sum(log_likelihood)\n\n# Ajustement de la copule (MLE)\ndef fit_copula(U, copula_type):\n    if copula_type in [\"gaussian\", \"gumbel\", \"clayton\", \"frank\"]:\n        if copula_type==\"gaussian\":\n            x0 = [0.6]\n        else:\n            x0 = [3]\n        bounds = [(1e-5, 10)] if copula_type != \"gaussian\" else [(-0.99, 0.99)]\n    elif copula_type == \"student\":\n        x0 = [0.6, 3]  # rho et df\n        bounds = [(-0.99, 0.99), (2, 30)]\n\n    result = minimize(negative_log_likelihood, x0, args=(U, copula_type), bounds=bounds, method='Nelder-Mead')\n\n    if not result.success:\n        raise RuntimeError(f\"MLE failed for {copula_type} copula: {result.message}\")\n\n    return result.x"
  },
  {
    "objectID": "3A/value-at-risk/pj_copules.html#iii.4.-mod√©lisation-la-structure-de-d√©pendance-au-moyen-des-copules-param√©triques",
    "href": "3A/value-at-risk/pj_copules.html#iii.4.-mod√©lisation-la-structure-de-d√©pendance-au-moyen-des-copules-param√©triques",
    "title": "Projet de gestion de risques multiples",
    "section": "III.4. Mod√©lisation la structure de d√©pendance au moyen des copules param√©triques",
    "text": "III.4. Mod√©lisation la structure de d√©pendance au moyen des copules param√©triques\nPour mod√©liser la structure de d√©pendance entre les d√©fauts, de mani√®re pr√©cise, nous utiliserons les copules. Une copule est une fonction de r√©partition multivari√©e de marginales uniformes sur \\([0,1]\\). Dans le cas bivari√©, on a:\n\\[\n\\mathrm{C}\\left(\\mathrm{u}_1, \\mathrm{u}_{\\mathrm{2}}\\right)=\\mathrm{P}\\left[\\mathrm{U}_1 \\leq \\mathrm{u}_1,\\mathrm{U}_{\\mathrm{2}} \\leq \\mathrm{u}_{\\mathrm{2}}\\right]\n\\]\nDans le cadre de ce projet, nous allons √©tudier deux principales familles de copules pr√©sent√©s dans le tableau :\n\nCopules elliptiques : gaussienne, Student\nCopules archim√©diennes : Clayton, Gumbel, Frank\n\n\\[\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{l|c|cc}\n\\hline\nFamille & Nom    & Copule $C(u,v)$   & Param√®tres \\\\ \\hline\n\\multirow{2}{*}{Elliptique} & Gaussien & $\\Phi_{\\Sigma}\\left(\\Phi^{-1}(u),\\Phi^{-1}(v)\\right)$ & $\\rho$ \\\\\n& Student &  $T_{\\Sigma}\\left(T_{\\nu}^{-1}(u),T_{\\nu}^{-1}(v)\\right)$ & $\\rho, \\nu$ \\\\\n\\midrule\n\\multirow{3}{*}{Archim√©dienne} & Frank  & $\\frac{1}{\\theta} \\left(-\\ln(1+ \\frac{(e^{-\\theta u}  - 1)(e^{-\\theta v} - 1)}{e^{-\\theta v} - 1})\\right)$ & $\\theta \\ne 0$ \\\\\n& Gumbel & $\\exp\\left(-\\left((- \\ln(u))^{\\theta} + (- \\ln(v))^{\\theta}\\right)^{\\frac{1}{\\theta}}\\right)$ & $\\theta \\geq 1$ \\\\\n& Clayton  & $(u^{-\\theta} + v^{-\\theta} -1)^{-\\frac{1}{\\theta}}$ & $\\theta &gt; 0$\n\\\\ \\hline\n\\end{tabular}\\\\\n{\\footnotesize *$\\Sigma$ est la matrice de variance covariance. }\n\\caption{Copules archim√©diennes bivari√©es les plus courantes.}\n\\label{tab:copule_family}\n\\end{table}\n\\]\nIl s‚Äôagira apr√®s estimation des param√®tres et des tests d‚Äôajustement, la copule la plus ad√©quate pour mod√©liser au mieux la d√©pendance entre les variables √©tudi√©es.\nPour l‚Äôestimation des param√®tres des copules s√©lectionn√©es, plusieurs approches m√©thodologiques s‚Äôoffrent √† nous: la m√©thode des moments, la m√©thode du maximum de vraisemblance et l‚Äôapproche IFM. Nous privil√©gierons l‚Äôapproche IFM (Inference Functions for Margins) pr√©sent√© ci dessous (algo \\(\\ref{IFM}\\)). Cet algorithme a l‚Äôavantage d‚Äô√™tre plus rapide que la m√©thode du maximum de vraisemblance.\nPour l‚Äô√©valuation de l‚Äôajustement des copules √† la structure de d√©pendance d‚Äôun √©chantillon, nous utiliserons des outils graphiques tels que le d√©pendogramme, pr√©sent√© pr√©cedemment, et le Kendall plot.\nLe Kendall plot permet une comparaison directe entre la copule empirique et la copule th√©orique. Plus le Kendall plot se rapproche d‚Äôune droite, plus l‚Äôajustement entre la structure de d√©pendance de l‚Äô√©chantillon et la copule estim√©e sur ce m√™me √©chantillon est bon.\n\nIII.4.1 Copule gaussienne\n\nIII.4.1.a. Estimation des param√®tres de la copule gaussienne\nDans le cadre de la copule gaussienne, il nous faut la matrice de variance qui est donn√©e par :\n\\[\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\]\nDans notre cas, le seul param√®tre √† estimer est le coefficient de corr√©lation de pearson. Nous allons donc estimer le coefficient de corr√©lation de pearson entre les rendements des actions de BNP et SG.\n\nrho = fit_copula(u_obs, \"gaussian\")[0]\n\nmat= np.array([[1, rho], [rho, 1]])\nmat\n\narray([[1.        , 0.86097656],\n       [0.86097656, 1.        ]])\n\n\n\n\nIII.4.1.b. Simulation de la copule gaussienne\nPour simuler la copule gaussienne, nous allons utiliser la m√©thode de distribution puisque la loi est facile √† impl√©menter.\n\n# # Simuler r√©alisation W suivant une loi normale centr√©e multivari√©e\n# import scipy.stats as stats\n\n# np.random.seed(0)\n# n = 1000\n# W = np.random.multivariate_normal([0, 0], mat, n)\n\n# # Calculer U1 et U2\n# U = np.zeros((n, 2))\n\n# for i in range(n):\n#     U[i,0] = stats.norm.cdf(W[i,0])\n#     U[i,1] = stats.norm.cdf(W[i,1])\n\n\nn = u_obs.shape[0]\n_ = GaussianCopula(corr = rho).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.1.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule gaussienne\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Kendall plot\nimport numpy as np\n\ndef calculer_Hi_vect(U):\n    n=len(U)\n    H = np.zeros(n, dtype=int)\n    # Comparaison de chaque paire une seule fois\n    for i in range(n):\n        # Cr√©er des masques bool√©ens pour les conditions\n        cond1 = U[:, 0] &lt;= U[i, 0]  # u_{1,j} &lt;= u_{1,i}\n        cond2 = U[:, 1] &lt;= U[i, 1]  # u_{2,j} &lt;= u_{2,i}\n\n        # Appliquer les conditions et exclure le cas o√π i == j\n        H[i] = (np.sum(np.logical_and(cond1, cond2)) - 1)\n\n    return H/(len(U)-1)\n\n\ndef kendall_plot(U,S=1000,copula=\"gaussian\",rho=None,nu=None,theta=None):\n\n    H_i = calculer_Hi_vect(U)\n    n = len(U)\n\n    H_means = np.zeros((S, n)) # S lignes et n colonnes\n    for s in range(S):\n        if copula == 'gaussian':\n            X_ = GaussianCopula(corr=rho).rvs(n)\n        elif copula == 'student':\n            X_ = StudentTCopula(df = nu, corr = rho).rvs(n)\n        elif copula == \"gumbel\":\n            X_= GumbelCopula(theta = theta).rvs(n)\n        elif copula == \"clayton\":\n            X_= ClaytonCopula(theta = theta).rvs(n)\n        elif copula == \"frank\":\n            X_= FrankCopula(theta = theta).rvs(n)\n        U_=pseudo_observations(X_)\n        H_means[s] = np.sort(calculer_Hi_vect(U_))\n\n    H_mean = np.mean(H_means, axis=0) # axis=0 pour moyenne par colonne\n\n    x,y = np.sort(H_i), np.sort(H_mean)\n    print(x.shape, y.shape)\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(x, y)\n    plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n    plt.title(f\"Kendall plot for {copula} copula\")\n    plt.xlabel(\"i\")\n    plt.ylabel(\"Kendall\")\n    plt.grid(True)\n    plt.show()\n\nS=1000\nkendall_plot(u_obs,S,copula=\"gaussian\",rho=rho)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.1.d. Test d‚Äôad√©quation\n\n# Copule empirique Cn(u)\ndef empirical_copula_cdf(U, u):\n    \"\"\"Calcule la copule empirique Cn(u).\"\"\"\n    return np.mean(np.all(U &lt;= u, axis=1))\n\n# Statistique de Cram√©r-von Mises\ndef cramer_von_mises_stat(U, copula):\n    \"\"\"Calcule la statistique de test Tn.\"\"\"\n    n = len(U)\n    Tn = 0.0\n    for i in range(n):\n        u_i = U[i]\n        Cn = empirical_copula_cdf(U, u_i)  # Copule empirique Cn(u_i)\n        C_theta = copula.cdf([u_i])  # Copule th√©orique estim√©e CŒ∏(u_i)\n        Tn += (Cn - C_theta) ** 2\n    return Tn\n\n# Test d‚Äôad√©quation complet avec bootstrap param√©trique\ndef adequation_test(X, copula_type=\"gaussian\", M=500):\n    \"\"\"\n    Test d'ad√©quation de Genest & R√©millard (2008) pour une copule avec\n    statistique de Cram√©r-von Mises et bootstrap param√©trique.\n    \"\"\"\n    # Pseudo-observations\n    U = pseudo_observations(X)\n    n = len(U)\n\n    # Estimation MLE de la copule sur les donn√©es\n    params = fit_copula(U, copula_type)\n    copula = get_copula(copula_type, params)\n\n    # Calcul de la statistique observ√©e Tn\n    T_obs = cramer_von_mises_stat(U, copula)\n\n    # Bootstrap param√©trique\n    T_boot = []\n    for _ in range(M):\n        # 1. Simulation d‚Äôun √©chantillon sous la copule ajust√©e\n        U_boot = copula.rvs(n)\n\n        # 2. R√©-estimation de la copule sur U_boot\n        params_boot = fit_copula(U_boot, copula_type)\n        copula_boot = get_copula(copula_type, params_boot)\n\n        # 3. Calcul de Tn pour cet √©chantillon bootstrap\n        T_boot.append(cramer_von_mises_stat(U_boot, copula_boot))\n\n    # Calcul de la p-value (proportion des T_boot sup√©rieurs √† T_obs)\n    p_value = np.mean(np.array(T_boot) &gt;= T_obs)\n\n    return {\n        \"copula_type\": copula_type,\n        \"params\": params,\n        \"T_obs\": T_obs,\n        \"p_value\": p_value\n    }\n\nfrom pprint import pprint\nresult_gaussian = adequation_test(X, copula_type=\"gaussian\")\npprint(result_gaussian)\n\n{'T_obs': np.float64(0.054564311624880506),\n 'copula_type': 'gaussian',\n 'p_value': np.float64(0.886),\n 'params': array([0.86097656])}\n\n\n\n\n\nIII.4.2 Copule de student\n\nIII.4.2.a. Estimation des param√®tres de la copule student\n\nrho,nu = fit_copula(u_obs, \"student\")\n\nprint(\"rho = \", rho)\nprint(\"nu = \", nu)\n\nrho =  0.8496143388748167\nnu =  2.0\n\n\n\n\nIII.4.2.b. Simulation de la copule de student\n\nn = u_obs.shape[0]\n_ = StudentTCopula(corr = rho, df=nu).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.2.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de student\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"student\",rho=rho,nu=nu)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAttention, nous avons utilis√© le package copula.api de statsmodels pour impl√©menter les copules. Cependant, la classe correspondant √† la copule de Student ne permet pas d‚Äôobtenir une fonction de r√©partition (voir lien).\nEn utilisant un environnement virtuel, il a √©t√© possible de modifier le fichier statsmodels/distributions/copula/elliptical.py du package afin d‚Äôimpl√©menter la m√©thode pour la fonction de r√©partition. Vous trouverez ce fichier ci-joint afin de garantir le bon fonctionnement du code si vous devez le relancer.\n\n\n\n\nIII.4.2.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_std = adequation_test(X, copula_type=\"student\") # ATTENTION DIFFICULTE POUR ESSTIMER COPULE STUDENT\npprint(result_std)\n\n{'T_obs': np.float64(0.05938600115289895),\n 'copula_type': 'student',\n 'p_value': np.float64(0.842),\n 'params': array([0.84961434, 2.        ])}\n\n\n\n\n\nIII.4.3 Copule de clayton\n\nIII.4.3.a. Estimation des param√®tres de la copule clayton\n\ntheta = fit_copula(u_obs, \"clayton\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  2.4768310546874988\n\n\n\n\nIII.4.3.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = ClaytonCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.3.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de clayton\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nS=1000\nkendall_plot(u_obs,S,copula=\"clayton\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.3.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_clayton= adequation_test(X, copula_type=\"clayton\") \npprint(result_clayton)\n\n{'T_obs': array([0.86477661]),\n 'copula_type': 'clayton',\n 'p_value': np.float64(0.012),\n 'params': array([2.47683105])}\n\n\n\n\n\nIII.4.4 Copule de gumbel\n\nIII.4.4.a. Estimation des param√®tres de la copule gumbel\n\ntheta = fit_copula(u_obs, \"gumbel\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  2.9939208984374996\n\n\n\n\nIII.4.4.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = GumbelCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.4.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de gumbel\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"gumbel\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.4.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_gumbel = adequation_test(X, copula_type=\"gumbel\") \npprint(result_gumbel)\n\n{'T_obs': array([0.03485682]),\n 'copula_type': 'gumbel',\n 'p_value': np.float64(0.976),\n 'params': array([2.9939209])}\n\n\n\n\n\nIII.4.4 Copule de frank\n\nIII.4.4.a. Estimation des param√®tres de la copule frank\n\ntheta = fit_copula(u_obs, \"frank\")[0]\n\nprint(\"theta = \", theta)\n\ntheta =  10.0\n\n\n\n\nIII.4.4.b. Simulation de la copule de clayton\n\nn = u_obs.shape[0]\n_ = FrankCopula(theta= theta).rvs(n)\nu_est = pseudo_observations(_)\n\n\n\nIII.4.4.c. Test d‚Äôajustement\n\nplt.figure(figsize=(10, 5))\nplt.scatter(u_obs[:,0], u_obs[:,1], label = \"Donn√©es empiriques\", color = \"grey\")\nplt.scatter(u_est[:,0], u_est[:,1], label = \"Copule de frank\", color = \"red\")\nplt.title(\"D√©pendogrammes\")\nplt.xlabel(\"u1\")\nplt.ylabel(\"u2\")\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nkendall_plot(u_obs,S,copula=\"frank\",theta=theta)\n\n(999,) (999,)\n\n\n\n\n\n\n\n\n\n\n\nIII.4.4.d. Test d‚Äôad√©quation\n\nfrom pprint import pprint\nresult_frank = adequation_test(X, copula_type=\"frank\") \npprint(result_frank)\n\n{'T_obs': array([0.12454173]),\n 'copula_type': 'frank',\n 'p_value': np.float64(0.432),\n 'params': array([10.])}\n\n\n\n\n\nIII.5. R√©sultats\nNos r√©sultats mettent en √©vidence l‚Äôimportance de la structure de d√©pendance dans l‚Äô√©valuation du risque de cr√©dit. Apr√®s avoir test√© plusieurs copules param√©triques, nous avons retenu la copule de Gumbel comme la plus appropri√©e, en raison de son bon ajustement aux donn√©es et de sa capacit√© √† capturer les asym√©tries et les queues de distribution lourdes, essentielles dans un contexte de crise financi√®re.\n\ndict_list = [result_gaussian, result_std, result_clayton, result_gumbel, result_frank]\n\n# Convert to DataFrame\ndf = pd.DataFrame(dict_list)\ndf\n\n\n\n\n\n\n\n\ncopula_type\nparams\nT_obs\np_value\n\n\n\n\n0\ngaussian\n[0.8609765625000002]\n0.054564\n0.886\n\n\n1\nstudent\n[0.8496143388748167, 2.0]\n0.059386\n0.842\n\n\n2\nclayton\n[2.4768310546874988]\n[0.8647766118220173]\n0.012\n\n\n3\ngumbel\n[2.9939208984374996]\n[0.034856823632549765]\n0.976\n\n\n4\nfrank\n[10.0]\n[0.12454172887238589]\n0.432"
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La r√©glementation prudentielle",
    "section": "",
    "text": "La r√©glementation prudentielle a √©t√© initi√©e par le d√©veloppement des march√©s financiers et des chocs aliment√©s par diverses crises financi√®res. Face √† ce constat, les autorit√©s de contr√¥le bancaire ainsi que les autorit√©s de march√© ont pris des d√©cisions pour r√©guler les march√©s. C‚Äôest notamment le r√¥le qu‚Äôoccupe le Comit√© de B√¢le ou la Commission bancaire, qui ont pour objectif de renforcer la stabilit√© des march√©s financiers. En France, l‚ÄôACPR (Autorit√© de Contr√¥le Prudentiel et de R√©solution) et la Banque de France sont membres du Comit√© de B√¢le et participent √† ses travaux et d√©cisions.\nIl existe par ailleurs plusieurs textes r√©glementaires ou documents relatifs au risque de march√©. Parmi ces textes, on peut citer le document de r√©f√©rence pour calculer le ratio de solvabilit√© de la Commission bancaire, intitul√© ‚ÄúModalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL‚Äù, actualis√© tous les ans par l‚ÄôACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "title": "La r√©glementation prudentielle",
    "section": "Approche standard de mesure du risque de march√©",
    "text": "Approche standard de mesure du risque de march√©\nL‚Äôapproche standard de mesure du risque de march√© consiste √† calculer les exigences en fonds propres pour chaque cat√©gorie de risque, √† savoir :\n\nle risque de taux (g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque li√© aux titres de propri√©t√©(g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque de change calcul√© sur l‚Äôensemble des op√©rations appartenant aussi bien au portefeuille de n√©gociation ou non;\nle risque sur mati√®res premi√®res calcul√© sur l‚Äôensemble des op√©rations du portefeuille de n√©gociation ou non;\nles risques op√©rationnels calcul√©s sur les options associ√©es √† chachune des cat√©gories de risque cit√©es ci-dessus.\n\nPar la suite, il s‚Äôagit de les additionner de mani√®re arithm√©tique. Par exemple, pour les titres de propri√©t√©, l‚Äôexigence de fonds propres est la somme de l‚Äôexigence de fonds propres pour le risque g√©n√©ral et l‚Äôexigence de fonds propres pour le risque sp√©cifique.\nPour le calcul des exigences de fonds propres au titre des risques de march√©, il faut tout d‚Äôabord d√©terminer les positions nettes. Les positions de titrisation log√©es dans le portefeuille de n√©gociation sont trait√©es comme tout instrument de dette au titre du risque de taux.\nPour le risque sp√©cifique, l‚Äôexigence en fonds propres sera la somme des positions nettes multipli√©es par un coefficient de pond√©ration (2%, 4%, 8% ou 12%) choisi en fonction de la liquidit√© et la diversification de la position. Pour le risque g√©n√©ral, l‚Äôexigence en fonds propres est la somme des positions nettes globales (pour chaque march√© national) multipli√©es par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-mod√®le-interne",
    "href": "3A/reglementation_prudentielle.html#approche-mod√®le-interne",
    "title": "La r√©glementation prudentielle",
    "section": "Approche mod√®le interne",
    "text": "Approche mod√®le interne\nL‚Äôapproche mod√®le interne est une m√©thode de calcul des exigences en fonds propres pour le risque de march√© qui permet aux √©tablissements de calculer leurs propres exigences. L‚Äôexigence en fonds propres est g√©n√©ralement un calcul de la VaR. Cette approche est soumise √† des conditions strictes et √† une validation par l‚ÄôACPR.\nConcernant l‚Äôutilisation conjointe des mod√®les internes et de l‚Äôapproche standard, la position de la commission pr√™te une attention particuli√®re √† la permanence des m√©thodes ainsi qu‚Äô√† leur √©volution. L‚Äôobjectif est de s‚Äôorienter vers un mod√®le global qui tient compte de l‚Äôensemble des risques de march√©.\n\nAinsi, un √©tablissement commen√ßant √† utiliser des mod√®les pour une ou plusieurs cat√©gories de facteurs de risque doit en principe √©tendre progressivement ce syst√®me √† tous ses risques √† la m√©thodologie standardis√©e (√† moins que la Commission Bancaire ne lui ait retir√© son agr√©ment pour ses mod√®les).\n\nPour une banque, la construction d‚Äôun mod√®le interne doit permettre de fournir une mesure plus √©conomique du risque de march√©. Au titre de l‚Äôarticle 363 du CRR (R√®glement sur les exigences de fonds propres), l‚Äôautorit√© comp√©tente autorise les √©tablissements assujettis √† utiliser leurs mod√®les internes pour calculer les exigences de fonds propres pour risques de march√©, apr√®s avoir v√©rifi√© qu‚Äôils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3√®me partie du CRR [@journal]. L‚Äôautorisation d‚Äôutiliser des mod√®les internes accord√©e par les autorit√©s comp√©tentes est requise pour chaque cat√©gorie de risques (risque g√©n√©ral et sp√©cifique li√©s aux actions et titres de cr√©ance, risque de change et risque sur mati√®res premi√®res), et elle n‚Äôest accord√©e que si le mod√®le interne couvre une part importante des positions d‚Äôune certaine cat√©gorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives √† la mesure du risque (articles 367) mais aussi d‚Äôordre g√©n√©ral (article 365).\n\nExigences g√©n√©rales\nLe calcul de la valeur en risque vis√©e √† l‚Äôarticle 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprim√© en centiles et unilat√©ral, de 99 %;\nune p√©riode de d√©tention de dix jours;\nune p√©riode effective d‚Äôobservation historique d‚Äôau moins un an, √† moins qu‚Äôune p√©riode d‚Äôobservation plus courte ne soit justifi√©e par une augmentation significative de la volatilit√© des prix;\ndes mises √† jour au moins mensuelles des s√©ries de donn√©es.\n\nL‚Äô√©tablissement peut utiliser des mesures de la valeur en risque calcul√©es sur la base de p√©riodes de d√©tention inf√©rieures √† dix jours, qu‚Äôil porte √† dix jours selon une m√©thode appropri√©e qu‚Äôil revoit r√©guli√®rement.\nChaque √©tablissement doit √©galement calculer, au moins hebdomadairement, une ‚Äúvaleur en risque en situation de tensions‚Äù (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit √™tre calcul√©e conform√©ment aux m√™mes exigences que la VaR standard √©nonc√©es plus haut (intervalle de confiance de 99% etc.). Cependant, les donn√©es d‚Äôentr√©e du mod√®le de Stressed VaR doivent √™tre calibr√©es par rapport √† une p√©riode historique de tensions financi√®res significatives d‚Äôau moins 12 mois, pertinente pour le portefeuille de l‚Äô√©tablissement. Le choix de cette p√©riode de tensions historiques fait l‚Äôobjet d‚Äôun examen au moins annuel par l‚Äô√©tablissement, qui en communique les r√©sultats aux autorit√©s comp√©tentes. L‚Äôobjectif est de s‚Äôassurer que la Stressed VaR refl√®te de mani√®re ad√©quate les risques auxquels l‚Äô√©tablissement serait expos√© en p√©riode de crise financi√®re.\nPour r√©sumer, les √©tablissements doivent calculer la perte potentielle quotidiennement pour une p√©riode de d√©tention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent √©galement calculer une Stressed VaR au moins une fois par semaine, en utilisant des donn√©es historiques de p√©riodes de tensions financi√®res significatives.\nNotons \\(VaR(t)\\) la valeur en risque √† la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions √† la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) √† la date t pour le risque de march√© sont calcul√©es comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\no√π \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu‚Äôon vera plus tard.\nDans des p√©riodes normales, l‚Äôexigence en fonds propres sera donc la somme d‚Äôun multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n‚Äôest que dans les p√©riodes de crises financi√®res que l‚Äôexigence en fonds propres correspond √† la VaR ou √† la sVaR du jour pr√©c√©dent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est √©gal √† la somme du chiffre 3, au minimum, et d‚Äôun cumulateur compris entre 0 et 1 conform√©ment au tableau 1. Ce cumulateur d√©pend du nombre de d√©passements, sur les 250 derniers jours ouvr√©s, mis en √©vidence par les contr√¥les a posteriori de la mesure de la valeur en risque, au sens de l‚Äôarticle 365, paragraphe 1, effectu√©s par l‚Äô√©tablissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqu√©s depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqu√©s depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.d√©passements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque sp√©cifique, tout mod√®le interne utilis√© pour calculer les exigences de fonds propres et tout mod√®le interne utilis√© pour la n√©gociation en corr√©lation satisfont aux exigences suppl√©mentaires suivantes:\n\nle mod√®le interne explique la variation historique des prix √† l‚Äôint√© rieur du portefeuille;\nil refl√®te la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement d√©favorable;\nil est valid√© par des contr√¥les a posteriori(backtesting) visant √† √©tablir si le risque sp√©cifique a √©t√© correctement pris en compte. Si l‚Äô√©tablissement effectue ces contr√¥les a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de mani√®re coh√©rente;\nil tient compte du risque de base li√© √† la signature et, en particulier, il est sensible aux diff√©rences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d‚Äô√©v√©nement.\n\nLe risque sp√©cifique vise √† tenir compte du risque de contrepartie li√© √† l‚Äôemetteur de l‚Äôinstrument.\nPour en savoir plus, reportez au r√®glement (UE) No 575/2013 du parlement europ√©en du journal officiel de l‚ÄôUnion Europ√©enne, appel√© aussi r√®glement CRR. (voir aussi la notice 2020 relative aux ¬´ Modalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV¬ª)."
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "Le mod√®le classique de volatilit√© stochastique est d√©fini par les √©quations suivantes :\n\nProcessus des rendements :\n\\[ r_t = \\exp(x_t / 2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0,1) \\]\nProcessus de la volatilit√© logarithmique :\n\\[ x_t = \\mu + \\phi x_{t-1} + \\sigma_t \\eta_t, \\quad \\eta_t \\sim N(0,1) \\]\n\n\n( x_t ) suit un processus autor√©gressif de premier ordre (AR(1)) et suit une distribution normale conditionnelle : \\[ p(x_t) \\sim N(\\frac{\\mu}{1-\\phi} , \\frac{\\sigma_t^2}{1-\\phi^2}) \\]\n\\[ x_t | x_{t-1} \\sim N(\\mu + \\phi x_{t-1}, \\sigma_t^2) \\]\n( r_t ) suit une distribution normale conditionnelle :\n\\[ r_t | x_t \\sim N(0, \\exp(x_t)) \\]\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\nn &lt;- 252\nmu &lt;- -0.8\nphi &lt;- 0.9\nsigma_squared &lt;- 0.09\n\nx &lt;- numeric(n)  # Log-volatilit√©\nr &lt;- numeric(n)  # Rendements simul√©s\n\nfor (t in 1:n) {\n  if (t == 1) {\n    # Densit√© de transition stationnaire de x_t\n    x[t] &lt;- rnorm(1, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n  } else {\n    # √âvolution de l'√©tat\n    x[t] &lt;- mu + phi *x[t-1] + sqrt(sigma_squared) * rnorm(1, mean = 0, sd = 1)\n  }\n  # Simulation des rendements\n  r[t] &lt;- exp(x[t] / 2) * rnorm(1, mean = 0, sd = 1)\n}\n\n# extraction dans fichier csv\nwrite.csv(data.frame(r, x), \"true_sv_taylor.csv\", row.names = FALSE)\n\n\npar(mfrow=c(1,2))\nplot(x, lwd = 2, type = \"l\", col = \"blue\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© simul√©\")\nplot(r, lwd = 2, type = \"l\", col = \"red\", ylab = \"Rendements\", xlab = \"Temps\", main = \"Rendements simul√©s\")\n\n\n\n\n\n\n\n\n\n\n\nparams &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "params &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "set.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap avec les rendements",
    "text": "Filtre bootstrap avec les rendements\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Definition des variables\n# D√©finition des param√®tres\n\nparams &lt;- c(mu,phi,sigma_squared)\n\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t,] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ] - mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nUtilisation de library(pmhtutorial)\n\nlibrary(pmhtutorial)\n\n# particleFilterSVmodel takes sigma as parameters\nparams[3] &lt;- sqrt(params[3])\nx_hat_2&lt;- particleFilterSVmodel(r,params,M)\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"log-volatilit√©\")\nlines(x_hat_2$xHatFiltered, type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© estim√©\")\n# legend\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat_2$xHatFiltered / 2) * rnorm(length(x_hat_2$xHatFiltered), mean = 0, sd = 1)\n\n# Superposition des trajectoires\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",\n  col=\"#1B9E77\", main=\"True returns\")\n\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", \n     main = \"Estimated returns\",bty=\"n\")\n\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap sur le mod√®le log-sv de taylor",
    "text": "Filtre bootstrap sur le mod√®le log-sv de taylor\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nparams &lt;- c(mu,phi,sigma_squared)\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ]-mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html",
    "href": "3A/proc_stochastique/modele_heston.html",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "",
    "text": "Le but de ce TP est de calculer des prix d‚Äôoptions sous le mod√®le d‚ÄôHeston puis de calibrer ce mod√®le par filtrage. On consid√®re le mod√®le suivant :\n\\[\n\\begin{cases}\ndS_s = S_s \\left( rds + \\sqrt{v_s} dW_s^1 \\right) \\\\\ndv_s = \\kappa (\\beta - v_s) ds + \\sigma \\sqrt{v_s} dW_s^2 \\\\\ndW_s^1 dW_s^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\no√π \\(W_s^1\\) et \\(W_s^2\\) sont deux mouvements browniens et \\(r\\) est le taux sans risque. Pour ce mod√®le, les rendements sont mod√©lis√©s par un mouvement brownien g√©om√©trique avec une variance stochastique.\nLa volatilit√© non observ√©e \\(v_t\\) est d√©termin√©e par un processus stochastique de retour √† la moyenne (1) introduit en 1985 par Cox, Ingersoll et Ross pour la mod√©lisation des taux d‚Äôint√©r√™t √† court terme.\nLe param√®tre \\(\\kappa\\) est le param√®tre de retour √† la moyenne positive, \\(\\beta\\) est le param√®tre positif √† long terme et \\(\\eta\\) la volatilit√© positive du param√®tre de variance. De plus, Heston a introduit une corr√©lation entre les deux mouvements browniens \\(W_s^1\\) et \\(W_s^2\\), repr√©sent√©e par le param√®tre \\(\\rho\\) appartenant √† \\([-1,1]\\)."
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la forme close",
    "text": "Avec la forme close\nSoit un Call de strike K et √† √©ch√©ance \\(\\tau\\) sous le mod√®le (1) avec les param√®tres suivants : \\(\\kappa\\) = 4,\\(\\beta\\) = 0.03,\\(\\sigma\\) = 0.4,r =0.05,\\(\\rho\\)=‚àí0.5,\\(\\tau\\) = 1, \\(S_0\\) = K=100,\\(v_0\\) = \\(\\beta\\).\nPour calculer le prix d‚Äôun Call, on peut utiliser la formule close de Heston (Heston 1993) :\n\\[\nC(S_0, K, \\tau) = S_0 P_1 - K e^{-r \\tau} P_2\n\\]\navec :\n\\[\nP_j(x, \\nu, T, \\ln(K)) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\Re \\left( \\frac{e^{-i \\ln(K) u} f_j(x,\\nu,t,u)}{i u} \\right) du\n\\]\no√π :\n\\[\nx = \\ln(S_t), \\quad f(x,\\nu,t,u) = \\exp(C(t,u) + D(t,u) \\nu + i \\phi x)\n\\]\net :\n\\[\nC(T-t = \\tau, \\phi) = r i \\phi t + \\frac{a}{\\sigma^2} \\left( (bj - \\rho \\sigma \\phi i + d)\\tau - 2 \\ln \\left( \\frac{1 - g e^{d \\tau}}{1 - g} \\right) \\right)\n\\]\n\\[\nD(T-t = \\tau, \\phi) = \\left( \\frac{bj - \\rho \\sigma \\phi i + d}{\\sigma^2} \\right) \\left( \\frac{1 - e^{d \\tau}}{1 - g e^{d \\tau}} \\right)\n\\]\n\\[\ng = \\frac{bj - \\rho \\sigma \\phi i + d}{bj - \\rho \\sigma \\phi i - d}\n\\]\n\\[\nd = \\sqrt{(\\rho \\sigma \\phi i - bj)^2 - \\sigma^2 (2 u_j \\phi i - \\phi^2)}\n\\]\n\\[\nu_1 = 1/2, \\quad u_2 = -1/2, a = \\lambda, b = \\kappa \\beta, \\quad t_1 = \\kappa - \\rho \\sigma, \\quad t_2 = \\kappa\n\\]\nPour ce faire, nous allons utiliser la fonction Heston_Call_Function.R qui permet de calculer le prix d‚Äôun Call sous le mod√®le d‚ÄôHeston avec la formule close.\n\n# Param√®tres\nkappa &lt;- 4\nbeta &lt;- 0.03\nsigma &lt;- 0.4\nr &lt;- 0.05\nrho &lt;- -0.5\ntau &lt;- 1\nS0&lt;- 100\nK &lt;- 100\nv0 &lt;- beta\n\n# Import Heston_Call_Function.R\nsource(\"data/Heston_Call_Function.R\")\n\n# Calcul du prix du Call\nCall_Heston &lt;- HestonCallClosedForm(lambda = kappa, vbar = beta, eta = sigma, rho = rho, v0 = v0, r = r, tau = tau, S0 = S0, K = K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.410405"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)",
    "text": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)\nLorsqu‚Äôon a pas acc√®s √† la formule close, on peut utiliser la m√©thode de Monte Carlo pour calculer le prix d‚Äôun Call. Il s‚Äôagit de simuler le mod√®le (1) et de calculer le prix du Call √† partir des simulations. Pour simuler le mod√®le (1), on peut utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston (Euler and Milstein Discretization, Fabrice Douglas Rouah) ou utiliser la formule de Ito pour le mod√®le de Heston.\nDans notre cas, nous allons utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston pour simuler le mod√®le (1) comme suit : \\[\n\\begin{cases}\nS_t = S_{t-1} \\left(1 + r \\Delta + \\sqrt{\\Delta v_t} W_t^1 \\right) \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\beta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}} \\Delta W_t^2 \\right| \\\\[10pt]\n\\text{Cov}(W_t^1, W_t^2) = \\rho\n\\end{cases}\n\\]\navec \\(W_t^1\\) et \\(W_t^2\\) des variables al√©atoires gaussiennes centr√©es r√©duites et corr√©l√©es entre elles telles que \\(\\text{Cov}(W_t^1, W_t^2) = \\rho\\). De plus, \\(\\Delta = \\frac{\\tau}{n}\\) est le pas de discr√©tisation, avec \\(n\\) le nombre de pas de discr√©tisation.\nDans notre cas, on d√©finit \\(n = 100\\) et on simule \\(M = 1000\\) mod√®le (1) pour calculer le prix d‚Äôun Call.\n\nHestonCallMC &lt;- function(M, N, lambda, vbar, eta, rho, v0, r, tau, S0, K){\n  # M: Number of Monte Carlo simulations\n  # N: Number of time steps\n  \n  set.seed(123)\n  dt &lt;- tau / N  # Time step\n\n  # Store final stock prices\n  ST &lt;- numeric(M)\n  \n  for (i in 1:M){\n    S &lt;- numeric(N+1)\n    v &lt;- numeric(N+1)\n    \n    S[1] &lt;- S0\n    v[1] &lt;- v0\n    \n    for (t in 1:N){\n      # Generate correlated Brownian motions\n      W1 &lt;- rnorm(1)\n      W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n      \n      # Euler discretization of variance process (ensure non-negativity)\n      v[t+1] &lt;- abs(v[t] + lambda * (vbar - v[t]) * dt + eta * sqrt(v[t] * dt) * W1)\n      \n      # Euler discretization of the stock price process (log-normal form)\n      S[t+1] &lt;- S[t] * exp((r - 0.5 * v[t]) * dt + sqrt(v[t] * dt) * W2)\n    }\n    \n    # Store final stock price\n    ST[i] &lt;- S[N+1]\n  }\n\n  # Compute Call option price using Monte Carlo method\n  Call &lt;- exp(-r * tau) * mean(pmax(ST - K, 0), na.rm=TRUE)\n  \n  return(Call)\n}\n\nM &lt;- 1000\nN &lt;- 100\nCall_Heston &lt;-HestonCallMC(M,N, kappa, beta, sigma, rho, v0, r, tau, S0, K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.797915"
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html",
    "href": "3A/proc_stochastique/APF_filter.html",
    "title": "APF filter",
    "section": "",
    "text": "Le filtre particulaire APF (Auxiliary Particle Filter) est un filtre particulaire qui utilise des particules auxiliaires pour estimer la densit√© de probabilit√© de l‚Äô√©tat cach√©. Il est utilis√© pour estimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. Dans cet article, nous allons √©tudier la performance du filtre APF en utilisant un exemple simple.\nComme tout filtre particulaire, il est necessaire de sp√©cifier la distribution a priori de l‚Äô√©tat, i.e.¬†\\(p(x_0)\\), la distribution de transition, i.e.¬†\\(p(v_t|v_{t-1})\\) et la vraisemblance, i.e.¬†\\(p(y_t|v_t)\\).\nDans le mod√®le de Heston sp√©cifi√© en (1), on consid√®re que : \\[\np(v_t|v_{t-1})=2c\\chi^2(2cx_k; 2q + 2; 2ce^{-\\kappa \\Delta} x_{k‚àí1}),\n\\]\n\\[p(v_1) = \\Gamma(v_1; a,b)\\] o√π \\(a = \\frac{2 \\kappa \\theta}{\\sigma^2}\\) et \\(b = \\frac{2 \\kappa}{\\sigma^2}\\),\net \\(p(y_t)|v_t) = N(0,h)\\).\n\\[\n\\begin{cases}\ndS_t = S_t \\left( rds + \\sqrt{v_t} dW_t^1 \\right) \\\\\ndv_t = \\kappa (\\theta - v_t) ds + \\sigma \\sqrt{v_t} dW_t^2 \\\\\ndW_t^1 dW_t^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\nPour tester la pertinence de l‚ÄôAPF, nous allons utiliser les param√®tres suivants \\(\\Phi = (\\theta = 0.03, \\kappa = 4, \\sigma = 0.4, \\kappa = -0.87, \\rho = 0.5)\\). Pour passer en temps discret et assurer la positivit√© de la volatilit√©, nous utilisosn le schema d‚Äôeuler (√† \\(|v_t|\\)) suivant :\n\\[\n\\begin{cases}\ny_t = C(t, \\theta, v_t, S_t, K, \\tau) + \\varepsilon_t, \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\theta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}\\Delta} (\\rho w_t^1 + \\sqrt{1-\\rho^2}w_t^2) \\right|, \\\\[10pt]\nS_t = S_{t-1} \\left(1 + \\mu \\Delta + \\sqrt{\\Delta v_t} w_t^1 \\right), \\\\[10pt]\n\\end{cases}\n\\]\no√π \\(\\varepsilon_t \\sim N(0,h=0.01)\\), et \\(w_t^1, w_t^2\\) sont des variables al√©atoires gaussiennes et ind√©pendantes.\n\nNous avons utilis√© un sch√©ma d‚Äôeuler modifi√© pour garantir la positivit√© de la volatilit√©. En effet, la volatilit√© doit √™tre positive dans le mod√®le de Heston, et le sch√©ma d‚Äôeuler standard peut produire des valeurs n√©gatives. En prenant la valeur absolue de la volatilit√© √† chaque √©tape, nous nous assurons que les valeurs restent positives. Il aurait √©t√© √©galement possible d‚Äôutiliser le sch√©ma d‚Äôeuler √† \\(ln(v_t)\\) (via le lemme de ito) pour garantir la positivit√© de la volatilit√©.\n\n\nrm(list=ls())\n\n################ Simulation de la trajectoire de St et vt ################ \nHeston_sim &lt;- function(N, kappa, theta, sigma, rho, v0, mu, tau, S0){\n  # N: Number of time steps\n  dt &lt;- tau / N  # Time step\n\n  # Store stock prices and volatilities\n  S &lt;- numeric(N+1)\n  v &lt;- numeric(N+1)\n  \n  S[1] &lt;- S0\n  v[1] &lt;- v0\n  \n  for (t in 1:N){\n    # Generate correlated Brownian motions\n    W1 &lt;- rnorm(1)\n    W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n    \n    # Euler discretization of variance process (ensure non-negativity)\n    v[t+1] &lt;- abs(v[t] + kappa * (theta - v[t]) * dt + sigma * sqrt(v[t] * dt) * W2)\n    \n    # Euler discretization of the stock price process \n    S[t+1] &lt;- S[t] * (1+ mu*dt + sqrt(v[t+1] * dt) * W1)\n  }\n  \n  return(list(v_t = v, S_t=S))\n}\n\nAvec les √©tapes 1 et 2, nous obtenons les trajectoires de volatilit√© instantan√©e et de prix d‚Äôaction suivantes :\n\nset.seed(123)\nN &lt;- 300\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nrho &lt;- 0.5\nv0 &lt;- 0.03\nmu &lt;- 0.1\ntau&lt;-1\nS0 &lt;- 100\n\nres &lt;- Heston_sim(N=N, kappa=kappa, theta=theta, sigma=sigma, rho=rho, v0=v0, mu=mu, tau=tau, S0=S0)\n\npar(mfrow=c(1,2))\nplot(res$v_t, type=\"l\", main=\"Processus de volatilit√© simul√©\", xlab = \"Time step\", ylab = \"Volatilit√©\") \nplot(res$S_t, type=\"l\", main=\"Processus de prix du sous-jacent simul√©\", xlab = \"Time step\", ylab = \"Prix\") \n\n\n\n\n\n\n\n\n\n\nLa proc√©dure de simulation est la suivante :\n\nTout d‚Äôabord, une trajectoire de 300 pas de temps de la variance instantan√©e sera simul√©e pour un pas de 1 jour, en commen√ßant par \\(v_0\\) = 0,03.\nConditionnellement √† cette trajectoire, une trajectoire correspondante du prix de l‚Äôaction sera alors g√©n√©r√©e.\nNous calculons, √† l‚Äôaide du mod√®le de Heston, les prix d‚Äôoptions bruit√©s pour trois types d‚Äôoptions diff√©rents : une option √† la monnaie (ATM), une option dans la monnaie (ITM) et une option mixte (50 % ITM / 50 % OTM).\n\nNous avons choisi trois types d‚Äôoptions afin d‚Äôobserver comment le filtre APF se comporte dans diff√©rentes conditions de march√© :\n\nOption √† la monnaie (ATM) :\n\nPrix d‚Äôexercice : K = 1 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ATM sont souvent utilis√©es pour l‚Äôestimation de la volatilit√© implicite, car elles sont les plus liquides et pr√©sentent un delta proche de 0.5, ce qui les rend sensibles aux variations du sous-jacent.\n\nOption dans la monnaie (ITM) :\n\nPrix d‚Äôexercice : K = 0.95 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ITM ont une valeur intrins√®que √©lev√©e et une volatilit√© implicite plus stable. Elles sont moins sensibles aux fluctuations imm√©diates du march√© mais permettent d‚Äô√©valuer l‚Äôimpact du filtre APF dans des conditions de faible variance du prix d‚Äôoption.\n\nOption mixte (50 % ITM / 50 % OTM) :\n\nPrix d‚Äôexercice : K = 117\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Cette approche permet de tester le filtre APF dans un sc√©nario r√©aliste de portefeuille d‚Äôoptions o√π des positions ITM et OTM sont combin√©es. L‚Äôobjectif est d‚Äôanalyser si le filtre reste stable lorsque l‚Äôon m√©lange des options avec des sensibilit√©s diff√©rentes aux mouvements du sous-jacent et aux variations de volatilit√©.\n\n\nPourquoi tester diff√©rentes configurations d‚Äôoptions ?\nL‚Äôobjectif de cette analyse est de v√©rifier comment le filtre APF se comporte en pr√©sence de conditions de march√© vari√©es :\n\nOptions ATM : impact fort de la volatilit√©, mais moins sujettes au risque de gamma.\nOptions ITM : faible sensibilit√© √† la volatilit√© implicite, mais risque de couverture plus limit√©.\nOptions mixtes : √©valuation de la robustesse du filtre lorsque plusieurs types d‚Äôoptions coexistent dans un m√™me portefeuille.\n\nCes tests permettent de comparer la pr√©cision du filtre en fonction de la position de l‚Äôoption par rapport au prix du sous-jacent.\n\n################ Simulation du prix des options ATM ################ \nsource(\"data/Heston_Call_Function.R\")\nset.seed(123)\n\nv_t &lt;- res$v_t \nS_t &lt;- res$S_t\nh &lt;- 0.01\n\n# Prix d'option K=1*S, tau = 0.5 =&gt; A la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 1\nATM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nATM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\n\nres$ATM &lt;- ATM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options ITM ################ \n# Prix d'option K=0.95*S, tau = 0.5 =&gt; Hors de la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 0.95\n\nITM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nITM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\nres$ITM &lt;- ITM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options mixte ################ \nsummary(S_t)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  99.49  104.36  117.23  119.24  128.88  156.89 \n\n# Prix d'option K=117, tau = 0.5  =&gt; moiti√© ITM et moiti√© OTM\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 117\n\nMIXTE &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nMIXTE[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K)\n}\nres$MIXTE &lt;- MIXTE + rnorm(1,mean=0,sd=sqrt(h))\n\n\n\nEn simulant le prix de ces options, nous constatons sans surprise que les options mixte ont un prix plus √©lev√© plus on se rapproche de la maturit√©. Cependant, les options ITM et ATM ont des prix plus stables, avec une l√©g√®re augmentation pour les options ITM.\n\nN &lt;- length(res$ITM)  \ndt &lt;- tau / N\ntime_axis &lt;- seq(0, tau, length.out = N)  # Axe des temps, de 0 √† tau\n\nplot(time_axis,res$ITM, type=\"l\", main = \"Evolution du prix des options \", ylim = c(0,50), col = \"red\", ylab = \"Prix\", xlab = \"Temps\")\nlines(time_axis,res$ATM, type = \"l\",col=\"blue\")\nlines(time_axis,res$MIXTE, type='l', col= \"darkgreen\")\n# legend\nlegend(\"topleft\", legend=c(\"ITM\", \"ATM\", \"Mixte\"), col=c(\"red\", \"blue\",'darkgreen'), lty=1:1, cex=0.8, title=\"Types d'option\")\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôobjectif est de comparer les r√©sultats de l‚ÄôAPF sur du filtre particulaire bootstrap en termes d‚Äôerreurs d‚Äôajustement, mesur√©es par les erreurs quadratiques moyennes (RMSE) de l‚Äôajustement de la variance et de l‚Äôajustement des prix des options, dans diff√©rents cas de donn√©es.\nFiltre bootstrap :\nLe filtre boostrap fonctionne de la mani√®re suivante :\n\n\n\nBootstrap filter\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre bootstrap de la mani√®re suivante :\n\nBootstrapParticleFilter &lt;- function(y, S, v, K, tau = 0.5, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, rho = 0.5, r = 0.05, dt = 1, h = 0.01) {\n  \n  # Param√®tres suppl√©mentaires\n  sigma_epsilon &lt;- sqrt(h)\n\n  # Param√®tres de la loi stationnaire de v\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation\n  n &lt;- length(y)\n  v_hat &lt;- numeric(n)\n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire bootstrap\n  for (t in 1:n) {\n    if (t == 1) {\n      # Initialisation des particules √† t = 0\n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      \n      # Poids initiaux bas√©s sur la densit√© de la loi stationnaire\n      w[t, ] &lt;- dgamma(v_particle[t, ], shape = alpha1, rate = 1/alpha2)\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # Estimation initiale\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    } else {\n      # √âtape de pr√©diction (propagation des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                    mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                    sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n      \n      #  Calcul du prix du Call pour chaque particule\n      C &lt;- numeric(M)\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # Calcul pour chaque particule avec le bon strike\n      for (i in 1:M) {\n          C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                       v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n\n      \n      # Mise √† jour des poids (vraisemblance observation-conditionnelle)\n      w[t, ] &lt;- dnorm(y[t], mean = C, sd = sqrt(sigma_epsilon))\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # R√©√©chantillonnage\n      index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n      v_particle[t, ] &lt;- v_particle[t, index]\n      \n      # Poids uniformes apr√®s resampling\n      w_normalized[t, ] &lt;- rep(1 / M, M)\n\n      # Estimation de la volatilit√© instantan√©e\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  return(v_hat)\n}\n\nFiltre APF :\nLe filtre particulaire auxilaire fonctionne de la mani√®re suivante :\n\n\n\nAPF\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre APF de la mani√®re suivante :\n\nAPF_Heston &lt;- function(y, S, v,K, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, \n                       lambda = -0.87, rho = 0.5, r = 0.05, tau = 0.5, dt = 1, h = 0.01) {\n  \n  # Param√®tres de la loi stationnaire de v\n  sigma_epsilon &lt;- sqrt(h)\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation des param√®tres\n  n &lt;- length(y)\n  \n  # Initialisation des vecteurs/matrices\n  v_hat &lt;- numeric(n)                   \n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  mu &lt;- matrix(nrow = n, ncol = M)        \n  py_mu &lt;- matrix(nrow = n, ncol = M) \n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire APF\n  for (t in 1:n) {\n    if (t == 1) {\n      \n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      w[t, ] &lt;- rep(1 / M, M)\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n      \n    } else {\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # 1 - Pr√©-s√©lection\n      mu[t, ] &lt;- abs(v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt)\n      \n      mean_pymu &lt;- numeric(M)\n      for (i in 1:M) {\n        mean_pymu[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                             v0 = mu[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      py_mu[t, ] &lt;- dnorm(y[t], mean = mean_pymu, sd = sigma_epsilon)\n\n      # Poids pour la pr√©-s√©lection\n      w[t-1, ] &lt;- py_mu[t, ] * w_normalized[t-1, ]\n      \n      # 2 - Resampling (√©chantillonnage)\n      index &lt;- sample(1:M, size = M, replace = TRUE, prob = w[t-1, ])\n\n      # Mise √† jour des particules\n      v_particle[t-1, ] &lt;- v_particle[t-1, index]\n      \n      # 3 - Propagation (√©volution des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                   mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                   sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n\n      # Mise √† jour des poids\n      C &lt;- numeric(M)\n      for (i in 1:M) {\n        C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                     v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      likelihood &lt;- dnorm(y[t], mean = C, sd = sigma_epsilon)\n      w[t, ] &lt;- likelihood / (py_mu[t, index] + 1e-12)  # Protection pour √©viter la division par z√©ro\n      \n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  # Retourner les r√©sultats\n  return(v_hat)\n}\n\nDiff√©rence entre le filtre bootstrap et le filtre APF :\nLe filtre bootstrap et le filtre APF sont deux m√©thodes de filtrage particulaire qui permettent d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. La principale diff√©rence entre ces deux m√©thodes r√©side dans la mani√®re dont elles mettent √† jour les poids des particules, et ainsi comme la distribution d‚Äôimportance q est construite. Par d√©finition, les poids sont d√©finis par : \\[\nw(x_t) = w(x_{t-1}) \\frac{p(y_t|x_t)p(x_t|x_{t-1})}{q(x_t|x_{t-1},y_t)},\n\\] o√π \\(p(y_t|x_t)\\) est la vraisemblance, \\(p(x_t|x_{t-1})\\) est la distribution de transition, et \\(q(x_t|x_{t-1},y_t)\\) est la distribution d‚Äôimportance.\nDans le cadre du filtre bootstrap, la distribution d‚Äôimportance est d√©finie comme suit : \\[\nq(x_t|x_{t-1},y_t) = p(x_t|x_{t-1}).\n\\] De ce fait, \\(w(x_t) = w(x_{t-1}) p(y_t|x_t)\\).Cela signifie que les poids sont mis √† jour en fonction de la distribution de transition et de la vraisemblance.\nDans le cadre du filtre APF, la distribution d‚Äôimportance se rapproche de la distribution optimale, vu dans la litt√©rature comme √©tant \\(q(x_t|x_{t-1},y_t) = p(x_t|x_{t-1},y_t)\\). De ce fait, les poids sont mis √† jour en fonction de la vraisemblance conditionnelle, ce qui permet d‚Äôam√©liorer la pr√©cision de l‚Äôestimation de l‚Äô√©tat cach√©, et donc de l‚Äôestimation de la volatilit√© dans notre cas."
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html#mod√®le-utilis√©",
    "href": "3A/proc_stochastique/APF_filter.html#mod√®le-utilis√©",
    "title": "APF filter",
    "section": "",
    "text": "Le filtre particulaire APF (Auxiliary Particle Filter) est un filtre particulaire qui utilise des particules auxiliaires pour estimer la densit√© de probabilit√© de l‚Äô√©tat cach√©. Il est utilis√© pour estimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. Dans cet article, nous allons √©tudier la performance du filtre APF en utilisant un exemple simple.\nComme tout filtre particulaire, il est necessaire de sp√©cifier la distribution a priori de l‚Äô√©tat, i.e.¬†\\(p(x_0)\\), la distribution de transition, i.e.¬†\\(p(v_t|v_{t-1})\\) et la vraisemblance, i.e.¬†\\(p(y_t|v_t)\\).\nDans le mod√®le de Heston sp√©cifi√© en (1), on consid√®re que : \\[\np(v_t|v_{t-1})=2c\\chi^2(2cx_k; 2q + 2; 2ce^{-\\kappa \\Delta} x_{k‚àí1}),\n\\]\n\\[p(v_1) = \\Gamma(v_1; a,b)\\] o√π \\(a = \\frac{2 \\kappa \\theta}{\\sigma^2}\\) et \\(b = \\frac{2 \\kappa}{\\sigma^2}\\),\net \\(p(y_t)|v_t) = N(0,h)\\).\n\\[\n\\begin{cases}\ndS_t = S_t \\left( rds + \\sqrt{v_t} dW_t^1 \\right) \\\\\ndv_t = \\kappa (\\theta - v_t) ds + \\sigma \\sqrt{v_t} dW_t^2 \\\\\ndW_t^1 dW_t^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\nPour tester la pertinence de l‚ÄôAPF, nous allons utiliser les param√®tres suivants \\(\\Phi = (\\theta = 0.03, \\kappa = 4, \\sigma = 0.4, \\kappa = -0.87, \\rho = 0.5)\\). Pour passer en temps discret et assurer la positivit√© de la volatilit√©, nous utilisosn le schema d‚Äôeuler (√† \\(|v_t|\\)) suivant :\n\\[\n\\begin{cases}\ny_t = C(t, \\theta, v_t, S_t, K, \\tau) + \\varepsilon_t, \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\theta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}\\Delta} (\\rho w_t^1 + \\sqrt{1-\\rho^2}w_t^2) \\right|, \\\\[10pt]\nS_t = S_{t-1} \\left(1 + \\mu \\Delta + \\sqrt{\\Delta v_t} w_t^1 \\right), \\\\[10pt]\n\\end{cases}\n\\]\no√π \\(\\varepsilon_t \\sim N(0,h=0.01)\\), et \\(w_t^1, w_t^2\\) sont des variables al√©atoires gaussiennes et ind√©pendantes.\n\nNous avons utilis√© un sch√©ma d‚Äôeuler modifi√© pour garantir la positivit√© de la volatilit√©. En effet, la volatilit√© doit √™tre positive dans le mod√®le de Heston, et le sch√©ma d‚Äôeuler standard peut produire des valeurs n√©gatives. En prenant la valeur absolue de la volatilit√© √† chaque √©tape, nous nous assurons que les valeurs restent positives. Il aurait √©t√© √©galement possible d‚Äôutiliser le sch√©ma d‚Äôeuler √† \\(ln(v_t)\\) (via le lemme de ito) pour garantir la positivit√© de la volatilit√©.\n\n\nrm(list=ls())\n\n################ Simulation de la trajectoire de St et vt ################ \nHeston_sim &lt;- function(N, kappa, theta, sigma, rho, v0, mu, tau, S0){\n  # N: Number of time steps\n  dt &lt;- tau / N  # Time step\n\n  # Store stock prices and volatilities\n  S &lt;- numeric(N+1)\n  v &lt;- numeric(N+1)\n  \n  S[1] &lt;- S0\n  v[1] &lt;- v0\n  \n  for (t in 1:N){\n    # Generate correlated Brownian motions\n    W1 &lt;- rnorm(1)\n    W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n    \n    # Euler discretization of variance process (ensure non-negativity)\n    v[t+1] &lt;- abs(v[t] + kappa * (theta - v[t]) * dt + sigma * sqrt(v[t] * dt) * W2)\n    \n    # Euler discretization of the stock price process \n    S[t+1] &lt;- S[t] * (1+ mu*dt + sqrt(v[t+1] * dt) * W1)\n  }\n  \n  return(list(v_t = v, S_t=S))\n}\n\nAvec les √©tapes 1 et 2, nous obtenons les trajectoires de volatilit√© instantan√©e et de prix d‚Äôaction suivantes :\n\nset.seed(123)\nN &lt;- 300\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nrho &lt;- 0.5\nv0 &lt;- 0.03\nmu &lt;- 0.1\ntau&lt;-1\nS0 &lt;- 100\n\nres &lt;- Heston_sim(N=N, kappa=kappa, theta=theta, sigma=sigma, rho=rho, v0=v0, mu=mu, tau=tau, S0=S0)\n\npar(mfrow=c(1,2))\nplot(res$v_t, type=\"l\", main=\"Processus de volatilit√© simul√©\", xlab = \"Time step\", ylab = \"Volatilit√©\") \nplot(res$S_t, type=\"l\", main=\"Processus de prix du sous-jacent simul√©\", xlab = \"Time step\", ylab = \"Prix\") \n\n\n\n\n\n\n\n\n\n\nLa proc√©dure de simulation est la suivante :\n\nTout d‚Äôabord, une trajectoire de 300 pas de temps de la variance instantan√©e sera simul√©e pour un pas de 1 jour, en commen√ßant par \\(v_0\\) = 0,03.\nConditionnellement √† cette trajectoire, une trajectoire correspondante du prix de l‚Äôaction sera alors g√©n√©r√©e.\nNous calculons, √† l‚Äôaide du mod√®le de Heston, les prix d‚Äôoptions bruit√©s pour trois types d‚Äôoptions diff√©rents : une option √† la monnaie (ATM), une option dans la monnaie (ITM) et une option mixte (50 % ITM / 50 % OTM).\n\nNous avons choisi trois types d‚Äôoptions afin d‚Äôobserver comment le filtre APF se comporte dans diff√©rentes conditions de march√© :\n\nOption √† la monnaie (ATM) :\n\nPrix d‚Äôexercice : K = 1 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ATM sont souvent utilis√©es pour l‚Äôestimation de la volatilit√© implicite, car elles sont les plus liquides et pr√©sentent un delta proche de 0.5, ce qui les rend sensibles aux variations du sous-jacent.\n\nOption dans la monnaie (ITM) :\n\nPrix d‚Äôexercice : K = 0.95 √ó S_t\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Les options ITM ont une valeur intrins√®que √©lev√©e et une volatilit√© implicite plus stable. Elles sont moins sensibles aux fluctuations imm√©diates du march√© mais permettent d‚Äô√©valuer l‚Äôimpact du filtre APF dans des conditions de faible variance du prix d‚Äôoption.\n\nOption mixte (50 % ITM / 50 % OTM) :\n\nPrix d‚Äôexercice : K = 117\nTemps avant l‚Äô√©ch√©ance : œÑ = 0.5 Cette approche permet de tester le filtre APF dans un sc√©nario r√©aliste de portefeuille d‚Äôoptions o√π des positions ITM et OTM sont combin√©es. L‚Äôobjectif est d‚Äôanalyser si le filtre reste stable lorsque l‚Äôon m√©lange des options avec des sensibilit√©s diff√©rentes aux mouvements du sous-jacent et aux variations de volatilit√©.\n\n\nPourquoi tester diff√©rentes configurations d‚Äôoptions ?\nL‚Äôobjectif de cette analyse est de v√©rifier comment le filtre APF se comporte en pr√©sence de conditions de march√© vari√©es :\n\nOptions ATM : impact fort de la volatilit√©, mais moins sujettes au risque de gamma.\nOptions ITM : faible sensibilit√© √† la volatilit√© implicite, mais risque de couverture plus limit√©.\nOptions mixtes : √©valuation de la robustesse du filtre lorsque plusieurs types d‚Äôoptions coexistent dans un m√™me portefeuille.\n\nCes tests permettent de comparer la pr√©cision du filtre en fonction de la position de l‚Äôoption par rapport au prix du sous-jacent.\n\n################ Simulation du prix des options ATM ################ \nsource(\"data/Heston_Call_Function.R\")\nset.seed(123)\n\nv_t &lt;- res$v_t \nS_t &lt;- res$S_t\nh &lt;- 0.01\n\n# Prix d'option K=1*S, tau = 0.5 =&gt; A la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 1\nATM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nATM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\n\nres$ATM &lt;- ATM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options ITM ################ \n# Prix d'option K=0.95*S, tau = 0.5 =&gt; Hors de la monnaie\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 0.95\n\nITM &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nITM[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K * S_t[i])\n}\nres$ITM &lt;- ITM + rnorm(1,mean=0,sd=sqrt(h))\n\n################ Simulation du prix des options mixte ################ \nsummary(S_t)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  99.49  104.36  117.23  119.24  128.88  156.89 \n\n# Prix d'option K=117, tau = 0.5  =&gt; moiti√© ITM et moiti√© OTM\nr &lt;- 5/100\ntau &lt;- 0.5\nK &lt;- 117\n\nMIXTE &lt;- numeric(N+1)\n\nfor(i in c(0:N+1)){\nMIXTE[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, v0 = v_t[i], r = r, tau = tau, S0 = S_t[i], K = K)\n}\nres$MIXTE &lt;- MIXTE + rnorm(1,mean=0,sd=sqrt(h))\n\n\n\nEn simulant le prix de ces options, nous constatons sans surprise que les options mixte ont un prix plus √©lev√© plus on se rapproche de la maturit√©. Cependant, les options ITM et ATM ont des prix plus stables, avec une l√©g√®re augmentation pour les options ITM.\n\nN &lt;- length(res$ITM)  \ndt &lt;- tau / N\ntime_axis &lt;- seq(0, tau, length.out = N)  # Axe des temps, de 0 √† tau\n\nplot(time_axis,res$ITM, type=\"l\", main = \"Evolution du prix des options \", ylim = c(0,50), col = \"red\", ylab = \"Prix\", xlab = \"Temps\")\nlines(time_axis,res$ATM, type = \"l\",col=\"blue\")\nlines(time_axis,res$MIXTE, type='l', col= \"darkgreen\")\n# legend\nlegend(\"topleft\", legend=c(\"ITM\", \"ATM\", \"Mixte\"), col=c(\"red\", \"blue\",'darkgreen'), lty=1:1, cex=0.8, title=\"Types d'option\")\n\n\n\n\n\n\n\n\n\n\n\nL‚Äôobjectif est de comparer les r√©sultats de l‚ÄôAPF sur du filtre particulaire bootstrap en termes d‚Äôerreurs d‚Äôajustement, mesur√©es par les erreurs quadratiques moyennes (RMSE) de l‚Äôajustement de la variance et de l‚Äôajustement des prix des options, dans diff√©rents cas de donn√©es.\nFiltre bootstrap :\nLe filtre boostrap fonctionne de la mani√®re suivante :\n\n\n\nBootstrap filter\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre bootstrap de la mani√®re suivante :\n\nBootstrapParticleFilter &lt;- function(y, S, v, K, tau = 0.5, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, rho = 0.5, r = 0.05, dt = 1, h = 0.01) {\n  \n  # Param√®tres suppl√©mentaires\n  sigma_epsilon &lt;- sqrt(h)\n\n  # Param√®tres de la loi stationnaire de v\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation\n  n &lt;- length(y)\n  v_hat &lt;- numeric(n)\n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire bootstrap\n  for (t in 1:n) {\n    if (t == 1) {\n      # Initialisation des particules √† t = 0\n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      \n      # Poids initiaux bas√©s sur la densit√© de la loi stationnaire\n      w[t, ] &lt;- dgamma(v_particle[t, ], shape = alpha1, rate = 1/alpha2)\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # Estimation initiale\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    } else {\n      # √âtape de pr√©diction (propagation des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                    mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                    sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n      \n      #  Calcul du prix du Call pour chaque particule\n      C &lt;- numeric(M)\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # Calcul pour chaque particule avec le bon strike\n      for (i in 1:M) {\n          C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                       v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n\n      \n      # Mise √† jour des poids (vraisemblance observation-conditionnelle)\n      w[t, ] &lt;- dnorm(y[t], mean = C, sd = sqrt(sigma_epsilon))\n      \n      # Normalisation des poids\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      \n      # R√©√©chantillonnage\n      index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n      v_particle[t, ] &lt;- v_particle[t, index]\n      \n      # Poids uniformes apr√®s resampling\n      w_normalized[t, ] &lt;- rep(1 / M, M)\n\n      # Estimation de la volatilit√© instantan√©e\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  return(v_hat)\n}\n\nFiltre APF :\nLe filtre particulaire auxilaire fonctionne de la mani√®re suivante :\n\n\n\nAPF\n\n\nDans le cadre du mod√®le de heston, nous avons impl√©ment√© le filtre APF de la mani√®re suivante :\n\nAPF_Heston &lt;- function(y, S, v,K, M = 200, theta = 0.03, kappa = 4, sigma = 0.4, \n                       lambda = -0.87, rho = 0.5, r = 0.05, tau = 0.5, dt = 1, h = 0.01) {\n  \n  # Param√®tres de la loi stationnaire de v\n  sigma_epsilon &lt;- sqrt(h)\n  alpha1 &lt;- (2 * kappa * theta) / (sigma^2)\n  alpha2 &lt;- (sigma^2) / (2 * kappa)\n\n  # Initialisation des param√®tres\n  n &lt;- length(y)\n  \n  # Initialisation des vecteurs/matrices\n  v_hat &lt;- numeric(n)                   \n  v_particle &lt;- matrix(nrow = n, ncol = M)\n  mu &lt;- matrix(nrow = n, ncol = M)        \n  py_mu &lt;- matrix(nrow = n, ncol = M) \n  w &lt;- matrix(nrow = n, ncol = M)\n  w_normalized &lt;- matrix(nrow = n, ncol = M)\n\n  # Filtre particulaire APF\n  for (t in 1:n) {\n    if (t == 1) {\n      \n      v_particle[t, ] &lt;- rgamma(M, shape = alpha1, rate = 1/alpha2)\n      w[t, ] &lt;- rep(1 / M, M)\n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n      \n    } else {\n      \n      # Si K est un unique scalaire (strike constant), on le conserve\n      # Si K est un vecteur, on prend K[t] comme strike au temps t\n      if (length(K) == 1) {\n          Kt &lt;- K  # Strike constant\n      } else {\n          Kt &lt;- K[t]  # Strike sp√©cifique au temps t\n      }\n      \n      # 1 - Pr√©-s√©lection\n      mu[t, ] &lt;- abs(v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt)\n      \n      mean_pymu &lt;- numeric(M)\n      for (i in 1:M) {\n        mean_pymu[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                             v0 = mu[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      py_mu[t, ] &lt;- dnorm(y[t], mean = mean_pymu, sd = sigma_epsilon)\n\n      # Poids pour la pr√©-s√©lection\n      w[t-1, ] &lt;- py_mu[t, ] * w_normalized[t-1, ]\n      \n      # 2 - Resampling (√©chantillonnage)\n      index &lt;- sample(1:M, size = M, replace = TRUE, prob = w[t-1, ])\n\n      # Mise √† jour des particules\n      v_particle[t-1, ] &lt;- v_particle[t-1, index]\n      \n      # 3 - Propagation (√©volution des particules)\n      v_particle[t, ] &lt;- abs(rnorm(M, \n                                   mean = v_particle[t-1, ] + kappa * (theta - v_particle[t-1, ]) * dt, \n                                   sd = sigma * sqrt(v_particle[t-1, ] * dt)))\n\n      # Mise √† jour des poids\n      C &lt;- numeric(M)\n      for (i in 1:M) {\n        C[i] &lt;- HestonCallClosedForm(lambda = kappa, vbar = theta, eta = sigma, rho = rho, \n                                     v0 = v_particle[t, i], r = r, tau = tau, S0 = S[t], K = Kt)\n      }\n      likelihood &lt;- dnorm(y[t], mean = C, sd = sigma_epsilon)\n      w[t, ] &lt;- likelihood / (py_mu[t, index] + 1e-12)  # Protection pour √©viter la division par z√©ro\n      \n      w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n      v_hat[t] &lt;- sum(w_normalized[t, ] * v_particle[t, ])\n    }\n  }\n\n  # Retourner les r√©sultats\n  return(v_hat)\n}\n\nDiff√©rence entre le filtre bootstrap et le filtre APF :\nLe filtre bootstrap et le filtre APF sont deux m√©thodes de filtrage particulaire qui permettent d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique non lin√©aire. La principale diff√©rence entre ces deux m√©thodes r√©side dans la mani√®re dont elles mettent √† jour les poids des particules, et ainsi comme la distribution d‚Äôimportance q est construite. Par d√©finition, les poids sont d√©finis par : \\[\nw(x_t) = w(x_{t-1}) \\frac{p(y_t|x_t)p(x_t|x_{t-1})}{q(x_t|x_{t-1},y_t)},\n\\] o√π \\(p(y_t|x_t)\\) est la vraisemblance, \\(p(x_t|x_{t-1})\\) est la distribution de transition, et \\(q(x_t|x_{t-1},y_t)\\) est la distribution d‚Äôimportance.\nDans le cadre du filtre bootstrap, la distribution d‚Äôimportance est d√©finie comme suit : \\[\nq(x_t|x_{t-1},y_t) = p(x_t|x_{t-1}).\n\\] De ce fait, \\(w(x_t) = w(x_{t-1}) p(y_t|x_t)\\).Cela signifie que les poids sont mis √† jour en fonction de la distribution de transition et de la vraisemblance.\nDans le cadre du filtre APF, la distribution d‚Äôimportance se rapproche de la distribution optimale, vu dans la litt√©rature comme √©tant \\(q(x_t|x_{t-1},y_t) = p(x_t|x_{t-1},y_t)\\). De ce fait, les poids sont mis √† jour en fonction de la vraisemblance conditionnelle, ce qui permet d‚Äôam√©liorer la pr√©cision de l‚Äôestimation de l‚Äô√©tat cach√©, et donc de l‚Äôestimation de la volatilit√© dans notre cas."
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html#option-√†-la-monnaie-atm",
    "href": "3A/proc_stochastique/APF_filter.html#option-√†-la-monnaie-atm",
    "title": "APF filter",
    "section": "2.1 Option √† la monnaie (ATM)",
    "text": "2.1 Option √† la monnaie (ATM)\n\n2.1.1 Filtre bootstrap\n\n##################### D√©finition des param√®tres #####################\n# D√©finition des param√®tres phi\ntheta &lt;- 0.03\nkappa &lt;- 4\nsigma &lt;- 0.4\nlambda &lt;- -0.87 # Prime de risque\nrho &lt;- 0.5\n\n# Param√®tres de Heston \nh &lt;- 0.01\nr &lt;- 0.05\n\n# Prix de call (y), du sous-jacent (S), et volatilit√© instantan√©e (v)\ny &lt;- res$ATM\nS &lt;- res$S_t\nv&lt;- res$v_t\n\n# Initialisation des param√®tres\nn &lt;- length(y)  # Nombre d'observations\nM &lt;- 200      # Nombre de particules\ndt &lt;- 1\n\n\nset.seed(123)\n\n# Param√®tres de l'option \nK &lt;- 1 * S\ntau&lt;- 0.5\n\n# Exemple d'appel avec tes donn√©es \"res\"\net_boot_atm &lt;- system.time({\nboot_atm &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_atm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_atm)\n\n   user  system elapsed \n 11.832   0.597  12.488 \n\nrmse_boot_atm &lt;- sqrt(mean((boot_atm - v)^2))\ncat(\"RMSE :\", rmse_boot_atm)\n\nRMSE : 0.001402773\n\n\n\n\n2.1.2 Filtre APF\n\nset.seed(123)\net_apf_atm &lt;- system.time({\napf_atm &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_atm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_atm)\n\n   user  system elapsed \n 23.626   1.113  24.874 \n\nrmse_apf_atm &lt;- sqrt(mean((apf_atm - v)^2))\ncat(\"RMSE :\", rmse_apf_atm)\n\nRMSE : 0.001889304"
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html#option-√†-la-monnaie-itm",
    "href": "3A/proc_stochastique/APF_filter.html#option-√†-la-monnaie-itm",
    "title": "APF filter",
    "section": "2.2 Option √† la monnaie (ITM)",
    "text": "2.2 Option √† la monnaie (ITM)\n\n2.2.1 Filtre bootstrap\n\nset.seed(123)\ny &lt;- res$ITM\nK &lt;- 0.95 * S\n\net_boot_itm &lt;- system.time({\nboot_itm &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_itm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_itm)\n\n   user  system elapsed \n 11.435   0.521  12.012 \n\nrmse_boot_itm &lt;- sqrt(mean((boot_itm - v)^2))\ncat(\"RMSE :\", rmse_boot_itm)\n\nRMSE : 0.00152023\n\n\n\n\n2.2.2 APF\n\net_apf_itm &lt;- system.time({\napf_itm &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_itm, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_itm)\n\n   user  system elapsed \n 22.633   1.017  23.794 \n\nrmse_apf_itm &lt;- sqrt(mean((apf_itm - v)^2))\ncat(\"RMSE :\", rmse_apf_itm)\n\nRMSE : 0.001850903"
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html#options-mixtes",
    "href": "3A/proc_stochastique/APF_filter.html#options-mixtes",
    "title": "APF filter",
    "section": "2.3 Options mixtes",
    "text": "2.3 Options mixtes\n\n2.3.1 Filtre bootstrap\n\nset.seed(123)\ny &lt;- res$MIXTE\nK &lt;- 117\n\net_boot_mixte &lt;- system.time({\nboot_mixte &lt;- BootstrapParticleFilter(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre bootstrap\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(boot_mixte, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_boot_mixte)\n\n   user  system elapsed \n 11.923   0.444  12.371 \n\nrmse_boot_mixte&lt;- sqrt(mean((boot_mixte - v)^2))\ncat(\"RMSE :\", rmse_boot_mixte)\n\nRMSE : 0.01595637\n\n\n\n\n2.3.2 APF\n\net_apf_mixte &lt;- system.time({\napf_mixte &lt;- APF_Heston(y=y, S=S, v=v, K=K, tau = tau, M = M, theta = theta, kappa = kappa, sigma = sigma,  rho = rho, r = r, dt = dt, h = h)\n})\n\n\nplot(v, type = \"l\", col = \"black\",  main = \"Estimation de la volatilit√© avec un filtre APF\",xlab = \"Temps\", ylab = \"Volatilit√©\")\nlines(apf_mixte, type = \"l\", col = \"red\")\nlegend(\"topright\", legend = c(\"Vol. obs.\", \"Vol. est.\"), col = c(\"black\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\n\nprint(et_apf_mixte)\n\n   user  system elapsed \n 23.635   0.869  24.509 \n\nrmse_apf_mixte &lt;- sqrt(mean((apf_mixte - v)^2))\ncat(\"RMSE :\", rmse_apf_mixte)\n\nRMSE : 0.01475175"
  },
  {
    "objectID": "3A/proc_stochastique/APF_filter.html#comparaison-des-performances",
    "href": "3A/proc_stochastique/APF_filter.html#comparaison-des-performances",
    "title": "APF filter",
    "section": "2.4 Comparaison des performances",
    "text": "2.4 Comparaison des performances\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Cr√©ation du data.frame avec les colonnes demand√©es\ncomp_perf &lt;- data.frame(\n    M√©thode = c(\"APF\", \"APF\", \"APF\", \"BOOT\", \"BOOT\", \"BOOT\"),\n    Type_Option = c(\"ATM\", \"ITM\", \"MIXTE\", \"ATM\", \"ITM\", \"MIXTE\"),\n    RMSE = c(rmse_apf_atm, rmse_apf_itm, rmse_apf_mixte, rmse_boot_atm, rmse_boot_itm, rmse_boot_mixte)\n)\n\n# Conversion : M√©thode comme colonne\ncomp_perf_wide &lt;- pivot_wider(\n    comp_perf,\n    names_from = M√©thode,   # Ce qui devient les colonnes\n    values_from = RMSE      # Ce qui remplit les cases\n)\n\ncomp_perf_wide &lt;- bind_rows(\n    comp_perf_wide,\n    data.frame(\n        Type_Option = \"Moyenne\",\n        APF = mean(c(rmse_apf_atm, rmse_apf_itm, rmse_apf_mixte), na.rm = TRUE),\n        BOOT = mean(c(rmse_boot_atm, rmse_boot_itm, rmse_boot_mixte), na.rm = TRUE)\n    )\n)\n# Affichage du r√©sultat\ncomp_perf_wide\n\n\n\n\n\nType_Option\nAPF\nBOOT\n\n\n\n\nATM\n0.0018893\n0.0014028\n\n\nITM\n0.0018509\n0.0015202\n\n\nMIXTE\n0.0147518\n0.0159564\n\n\nMoyenne\n0.0061640\n0.0062931\n\n\n\n\n\n\n\n# Comparaison des temps d'ex√©cution\ncomp_temps &lt;- data.frame(\n    M√©thode = c(\"APF\", \"APF\", \"APF\", \"BOOT\", \"BOOT\", \"BOOT\"),\n    Type_Option = c(\"ATM\", \"ITM\", \"MIXTE\", \"ATM\", \"ITM\", \"MIXTE\"),\n    Temps = c(et_apf_atm[3], et_apf_itm[3], et_apf_mixte[3], et_boot_atm[3], et_boot_itm[3], et_boot_mixte[3])\n)\n\n# Conversion : M√©thode comme colonne\ncomp_temps_wide &lt;- pivot_wider(\n    comp_temps,\n    names_from = M√©thode,   # Ce qui devient les colonnes\n    values_from = Temps      # Ce qui remplit les cases\n)\n\ncomp_temps_wide &lt;- bind_rows(\n    comp_temps_wide,\n    data.frame(\n        Type_Option = \"Moyenne\",\n        APF = mean(c(et_apf_atm[3], et_apf_itm[3], et_apf_mixte[3]), na.rm = TRUE),\n        BOOT = mean(c(et_boot_atm[3], et_boot_itm[3], et_boot_mixte[3]), na.rm = TRUE)\n    )\n)\n# Affichage du r√©sultat\ncomp_temps_wide\n\n\n\n\n\nType_Option\nAPF\nBOOT\n\n\n\n\nATM\n24.87400\n12.48800\n\n\nITM\n23.79400\n12.01200\n\n\nMIXTE\n24.50900\n12.37100\n\n\nMoyenne\n24.39233\n12.29033\n\n\n\n\n\n\nEn comparant les performances de l‚ÄôAPF √† celles du filtre bootstrap, nous constatons que les deux filtres ont des performances assez similaires en termes d‚Äôerreurs d‚Äôajustement (RMSE) pour les options ATM et ITM. Cependant, l‚ÄôAPF semble √™tre plus efficace pour les options mixtes, avec des erreurs d‚Äôajustement plus faibles que le filtre bootstrap. Cependant, en termes de temps d‚Äôex√©cution, le filtre bootstrap est plus rapide que l‚ÄôAPF pour les trois types d‚Äôoptions. En moyenne, l‚ÄôAPF est plus lent que le filtre bootstrap, mais offre une meilleure pr√©cision tout option confondue.\nPou connaitre la robustesse de ces estimations, nous avons fait du bootstrap pour estimer la distribution des RMSE, et ainsi obtenir des intervalles de confiance pour ces estimations, de m√™me que des p-valeurs. Le code ayant servi √† la simulation est disponible ici.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggpubr)\n\n# Charger les r√©sultats du bootstrap\nres_ATM &lt;- readRDS(\"results_sim/res_ATM_min.rds\")\nres_ITM &lt;- readRDS(\"results_sim/res_ITM_min.rds\")\nres_MIXTE &lt;- readRDS(\"results_sim/res_MIXTE_min.rds\")\n\n# Extraire les RMSE\nrmse_bootstrap_atm &lt;- res_ATM$t[, 1]\nrmse_apf_atm_b &lt;- res_ATM$t[, 2]\n\nrmse_bootstrap_itm &lt;- res_ITM$t[, 1]\nrmse_apf_itm_b &lt;- res_ITM$t[, 2]\n\nrmse_bootstrap_mixte &lt;- res_MIXTE$t[, 1]\nrmse_apf_mixte_b &lt;- res_MIXTE$t[, 2]\n\n# Cr√©er un dataframe avec les valeurs RMSE pour les trois sc√©narios\ndf_rmse_boxplot &lt;- data.frame(\n  M√©thode = rep(c(\"Bootstrap Filter\", \"APF-Heston\"), each = c(length(rmse_bootstrap_atm), length(rmse_apf_atm_b), length(rmse_bootstrap_itm), length(rmse_apf_itm_b),length(rmse_bootstrap_mixte), length(rmse_apf_mixte_b))),\n  RMSE = c(rmse_bootstrap_atm, rmse_apf_atm_b,\n           rmse_bootstrap_itm, rmse_apf_itm_b,\n           rmse_bootstrap_mixte, rmse_apf_mixte_b),\n  Sc√©nario = rep(c(\"ATM\", \"ATM\", \"ITM\", \"ITM\", \"MIXTE\", \"MIXTE\"), each = c(length(rmse_bootstrap_atm), length(rmse_apf_atm_b),\nlength(rmse_bootstrap_itm), length(rmse_apf_itm_b),\nlength(rmse_bootstrap_mixte), length(rmse_apf_mixte_b)))\n)\n\nWarning in rep(c(\"Bootstrap Filter\", \"APF-Heston\"), each =\nc(length(rmse_bootstrap_atm), : first element used of 'each' argument\n\n\nWarning in rep(c(\"ATM\", \"ATM\", \"ITM\", \"ITM\", \"MIXTE\", \"MIXTE\"), each =\nc(length(rmse_bootstrap_atm), : first element used of 'each' argument\n\n# Calcul des p-valeurs\n\n# kruskall wallis\ntest_atm &lt;- kruskal.test(list(rmse_bootstrap_atm, rmse_apf_atm_b))$p.value\ntest_itm &lt;- kruskal.test(list(rmse_bootstrap_itm, rmse_apf_itm_b))$p.value\ntest_mixte &lt;- kruskal.test(list(rmse_bootstrap_mixte, rmse_apf_mixte_b))$p.value\n\n# test_atm &lt;- t.test(rmse_bootstrap_atm, rmse_apf_atm_b)$p.value\n# test_itm &lt;- t.test(rmse_bootstrap_itm, rmse_apf_itm_b)$p.value\n# test_mixte &lt;- t.test(rmse_bootstrap_mixte, rmse_apf_mixte_b)$p.value\n\n# Cr√©ation du boxplot avec facet_wrap et ajout des p-valeurs en caption\nggplot(df_rmse_boxplot, aes(x = M√©thode, y = RMSE, fill = M√©thode)) +\n  geom_boxplot(alpha = 0.6) +\n  facet_wrap(~Sc√©nario, scales = \"free_y\") +\n  labs(title = \"Comparaison des RMSE (Bootstrap vs APF) selon les sc√©narios\",\n       x = \"M√©thode de Filtrage\",\n       y = \"RMSE\",\n       caption = paste(\"p-values t-test: ATM =\", formatC(test_atm, format = \"e\", digits = 2),\n                       \"| ITM =\", formatC(test_itm, format = \"e\", digits = 2),\n                       \"| MIXTE =\", formatC(test_mixte, format = \"e\", digits = 2))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n# Concat√©ner toutes les RMSE\ntotal_rmse_bootstrap &lt;- c(rmse_bootstrap_atm, rmse_bootstrap_itm, rmse_bootstrap_mixte)\ntotal_rmse_apf &lt;- c(rmse_apf_atm_b, rmse_apf_itm_b, rmse_apf_mixte_b)\n\n# Kruskal wallis test\nglobal_pval &lt;- kruskal.test(list(total_rmse_bootstrap, total_rmse_apf))$p.value\n\n# global_pval &lt;- t.test(total_rmse_bootstrap, total_rmse_apf)$p.value\n\n# Boxplot global\nggboxplot(df_rmse_boxplot, x = \"M√©thode\", y = \"RMSE\", fill = \"M√©thode\",\n          ylab = \"RMSE\", xlab = \"M√©thode de Filtrage\",\n          title = \"Comparaison des RMSE (Bootstrap vs APF) pour les trois sc√©narios\") +\n  labs(caption = paste(\"p-values: Global =\", formatC(global_pval, format = \"e\", digits = 2))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nApproche pvalue :\n\nrmse_diff_atm &lt;- rmse_bootstrap_atm - rmse_apf_atm_b\nrmse_diff_atm_obs &lt;- rmse_boot_atm - rmse_apf_atm\n\nmean(rmse_diff_atm_obs &gt; rmse_diff_atm)\n\n[1] 0.84\n\nrmse_diff_itm &lt;- rmse_bootstrap_itm - rmse_apf_itm_b\nrmse_diff_itm_obs &lt;- rmse_boot_itm - rmse_apf_itm\n\nmean(rmse_diff_itm_obs &gt; rmse_diff_itm)\n\n[1] 0.38\n\nrmse_diff_mixte &lt;- rmse_bootstrap_mixte - rmse_apf_mixte_b\nrmse_diff_mixte_obs &lt;- rmse_boot_mixte - rmse_apf_mixte\n\nmean(rmse_diff_mixte_obs &gt; rmse_diff_mixte)\n\n[1] 1\n\n\n\nConclusion : L‚ÄôAPF est une m√©thode plus pr√©cise que le filtre bootstrap pour estimer la volatilit√© instantan√©e dans le cadre d‚Äôun mod√®le de Heston o√π le prix est bruit√© \\(h=0.01\\), mais elle est √©galement plus lente en termes de temps d‚Äôex√©cution. Le choix entre ces deux m√©thodes d√©pendra donc des besoins sp√©cifiques de l‚Äôanalyse, en fonction de la pr√©cision et de la vitesse d‚Äôex√©cution requises.\nPour tester la robustesse de ces estimations, il aurait fallu faire du bootstrap afin d‚Äôestimer la distribution des RMSE, et ainsi obtenir des intervalles de confiance pour ces estimations.\nIl aurait √©t√© interessant de tester les filtres dans d‚Äôautres cas de figures."
  },
  {
    "objectID": "3A/gestion_actifs/TP-2.html",
    "href": "3A/gestion_actifs/TP-2.html",
    "title": "Asset management : Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "",
    "text": "Le risque de liquidit√© est l‚Äôune des quatre grandes cat√©gories de risques auxquels une banque peut √™tre expos√©e. Il concerne √† la fois la liquidit√© du march√© et le risque op√©rationnel de financement.\nAinsi, le risque de liquidit√© est crucial dans la gestion d‚Äôun portefeuille, notamment en p√©riode de tensions sur les march√©s. Si un investisseur doit liquider un actif, il peut √™tre contraint de le vendre √† un prix inf√©rieur √† sa valeur fondamentale, entra√Ænant ainsi une perte importante.\nPour √©valuer ce risque, plusieurs indicateurs sont utilis√©s :\nDans le cadre de notre √©tude, nous nous concentrons sur le profil d‚Äô√©coulement d‚Äôun portefeuille compos√© de 10 actifs, et ce dans les quatre sc√©narios suivants :\n1. Conditions normales avec d√©formation : On consid√®re les volumes moyens de march√©, que l‚Äôon d√©forme pour obtenir les quantit√©s effectivement liquid√©es.\nLa liquidation d√©bute avec les actifs les plus liquides et se termine avec les plus illiquides.\nCe processus entra√Æne une d√©formation du portefeuille :\nles premiers investisseurs r√©cup√®rent une part plus liquide, tandis que ceux qui restent se retrouvent avec des actifs plus illiquides.\n2. Conditions normales sans d√©formation : Dans ce cas, on cherche √† pr√©server l‚Äô√©quilibre du portefeuille, afin de ne pas p√©naliser les investisseurs restants.\nLa liquidation est r√©partie uniform√©ment, sans privil√©gier les actifs les plus liquides.\n3. Conditions stress√©es avec d√©formation : Les march√©s sont en tension, les volumes chutent drastiquement.\nOn liquide en priorit√© les actifs les plus liquides, ce qui accentue l‚Äôilliquidit√© r√©siduelle du portefeuille.\n4. Conditions stress√©es sans d√©formation : M√™me en p√©riode de stress, on cherche √† conserver une proportionnalit√© √©quitable dans la liquidation, pour √©viter une concentration d‚Äôactifs illiquides pour les investisseurs restants.\n√âtapes de mise en ≈ìuvre\nPour chaque sc√©nario, on proc√®de selon les √©tapes suivantes :\nNous utiliserons un portefeuille fictif de 10 actifs du CAC, √† savoir :"
  },
  {
    "objectID": "3A/gestion_actifs/TP-2.html#pr√©sence-de-d√©formation",
    "href": "3A/gestion_actifs/TP-2.html#pr√©sence-de-d√©formation",
    "title": "Asset management : Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Pr√©sence de d√©formation",
    "text": "Pr√©sence de d√©formation\nLa d√©formation dans le cadre de la liquidation d‚Äôun portefeuille d‚Äôactifs fait r√©f√©rence √† la mani√®re dont les quantit√©s liquid√©es sont r√©parties entre les diff√©rents actifs du portefeuille. En effet, lorsque l‚Äôon liquide un portefeuille, on ne peut pas simplement vendre tous les actifs en m√™me temps, compte tenu de leur liquidit√© respective. La d√©formation se produit lorsque l‚Äôon liquide d‚Äôabord les actifs les plus liquides, ce qui entra√Æne une concentration d‚Äôactifs illiquides dans le portefeuille restant. Cela peut avoir un impact significatif sur la valeur du portefeuille et sur le risque de liquidit√©.\n\nSous conditions normales avec d√©formation (waterfall liquidation)\nPour conna√Ætre la quantit√© d‚Äôactifs liquid√©s chaque jour, on fait l‚Äôhypoth√®se que les liquidations futures se feront aux prix observ√©s aujourd‚Äôhui. En pratique, ce que l‚Äôon peut r√©ellement liquider en une journ√©e correspond √† la quantit√© que l‚Äôon peut vendre sans impacter le prix, c‚Äôest-√†-dire min(quantit√© liquidable en 1 jour, quantit√© restant dans le portefeuille).\nSur cette base, on peut calculer la valeur de march√© du portefeuille, aussi bien au moment initial qu‚Äô√† chaque jour de liquidation. Cette valeur est g√©n√©ralement exprim√©e en pourcentage de l‚Äôencours total.\nOn peut ensuite calculer le cumul des pourcentages liquid√©s au fil des jours. Cette courbe cumulative repr√©sente ce que l‚Äôon appelle le profil d‚Äô√©coulement du portefeuille.\nDans le code ci-dessous, nous pr√©sentons les quantit√©s liquid√©es par jour pour chaque actif.\n\nüí° Par convention, le jour 0 correspond √† la quantit√© initialement d√©tenue dans le portefeuille, avant toute op√©ration de liquidation.\n\n\n\nShow the code\n#---------------------------#\n# Liquidation du portefeuille\n#---------------------------#\n\nADV[\"Quantity liquidated\"] = 0  # Initialement, rien n'est liquid√©\n\n# Au jour 0, on a liquid√© 0. La colonne 0 sert de quantit√© initiale\nquantity_liquidated_per_day = [ADV[\"Quantity\"]]\n\nwhile (ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]).sum() &gt; 0 : # Tant qu'il y a des liquidations √† faire\n    # Calculer la quantit√© liquide au jour i\n    liquidated_today = np.minimum(ADV[\"Quantity in 1day\"], ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"])\n    \n    # Mettre √† jour les quantit√©s liquid√©es dans le DataFrame\n    ADV[\"Quantity liquidated\"] += liquidated_today\n    \n    # Stocker les quantit√©s liquid√©es ce jour dans une liste\n    quantity_liquidated_per_day.append(liquidated_today)\n\nliquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\nliquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n\nliquidation_df\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n1909146.0\n365490.0\n365490.0\n365490.0\n365490.0\n365490.0\n81696.0\n0.0\n0.0\n\n\nGLE.PA\n1512657.0\n704858.0\n704858.0\n102941.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nHO.PA\n132823.0\n78067.0\n54756.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nENGI.PA\n5021391.0\n1214404.0\n1214404.0\n1214404.0\n1214404.0\n163775.0\n0.0\n0.0\n0.0\n\n\nCAP.PA\n548385.0\n101628.0\n101628.0\n101628.0\n101628.0\n101628.0\n40245.0\n0.0\n0.0\n\n\nCA.PA\n1909319.0\n601683.0\n601683.0\n601683.0\n104270.0\n0.0\n0.0\n0.0\n0.0\n\n\nORA.PA\n11300313.0\n1536260.0\n1536260.0\n1536260.0\n1536260.0\n1536260.0\n1536260.0\n1536260.0\n546493.0\n\n\nAC.PA\n667882.0\n130034.0\n130034.0\n130034.0\n130034.0\n130034.0\n17712.0\n0.0\n0.0\n\n\nOR.PA\n331164.0\n91812.0\n91812.0\n91812.0\n55728.0\n0.0\n0.0\n0.0\n0.0\n\n\nACA.PA\n4083121.0\n1388400.0\n1388400.0\n1306321.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nShow the code\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=5)\nprice_data = get_data(start_date, end_date, index, assets_ticker, column=\"Close\")\n\nprice_data[\"portfolio_data\"].head()\nprice_dict = price_data[\"portfolio_data\"].iloc[-1].to_dict()\n\nprint(\"=\"*50)\nprint(\"Prix des actifs √† la date du jour\")\nprint(\"=\"*50)\nfor ticker, price in price_dict.items():\n    print(f\"Le prix de l'actif {selected_assets[ticker]} est de {price} ‚Ç¨\")\n\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n==================================================\nPrix des actifs √† la date du jour\n==================================================\nLe prix de l'actif Accor est de 42.790000915527344 ‚Ç¨\nLe prix de l'actif Cr√©dit agricole est de 16.94499969482422 ‚Ç¨\nLe prix de l'actif Carrefour est de 13.255000114440918 ‚Ç¨\nLe prix de l'actif Capgemini est de 145.3000030517578 ‚Ç¨\nLe prix de l'actif Engie est de 18.059999465942383 ‚Ç¨\nLe prix de l'actif Soci√©t√© g√©n√©rale est de 42.540000915527344 ‚Ç¨\nLe prix de l'actif Thales est de 246.60000610351562 ‚Ç¨\nLe prix de l'actif L'Oreal est de 345.1000061035156 ‚Ç¨\nLe prix de l'actif Orange est de 11.850000381469727 ‚Ç¨\nLe prix de l'actif Sanofi est de 103.4000015258789 ‚Ç¨\n\n\n\n\n\n\n\nShow the code\n# Valeur liquide des actions par jour de liquidation\nmarket_value =[\n    price_dict[ticker] * liquidation_df.loc[ticker]\n    for ticker in selected_assets\n]\n\nmarket_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\nmarket_value\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n1.974057e+08\n3.779167e+07\n3.779167e+07\n3.779167e+07\n3.779167e+07\n3.779167e+07\n8.447367e+06\n0.000000e+00\n0.000000e+00\n\n\nGLE.PA\n6.434843e+07\n2.998466e+07\n2.998466e+07\n4.379110e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nHO.PA\n3.275415e+07\n1.925132e+07\n1.350283e+07\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nENGI.PA\n9.068632e+07\n2.193214e+07\n2.193214e+07\n2.193214e+07\n2.193214e+07\n2.957776e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nCAP.PA\n7.968034e+07\n1.476655e+07\n1.476655e+07\n1.476655e+07\n1.476655e+07\n1.476655e+07\n5.847599e+06\n0.000000e+00\n0.000000e+00\n\n\nCA.PA\n2.530802e+07\n7.975308e+06\n7.975308e+06\n7.975308e+06\n1.382099e+06\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nORA.PA\n1.339087e+08\n1.820468e+07\n1.820468e+07\n1.820468e+07\n1.820468e+07\n1.820468e+07\n1.820468e+07\n1.820468e+07\n6.475942e+06\n\n\nAC.PA\n2.857867e+07\n5.564155e+06\n5.564155e+06\n5.564155e+06\n5.564155e+06\n5.564155e+06\n7.578965e+05\n0.000000e+00\n0.000000e+00\n\n\nOR.PA\n1.142847e+08\n3.168432e+07\n3.168432e+07\n3.168432e+07\n1.923173e+07\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\nACA.PA\n6.918848e+07\n2.352644e+07\n2.352644e+07\n2.213561e+07\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n\n\n\n\n\n\n\nShow the code\n# Calcul de la valeur de march√© initiale et totale\nmarket_value_0 = market_value.iloc[:, 0]\ntotal_market_value_0 = market_value_0.sum()\n\nprint(f\" La valeur de march√© initiale est de {total_market_value_0}‚Ç¨\")\n\n# Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\ncumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\ncumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\ncumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\ncumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\nweights = {}\nfor ticker in assets_ticker :\n    weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\nweights = pd.DataFrame(weights).T\nweights.head()\n\n\n La valeur de march√© initiale est de 836143533.8764086‚Ç¨\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSAN.PA\n0.236091\n0.255194\n0.289688\n0.328122\n0.336963\n0.145800\n0.0\n0.0\nNaN\n\n\nGLE.PA\n0.076959\n0.054941\n0.010413\n0.000000\n0.000000\n0.000000\n0.0\n0.0\nNaN\n\n\nHO.PA\n0.039173\n0.021589\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\nNaN\n\n\nENGI.PA\n0.108458\n0.109925\n0.111341\n0.097190\n0.021555\n0.000000\n0.0\n0.0\nNaN\n\n\nCAP.PA\n0.095295\n0.103785\n0.119248\n0.138154\n0.150224\n0.100928\n0.0\n0.0\nNaN\n\n\n\n\n\n\n\n\n\nShow the code\nplt.figure(figsize=(10, 4))\n\n# Barplot empil√©\nbottom = None\nfor asset in weights.index:\n    plt.bar(\n        pd.to_numeric(weights.columns),  \n        weights.loc[asset],  \n        bottom=bottom,  \n        label=selected_assets[asset]  \n    )\n    bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\nplt.xlabel(\"Days of Liquidation\")\nplt.ylabel(\"Portfolio Weights\")\nplt.title(\"D√©formation du portefeuille\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nNous constatons avec ce profile de liquidation que les actifs les plus liquides sont liquid√©s en premier, ce qui entra√Æne une concentration d‚Äôactifs illiquides dans le portefeuille restant. Cela peut avoir un impact significatif sur la valeur du portefeuille et sur le risque de liquidit√©. En effet, √† la fin de la p√©riode de liquidation, le portefeuille est uniquement constitu√© de l‚Äôactif Orange, qui nous avons vu pr√©c√©demment est l‚Äôactif le plus illiquide.\n\nüí° Bon √† savoir : Pour un fonds de droit fran√ßais r√©glement√©, il est interdit d‚Äôinvestir plus de 5‚ÄØ% du portefeuille dans un seul √©metteur. Cependant, √† titre exceptionnel, il est possible d‚Äôinvestir jusqu‚Äô√† 10‚ÄØ% dans certains titres, √† condition que la somme des expositions sup√©rieures √† 5‚ÄØ% ne d√©passe pas 40‚ÄØ% du portefeuille. C‚Äôest ce que l‚Äôon appelle la r√®gle des 5/10/40, un ratio r√©glementaire applicable aux OPC (organismes de placement collectif). Toutes les pertes li√©es √† un non-respect de ce ratio doivent √™tre support√©es par la soci√©t√© de gestion. De plus, tout d√©passement doit √™tre d√©clar√© √† l‚ÄôAMF.\n\n\n\nShow the code\n# Valeur liquide du portefeuille\nmarket_value_df = pd.DataFrame()\n\nmarket_value_df[\"market_value\"] = market_value.sum(axis=0)\n\n# Calculer la valeur liquide relative par rapport au jour 0\nmarket_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n\n# Calculer la valeur cumul√©e liquide relative du portefeuille\nmarket_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n\nmarket_value_df\n\n\n\n\n\n\n\n\n\nmarket_value\nrelative value\ncumulative value\n\n\n\n\n0\n8.361435e+08\n1.000000\n0.000000\n\n\n1\n2.106812e+08\n0.251968\n0.251968\n\n\n2\n2.049327e+08\n0.245093\n0.497061\n\n\n3\n1.644335e+08\n0.196657\n0.693718\n\n\n4\n1.188730e+08\n0.142168\n0.835886\n\n\n5\n7.928483e+07\n0.094822\n0.930708\n\n\n6\n3.325754e+07\n0.039775\n0.970483\n\n\n7\n1.820468e+07\n0.021772\n0.992255\n\n\n8\n6.475942e+06\n0.007745\n1.000000\n\n\n\n\n\n\n\n\n\nShow the code\nmarket_value_df = market_value_df.iloc[1:]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,  # Center text\n        height,  # Position slightly above the bar\n        f'{height:.2f}',  # Format with 2 decimal places\n        ha='center',  # Center horizontally\n        va='bottom',  # Position text at the bottom\n        fontsize=10, color=\"black\"\n    )\n\nplt.xlabel(\"Days\")\nplt.ylabel(\"Cumulative Value (%)\")\nplt.title(\"Profil de liquidation du portefeuille\")\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPour voir ce qui arrive au profil d‚Äô√©coulement lorsque les quantit√©s varient, on va utiliser un facteur de modulation de la quantit√©. Cela permet de d√©terminer quelle est la taille cible du portefeuille qui permet d‚Äôavoir la liquidit√© pour un certain niveau en nombre de jours qu‚Äôon se fixe. Cet exercice est fait une seule fois √† l‚Äôinitialisation du portefeuille.\nPour facilier l‚Äôimpl√©mentation, nous allons utiliser une fonction liquidation_profile qui va nous permettre de calculer le profil d‚Äô√©coulement du portefeuille, en int√©grant le facteur de modulation. Lorsqu‚Äôon ne souhaite pas de modulation, on peut simplement passer un facteur de modulation de 1.\n\n\nShow the code\ndef waterfall_liquidation(ADV, price_dict, selected_assets, fact_modulation=0.30, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumul√©es.\n    \"\"\"\n    \n    # Initialisation des quantit√©s liquid√©es\n    ADV = ADV.copy()\n    ADV[\"Quantity\"] = round(ADV[\"Quantity\"] * fact_modulation)\n\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"]]\n    \n    time_elapsed = 0 \n    # Calcul des quantit√©s liquid√©es par jour\n    while (ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]).sum() &gt; 0 :\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]\n        )\n        ADV[\"Quantity liquidated\"] += liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n        time_elapsed += 1\n    \n    print(f\"Temps de liquidation du portefeuille : {time_elapsed} jours\")\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    #---------------------------#\n    # Calcul des poids par jour #\n    #---------------------------#\n\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(10, 4))\n        \n\n        # Barplot empil√©\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns), \n                weights.loc[asset],  \n                bottom=bottom, \n                label=selected_assets[asset]  \n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    #---------------------------#\n    #   Profil de liquidation   #\n    #---------------------------#\n\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n        bars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(\n                bar.get_x() + bar.get_width() / 2,  # Center text\n                height,  # Position slightly above the bar\n                f'{height:.2f}',  # Format with 2 decimal places\n                ha='center',  # Center horizontally\n                va='bottom',  # Position text at the bottom\n                fontsize=10, color=\"black\"\n            )\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n\n    return market_value_df, market_value, weights\n\n\nNous constatons qu‚Äôen utilisant un facteur de modulation de 0.3, le temps de liquidation est r√©duit √† 3 jours, ce qui est plus rapide que le temps de liquidation initial de 8 jours. Cela montre l‚Äôimpact significatif de la modulation sur le profil de liquidation du portefeuille.\n\n\nShow the code\nfact_modulation= 0.3\nnew_market_value_df, new_market_value, new_weights = waterfall_liquidation(ADV, price_dict, selected_assets, fact_modulation, plot_graphs=True)\n\n\nTemps de liquidation du portefeuille : 3 jours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSous conditions stress√©es avec d√©formation\nPour stresser le portefeuille, √† la baisse, on va diviser la profondeur de march√© par 2. Cela signifie que l‚Äôon peut vendre moins d‚Äôactifs sans impacter le prix de mani√®re significative. En cons√©quence, le temps de liquidation augmente, car il faut plus de temps pour liquider la m√™me quantit√© d‚Äôactifs. Il est √©galement possible d‚Äôavoir des conditions stress√©es √† la hausse, o√π l‚Äôon multiplie la profondeur de march√© par 2. Cela signifie que l‚Äôon peut vendre plus d‚Äôactifs sans impacter le prix de mani√®re significative. En cons√©quence, le temps de liquidation diminue, car il faut moins de temps pour liquider la m√™me quantit√© d‚Äôactifs.\nDans notre cas, en stressant le portefeuille √† la baisse, nous constatons que le temps de liquidation augmente. On passe de 8 jours de liquidatin √† 15 jours.\n\n\nShow the code\n#---------------------------#\n# Stress Test\n#---------------------------#\nADV_stressed = ADV.copy()\n\n# Quantit√© journali√®re\nmarket_depth = (20/100)/2  # On stresse la liquidit√© √† la baisse\nADV_stressed[\"Quantity in 1day\"] = round(ADV_stressed[\"ADV\"] * market_depth)\n\n# Calcul du nombre de jours de liquidation\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Quantity\"]/ADV_stressed[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\n\ntime_elapsed = ADV_stressed['Days of liquidation'].max()\nprint(f\"Temps de liquidation du portefeuille stress√©: {time_elapsed} jours\")\n\n\nTemps de liquidation du portefeuille stress√©: 15 jours\n\n\n\n\nShow the code\nstressed_market_value_df, stressed_market_value, stressed_weights = waterfall_liquidation(ADV=ADV_stressed, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\n\nTemps de liquidation du portefeuille : 15 jours"
  },
  {
    "objectID": "3A/gestion_actifs/TP-2.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "href": "3A/gestion_actifs/TP-2.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "title": "Asset management : Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Absence de d√©formation du portefeuille (pro forma)",
    "text": "Absence de d√©formation du portefeuille (pro forma)\nNous avons vu dans la section pr√©c√©dente que, lors de la liquidation d‚Äôun portefeuille, on a tendance √† commencer par les actifs les plus liquides. Cela conduit √† une concentration progressive d‚Äôactifs illiquides dans le portefeuille r√©siduel, ce qui peut affecter significativement sa valeur et accro√Ætre le risque de liquidit√©.\nDans cette section, nous allons √©tudier comment √©viter cette d√©formation en proc√©dant √† une liquidation proportionnelle : l‚Äôobjectif est de pr√©server la r√©partition initiale du portefeuille tout au long du processus de liquidation.\nPour cela, on commence par estimer, comme pr√©c√©demment, la quantit√© liquidable en un jour pour chaque titre. Cette estimation permet d‚Äôen d√©duire le pourcentage liquidable quotidien par rapport √† la position totale sur chaque actif. Si l‚Äôon souhaite que tous les titres soient liquid√©s √† la m√™me vitesse, il faut adopter le rythme de liquidation de l‚Äôactif le plus lent. Ainsi, on calcule le pourcentage liquidable en un jour pour chaque titre, puis on en retient le minimum. Ce minimum d√©finit alors le pourcentage quotidien de liquidation appliqu√© √† l‚Äôensemble du portefeuille. Ce proc√©d√© allonge la dur√©e totale de liquidation, mais il permet de conserver une structure de portefeuille stable. N√©anmoins, une l√©g√®re d√©formation peut subsister, notamment en raison des arrondis et des limites pratiques de liquidit√© sur certains actifs.\n\n\nShow the code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef pro_forma_liquidation(ADV, price_dict, selected_assets, fact_modulation=0.30, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation pro forma et visualise les graphiques des poids et des valeurs cumul√©es.\n    \"\"\"\n\n    ADV = ADV.copy()\n    ADV[\"Quantity\"] = ADV[\"Quantity\"] * fact_modulation\n\n    time_elapsed = ADV['Days of liquidation'].max()\n\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"]]\n    \n    min_liquidated_today = np.ones(len(ADV[\"Quantity\"]))\n    time_elapsed = 0 \n\n    # Calcul des quantit√©s liquid√©es par jour\n    while min_liquidated_today.sum() &gt; 0:  \n\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"]\n        ) / ADV[\"Quantity\"]\n        min_liquidated_today = round(np.min(liquidated_today) * ADV[\"Quantity\"])# On liquide √† la vitesse de l'actif le moins liquide\n        ADV[\"Quantity liquidated\"] += min_liquidated_today\n        quantity_liquidated_per_day.append(min_liquidated_today)\n        time_elapsed += 1\n    \n    print(f\"Temps de liquidation du portefeuille : {time_elapsed} jours\")\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    #---------------------------#\n    # Calcul des poids par jour\n    #---------------------------#\n\n    # Calcul de la valeur de march√© initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n    \n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns), \n                weights.loc[asset],  \n                bottom=bottom, \n                label=selected_assets[asset]  \n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n        plt.show()\n\n    \n    #---------------------------#\n    #   Profil de liquidation   #\n    #---------------------------#\n    \n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(10, 4))\n        bars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(\n                bar.get_x() + bar.get_width() / 2,  # Center text\n                height,  # Position slightly above the bar\n                f'{height:.2f}',  # Format with 2 decimal places\n                ha='center',  # Center horizontally\n                va='bottom',  # Position text at the bottom\n                fontsize=10, color=\"black\"\n            )\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\n\nSous condition normale sans d√©formation\n\n\nShow the code\nproforma_market_value_df, proforma_market_value, proforma_weights = pro_forma_liquidation(ADV=ADV, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\n\nTemps de liquidation du portefeuille : 9 jours\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSous conditions stress√©es sans d√©formation\n\n\nShow the code\nproforma_s_market_value_df, proforma_s_market_value, proforma_s_weights = pro_forma_liquidation(ADV=ADV_stressed, price_dict=price_dict, selected_assets=selected_assets, fact_modulation=1, plot_graphs=True)\n\n\nTemps de liquidation du portefeuille : 16 jours"
  },
  {
    "objectID": "3A/gestion_actifs/TP-2.html#conclusion",
    "href": "3A/gestion_actifs/TP-2.html#conclusion",
    "title": "Asset management : Risques de liquidit√© (profil d‚Äô√©coulement / de liquidation d‚Äôun portefeuille d‚Äôactifs)",
    "section": "Conclusion",
    "text": "Conclusion\nLa liquidation d‚Äôun portefeuille est un exercice complexe qui n√©cessite de prendre en compte plusieurs facteurs : la liquidit√© des actifs, la taille du portefeuille, et les conditions de march√©. En utilisant des outils tels que le profil d‚Äô√©coulement (ou profil de liquidation), les investisseurs peuvent mieux anticiper l‚Äôimpact potentiel de leurs transactions sur les prix de march√©, et ainsi minimiser l‚Äôeffet de march√©.\nLe processus de liquidation choisi peut conduire √† des rythmes de sortie diff√©rents, et il est essentiel d‚Äôen tenir compte dans la gestion active du portefeuille.\nPour pr√©server la liquidit√© d‚Äôun portefeuille, notamment en p√©riode de tension sur les march√©s, plusieurs m√©canismes r√©glementaires et op√©rationnels existent. Ils visent √† prot√©ger les investisseurs restants et √† maintenir la qualit√© du portefeuille. En cas de crise financi√®re ou de forte volatilit√©, les soci√©t√©s de gestion de portefeuille (SGP) peuvent activer ces dispositifs, conform√©ment aux dispositions pr√©vues par la r√©glementation.\nCes mesures permettent d‚Äô√©viter des ventes forc√©es d‚Äôactifs et de limiter les effets de contagion sur le reste du portefeuille.\nPrincipaux m√©canismes de gestion de la liquidit√© :\n\nLes gates (plafonnement de rachat)\nLes gates permettent de limiter les rachats quotidiens √† un certain pourcentage de l‚Äôactif net du fonds. En g√©n√©ral, si les rachats d√©passent 5‚ÄØ% de l‚Äôactif net, la SGP a le droit (mais non l‚Äôobligation) de n‚Äôhonorer que les premiers 5‚ÄØ% et de reporter le reste sur les jours suivants, en fonction des conditions de march√©.\nCe m√©canisme permet de r√©duire l‚Äôimpact sur les prix de march√©, en √©vitant une vente massive d‚Äôactifs en un seul jour. C‚Äôest une mesure de protection des porteurs restants, bien que son activation soit souvent per√ßue comme un signal n√©gatif.\nL‚Äôexistence du m√©canisme des gates doit figurer dans le prospectus du fonds, sauf justification sp√©cifique de la SGP. Il convient de noter que l‚Äôactivation des gates est optionnelle.\nLa suspension des souscriptions et des rachats\nIl s‚Äôagit d‚Äôune mesure plus radicale, qui consiste √† geler temporairement les op√©rations d‚Äôentr√©e et de sortie du fonds. Elle est utilis√©e dans des situations exceptionnelles, telles que des crises de march√© ou une volatilit√© extr√™me, lorsque la valorisation des actifs devient incertaine ou que la liquidit√© dispara√Æt. Ce m√©canisme vise √† : pr√©server l‚Äô√©galit√© de traitement entre les investisseurs, √©viter des ventes pr√©cipit√©es d‚Äôactifs, stabiliser la structure du portefeuille. La suspension doit √™tre justifi√©e et temporaire, et elle est lev√©e d√®s que les conditions de march√© se normalisent.\nLes m√©canismes de d√©formation du portefeuille\nIl s‚Äôagit de mesures proactives consistant √† adapter la composition du portefeuille lors des rachats, pour pr√©server la liquidit√© r√©siduelle. Concr√®tement, la SGP peut choisir de vendre en priorit√© les actifs les plus liquides, ce qui permet de r√©pondre rapidement aux demandes de rachat sans impacter significativement les prix. Ces m√©canismes sont particuli√®rement utilis√©s en cas de rachats massifs, pour : limiter l‚Äôimpact de march√©, prot√©ger les investisseurs restants, √©viter une d√©stabilisation du portefeuille.\nN√©anmoins, cela entra√Æne une d√©formation du portefeuille, c‚Äôest-√†-dire que les investisseurs qui restent se retrouvent avec une part moins liquide du portefeuille initial. Ce compromis doit √™tre g√©r√© avec prudence."
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "",
    "text": "L‚Äôanalyse financi√®re constitue l‚Äôensemble des outils permettant de donner un avis objectif d‚Äôune organisation (entreprises, fondations, etc.) sur la sant√© finani√®re et les risques financiers auxquels elle sera confront√©e. Il s‚Äôagit de determiner quels sont les crit√®res d‚Äôune sant√© financi√®re, qu‚Äôest le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le g√®re-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financi√®re d‚Äôune entreprise. Il s‚Äôagit du bilan et du compte de r√©sultat. Ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise. Comprendre comment ils fonctionnent permet de mieux appr√©hender la situation financi√®re d‚Äôune banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un √©tat des lieux de la situation patrimoniale de l‚Äôentreprise √† un moment donn√©. Il est compos√© de deux parties : l‚Äôactif et le passif. L‚Äôactif ou l‚Äôemploi regroupe l‚Äôensemble des biens et des droits de l‚Äôentreprise tandis que le passif regroupe l‚Äôensemble des ressources de l‚Äôentreprise (d‚Äôo√π vient l‚Äôargent et o√π peut-on s‚Äôen procurer). Le bilan est √©quilibr√© en valeur nette, c‚Äôest-√†-dire que l‚Äôactif est √©gal au passif.\nLe compte de r√©sultats, quant √† lui, est un document qui permet de faire un √©tat des lieux des performances de l‚Äôentreprise sur une p√©riode donn√©e (il r√©sume les b√©n√©fices ou pertes g√©n√©r√©es). Il est compos√© du d√©tail des produits et des charges de l‚Äôentreprise. Les produits sont les √©l√©ments qui g√©n√®rent des revenus pour l‚Äôentreprise tandis que les charges sont les √©l√©ments qui g√©n√®rent des d√©penses pour l‚Äôentreprise. Le compte de r√©sultat alimente par ailleurs la partie ‚Äúr√©sultat de l‚Äôexercice‚Äù du bilan comptable.\nLe coeur de l‚Äôentreprise √† analyser comme ressources suppl√©mentaires dans le compte de r√©sultat est l‚Äôensembles des charges financi√®res & exceptionnelles ainsi que l‚Äôensemble des produits d‚Äôexploitation et financiers. Ces √©l√©ments cl√©s permettent de d√©terminer la rentabilit√© de l‚Äôentreprise. En effet, si les charges sont sup√©rieures aux produits, l‚Äôentreprise est en perte. Si les produits sont sup√©rieurs aux charges, l‚Äôentreprise est en b√©n√©fice.\nIl est important de noter que ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d‚Äôanalyser la situation financi√®re d‚Äôune entreprise, il faut donc le remodeler en un bilan ‚Äúfonctionnel‚Äù pour pouvoir l‚Äôanalyser. Le bilan fonctionnel est un document qui permet de faire un √©tat des lieux de la situation financi√®re de l‚Äôentreprise en fonction de son activit√©, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d‚Äôinvestissement √† long terme\nEmplois stables\n\nactifs immobilis√©s en valeur brute\n\nCycle de financement √† long terme\nRessources stables\n\nCapitaux propres,\nEmprunts √† long terme,\nAmortissements et d√©pr√©ciation,\nProvisions pour risques\n\n\n\nCycle d‚Äôexploitation\nEmplois d‚Äôexploitation\n\nStocks et encours\nCr√©ances\n\nCycle d‚Äôexploitation\nRessources d‚Äôexploitation\n\nDettes circulantes\n\n\n\nTr√©sorerie active\n\nDisponibilit√©s\n\nTr√©sorerie passive\n\nD√©couverts bancaires\n\n\n\n\nLes ressources stables font r√©f√©rence aux ressources saines du bilan etfont face aux emplois stables. La tr√©sorerie passive fait r√©f√©rence aux d√©couverts bancaires. Il est important de souligner qu‚Äôune tr√©sorerie passive est per√ßue n√©gativement dans le bilan fonctionnel. En effet, une tr√©sorerie passive signifie que l‚Äôentreprise a des dettes √† court terme qui ne sont pas couvertes par des actifs √† court terme d‚Äôo√π la n√©cessit√© d‚Äôavoir des d√©couverts bancaires.\nNb : La provision pour le risque peuve √™tre consid√©r√©e comme une ressource stable ou une ressource d‚Äôexploitation en fonction de l‚Äôentreprise. Tout d√©pend de la longevit√© des provisions.\n\nEquilibre financier\nNous dirons qu‚Äôil y a √©quilibre financier lorsque :\n\nLes emplois stables soient enti√®rement financ√©s par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a nec√©ssit√© d‚Äôun fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) d√©pend du cycle d‚Äôexploitation (entre autre, la rapidit√© de rotation des stocks et des cr√©ances). Il doit couvrir les besoins de financement du cycle d‚Äôexploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, cr√©ances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la tr√©sorerie (\\(\\text{Tr√©sorerie}=FDR-BFR\\)). Si la tr√©sorerie est positive, il y a √©quilibre financier. Cel√† signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d‚Äôexploitation. Lorsqu‚Äôil est n√©gatif, il faut trouver des ressources pour financer le cycle d‚Äôexploitation. Si la tr√©sorerie est nulle, il y a √©quilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-m√™me le cycle d‚Äôexploitation de l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le cr√©dit fournisseur. Il est important de noter que le cr√©dit fournisseur est une source de financement gratuite pour l‚Äôentreprise. C‚Äôest le cas des E-commerce o√π les acteurs encaissent leurs clients avant m√™me d‚Äôacheter les stocks aupr√®s des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transform√© en ressources en fonds de roulement, cel√† est une situation tr√®s favorable pour l‚Äôentreprise et est appel√©e ‚Äúcr√©dit inter-entreprises‚Äù."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Analyse du compte de r√©sultat",
    "text": "Analyse du compte de r√©sultat\nNous pouvons faire les m√™mes critiques faites au bilan comptable sur le compte de r√©sultat. En effet, le compte de r√©sultat est con√ßu de sorte √† fournir des informations au seul d√©tenteur du capital, √† savoir les actionnaires. Il fait apparaitre uniquement le b√©n√©fice ou la perte. C‚Äôest un document d‚Äôint√©r√™t pour l‚ÄôEtat pour d√©terminer si un pays est en croissance ou en r√©cession. Pour en faire un vrai diagnostic financier, il faut le d√©couper en sous-soldes appel√©s ‚Äúsoldes interm√©diaires de gestion‚Äù (SIG). Les SIG permettent de d√©terminer la rentabilit√© de l‚Äôentreprise, sa capacit√© d‚Äôautofinancement, sa capacit√© de remboursement, sa capacit√© de financement, etc.\nIl existe 9 soldes interm√©diaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajout√©e\nL‚Äôexc√©dent brut d‚Äôexploitation (EBE)\nLe r√©sultat d‚Äôexploitation\nLe r√©sultat courant avant imp√¥t\nLe r√©sultat exceptionnel\nLe r√©sultat net\nLa plus ou moins value de cession\n\nSelon la th√©orie de prise de d√©cisions, il y a deux grands types de d√©cisions : des d√©cisions qui permettent de cr√©er de la riches (Marge co., production et valeur ajout√©e) et des d√©cisions qui permettent de distribuer/d√©penser de la richesse (EBE, r√©sultat d‚Äôexploitation, r√©sultat courant avant imp√¥t, r√©sultat exceptionnel, r√©sultat net et plus ou moins value de cession). Lorsqu‚Äôon d√©pense la riches, il faudrait qu‚Äôelle soit bien d√©pens√©e.\n\nSoldes de cr√©ation de richesse\nLes soldes qui contribuent √† la cr√©ation de richesse sont la marge commerciale, la production et la valeur ajout√©e :\n\nLa marge commerciale est la diff√©rence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C‚Äôest un solde des entreprises commerciales (par exemple, les supermarch√©s). Pour une entreprise qui n‚Äôont pas de marchandises, le marge commerciale est nulle.\nLa production de l‚Äôexercice est la somme des produits vendus(\\(\\pm\\) les produits stock√©es) et des produits immobilis√©es par l‚Äôentreprise (certaines entreprises peuvent se vendre des produits √† elles-m√™mes). C‚Äôest un solde des entreprises industrielles.\nLa valeur ajout√©e est la richesse cr√©√©e par l‚Äôentreprise. C‚Äôest la somme des marges commerciales, de la production de l‚Äôexercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services ext√©rieurs).\n\nLa valeur ajout√© est un indicateur tr√®s suivi par l‚ÄôEtat pour d√©terminer le produit int√©rieur brut (PIB) afin de d√©terminer si un pays est en croissance ou en r√©cession. Par ailleurs, la valeur ajout√©e divis√©e par le nombre de salari√©s permet de d√©terminer le niveau de technicit√© de l‚Äôentreprise. Plus la valeur ajout√©e par salari√© est √©lev√©e, plus l‚Äôentreprise est techniquement avanc√©e.\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© √©conomique\nIl existe 5 tiers √† qui l‚Äôentreprise redistribue la VA (rang√©e par ordre de priorit√©) :\n\nLe personnel (√† travers les salaires),\nL‚ÄôEtat (√† travers les imp√¥ts),\nLe capital technique (via les amortissements),\nLes banques (via les int√©r√™ts),\nLes actionnaires ou les associ√©s (via le b√©n√©fice comptable)\n\nLes soldes qui permettent de financer l‚Äôactivit√© √©conomique (Etat, personnel, capital technique) sont l‚Äôexc√©dent brut d‚Äôexploitation et le r√©sultat d‚Äôexploitation :\n\nLe solde EBE r√©mun√®re le personnel et l‚ÄôEtat. Il repr√©sente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d‚Äôentreprise pour l‚ÄôEtat. Un EBE positif signifie que l‚Äôentreprise est capable de r√©mun√©rer le personnel et l‚ÄôEtat, et donc de financer l‚Äôemploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajout√©e} - \\text{Imp√¥ts, t√¢xes et versements assimil√©s} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de r√©sultat d‚Äôexploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de r√©mun√©rer le capital technique (machines etc.) et appartient √† tout ceux qui d√©pendent du capital financier et mesure les performances industrielles et commerciales de l‚Äôentreprise.\n\n\\[\\begin{align*}\n\\text{R√©sultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© financi√®re\nLes soldes qui permettent de financer l‚Äôactivit√© financi√®re (banques, actionnaires) sont le r√©sultat courant avant imp√¥t, le r√©sultat exceptionnel, le r√©sultat net et la plus ou moins value de cession :\n\nLe r√©sultat courant avant imp√¥t est le solde qui permet de r√©mun√©rer les banques. Il est un indicateur de la capacit√© de l‚Äôentreprise √† rembourser ses dettes et est un t√©moin de l‚Äôincidence de la politique financi√®re de l‚Äôentreprise sur son r√©sultat. Il faut distinguer les int√©r√™ts √† long terme et ceux de court terme. Plus ceux ci sont li√©s √† des dettes de court terme (ex. : d√©couverts), on peut dire que l‚Äôentreprise est en difficult√© financi√®re tandis que l‚Äôendettement √† long terme est un signe de bonne sant√© financi√®re, car il est voulu plut√¥t que subi. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat courant avant imp√¥t} &= \\text{R√©sultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financi√®res}\n\\end{align*}\\]\n\nLe r√©sultat exceptionnel est le solde qui est le moins analys√© car il est souvent li√© √† des √©v√®nements exceptionnels (ex. : vente d‚Äôun bien immobilier). Il est calcul√© comme √©tant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe r√©sultat net est le solde qui permet de r√©mun√©rer les actionnaires. C‚Äôest le solde en bas du compte de r√©sultat. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat net} &= \\text{R√©sultat courant avant imp√¥t} + \\text{R√©sultat exceptionnel} \\\\\n&- \\text{participations des salari√©s} - \\text{Imp√¥ts sur les b√©n√©fices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu‚Äôune entreprise vend une immobilisation. Ce ratio permet de d√©terminer si l‚Äôentreprise a vendu une immobilisation √† un prix sup√©rieur ou inf√©rieur √† sa valeur comptable. Cel√† constitue un temoin d‚Äôalerte sur la sant√© de l‚Äôentreprise et permet de d√©terminer si l‚Äôentreprise est en difficult√© financi√®re (car rien ne l‚Äôoblige √† vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "La capacit√© d‚Äôautofinancement",
    "text": "La capacit√© d‚Äôautofinancement\nLa capacit√© d‚Äôautofinancement (CAF) est un indicateur qui permet de d√©terminer si l‚Äôentreprise est capable de financer ses investissements sans recourir √† des financements ext√©rieurs. Elle regroupe la capacit√© √† d√©gager de la liquidit√©. Il n‚Äôy a pas de correspondance entre la tr√©sorerie et le b√©n√©fice. En effet, une entreprise peut √™tre en b√©n√©fice mais en difficult√© financi√®re. Pour la calculer, il faut √©liminer les sommes non encaissanles et non d√©caissables (ex. : Dotations, provision, reprise sur amortissements, les √©critures exceptionnelles).\nPour passer du b√©n√©fice √† la CAF, on ne conserve que les √©l√©ments qui sont encaissables et d√©caissables et est calcul√©e comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges d√©caissables (int√©r√™t bancaire, imp√¥t sur b√©n√©fice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu vers√© par l‚Äôentreprise √† ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas √† elle toute seule de d√©terminer l‚Äôautofinancement de l‚Äôentreprise. Dans le cadre l√©gal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu‚Äô√† 95% du b√©n√©fice comptable impos√© par l‚ÄôEtat et garder 5% √† l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende l√©gal‚Äù. Au del√† de 10%, les actionnaires peuvent retirer jusqu‚Äô√† 100% du b√©n√©fice comptable. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende statutaire‚Äù.\nAinsi, l‚Äôautofinancement est la somme qui reste de la CAF apr√®s le dividende l√©gal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaiss√©s par les actionnaires d√©termine la politique d‚Äôautofinancement de l‚Äôentreprise.\nL‚Äôautofinancement est essentiel pour l‚Äôentreprise car il permet de:\n\nrembourser les emprunts,\nam√©liorer la tr√©sorerie,\ncouvrir les risque de l‚Äôentreprise (provisions pour risque),\nfinancer l‚Äôexploitation (stocks & cr√©ances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html",
    "href": "3A/Apprentisage-stat/Tp3.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "AdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\nHence, for M weak learners, the final prediction is given by \\(g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)\\) where \\(g_m(x)\\) is the m-th weak learner and \\(\\alpha_m\\) is the weight associated with the m-th weak learner. The optimisation problem of AdaBoost is given by:\n\\[ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) \\]\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function \\(l(y,f(x))=\\exp(-yf(x))\\), \\(y \\in \\{-1,+1\\}\\). It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n\nInitialize the observation weights \\(w_i^{(1)} = 1/n\\) for \\(i=1,\\dots,n\\)\n\n\nFor m=1 to M:\n\n\n\nFit a weak learner \\(g_m(x)\\) to the training data using weights \\(w_i^{(m)}\\)\n\n\nCompute the error rate \\(err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))\\) where \\(I\\) is the indicator function\n\n\nCompute the weight \\(\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)\\)\n\n\nUpdate the weights \\(w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)\\)\n\n\n\nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems. It is a generalization of the AdaBoost algorithm to multi-class classification problems.\nTo inspect how the errors and the weights vary with the number of iterations, we will the function make_gaussian_quantiles from sklearn. This function generates a multi-dimensional standard normale distribution with a given number of samples \\(n\\) per class \\(K\\). We will generate a dataset of size \\(n=2000\\) with \\(K=3\\) classes and \\(d=10\\) features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "href": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "title": "Gradient boosting",
    "section": "",
    "text": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "href": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "title": "Gradient boosting",
    "section": "",
    "text": "# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html",
    "href": "3A/Apprentisage-stat/Tp1.html",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]\n\n\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "If we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "1. Ridge regression",
    "text": "1. Ridge regression\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter \\(\\lambda\\) by cross-validation. We will use the function RidgeCV from the sklearn library to perform the cross-validation. Since we didn‚Äôt center the covariables, we will set the parameter fit_intercept to True in order to include an intercept in the model.\nBy default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter Œª. This is called regularization path. We can do this by fitting the model for different values of Œª and store the coefficients.\n\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'Œª = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter \\(\\lambda\\) wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients.\n In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. \n\na. Check on the intercept value of the model using lambda found by cross validation\n\nprint(f'Intercept value : {ridge_cv.intercept_}')\n\nIntercept value : 152.28937186306018\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "2. Lasso regression",
    "text": "2. Lasso regression\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation. It is important to perform a cross-validation in order to choose the best value of the regularization parameter \\(\\lambda\\) as we have demonstrated in the first activity. For the lasso regression, we will use the function LassoCV from the sklearn library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n\n\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Œª = {lasso_cv.alpha_:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter \\(\\lambda\\) given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of \\(\\lambda\\), we can select more or less variables.\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n\na. Check on the intercept value of the model using lambda found by cross validation\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n\nIntercept value : 151.95282341561403"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "3. Quality of the models (ridge regression vs lasso regression)",
    "text": "3. Quality of the models (ridge regression vs lasso regression)\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\\[\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2\\]\nwhere \\(\\hat g(x)\\) is the predicted value of the output variable y given the input variable x, \\(\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j\\).\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model."
  },
  {
    "objectID": "00_Assets/markets_intro.html",
    "href": "00_Assets/markets_intro.html",
    "title": "Introduction aux march√©s financiers",
    "section": "",
    "text": "Dans le secteur financier, le march√© d√©signe l‚Äôendroit o√π se rencontrent l‚Äôoffre et la demande de services ou de produits financiers. Il regroupe de nombreux acteurs, notamment les banques, les institutions financi√®res, les entreprises et les particuliers. Parmi les diff√©rents produits financiers √©chang√©s, on trouve les actions, les obligations, les produits d√©riv√©s, les devises et les mati√®res premi√®res‚Ä¶\nDans cet article, nous allons examiner trois types de march√©s financiers, √† savoir le march√© des actions, des obligations et des produits d√©riv√©s, leurs caract√©ristiques ainsi que leur mode de fonctionnement."
  },
  {
    "objectID": "00_Assets/markets_intro.html#march√©s-des-actions",
    "href": "00_Assets/markets_intro.html#march√©s-des-actions",
    "title": "Introduction aux march√©s financiers",
    "section": "March√©s des actions",
    "text": "March√©s des actions\nLe march√© des actions permet aux entreprises de lever des fonds en √©mettant des titres de propri√©t√© (actions) que les investisseurs peuvent acheter. Ces actions sont ensuite √©chang√©es sur des bourses telles que Euronext Paris ou le New York Stock Exchange. Les prix des actions varient en fonction de l‚Äôoffre et de la demande, influenc√©es par les performances de l‚Äôentreprise, les conditions √©conomiques et les sentiments du march√©.\nLes acteurs principaux de ce march√© sont les entreprises √©mettrices, les investisseurs institutionnels (fonds de pension, compagnies d‚Äôassurance) et les investisseurs individuels. Pour acheter ou vendre des actions, les investisseurs passent g√©n√©ralement par des courtiers en ligne ou des banques d‚Äôinvestissement. Ils emmettent des ordres d‚Äôachat (bid) ou de vente(ask), qu‚Äôon peut visualiser dans un carnet d‚Äôordres, qui sont ensuite ex√©cut√©s sur le march√©. Il existe plusieurs types d‚Äôordres :\n\nOrdre au march√© : achat ou vente imm√©diate au meilleur prix disponible.\nOrdre √† cours limit√© : permet d‚Äôacheter ou de vendre une quantit√© de titres avec une condition de prix, donc avec une ma√Ætrise du cours auquel l‚Äôordre sera ex√©cut√©.Il permet ainsi de fixer le cours d‚Äôobjectif d‚Äôachat ou de vente.\nOrdre √† seuil de d√©clenchement : il s‚Äôagit d‚Äôun ordre qui ne sera ex√©cut√© que si le prix atteint un certain seuil. Il est souvent utilis√© pour limiter les pertes ou s√©curiser les gains. Pour un achat, il est plac√© en dessous du prix actuel du march√©, et pour une vente, au dessus.\nOrdre √† plage de d√©clenchement : il s‚Äôagit d‚Äôun ordre qui ne sera ex√©cut√© que si le prix atteint un certain seuil, mais qui est limit√© dans le temps. Il est souvent utilis√© pour limiter les pertes ou s√©curiser les gains. Pour un achat, il est plac√© en dessous du prix actuel du march√©, et pour une vente, au dessus.\nOrdre suiveur : ce type d‚Äôordre repose sur le m√™me principe qu‚Äôun ordre √† seuil de d√©clenchement √† la diff√©rence que l‚Äôon saisira un pourcentage de variation et non un seuil en euros.\n\nIl existe √©galement d‚Äôautres types dordres tactiques (ordre s√©quence, etc.) qui permettent de g√©rer des strat√©gies d‚Äôinvestissement plus complexes.\n\nüí° En France, l‚ÄôAutorit√© des march√©s financiers (AMF) supervise le march√© pour assurer sa transparence et son bon fonctionnement.\n\n\nFixation du cours d‚Äôune action\nLe prix d‚Äôune action est d√©termin√© par le jeu de l‚Äôoffre et de la demande sur le march√©. Lorsque la demande pour une action augmente, son prix tend √† augmenter. Inversement, si l‚Äôoffre d√©passe la demande, le prix diminue. Le cours d‚Äôune action ne refl√®te pas directement le bid ni le ask, mais l‚Äôintersection effective entre eux, c‚Äôest-√†-dire l√† o√π une offre et une demande se sont crois√©es.\nCe prix est √©galement influenc√© par des facteurs fondamentaux tels que : les b√©n√©fices de l‚Äôentreprise, les dividendes distribu√©s, les perspectives de croissance, ainsi que les conditions √©conomiques g√©n√©rales.\n√Ä la cl√¥ture de la bourse, le cours de cl√¥ture correspond au dernier prix auquel l‚Äôaction a √©t√© √©chang√©e. Ce prix peut √™tre diff√©rent du prix d‚Äôouverture du lendemain en raison d‚Äô√©v√©nements ou de mouvements de march√© survenus en dehors des heures d‚Äôouverture (after-hours trading)."
  },
  {
    "objectID": "00_Assets/markets_intro.html#march√©-obligataire",
    "href": "00_Assets/markets_intro.html#march√©-obligataire",
    "title": "Introduction aux march√©s financiers",
    "section": "March√© obligataire",
    "text": "March√© obligataire\nLe march√© obligataire est un march√© o√π les investisseurs ach√®tent et vendent des obligations. Les obligations sont des titres de cr√©ance √©mis par des gouvernements ou des entreprises pour lever des fonds. Une obligation est un contrat entre un √©metteur et un investisseur, dans lequel l‚Äô√©metteur s‚Äôengage √† rembourser l‚Äôinvestisseur √† une date future, tout en lui versant des int√©r√™ts (appel√©s coupons) √† intervalles r√©guliers.\nDans ce contrat, l‚Äô√©metteur contracte une dette sur le march√© obligataire. L‚Äôinvestisseur pr√™te de l‚Äôargent en achetant l‚Äôobligation, et en √©change, il per√ßoit des int√©r√™ts jusqu‚Äô√† l‚Äô√©ch√©ance, date √† laquelle il r√©cup√®re le capital pr√™t√© (la valeur nominale).\nLes obligations sont g√©n√©ralement consid√©r√©es comme des investissements √† faible risque, car elles offrent souvent un rendement fixe et sont moins volatiles que les actions. Cependant, leur rendement est g√©n√©ralement inf√©rieur √† celui des actions, ce qui en fait un choix moins attrayant pour les investisseurs √† la recherche de rendements √©lev√©s.\nLes obligations peuvent √™tre class√©es selon leur √©metteur, leur dur√©e, et leur taux d‚Äôint√©r√™t :\n\nLes obligations d‚Äô√âtat sont √©mises par les gouvernements.\nLes obligations d‚Äôentreprise sont √©mises par des soci√©t√©s priv√©es ou publiques.\nLes obligations √† court terme ont une dur√©e inf√©rieure √† cinq ans, tandis que les obligations √† long terme ont une dur√©e sup√©rieure √† dix ans.\nLe taux d‚Äôint√©r√™t des obligations est g√©n√©ralement fixe, mais certaines peuvent avoir un taux variable.\nLes obligations sont √©galement class√©es selon leur notation de cr√©dit, qui √©value la capacit√© de l‚Äô√©metteur √† rembourser la dette.\n\n\nCaract√©ristiques d‚Äôune obligation\nUne obligation comprend plusieurs √©l√©ments cl√©s :\n\nIdentit√© de l‚Äô√©metteur : L‚Äôentit√© qui √©met l‚Äôobligation (√âtats, entreprises, banques, collectivit√©s‚Ä¶)\nDevise d‚Äô√©mission : La monnaie dans laquelle l‚Äôobligation est √©mise (euro, dollar, yen‚Ä¶)\nValeur nominale : Montant, not√©e N, que l‚Äô√©metteur s‚Äôengage √† rembourser √† l‚Äô√©ch√©ance.\nDate de maturit√© : Date, not√©e T, √† laquelle la valeur nominale doit √™tre rembours√©e √† l‚Äôinvestisseur.\nCoupon (C) : Int√©r√™ts, not√©s C, vers√©s p√©riodiquement √† l‚Äôinvestisseur jusqu‚Äô√† la date de maturit√©. Cela est g√©n√©ralement exprim√© en pourcentage de la valeur nominale afin de favoriser la comparaison entre les obligations.\n\n\nSch√©matisation d‚Äôune obligation (taux fixe, maturit√© 5 ans)\n\n\n\n\nValorisation d‚Äôune obligation\nLa valorisation d‚Äôune obligation consiste √† d√©terminer sa valeur actuelle, c‚Äôest-√†-dire le montant qu‚Äôun investisseur est pr√™t √† payer pour l‚Äôacheter aujourd‚Äôhui. La valeur d‚Äôune obligation d√©pend de plusieurs facteurs,\n\\[\nB(t,T) = \\sum_{t=1}^{T} \\frac{C}{(1+r)^t} + \\frac{N}{(1+r)^T}\n\\]\nComme on peut le constater, cette valeur d√©pend de plusieurs facteurs, notamment :\n\nLe taux d‚Äôint√©r√™t du march√© (\\(r\\)) : Si le taux d‚Äôint√©r√™t du march√© augmente, la valeur des obligations existantes diminue, car les investisseurs pr√©f√®rent acheter de nouvelles obligations offrant des rendements plus √©lev√©s.\nLa dur√©e restante jusqu‚Äô√† l‚Äô√©ch√©ance (\\(T-t\\)) : Plus la dur√©e restante est longue, plus la valeur de l‚Äôobligation est sensible aux variations des taux d‚Äôint√©r√™t.\nLe taux de coupon (\\(c\\)): Un taux de coupon plus √©lev√© rend l‚Äôobligation plus attrayante, ce qui augmente sa valeur.\n\nLa valeur d‚Äôune obligation d√©pend √©galement de la qualit√© de cr√©dit de l‚Äô√©metteur. Si la qualit√© de cr√©dit de l‚Äô√©metteur diminue, la valeur de l‚Äôobligation diminue √©galement, car les investisseurs per√ßoivent un risque accru de d√©faut.\n\nüí° Lorsqu‚Äôune obligation est sans coupon, on l‚Äôappelle une obligation z√©ro coupon. Dans ce cas, la valorisation de l‚Äôobligation est plus simple, car il n‚Äôy a pas de paiements d‚Äôint√©r√™ts p√©riodiques. La valeur actuelle d‚Äôune obligation z√©ro coupon est simplement la valeur nominale actualis√©e √† la date d‚Äô√©ch√©ance : \\(B(t,T) = \\frac{N}{(1+r)^{T-t}}\\)."
  },
  {
    "objectID": "00_Assets/markets_intro.html#march√©-des-produits-d√©riv√©s",
    "href": "00_Assets/markets_intro.html#march√©-des-produits-d√©riv√©s",
    "title": "Introduction aux march√©s financiers",
    "section": "March√© des produits d√©riv√©s",
    "text": "March√© des produits d√©riv√©s\nLes produits d√©riv√©s sont des instruments financiers dont la valeur d√©pend d‚Äôun actif sous-jacent. Parmi les produit d√©riv√©s, on trouve les options, les futures, les forwards, et les swaps. Ces instruments sont utilis√©s pour diverses raisons, notamment le hedging, la sp√©culation sur les mouvements de prix futurs, et l‚Äôarbitrage entre diff√©rents march√©s.\nUn trader est en position de hedging lorsqu‚Äôil a une exposition au prix d‚Äôun actif et prend une position dans un d√©riv√© pour compenser l‚Äôexposition. Dans une sp√©culation, le trader n‚Äôa pas d‚Äôexposition √† compenser. Il parie sur les mouvements futurs du prix de l‚Äôactif pour se faire un profit. L‚Äôarbitrage implique de prendre une position sur deux ou plusieurs march√©s diff√©rents pour avoir un profit sans risque, i.e.¬†avec une certitude de gain.\nExemple d‚Äôopportunit√© d‚Äôarbitrage : Soit un actif sous-jacent qui se n√©gocie √† 100 $ sur le march√© A et √† 105 $ sur le march√© B. Un trader peut acheter l‚Äôactif sur le march√© A et le vendre sur le march√© B, r√©alisant ainsi un profit de 5 $ par unit√© sans risque.\nLes produits d√©riv√©s sont √©chang√©s sur des march√©s organis√©s (march√©s √† terme) ou de gr√© √† gr√© (Over-The-Counter, OTC). Les march√©s organis√©s sont r√©glement√©s, centralis√©s et proposent des contrats standardis√©s (quantit√©, √©ch√©ance, actif sous-jacent, etc.). Des bourses de contrats √† terme telles que le CME (Chicago Mercantile Exchange) en sont des exemples. Ces march√©s garantissent l‚Äôex√©cution des contrats gr√¢ce √† une chambre de compensation, √† travers l‚Äôutilisation de marges, c‚Äôest-√†-dire le d√©p√¥t d‚Äôune fraction de la valeur totale du contrat pour ouvrir une position. Ces marges seront ajust√©es au fur et √† mesure, √† une fr√©quence quotidienne (daily settlement, marking to market) que le prix de l‚Äôactif sous-jacent fluctue pour tenir compte des gains ou des pertes r√©alis√©s sur la position. Cela permet aisi de r√©duire le risque de contrepartie.\n√Ä l‚Äôinverse, les march√©s OTC sont d√©centralis√©s, moins r√©glement√©s et permettent de conclure des contrats sur mesure directement entre deux contreparties.\nSur ces march√©s, les transactions sont d√©termin√©es par deux prix cl√©s : le bid et le ask. Le bid est le prix auquel un acheteur est pr√™t √† acheter l‚Äôactif, tandis que le ask est le prix auquel un vendeur est pr√™t √† vendre. La diff√©rence entre ces deux prix est appel√©e le bid-ask spread, qui est un indicateur de la liquidit√© du march√© :\n\nUn spread √©troit refl√®te un march√© liquide et comp√©titif (souvent sur march√©s organis√©s).\nUn spread large refl√®te une liquidit√© plus faible ou une incertitude √©lev√©e (fr√©quent sur les march√©s OTC).\n\n\nForward\nUn contrat forward est un accord entre deux parties pour acheter ou vendre un actif √† un prix convenu \\(K\\) √† une date future \\(T\\). Contrairement aux contrats √† terme (futures), les contrats forward sont n√©goci√©s de gr√© √† gr√© et ne sont pas standardis√©s. G√©n√©ralement, les contrats forward sont hedg√©s par des institutions financi√®res, car elles permettent de neutraliser le risque.\nLe payoff d‚Äôun contrat forward est donn√© par la formule suivante : \\[\n\\text{Payoff}_{\\text{long position}} = S_T - K, \\quad\n\\text{Payoff}_{\\text{short position}} = K - S_T,\n\\]\no√π \\(S_T\\) est le prix de l‚Äôactif sous-jacent √† la date d‚Äô√©ch√©ance, et \\(K\\) est le prix convenu dans le contrat.\n\nüí° La vente d‚Äôun contrat √† terme correspond √† une position courte (short position), tandis que son achat correspond √† une position longue (long position).\n\nExemple : Soit une companie X qui doit payer ¬£10 millions le 31 mars, sachant qu‚Äôon est le 31 d√©cembre. Pour se prot√©ger contre la hausse du prix de la livre sterling, la compagnie X peut acheter un contrat forward, i.e.¬†il a l‚Äôobligation d‚Äôacheter le livre sterling √† un prix convenu. Supposons que le prix convenu soit de ¬£1,20. Si le prix de la livre sterling augmente √† ¬£1,30 √† l‚Äô√©ch√©ance, la compagnie X √©conomise ¬£1 million en utilisant le contrat forward. De ce fait, le risque est neutralis√©.\n\n\nSch√©matisation d‚Äôun contrat forward\n\n\n\n\n\nOptions\nLes options sont des produits d√©riv√©s qui donnent √† l‚Äôacheteur le droit, mais pas l‚Äôobligation, d‚Äôacheter ou de vendre un actif sous-jacent √† un prix convenu (prix d‚Äôexercice) √† une date future, √©chang√©s sur des march√©s organis√©s ou de gr√© √† gr√©. Il existe deux types d‚Äôoptions : les options d‚Äôachat (call options) et les options de vente (put options).\n\nüí° On dit qu‚Äôon est en position longue (long position) lorsqu‚Äôon ach√®te une option, et en position courte (short position) lorsqu‚Äôon la vend. Le vendeur d‚Äôune option est appel√© le ‚Äúpreneur de risque‚Äù, car il doit honorer l‚Äôoption si l‚Äôacheteur d√©cide de l‚Äôexercer. -Ex : Si l‚Äôon est en position courte sur une option de vente (put option), on a l‚Äôobligation d‚Äôacheter l‚Äôactif sous-jacent √† un prix convenu si l‚Äôoption est exerc√©e. Si l‚Äôon est en position longue, on a le droit d‚Äôexiger la vente de l‚Äôactif sous-jacent √† un prix convenu si l‚Äôoption est exerc√©e.*\n\nL‚Äôon peut utiliser les options pour hedger ou pour sp√©culer sur la hausse ou la baisse de son prix. Les options sont √©galement utilis√©es pour g√©n√©rer des revenus suppl√©mentaires gr√¢ce √† la vente d‚Äôoptions.\nLe payoff d‚Äôune option est donn√© par la formule suivante : \\[\n\\text{Payoff}_{\\text{call}} = \\max(S_T - K, 0), \\quad\n\\text{Payoff}_{\\text{put}} = \\max(K - S_T, 0),\n\\] o√π \\(S_T\\) est le prix de l‚Äôactif sous-jacent √† la date d‚Äô√©ch√©ance, et \\(K\\) est le prix d‚Äôexercice de l‚Äôoption. Le prix d‚Äôexercice est le prix auquel l‚Äôacheteur de l‚Äôoption peut acheter ou vendre l‚Äôactif sous-jacent. Le prix d‚Äôexercice est fix√© au moment de l‚Äôachat de l‚Äôoption et reste constant jusqu‚Äô√† l‚Äô√©ch√©ance de l‚Äôoption.\n\n\n\nSch√©matisation d‚Äôune option d‚Äôachat (call option) co√ªtant 2.5‚Ç¨\n\n\n\n\n\nSch√©matisation d‚Äôune option de vente (put option) co√ªtant 2.5‚Ç¨\n\n\n\n\n\n\nFutures\nLes contrats √† terme (futures) sont des contrats standardis√©s n√©goci√©s sur des march√©s organis√©s, qui obligent l‚Äôacheteur √† acheter et le vendeur √† vendre un actif sous-jacent √† un prix convenu √† une date future. Contrairement aux contrats forward, les contrats futures sont standardis√©s en termes de taille de contrat, de lieux de livraison, de date d‚Äô√©ch√©ance et de qualit√© de l‚Äôactif sous-jacent. Il existe √©galement des limites de prix pour √©viter des fluctuations excessives. Le payoff d‚Äôun contrat future est similaire √† celui d‚Äôun contrat forward.\nPar ailleurs, les contrats futures sont r√©gl√©s quotidiennement (daily settlement) par le biais d‚Äôune chambre de compensation, qui garantit l‚Äôex√©cution des contrats et r√©duit le risque de contrepartie. Cela signifie que les gains et les pertes sont calcul√©s quotidiennement, et les marges sont ajust√©es en cons√©quence. Les traders doivent maintenir un niveau de marge minimum pour √©viter d‚Äô√™tre appel√©s √† d√©poser des fonds suppl√©mentaires (margin call) si la valeur de leur position diminue.\nDe plus, pour √©viter toute opportunit√© d‚Äôarbitrage, les prix des contrats futures convergent vers le prix au comptant de l‚Äôactif sous-jacent √† l‚Äô√©ch√©ance du contrat. Cela signifie qu‚Äô√† la date d‚Äô√©ch√©ance, le prix d‚Äôun contrat future doit √™tre tr√®s proche du prix spot de l‚Äôactif sous-jacent. Dans le cas contraire, les traders pourraient r√©aliser un profit sans risque en achetant ou vendant l‚Äôactif sur le march√© au comptant tout en prenant une position oppos√©e sur le march√© des futures.\n\n\nSwaps\nLes swaps sont des contrats d√©riv√©s dans lesquels deux parties √©changent des flux de tr√©sorerie bas√©s sur des actifs sous-jacents ou des indices. Il est structur√© pour une dur√©e d√©termin√©e avec des paiements p√©riodiques (par exemple annuels ou semestriels). Le plus souvent, ces calculs de ces flux de tr√©sorerie font intervenir des taux d‚Äôint√©r√™t, qui font du swap de taux d‚Äôint√©r√™t le type de swap le plus courant.\nUn swap de taux d‚Äôint√©r√™t (Interest Rate Swap) est un contrat d√©riv√© dans lequel deux contreparties √©changent des flux financiers futurs sur un montant notionnel, en se basant sur des taux d‚Äôint√©r√™t diff√©rents : L‚Äôun fixe, l‚Äôautre variable (g√©n√©ralement bas√© sur le LIBOR ou un taux de r√©f√©rence √©quivalent).\nLes swaps de taux sont souvent utilis√©s pour g√©rer l‚Äôexposition au risque de taux d‚Äôint√©r√™t, par exemple transformer une dette √† taux variable en dette √† taux fixe; transformer une cr√©ance √† taux fixe en cr√©ance √† taux variable.\n\n\n\n\n\n\nExemple\n\n\n\n\n\nSoit Amazon avec une dette de 100 millions ‚Ç¨ √† taux variable du libor + 0.1% par an, sur 3 ans. Pour se pr√©munir contre le risque de hausse des taux d‚Äôint√©r√™t, A souhaite convertir cette dette √† taux variable en une dette √† taux fixe. De fait, Amazon conclut un swap de taux d‚Äôint√©r√™t avec Alibaba sur un notionnel N=100 millions ‚Ç¨, selon lequel : - Amazon s‚Äôengage √† payer un taux fixe de 5%, - et √† recevoir de la part d‚ÄôAlibaba un taux variable bas√© sur le Libor 6-mois.\nDe ce fait, la charge nette d‚Äôint√©r√™t d‚ÄôAmazon devient $ (Libor + 0,1%) - (Libor - 5%) = 5,1%$, au lieu de \\(Libor + 0,1\\%\\). La dette d‚ÄôAmazon est ainsi transform√©e en une dette √† taux fixe de 5,1% par an.\nSupposons de m√™me qu‚ÄôAlibaba avait, au moment de l‚Äôentr√©e au swap, une dette qui a √©t√© contract√©e √† un taux de 5,1% par an. De m√™me, Alibaba, en entrant dans ce swap, transforme son exposition au taux fixe en une exposition au taux variable de \\(5,1\\% - (5\\% - Libor) = Libor + 0,1\\%\\). La dette d‚ÄôAlibaba est ainsi transform√©e en une dette √† taux variable de Libor + 0,1% par an.\n\n\nSch√©matisation du contrat swap\n\n\n\n\n\n\nEn pratique, dans un swap, le notionnel n‚Äôest pas √©chang√©, mais sert de base pour le calcul des flux de tr√©sorerie √©chang√©s entre les contreparties. Lorsque le notionnel (N) est √©chang√© √† la maturit√© du swap, une √©quivalence du contrat avec des obligations peut √™tre √©tablie. En effet, les flux de tr√©sorerie √©chang√©s dans un swap peuvent √™tre vus comme des positions longues et courtes sur des obligations √† taux fixe et variable.\nDans l‚Äôexemple pr√©c√©dent, la position d‚ÄôAmazon peut √™tre ainsi vue comme une position longue d‚Äôune obligation √† taux fixe de 5% et une position courte d‚Äôune obligation sur le taux variable du libor + 0,1%, avec N =100 millions d‚Äôeuros. La valeur de ce contrat de swap peut √™tre exprim√©e comme la diff√©rence entre les valeurs actualis√©es des flux de tr√©sorerie futurs des obligations √† taux fixe et variable : \\(B_{fix} - B_{variable}\\). De m√™me pour la position d‚ÄôAlibaba.\nG√©n√©ralement, les contreparties du swap ne communiquent pas directement entre elles. De fait, il existe des institutions financi√®res appel√©es ‚Äúcontreparties centrales‚Äù (CCP) qui agissent comme interm√©diaires pour les swaps, qui sont certaines un gain certain du fait du co√ªt de la mise en place du swap, et qui garantissent l‚Äôex√©cution des contrats. Ces institutions centralisent les transactions de swap, r√©duisant ainsi le risque de contrepartie et augmentant la liquidit√© du march√© des swaps.\nDe plus, puisque les besoins des contreparties ne sont pas exactement oppos√©s, il existe des march√©s de swaps o√π une offre de taux fixe en echange d‚Äôun libor (bid) est contrebalanc√©e par une demande de taux fixe en √©change d‚Äôun libor (ask). Le bid, ici, est le taux fixe que le market maker est pr√™t √† payer en √©change de la percetion du LIBOR, tandis que l‚Äôask est le taux fixe que le market maker est pr√™t √† recevoir en √©change du paiement du LIBOR. La moyenne entre le bid et l‚Äôask est le taux swap, qui est le taux auquel les contreparties sont pr√™tes √† √©changer des flux de tr√©sorerie. Par exemple, si le bid est de 4,20% et l‚Äôask est de 4.60%, le taux swap est de 4.40%.\n\n\nBid and oÔ¨Äer fixed rates in the swap market and swap rates (percent per annum).\n\n\nMaturit√©\nBid\nAsk\nTaux Swap\n\n\n\n\n2 ans\n3.85‚ÄØ%\n4.10‚ÄØ%\n3.975‚ÄØ%\n\n\n5 ans\n4.00‚ÄØ%\n4.30‚ÄØ%\n4.15‚ÄØ%\n\n\n7 ans\n4.10‚ÄØ%\n4.45‚ÄØ%\n4.275‚ÄØ%\n\n\n10 ans\n4.20‚ÄØ%\n4.60‚ÄØ%\n4.40‚ÄØ%\n\n\n30 ans\n4.35‚ÄØ%\n4.80‚ÄØ%\n4.575‚ÄØ%\n\n\n\n\nSi l‚Äôon consid√®re un swap o√π le taux fixe est √©gale au taux swap, on peut supposer que la valeur nette du swap est nulle, i.e.¬†il n‚Äôy a pas de gain ou de perte nette pour les deux contreparties. En effet, si le taux fixe est √©gal au taux variable du libor, alors les flux de tr√©sorerie √©chang√©s entre les contreparties sont √©gaux. Autrement dit, le montant que l‚Äôune des contreparties paie est exactement compens√© par le montant que l‚Äôautre contrepartie re√ßoit. Dans ce cas, \\(B_{fix} = B_{variable}\\).\nDe ce fait, en √©galant les flux de tr√©sorerie futurs actualis√©s des deux obligations au notionnel, l‚Äôon peut d√©terminer le taux zero coupon. Cette relation est utile pour d√©terminer la courbe de taux zero coupon, qui est la courbe des taux d‚Äôint√©r√™t sans risque pour des maturit√©s diff√©rentes. De fait, elle permet de d√©terminer le taux d‚Äôactualisation appropri√© pour les flux de tr√©sorerie futurs.\nComme le taux libor n‚Äôest disponible que pour des maturit√©s allant jusqu‚Äô√† 12 mois, le taux swap est souvent utilis√© comme un taux sans risque pour les pr√™ts √† long terme. En effet, le taux swap est observable pour des maturit√©s allant jusqu‚Äô√† 30 ans, ce qui en fait un taux de r√©f√©rence utile pour l‚Äôactualisation des flux de tr√©sorerie futurs.\nComme le taux LIBOR, le taux swap n‚Äôest pas un taux de pr√™t sans risque. Cependant, il est raisonnablement proche du taux sans risque dans des conditions de march√© normales (i.e.¬†sans crise financi√®re).\n\nSource : Options, Futures, and Other Derivatives, John C. Hull, 11th edition, Pearson ‚Äì Chapter 7.Swaps"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html",
    "href": "2A/App_sup/reg_lin.html",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "La r√©gression lin√©aire est une m√©thode d‚Äôapprentissage supervis√© qui vise √† √©valuer, lorsqu‚Äôil existe, la relation lin√©aire entre une variable d‚Äôint√©r√™t et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de donn√©es constitu√© de n √©chantillons iid (ind√©pendant et identiquement distribu√©), le mod√®le de regression lin√©aire s‚Äô√©crit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\no√π \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l‚Äôerreur, l‚Äôinformation que les autres variables explicatives ne donnent pas.\nL‚Äôhypoth√®se fondamentale de la r√©gression lin√©aire est l‚Äôexistence d‚Äôune relation lin√©aire entre la variable cible et les variables explicatives. Pour s‚Äôassurer de la pertinence de cette hypoth√®se avant de proc√©der √† la r√©gression lin√©aire (√† l‚Äôaide de visualisation ou de tests- spearman, pearson, etc.)\nL‚Äôhypoth√®se de rang plein est la seconde plus grande hypoth√®se, elle stipule que les variables explicatives ne soient pas corr√©l√©es entre elles. Cette condition est n√©cessaire pour garantir l‚Äôunicit√© des estimations des param√®tres du mod√®le et ainsi l‚Äôidentifiabilit√© du mod√®le √©tudi√©\nPar ailleurs pour que les estimations des param√®tres du mod√®le lin√©aire soient fiables, les erreurs du mod√®le, repr√©sent√©es par \\(\\xi_i\\), doivent r√©pondre √† plusieurs crit√®res :\n\nErreurs centr√©es : La moyenne attendue des erreurs doit √™tre nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le mod√®le ne pr√©sente pas de biais syst√©matique dans les pr√©dictions.\nHomosc√©dasticit√© : La variance des erreurs doit √™tre constante pour toutes les observations, exprim√©e par \\(V[\\xi_i] = \\sigma^2\\). Cette propri√©t√© garantit que la pr√©cision des estimations est uniforme √† travers la gamme des valeurs pr√©dites.\nD√©corr√©lation des erreurs : Les erreurs doivent √™tre mutuellement ind√©pendantes, c‚Äôest-√†-dire que la covariance entre toute paire d‚Äôerreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour √©viter les biais dans les estimations des param√®tres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d‚Äôobserver une hypoth√®se suppl√©mentaire sur la loi des erreurs. En effet, les erreurs sont souvent suppos√©es suivre une loi normale, c‚Äôest √† dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Cel√† nous permet de faire des inf√©rences sur les param√®tres du mod√®le et de construire des intervalles de confiance.\n\n\nToutes les hypoth√®ses √©tant respect√©es, et sous reserve qu‚Äôil n‚Äôy a pas de multicolin√©arit√© entre les variables explicatives du mod√®les i.e.¬†\\(X^T X\\) est inversible(l‚Äôhypoth√®se de rang plein est respect√©e), l‚Äôestimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carr√© ordinaire est donn√© par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD‚Äôapr√®s le th√©or√®me de Gauss-Markov, l‚Äôestimateur \\(\\hat \\beta\\) est le meilleur estimateur lin√©aire non biais√© des param√®tres du mod√®le. En effet, il est l‚Äôestimateur avec la plus petite variance, parmi les estimateurs lin√©aires sans biais qui existent. Cet estimateur est ainsi appel√© BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont suppos√©es suivre une loi normale, l‚Äôestimateur \\(\\hat \\beta\\) est √©galement l‚Äôestimateur du maximum de vraisemblance des param√®tres du mod√®le et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL‚Äôestimateur de la variance des erreurs \\(\\sigma^2\\) est donn√© par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carr√©s des r√©sidus.\n\n\n\nDans l‚Äôoptique de mesurer la qualit√© du mod√®le, plusieurs m√©triques sont utilis√©es : le R¬≤, le R¬≤ ajust√©, l‚Äôerreur quadratique moyenne (MSE), des crit√®res d‚Äôinformations (AIC, BIC) etc.\n\n\nLe coefficient de d√©termination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliqu√©e par le mod√®le. Il est d√©fini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carr√©s totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carr√©s des r√©sidus.\nN√©anmoins, le \\(R^2\\) n‚Äôest pas une mesure parfaite de la qualit√© du mod√®le. En effet, il augmente avec le nombre de variables explicatives, m√™me si ces variables n‚Äôont pas de lien avec la variable cible. Pour pallier √† ce probl√®me, le \\(R^2\\) ajust√© est utilis√©. Il est d√©fini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes crit√®res AIC et BIC sont des crit√®res d‚Äôinformation qui servent √† mesurer l‚Äôattache du mod√®le aux donn√©es que nous avons ajust√©s avec une p√©nalit√© li√© soit aux nombres de variables inclus dans le mod√®les et/ou la taille de l‚Äô√©chantillon √©tudi√©. De fait, plus l‚ÄôAIC ou le BIC est faible, meilleur est le mod√®le, car cela signifie qu‚Äôil a le mod√®le choisie a une probabilit√© plus √©lev√©e d‚Äô√™tre correct et une complexit√© plus faible.\nMath√©matiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces √©quations sont visibles, nous constatons que le BIC est un crit√®re plus parcimonieux que le crit√®re AIC en raison de la p√©nalisation qui est plus √©lev√© lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l‚Äô√©chantillon s√©lectionn√©."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#estimation-des-param√®tres",
    "href": "2A/App_sup/reg_lin.html#estimation-des-param√®tres",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "Toutes les hypoth√®ses √©tant respect√©es, et sous reserve qu‚Äôil n‚Äôy a pas de multicolin√©arit√© entre les variables explicatives du mod√®les i.e.¬†\\(X^T X\\) est inversible(l‚Äôhypoth√®se de rang plein est respect√©e), l‚Äôestimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carr√© ordinaire est donn√© par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD‚Äôapr√®s le th√©or√®me de Gauss-Markov, l‚Äôestimateur \\(\\hat \\beta\\) est le meilleur estimateur lin√©aire non biais√© des param√®tres du mod√®le. En effet, il est l‚Äôestimateur avec la plus petite variance, parmi les estimateurs lin√©aires sans biais qui existent. Cet estimateur est ainsi appel√© BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont suppos√©es suivre une loi normale, l‚Äôestimateur \\(\\hat \\beta\\) est √©galement l‚Äôestimateur du maximum de vraisemblance des param√®tres du mod√®le et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL‚Äôestimateur de la variance des erreurs \\(\\sigma^2\\) est donn√© par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carr√©s des r√©sidus."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "Dans l‚Äôoptique de mesurer la qualit√© du mod√®le, plusieurs m√©triques sont utilis√©es : le R¬≤, le R¬≤ ajust√©, l‚Äôerreur quadratique moyenne (MSE), des crit√®res d‚Äôinformations (AIC, BIC) etc.\n\n\nLe coefficient de d√©termination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliqu√©e par le mod√®le. Il est d√©fini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carr√©s totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carr√©s des r√©sidus.\nN√©anmoins, le \\(R^2\\) n‚Äôest pas une mesure parfaite de la qualit√© du mod√®le. En effet, il augmente avec le nombre de variables explicatives, m√™me si ces variables n‚Äôont pas de lien avec la variable cible. Pour pallier √† ce probl√®me, le \\(R^2\\) ajust√© est utilis√©. Il est d√©fini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes crit√®res AIC et BIC sont des crit√®res d‚Äôinformation qui servent √† mesurer l‚Äôattache du mod√®le aux donn√©es que nous avons ajust√©s avec une p√©nalit√© li√© soit aux nombres de variables inclus dans le mod√®les et/ou la taille de l‚Äô√©chantillon √©tudi√©. De fait, plus l‚ÄôAIC ou le BIC est faible, meilleur est le mod√®le, car cela signifie qu‚Äôil a le mod√®le choisie a une probabilit√© plus √©lev√©e d‚Äô√™tre correct et une complexit√© plus faible.\nMath√©matiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces √©quations sont visibles, nous constatons que le BIC est un crit√®re plus parcimonieux que le crit√®re AIC en raison de la p√©nalisation qui est plus √©lev√© lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l‚Äô√©chantillon s√©lectionn√©."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#simulation-des-donn√©es",
    "href": "2A/App_sup/reg_lin.html#simulation-des-donn√©es",
    "title": "La r√©gression lin√©aire",
    "section": "1. Simulation des donn√©es",
    "text": "1. Simulation des donn√©es\nPour √©valuer l‚Äôint√©r√™t de la regr√©ssion lin√©aire, nous allons simuler un √©chantillon de taille n=200, o√π la variable cible Y est une fonction lin√©aire de la variable explicative X. La vraie relation est donn√©e par \\(Y = 2 + 3X + \\epsilon\\), o√π \\(\\epsilon \\sim N(0, 1.6)\\). De fait le mod√®le lin√©aire est ad√©quat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un mod√®le lin√©aire simple √† nos donn√©es, nous obtenons une estimation des param√®tres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du mod√®le suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le-1",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le-1",
    "title": "La r√©gression lin√©aire",
    "section": "2. Evaluation du mod√®le",
    "text": "2. Evaluation du mod√®le\n\n2.1. Hypoth√®ses sur les erreurs et l‚Äôexistence d‚Äôune relation lin√©aire\nPour √©valuer la qualit√© du mod√®le, nous allons tracer les r√©sidus studentis√©s en fonction des valeurs ajust√©es. Les r√©sidus studentis√©s sont les r√©sidus divis√©s par l‚Äô√©cart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajust√©es\", ylab=\"R√©sidus studentis√©s\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les r√©ponses pr√©dites par le mod√®le (fitted values) augmentent, les r√©sidus restent globalement uniform√©ment distribu√©s de part et d‚Äôautre de 0. Cela montre, qu‚Äôen moyenne, la droite de r√©gression, est bien adapt√©e aux donn√©es, et donc que l‚Äôhypoth√®se de lin√©arit√© est acceptable.\nSi l‚Äôon observait une forme de trompette, cel√† reviendrait √† soulever une question sur l‚Äôh√©t√©rosc√©dascit√© des r√©sidus, tandis qu‚Äôune forme de banane rev√®le plut√¥t une relation de non-lin√©arit√©.\nLorsque le nuage de point n‚Äôa pas de structure particuli√®re, a priori l‚Äôhypoth√®se d‚Äôhomosc√©dascticit√© n‚Äôest pas remise en question, comme cela semble √™tre le cas ici. Attention : ces principes peuvent parfois √™tre mis en d√©faut et il vaut toujours mieux r√©aliser plusieurs contr√¥les diff√©rents.\nPour v√©rifier l‚Äôhypoth√®se d‚Äôhomosc√©dasticit√©, nouspouvons √©galement utiliser le test de Breusch-Pagan. Ce test est bas√© sur la r√©gression des carr√©s des r√©sidus sur les variables explicatives. Si le test est significatif, l‚Äôhypoth√®se d‚Äôhomosc√©dasticit√© est rejet√©e.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a n√©cessit√© le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l‚Äôhypoth√®se de non corr√©lation des r√©sidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est bas√© sur l‚Äôautocorr√©lation des r√©sidus. Si le test est significatif, l‚Äôhypoth√®se de non corr√©lation des r√©sidus est rejet√©e.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l‚Äôhypoth√®se de normalit√© des r√©sidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est bas√© sur la comparaison des r√©sidus avec une loi normale. Si le test est significatif, l‚Äôhypoth√®se de normalit√© des r√©sidus est rejet√©e.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l‚Äôhypoth√®se de normalit√© des r√©sidus n‚Äôest pas rejet√©e.\n\n\n2.2. Qualit√© du mod√®le\nPour √©valuer la qualit√© du mod√®le, nous allons calculer le coefficient de d√©termination \\(R^2\\) et le \\(R^2\\) ajust√©.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajust√© de 0.97. Cela signifie que 97% de la variance de la variable cible est expliqu√©e par le mod√®le. Notre mod√®le de r√©gression lin√©aire est bien ajust√© √† nos donn√©es."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html",
    "href": "3A/Apprentisage-stat/Tp2.html",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear.\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable. In particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. The optimization problem of KRR is given by: \\[\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n\\] where \\(x_i\\) is the \\(i\\)-th row of the matrix \\(X\\) and \\(y_i\\) is the \\(i\\)-th element of the vector \\(y\\). The parameter \\(\\lambda\\) is the regularization parameter. The solution of the optimization problem is given by:\n\\[\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n\\]\nwhere \\(I_d\\) and \\(I_n\\) are the identity matrix.\nIn prediction, the target variable is given by: \\[\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n\\] where \\(\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}\\). We easily see that the prediction is a linear combination of the scalar product between the test point \\(x^*\\) and the training points \\(x_i\\), we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\\[\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n\\]\nwhere \\(K(x_i, x^*)\\) is the kernel function.\n\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n\nShow the code\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow the code\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n\nShow the code\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\n\nShow the code\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\n\nBest parameters by CV : {'alpha': np.float64(0.07196856730011521), 'gamma': np.float64(35.564803062231285)}\n\n\n\n\nShow the code\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\n\nRoot mean square error:  22.98\n\n\n\n\nShow the code\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\n\nShow the code\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by \\(\\tilde{x}=\\frac{\\sqrt(2)}{\\pi}\\cos(\\pi x)\\) and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n\nShow the code\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n\n\nShow the code\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n\nShow the code\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\n\nShow the code\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\n\n\nShow the code\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n\n{'alpha': np.float64(0.07196856730011521)}\n\n\n\n\nShow the code\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n\nShow the code\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow the code\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n\nShow the code\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\n\nShow the code\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\n\nBest parameters by CV : {'alpha': np.float64(0.07196856730011521), 'gamma': np.float64(35.564803062231285)}\n\n\n\n\nShow the code\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\n\nRoot mean square error:  22.98\n\n\n\n\nShow the code\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Now we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\n\nShow the code\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by \\(\\tilde{x}=\\frac{\\sqrt(2)}{\\pi}\\cos(\\pi x)\\) and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n\nShow the code\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n\n\nShow the code\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n\nShow the code\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "href": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Linear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\n\nShow the code\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\n\n\nShow the code\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n\n{'alpha': np.float64(0.07196856730011521)}\n\n\n\n\nShow the code\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "title": "Kernel Trick and SVM",
    "section": "I. Toy dataset",
    "text": "I. Toy dataset\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\n\nShow the code\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n\n\nText(0.5, 1.0, 'Two moons dataset')\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n\n\n\n1. K-nearest neighbors\n\n\nShow the code\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\nBest parameters by CV : {'n_neighbors': np.int64(1)}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\n\nShow the code\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2. Logistic regression\n\n\nShow the code\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n\n\n\n\nShow the code\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3. SVM\n\n\nShow the code\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\n\nBest parameters by CV : {'C': np.float64(0.030888435964774818), 'gamma': np.float64(3.727593720314938)}\n\n\n\n\nShow the code\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\n\nShow the code\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4. Conclusion\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\nWhenever we have a classification problem, it is hence always useful to try the SVM and the KNN."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "title": "Kernel Trick and SVM",
    "section": "II. Image dataset",
    "text": "II. Image dataset\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255). We are going to use the SVM with the gaussian kernel to classify the images.\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\n\nShow the code\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n\n\nDescription of train dataset : \n\n\n\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n\n\n\n\nShow the code\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n\nShow the code\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n\n\n\n\nShow the code\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n\n\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\n\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n\nShow the code\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n\n\n\n\nShow the code\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html",
    "href": "3A/Apprentisage-stat/Tp4.html",
    "title": "Features selection",
    "section": "",
    "text": "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features.\nThe usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\nIn this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n\\[I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)\\]\nwhere \\(p(x,y)\\) is the joint probability distribution function of X and Y, and \\(p(x)\\) and \\(p(y)\\) are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\nIn order to have confidence in this measure, we will consider a bivariate gaussian variable \\(Z=(X,Y)\\) with mean \\(\\mu = (0,0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}\\). We will compute the mutual information between X and Y for a grid a \\(\\rho\\) between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to \\(-\\frac{1}{2} \\log(1-\\rho^2)\\).\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset.\n\n\n\nWe will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n\n\n\n\nWe can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "href": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "title": "Features selection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "href": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "title": "Features selection",
    "section": "",
    "text": "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "href": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "title": "Features selection",
    "section": "",
    "text": "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "href": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "title": "Features selection",
    "section": "",
    "text": "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/gestion_actifs/TP-1.html",
    "href": "3A/gestion_actifs/TP-1.html",
    "title": "Asset management : risque de march√©",
    "section": "",
    "text": "La gestion d‚Äôactifs (ou asset management) est l‚Äôart de g√©rer un portefeuille d‚Äôactifs financiers. Il s‚Äôagit de g√©rer l‚Äôargent de clients ‚Äî particuliers ou institutionnels ‚Äî avec pour objectif principal de maximiser le rendement tout en minimisant le risque.\nLes √©tapes usuelles de gestion du risque\nOn distingue g√©n√©ralement quatre grandes cat√©gories de risques :\nCe sont des mesures quantitatives du risque. Parmi les plus utilis√©es : la volatilit√©, qui mesure la variation des rendements d‚Äôun actif par rapport √† sa moyenne; la Value at Risk (VaR), qui mesure la perte maximale anticip√©e sur un portefeuille, avec un certain niveau de confiance \\(\\alpha\\), sur un horizon \\(T\\), la Tracking Error, qui mesure l‚Äô√©cart de performance entre un portefeuille et son indice de r√©f√©rence.\nIl s‚Äôagit de mettre en place des r√®gles de gestion ou des limites pour √©viter des d√©rives du portefeuille. Cela peut inclure :\nConstitution du portefeuille\nDans notre √©tude, nous allons constituer un portefeuille de 10 actions choisies dans l‚Äôindice CAC 40, en leur attribuant des poids al√©atoires.\nLes actifs retenus sont les suivants :\nPuisque nous travaillons avec un portefeuille d‚Äôactions, donc le principal risque est le risque de march√© actions. Nous allons donc nous int√©resser √† trois indicateurs de risque : - la volatilit√© ex-ante, - la Value at Risk ex-ante, - la Tracking Error ex-ante\nc‚Äôest-√†-dire des mesures anticip√©es, bas√©es sur la composition actuelle du portefeuille, et non sur des donn√©es historiques (ex-post)."
  },
  {
    "objectID": "3A/gestion_actifs/TP-1.html#r√©cup√©ration-des-donn√©es",
    "href": "3A/gestion_actifs/TP-1.html#r√©cup√©ration-des-donn√©es",
    "title": "Asset management : risque de march√©",
    "section": "R√©cup√©ration des donn√©es",
    "text": "R√©cup√©ration des donn√©es\n\n\nShow the code\n#------------------------------------#\n#---------- Package Imports ---------#\n#------------------------------------#\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n\n\nShow the code\ndef get_data(start_date, end_date, index_ticker, tickers):\n    \"\"\"\n    Extraction de donn√©es de cours d'actions\n    \"\"\"\n    # Extraction des prix historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    # Extraction des prix historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\n\n\n\nShow the code\nend_date = pd.to_datetime(\"2025-01-27\")\nstart_date = end_date - timedelta(days=2*365)\n\nselected_assets = {\n    \"SAN.PA\" : \"Sanofi\",\n    \"GLE.PA\" : \"Soci√©t√© g√©n√©rale\",\n    \"HO.PA\" : \"Thales\",\n    \"ENGI.PA\" : \"Engie\",\n    \"CAP.PA\" : \"Capgemini\",\n    \"CA.PA\" : \"Carrefour\",\n    \"ORA.PA\" : \"Orange\",\n    \"AC.PA\" : \"Accor\",\n    \"OR.PA\" : \"L'Oreal\",\n    \"ACA.PA\" : \"Cr√©dit agricole\"\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker)\n\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**********            20%                       ]  2 of 10 completed[*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nShow the code\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.tail()\n\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nCA.PA\nCAP.PA\nENGI.PA\nGLE.PA\nHO.PA\nOR.PA\nORA.PA\nSAN.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-01-20\n48.259998\n14.175\n13.540\n161.899994\n15.800\n29.465000\n149.600006\n340.799988\n10.290\n98.849998\n\n\n2025-01-21\n48.320000\n14.170\n13.470\n163.399994\n15.695\n29.320000\n151.149994\n341.200012\n10.325\n99.019997\n\n\n2025-01-22\n49.049999\n14.065\n13.145\n162.500000\n15.525\n29.320000\n152.149994\n349.649994\n10.230\n98.849998\n\n\n2025-01-23\n48.580002\n14.245\n13.225\n164.100006\n15.500\n29.985001\n152.850006\n354.549988\n10.205\n99.889999\n\n\n2025-01-24\n48.980000\n14.225\n13.300\n167.500000\n15.500\n30.065001\n152.300003\n358.450012\n10.155\n100.160004\n\n\n\n\n\n\n\n\n\nShow the code\nbenchmark_data = data[\"benchmark_data\"]\nbenchmark_data.head()\n\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2023-01-30\n7082.009766\n\n\n2023-01-31\n7082.419922\n\n\n2023-02-01\n7077.109863\n\n\n2023-02-02\n7166.270020\n\n\n2023-02-03\n7233.939941\n\n\n\n\n\n\n\n\n\nShow the code\n# On attribue des poids √©quitables pour chaque action\nweights_by_asset = {ticker: 1 / len(assets_ticker) for ticker in assets_ticker}\n\n\nPour connaitre la valeur totale des actifs du portefeuille, nous allons utiliser la notion d‚Äôasset under management (AUM) d√©fini comme suit :\n\\[\nAUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times P_i(T_n),\n\\]\no√π \\(\\omega_i\\) est le poids de l‚Äôactif \\(i\\) dans le portefeuille et \\(P_i(T_n)\\) est le prix de l‚Äôactif \\(i\\) √† la date \\(T_n\\).\nPuisque les rendements sont les seuls facteurs de risque de l‚ÄôAUM, nous allons nous int√©resser √† la variation de l‚ÄôAUM entre deux dates \\(T_n\\) et \\(T_{n+1}\\), soit :\n\\[\n\\Delta AUM(T_n, T_{n+1}) = AUM(T_{n+1}) - AUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times (P_i(T_{n+1}) - P_i(T_n)).\n\\]\n\n\nShow the code\naum_series = portfolio_data.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\naum_series\n\nAUM = pd.DataFrame(aum_series, columns=[\"AUM\"])\nAUM.head()\n\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-01-30\n82.874496\n\n\n2023-01-31\n82.658041\n\n\n2023-02-01\n82.421507\n\n\n2023-02-02\n83.536796\n\n\n2023-02-03\n84.256733\n\n\n\n\n\n\n\n\n\nShow the code\n# --------------------------------------------- #\n# Evolution de la valeur totale du portefeuille\n# ---------------------------------------------- #\n\nplt.figure(figsize=(12, 4))\nplt.plot(AUM, label=\"AUM\")\nplt.title(\"Evolution de l'actif sous gestion\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# On s'interesse aux variations/rendements de l'AUM\nAUM[\"Variation\"] = AUM[\"AUM\"].pct_change()\nAUM[\"Variation\"].head()\n\n\nDate\n2023-01-30         NaN\n2023-01-31   -0.002612\n2023-02-01   -0.002862\n2023-02-02    0.013532\n2023-02-03    0.008618\nName: Variation, dtype: float64\n\n\n\nEstimation de la volatilit√©\nPour estimer la volatilit√© du portefeuille, on peut calculer l‚Äô√©cart-type des variations de l‚ÄôAUM. On fait le choix de calculer une volatilit√© ex-ante en se basant sur les variation historiques des prix des actifs avec une profondeur historique de 2 ans. Vu qu‚Äôon a une volatilit√© quotidienne, on va l‚Äôannualiser en multipliant par \\(\\sqrt{252}\\).\nEn g√©n√©ral, sur le march√© action, la volatilit√© quotidienne est environ de 1% et la volatilit√© annuelle est entre 10% et 20%.\n\n\nShow the code\n# Calcul de la volatilit√© du portefeuille\nvolatility_portfolio = np.std(AUM[\"Variation\"])\nannualized_volatility_portfolio = volatility_portfolio * np.sqrt(252)\nprint(f\"Volatilit√© de la performance quotidienne : {volatility_portfolio : .2%}\")\nprint(f\"Volatilit√© de la performance annuelle : {annualized_volatility_portfolio : .2%}\")\n\n\nVolatilit√© de la performance quotidienne :  0.87%\nVolatilit√© de la performance annuelle :  13.75%\n\n\n\n\nShow the code\n# Calcul de la volatilit√© de l'indice CAC 40\n\nbenchmark_data[\"Variation\"] = benchmark_data[\"^FCHI\"].pct_change()\nvolatility_benchmark = np.std(benchmark_data[\"Variation\"])\nannualized_volatility_benchmark = volatility_benchmark * np.sqrt(252)\n\nprint(f\"Volatilit√© de l'indice CAC 40 : {volatility_benchmark : .2%}\")\nprint(f\"Volatilit√© de l'indice CAC 40 annuelle : {annualized_volatility_benchmark : .2%}\")\n\n\nVolatilit√© de l'indice CAC 40 :  0.84%\nVolatilit√© de l'indice CAC 40 annuelle :  13.37%\n\n\nNotre portefeuille nous fournit une volatatilit√© quotidienne sup√©rieure de 3bps √† la volatilit√© du CAC 40. On retrouve sur √† peu pr√®s la m√™me volatilit√© du portefeuille et celle du CAC 40. Il y a donc une certaine homog√©n√©it√© dans le portefeuille que nous avons constitu√©.\n\nüí° Note : bp = 0,01%\n\n\n\nEstimation de la tracking error/erreur de suivi\nLa tracking error est une mesure de l‚Äô√©cart entre la performance d‚Äôun portefeuille et celle de son indice de r√©f√©rence. Elle est calcul√©e comme la volatilit√© de la diff√©rence entre les rendements du portefeuille et de l‚Äôindice de r√©f√©rence :\n\\[\nTE = \\sqrt{Var(R_p - R_b)}\n\\]\nLa tracking error mesure l‚Äôincercitude du portefeuille par rapport √† l‚Äôindice de r√©f√©rence, c‚Äôest une mesure relative. Plus la tracking error est √©lev√©e, plus le portefeuille est risqu√©. On ne souhaite sous ou sur-performer l‚Äôindice de r√©f√©rence. On souhaite suivre v√©ritablement l‚Äôindice de r√©f√©rence.\nPour l‚Äôannualiser, on multiplie par \\(\\sqrt{252}\\) en supposant que les performances quotidiennes sont ind√©pendantes et donc un utilise l‚Äôadditivit√© des variances.\n\n\nShow the code\nperformance_relative = AUM[\"Variation\"] - benchmark_data[\"Variation\"]\n\nplt.figure(figsize=(12, 4))\nplt.plot(performance_relative, label=\"Performance\")\nplt.title(\"Performance du portefeuille par rapport √† l'indice CAC 40\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Performance\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Calcul de la tracking error\n\nTE = np.std(AUM[\"Variation\"] - benchmark_data[\"Variation\"]) \nprint(f\"Tracking error : {TE : .2%}\")\n\nTE_annualized = TE * np.sqrt(252)\nprint(f\"Tracking error annualis√© : {TE_annualized : .2%}\")\n\n\nTracking error :  0.51%\nTracking error annualis√© :  8.05%\n\n\n\n\nEstimation de la Value-at-Risk (VaR)\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de rescaling, i.e.¬†\\(VaR = \\sigma \\times \\Phi^{-1}(\\alpha)\\). 2. Approche param√©trique : On suppose que les rendements suivent une loi normale. 3. Approche Monte Carlo : On simule les rendements futurs.\nPuisqu‚Äôon s‚Äôint√©resse √† un portefeuille d‚Äôactions qui a un indice de r√©f√©rence, on peut √©galement calculer la VaR relative. La VaR relative suit une philosophie proche du tracking error. Elle se calcule sur les √©carts entre le portefeuille et le benchmark. Elle sert √† mesurer de combien mon portefeuille sous-performe par rapport √† l‚Äôindice de r√©f√©rence.\n\n\nShow the code\n# --------------------------------- #\n# VaR historique\n# --------------------------------- #\n\nseuil = 99/100\n\nVaR_hist_portfolio = np.percentile(AUM[\"Variation\"].dropna(), 100*(1- seuil))\nprint(f\"VaR historique sur le portefeuille : {- VaR_hist_portfolio : .2%}\")\nprint(f\"VaR historique sur 20 jours sur le portefeuille : {-VaR_hist_portfolio*np.sqrt(20) : .2%}\")\n\nprint(\"=*=\"*10)\n\nVaR_hist_benchmark = np.percentile(benchmark_data[\"Variation\"].dropna(), 100*(1 - seuil))\nprint(f\"VaR historique sur l'indice CAC 40 : {-VaR_hist_benchmark : .2%}\")\nprint(f\"VaR historique sur 20 jours sur l'indice CAC 40 : {-VaR_hist_benchmark*np.sqrt(20) : .2%}\")\n\n\nVaR historique sur le portefeuille :  2.29%\nVaR historique sur 20 jours sur le portefeuille :  10.25%\n=*==*==*==*==*==*==*==*==*==*=\nVaR historique sur l'indice CAC 40 :  2.13%\nVaR historique sur 20 jours sur l'indice CAC 40 :  9.53%\n\n\n\n\nShow the code\n# ---------------------------------- #\n# VaR param√©trique\n# ---------------------------------- #\n\n# PnL ~ N(mu, sigma) ==&gt; PnL = mu + sigma * Z, o√π Z ~ N(0,1)\n# P(PnL &gt; -VaR) = alpha &lt;=&gt; P(mu + sigma * Z &gt; -VaR) = alpha &lt;=&gt; P(Z &lt; (-VaR - mu) / sigma) = 1 - alpha\n# Donc, -VaR = mu + sigma * quantile(1 - alpha), o√π quantile(1 - alpha) est le quantile de la loi normale standard\n\n\nmu = np.mean(AUM[\"Variation\"].dropna())\nprint(f\"mu sur le portefeuille : {mu : .2}\")\nsigma = np.std(AUM[\"Variation\"].dropna())\nprint(f\"sigma sur le portefeuille : {sigma : .2}\")\n\nVaR_param_portfolio  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_portfolio : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_portfolio * np.sqrt(20): .2%}\")\n\nprint(\"=*=\"*10)\n\nmu = np.mean(benchmark_data[\"Variation\"].dropna())\nprint(f\"mu sur le benchmark: {mu : .2}\")\nsigma = np.std(benchmark_data[\"Variation\"].dropna())\nprint(f\"sigma sur le benchmark : {sigma : .2}\")\n\nVaR_param_benchmark  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_benchmark : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_benchmark * np.sqrt(20): .2%}\")\n\n\nmu sur le portefeuille :  0.00022\nsigma sur le portefeuille :  0.0087\nVaR param√©trique sur le portefeuille :  1.99%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.91%\n=*==*==*==*==*==*==*==*==*==*=\nmu sur le benchmark:  0.00026\nsigma sur le benchmark :  0.0084\nVaR param√©trique sur le portefeuille :  1.93%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.65%\n\n\n\n\nShow the code\n# ---------------------------------- #\n# VaR relative\n# ---------------------------------- #\n\nVaR_hist_relative = np.percentile(performance_relative.dropna(), 100*(1- seuil))\nprint(f\"VaR historique relative : {- VaR_hist_relative : .2%}\")\nprint(f\"VaR historique relative sur 20 jours : {-VaR_hist_relative*np.sqrt(20) : .2%}\")\n\nprint(\"=*=\"*10)\n\nmu = np.mean(performance_relative.dropna())\nprint(f\"mu des performances relatives: {mu : .2}\")\nsigma = np.std(performance_relative.dropna())\nprint(f\"sigma des performances relatives : {sigma : .2}\")\n\nVaR_param_relative  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique relative : {VaR_param_relative : .2%}\")\nprint(f\"VaR param√©trique relative sur 20 jours : {VaR_param_relative * np.sqrt(20): .2%}\")\n\n\nVaR historique relative :  1.07%\nVaR historique relative sur 20 jours :  4.80%\n=*==*==*==*==*==*==*==*==*==*=\nmu des performances relatives: -3.5e-05\nsigma des performances relatives :  0.0051\nVaR param√©trique relative :  1.18%\nVaR param√©trique relative sur 20 jours :  5.29%"
  },
  {
    "objectID": "3A/gestion_actifs/TP-1.html#conclusion",
    "href": "3A/gestion_actifs/TP-1.html#conclusion",
    "title": "Asset management : risque de march√©",
    "section": "Conclusion",
    "text": "Conclusion\nLe risque de march√© constitue un enjeu central dans la gestion d‚Äôactifs. Il est inh√©rent √† toute exposition aux march√©s financiers et doit, √† ce titre, √™tre mesur√©, surveill√© et encadr√© avec rigueur afin de pr√©server les int√©r√™ts des investisseurs.\nDes outils tels que la volatilit√©, la tracking error et la Value at Risk (VaR) permettent de quantifier l‚Äôincertitude li√©e aux rendements du portefeuille et d‚Äôanticiper les pertes potentielles dans des conditions normales de march√©.\nEn compl√©ment, les stress tests jouent un r√¥le fondamental : ils permettent d‚Äô√©valuer la r√©silience du portefeuille face √† des sc√©narios extr√™mes, souvent absents des donn√©es historiques, mais pourtant plausibles dans un contexte de crise."
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html",
    "href": "3A/gestion_actifs/TP-3.html",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "",
    "text": "Le risque de valorisation est le risque que la valeur d‚Äôun actif financier soit inf√©rieure √† sa valeur comptable ou √† sa valeur de march√©. Ce risque peut √™tre caus√© par des fluctuations des taux d‚Äôint√©r√™t, des changements dans la qualit√© de cr√©dit de l‚Äô√©metteur, ou d‚Äôautres facteurs √©conomiques et financiers. Dans le cadre de nos travaux, nous allons nous interesser √† la valorisation d‚Äôune obligation."
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#i.-valorisation-dune-obligation-sans-risque-de-d√©faut",
    "href": "3A/gestion_actifs/TP-3.html#i.-valorisation-dune-obligation-sans-risque-de-d√©faut",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "I. Valorisation d‚Äôune obligation sans risque de d√©faut",
    "text": "I. Valorisation d‚Äôune obligation sans risque de d√©faut\nNous souhaitons valoriser une obligation sans risque de d√©faut, i.e.¬†\\(\\lambda = 0\\). Pour ce faire, nous fixons les param√®tres suivants :\n\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t sans risque\n\\(\\lambda = 0\\) : l‚Äôintensit√© de d√©faut\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\\(c = 0.02\\) : le coupon annuel\n\\(R = 0.4\\) : le taux de recouvrement\n\\(n = 10\\) : le nombre de coupons\n\\(t = 0\\) : l‚Äôinstant pr√©sent\n\n\n\nShow the code\nt=0\nlambda_ = 0\nr = 2/100\nT = 10\nc = 2/100\nR = 40/100\nN=1\n\nB_t = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t}\")\n\n\nPrix de l'obligation vu √† t=0 : 0.9981933497987289\n\n\nEn valorisant l‚Äôobligation, nous obtenons un prix de \\(B_0 \\approx 0.99\\). Ce prix est proche du nominal, car les taux de coupons sont √©gaux au taux de march√©. L‚Äôobligation est dite au pair (Prix = Nominal) car elle r√©mun√®re au taux de march√©. Si \\(c &gt; r\\), le prix de l‚Äôobligation sera sup√©rieur au nominal, car le march√© se serait ru√© sur cette obligation, car elle offrirait plus que le taux de march√©. Sinon, le prix de l‚Äôobligation sera inf√©rieur au nominal, car le march√© serait plus r√©ticent.\n\nüí° La condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est \\(c \\approx r + \\lambda\\). C‚Äôest une obligation qui permet de recouvrir ausi bien le risque de taux (\\(r\\)) et le risque de cr√©dit li√© √† l‚Äôintensit√© de d√©faut (\\(\\lambda\\)). Pour voir ceci, nous avons impl√©ment√© ci-dessous une fonction qui permet d‚Äôextrait le coupon qui permet d‚Äôavoir une obligation au pair, avec les m√™mes param√®tres que pr√©c√©demment.\n\n\n\nShow the code\nt=0\nlambda_ = 1/100\nr = 2/100\nT = 10\nR = 40/100\nN=1\n\n# M√©thode de dichotomie\ndef trouver_coupon(t, T, r, lambda_, R, N, dt=1, tol=1e-6):\n    \"\"\"\n    Trouve le coupon c tel que la valeur de l'obligation soit √©gale √† N.\n    \"\"\"\n    def equation(c):\n        return pricing_bond(t, c, T, r, lambda_, R, N, dt) - N\n    \n    c_opt = bisect(equation, 0, 1, xtol=tol)  # Recherche de c dans l'intervalle [0,1]\n    return c_opt\n\nc_opt = trouver_coupon(t=t, T=T, r=r, lambda_=lambda_, R=R, N=N)\nc_opt\n\nprint(f\"Coupon pour avoir une obligation au pair vu √† t={t} : {c_opt:.2%}\")\nB_t = pricing_bond(t=t,c=c_opt,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t}\")\n\n\nCoupon pour avoir une obligation au pair vu √† t=0 : 2.64%\nPrix de l'obligation vu √† t=0 : 1.000007783902349\n\n\n\nDe plus, lorsque l‚Äôintensit√© de d√©faut est tr√®s grande, on retrouve un prix √† peu pr√®s √©gal au taux de recouvrement. En effet, la probabilit√© de d√©faut est tr√®s grande et donc la probabilit√© qu‚Äôil y ait un recouvrement est tr√®s √©lev√©e. (voir exemple ci-dessous)\n\n\n\nShow the code\nt=0\nlambda_ = 10\nr = 2/100\nT = 10\nc = 2/100\nR = 40/100\nN=1\n\n\nB_t = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\nprint(f\"Prix de l'obligation vu √† t={t} : {B_t:.4f}\")\nprint(f\"Taux de recouvrement : {R:.4f}\")\n\n\nPrix de l'obligation vu √† t=0 : 0.3992\nTaux de recouvrement : 0.4000"
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#ii.-evolution-du-prix-de-lobligation-en-fonction-du-temps",
    "href": "3A/gestion_actifs/TP-3.html#ii.-evolution-du-prix-de-lobligation-en-fonction-du-temps",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "II. Evolution du prix de l‚Äôobligation en fonction du temps",
    "text": "II. Evolution du prix de l‚Äôobligation en fonction du temps\nDurant la vie de l‚Äôobligation, son prix √©volue en fonction des paiements de coupons. √Ä chaque distribution de coupon, une chute du prix de l‚Äôobligation est observ√©e. Cette baisse s‚Äôexplique par le fait que, juste avant le versement, le prix de l‚Äôobligation int√®gre la valeur du coupon √† percevoir. Une fois le coupon pay√© aux d√©tenteurs, cette valeur dispara√Æt, entra√Ænant m√©caniquement une diminution du prix de l‚Äôobligation, jusqu‚Äô√† atteindre le nominal de l‚Äôobligation ainsi que le dernier coupon.\nToutefois, apr√®s cette chute li√©e au d√©tachement du coupon, la valeur de l‚Äôobligation remonte progressivement √† mesure que l‚Äô√©ch√©ance du prochain coupon approche. Ce ph√©nom√®ne cr√©e une √©volution en dents de scie, o√π chaque baisse correspond √† un paiement de coupon et chaque remont√©e traduit l‚Äôaccumulation de la valeur du prochain paiement attendu.\nBien que ce ph√©nom√®ne soit logique et attendu, il peut √™tre per√ßu n√©gativement car la forme en dents de scie pourrait donner l‚Äôimpression d‚Äôune d√©gradation de la qualit√© de l‚Äôobligation. C‚Äôest pourquoi on distingue deux types de prix :\n\nLe dirty price (prix sale) : il correspond au prix de l‚Äôobligation tel qu‚Äôaffich√© sur le march√©, int√©grant les variations dues aux paiements de coupons.\nLe clean price (prix net ou pied de coupon) : il correspond au prix de l‚Äôobligation ‚Äúnettoy√©‚Äù des coupons accumul√©s. Ce prix est obtenu en soustrayant les int√©r√™ts courus au dirty price. Ainsi, le clean price permet d‚Äô√©valuer plus pr√©cis√©ment la valeur intrins√®que de l‚Äôobligation sans √™tre pollu√© par les variations dues aux paiements p√©riodiques de coupons. C‚Äôest cette valeur qui est g√©n√©ralement utilis√©e pour comparer les obligations entre elles.\n\nPour illustrer cette √©volution, nous avons trac√© l‚Äô√©volution du prix de l‚Äôobligation en fonction du temps. Nous avons fix√© les param√®tres suivants :\n\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t sans risque\n\\(\\lambda = 0.01\\) : l‚Äôintensit√© de d√©faut\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\\(c = 0.03\\) : le coupon annuel\n\\(R = 0.4\\) : le taux de recouvrement\n\\(n = 10\\) : le nombre de coupons\n\\(t = 0\\) : l‚Äôinstant pr√©sent\n\n\n\nShow the code\ndef myFloor(x):\n    if x==0:\n        return 0\n    if x==np.floor(x):\n        return x-1\n    return np.floor(x)\n\ndef clean_price(t,c,T,r,lambda_,R,N,dt=1):\n    \"\"\"\n    Fonction qui calcule le prix d'une obligation propre.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    cc = c * (t - myFloor(t))\n\n    return B_t - cc\n\n\n\n\nShow the code\nt=0\nlambda_ = 1/100\nr = 2/100\nT = 10\nc = 3/100\nR = 40/100\nN=1\n\n\ndirty_prices = []\nclean_prices = []\ngrid_values_c = np.arange(0,T+0.001,0.001)\nfor t in grid_values_c:\n    dirty_prices.append(pricing_bond(t,c,T,r,lambda_,R,N))\n    clean_prices.append(clean_price(t,c,T,r,lambda_,R,N))\n\n\nimport matplotlib.pyplot as plt\nplt.plot(grid_values_c,dirty_prices, label=\"Dirty prices\")\nplt.plot(grid_values_c,clean_prices, label=\"Clean prices\")\nplt.title(\"Dirty prices vs Clean prices\")\nplt.xlabel(\"t\")\nplt.grid()\nplt.legend()\nplt.ylabel(\"Prix de l'obligation\")\n\n\nText(0, 0.5, \"Prix de l'obligation\")\n\n\n\n\n\n\n\n\n\n\na. Cas extr√™mes\nNous avons analys√© l‚Äô√©volution du prix de l‚Äôobligation dans deux cas extr√™mes afin d‚Äôobserver l‚Äôimpact du coupon sur la dynamique des prix. Toutes choses √©gales par ailleurs, nous avons modifi√© le coupon de l‚Äôobligation tout en conservant les autres param√®tres constants. Les deux sc√©narios √©tudi√©s sont les suivants :\n\n$ c = 0.01 $ : le coupon est inf√©rieur au taux d‚Äôint√©r√™t sans risque.\n\n$ c = 0.05 $ : le coupon est sup√©rieur au taux d‚Äôint√©r√™t sans risque.\n\n\nCas 1 : $ c = 0.01 $\nLorsque $ c = 1% $ et que ce coupon est inf√©rieur au taux d‚Äôint√©r√™t sans risque $ r $, la valeur de l‚Äôobligation √©volue de mani√®re sp√©cifique :\n\nAu d√©part, l‚Äôobligation est escompt√©e car le coupon est faible, et les investisseurs anticipent un rendement global inf√©rieur au taux du march√©.\n\n√Ä mesure que l‚Äô√©ch√©ance approche, l‚Äôincertitude sur le paiement du coupon dispara√Æt progressivement. Les investisseurs deviennent de plus en plus certains que le paiement aura bien lieu.\n\n√Ä la veille du paiement, l‚Äôobligation converge vers un prix proche de $ N + c $ pour le dirty price (car elle inclut le coupon accumul√©) et $ N $ pour le clean price (qui exclut le coupon accumul√©).\n\nCela signifie que l‚Äôobligation s‚Äôappr√©cie au fil du temps en raison de la certitude croissante du paiement des flux futurs. En d‚Äôautres termes, plus l‚Äô√©ch√©ance se rapproche, plus l‚Äôinvestisseur est assur√© de recevoir ses paiements, ce qui entra√Æne une augmentation progressive de la valeur de l‚Äôobligation.\n\n\nShow the code\nlambda_ = 1/100\nr = 2/100\nT = 10\nc1 = 1/100\nR = 40/100\nN=1\n\nclean_prices1 = []\ndirty_prices1 = []\nfor t in grid_values_c:\n    B_t_dirty = pricing_bond(t=t,c=c1,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    B_t_clean = clean_price(t=t,c=c1,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    clean_prices1.append(B_t_clean)\n    dirty_prices1.append(B_t_dirty)\n\nplt.plot(grid_values_c,clean_prices1, label=\"Clean prices\")\nplt.plot(grid_values_c,dirty_prices1, label=\"Dirty prices\")\nplt.title(\"Clean prices vs Dirty prices\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"t\")\nplt.ylabel(\"Prix de l'obligation\")\n\n\nText(0, 0.5, \"Prix de l'obligation\")\n\n\n\n\n\n\n\n\n\n\n\nCas 2 : $ c = 0.05 $\nLorsque $ c = 5% $ et que ce coupon est sup√©rieur au taux d‚Äôint√©r√™t sans risque $ r $, la dynamique du prix de l‚Äôobligation suit une √©volution inverse :\n\nAu d√©part, l‚Äôobligation est pris√©e au-dessus du nominal (elle se n√©gocie avec une prime). Cela s‚Äôexplique par le fait que son coupon g√©n√©reux attire les investisseurs, qui consid√®rent que le rendement offert par l‚Äôobligation compense largement le risque de cr√©dit et est plus attractif que les opportunit√©s de placement √† taux sans risque.\n\n√Ä mesure que l‚Äô√©ch√©ance approche, la valeur de l‚Äôobligation se depr√©cie progressivement. En effet, √† chaque p√©riode, l‚Äôinvestisseur re√ßoit un coupon √©lev√©, mais √† l‚Äô√©ch√©ance, il ne r√©cup√®re que le nominal $ N $, ce qui entra√Æne une correction progressive du prix de march√©.\n\n√Ä la veille du remboursement, l‚Äôobligation converge vers $ N + c $ pour le dirty price (qui inclut le dernier coupon √† verser) et vers $ N $ pour le clean price.\n\nAinsi, cette obligation se d√©pr√©cie progressivement jusqu‚Äô√† l‚Äô√©ch√©ance, car l‚Äôeffet attractif du coupon √©lev√© s‚Äôamenuise √† mesure que le remboursement du capital nominal devient imminent. En d‚Äôautres termes, l‚Äôobligation part d‚Äôune valeur sup√©rieure √† son nominal mais perd progressivement sa prime √† l‚Äôapproche de l‚Äô√©ch√©ance.\n\n\nShow the code\nlambda_ = 1/100\nr = 2/100\nT = 10\nc2 = 5/100\nR = 40/100\nN=1\n\nclean_prices2 = []\ndirty_prices2 = []\nfor t in grid_values_c:\n    B_t_dirty = pricing_bond(t=t,c=c2,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    B_t_clean = clean_price(t=t,c=c2,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    clean_prices2.append(B_t_clean)\n    dirty_prices2.append(B_t_dirty)\n\nplt.plot(grid_values_c,clean_prices2, label=\"Clean prices\")\nplt.plot(grid_values_c,dirty_prices2, label=\"Dirty prices\")\nplt.title(\"Clean prices vs Dirty prices\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"t\")\nplt.ylabel(\"Prix de l'obligation\")\n\n\nText(0, 0.5, \"Prix de l'obligation\")"
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#iii.-√©volution-du-prix-en-fonction-du-taux-dint√©r√™t",
    "href": "3A/gestion_actifs/TP-3.html#iii.-√©volution-du-prix-en-fonction-du-taux-dint√©r√™t",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "III. √âvolution du prix en fonction du taux d‚Äôint√©r√™t",
    "text": "III. √âvolution du prix en fonction du taux d‚Äôint√©r√™t\nLe prix d‚Äôune obligation est une fonction d√©croissante du taux d‚Äôint√©r√™t. En effet, plus le taux d‚Äôint√©r√™t est √©lev√©, plus la valeur actualis√©e des flux futurs (coupons et remboursement du nominal) est faible, ce qui r√©duit m√©caniquement le prix de l‚Äôobligation.\nDans cette analyse, il est inutile de distinguer le dirty price et le clean price, car la diff√©rence entre les deux ne d√©pend pas du taux d‚Äôint√©r√™t. De plus, en consid√©rant $t = 0 \\(, il n'y a pas encore d‚Äôint√©r√™ts courus (\\)c = 0 $), donc les deux prix co√Øncident.\n\nObligation au pair\nAutour de $c - = 2% $, le prix de l‚Äôobligation est √©gal au nominal. Cela s‚Äôexplique par le fait que le taux de coupon est exactement √©gal au taux de march√©. L‚Äôobligation est alors dite ‚Äúau pair‚Äù, car les investisseurs n‚Äôont ni prime ni d√©cote √† appliquer sur son prix.\n\n\nExplication de la relation n√©gative entre prix et taux\nLa relation n√©gative entre le prix d‚Äôune obligation et le taux d‚Äôint√©r√™t s‚Äôexplique par l‚Äôeffet de substitution avec les nouvelles √©missions obligataires.\n\nLorsque les taux d‚Äôint√©r√™t augmentent, de nouvelles obligations sont √©mises avec des coupons plus attractifs.\n\nEn cons√©quence, les obligations existantes, qui offrent un coupon fixe plus faible, deviennent moins int√©ressantes pour les investisseurs. Leur prix diminue afin d‚Äôajuster leur rendement effectif au nouveau niveau des taux du march√©.\n\nInversement, si les taux d‚Äôint√©r√™t baissent, les obligations existantes deviennent plus attractives puisqu‚Äôelles offrent un coupon plus √©lev√© que les nouvelles √©missions, ce qui entra√Æne une hausse de leur prix sur le march√© secondaire.\n\nAinsi, la sensibilit√© d‚Äôune obligation aux variations de taux d‚Äôint√©r√™t, appel√©e ‚Äúduration‚Äù, est un √©l√©ment cl√© dans l‚Äô√©valuation du risque de taux et la gestion de portefeuille obligataire.\n\n\nShow the code\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_r = np.arange(0,1,0.001) \nfor r in grid_values_r:\n    B_t_dirty = pricing_bond(t=t,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_r,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Prix de l'obligation en fonction du taux d'int√©r√™t\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"r\")\nplt.ylabel(\"Prix de l'obligation\")\n\n\nText(0, 0.5, \"Prix de l'obligation\")"
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#iv.-sensibilit√©-du-prix-de-lobligation-au-taux-dint√©r√™t-duration",
    "href": "3A/gestion_actifs/TP-3.html#iv.-sensibilit√©-du-prix-de-lobligation-au-taux-dint√©r√™t-duration",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "IV. Sensibilit√© du prix de l‚Äôobligation au taux d‚Äôint√©r√™t / Duration",
    "text": "IV. Sensibilit√© du prix de l‚Äôobligation au taux d‚Äôint√©r√™t / Duration\nLa duration est une mesure de la sensibilit√© du prix d‚Äôune obligation aux variations du taux d‚Äôint√©r√™t. Elle permet d‚Äô√©valuer le risque de taux, c‚Äôest-√†-dire l‚Äôimpact d‚Äôune variation des taux sur la valeur de l‚Äôobligation.\n\nDans le cas de l‚Äô√©volution du prix en fonction du taux d‚Äôint√©r√™t, la duration sera donc la pente de la courbe repr√©sentant cette relation.\n\nMath√©matiquement, la duration est d√©finie comme la d√©riv√©e du prix de l‚Äôobligation par rapport au taux d‚Äôint√©r√™t :\n\\[\n\\delta = - \\frac{d B_t}{d r} \\times \\frac{1}{B_t}\n\\]\nEn utilisant une approximation en diff√©rences finies, on exprime cette d√©riv√©e de la mani√®re suivante :\n\\[\n\\frac{d B_t}{d r}  \\approx \\frac{B_t(r+\\Delta r) - B_t(r)}{\\Delta r}\n\\]\nCette sensibilit√© permet de mesurer la variation du prix de l‚Äôobligation en r√©ponse √† une fluctuation du taux d‚Äôint√©r√™t, offrant ainsi une √©valuation directe du risque de taux auquel est expos√© l‚Äôinvestisseur. De ce fait, si les taux d‚Äôint√©r√™t bouge de \\(\\Delta r\\) = 1%, alors les prix bougeront de -sensibilit√© * \\(\\Delta r\\) .\nNous allons impl√©menter ce calcul en Python, en prenant \\(\\Delta r = 1\\) bp (soit \\(0.0001\\) en notation d√©cimale).\n\n\nShow the code\ndef sensivity_to_rate(t,c,T,r,lambda_,R,N,dt=1,dr= 0.01/100):\n    \"\"\"\n    Fonction qui calcule la sensibilit√© d'une obligation √† un taux d'int√©r√™t.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    B_t_plus = pricing_bond(t,c,T,r+dr,lambda_,R,N,dt)\n    sensivity = -((B_t_plus - B_t)/dr) * (1/B_t)\n    return sensivity\n\n\nSi les taux d‚Äôint√©r√™t bouge de \\(\\Delta r\\) = 1%, alors les prix bougeront de -sensibilit√© * \\(\\Delta r\\) = -8,64 * 1%. La duration va √™tre souvent proche de la maturit√©.\n\n\nShow the code\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nr = 2/100\nN=1\n\nsensivity_to_rate(t,c,T,r,lambda_,R,N)\n\n\nnp.float64(8.643982489102903)\n\n\nD‚Äôun point de vue graphique, il existe une certaine identit√© entre la maturit√© et la duration, car cette derni√®re peut √™tre interpr√©t√©e comme le barycentre des flux futurs de l‚Äôobligation. Plus ces flux sont concentr√©s dans le temps, plus leur pond√©ration affecte la sensibilit√© du prix aux variations des taux d‚Äôint√©r√™t.\n\nLien entre duration et maturit√©\n\nLa duration est une approximation de la dur√©e d‚Äôexposition au risque, ajust√©e en fonction des flux de paiements.\n\nElle est souvent proche de la maturit√© moyenne de l‚Äôobligation, bien que l√©g√®rement inf√©rieure (environ 80% de la maturit√© totale, en fonction des conditions de march√© et du niveau des coupons).\n\nAinsi, plus l‚Äô√©ch√©ance de l‚Äôobligation se rapproche, plus la duration tend √† augmenter, car les flux futurs deviennent plus proches dans le temps, rendant l‚Äôobligation plus sensible aux variations des taux d‚Äôint√©r√™t.\n\nLorsque la maturit√© de l‚Äôobligation approche, sa sensibilit√© au taux d‚Äôint√©r√™t tend √† augmenter. En effet, la duration peut √™tre interpr√©t√©e comme une mesure du temps moyen pond√©r√© pendant lequel l‚Äôinvestisseur est expos√© au risque de taux.\n\n\n\nShow the code\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_T = np.arange(1,20,1)\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_rate(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\n\n\nCas particulier : absence de coupon, de taux d‚Äôint√©r√™t et de risque de d√©faut\nLorsque le coupon, le taux d‚Äôint√©r√™t et l‚Äôintensit√© de d√©faut sont nuls, la duration est exactement √©gale √† la maturit√© de l‚Äôobligation. Puisque la duration peut √™tre interpr√©t√©e comme le barycentre des flux futurs de l‚Äôobligation, lorsque le coupon et l‚Äôintensit√© de d√©faut sont nuls, tous les flux sont concentr√©s √† l‚Äô√©ch√©ance, ce qui √©quivaut √† la maturit√© de l‚Äôobligation.\n\n\nShow the code\nt=0\nc = lambda_ = 10e-6\nR = 40/100\nN=1\n\n\ndirty_prices = []\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_rate(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© au taux en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\n\nDans le risque de taux, le principale indicateur est la duration."
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#v.-mod√®lisation-de-la-var",
    "href": "3A/gestion_actifs/TP-3.html#v.-mod√®lisation-de-la-var",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "V. Mod√®lisation de la VaR",
    "text": "V. Mod√®lisation de la VaR\n\nV.1. Approche par la sensibilit√©\nSelon le mod√®le de Hull et White, le taux d‚Äôint√©r√™t est mod√©lis√© par :\n\\[\ndr = \\theta ( \\mu - r) dt + \\sigma dW\n\\]\no√π \\(\\theta\\) est le coefficient de vitesse de r√©version, \\(\\mu\\) est le taux d‚Äôint√©r√™t moyen, \\(\\sigma\\) est la volatilit√© du taux d‚Äôint√©r√™t et \\(dW\\) est un mouvement brownien. Ce mod√®le a la sp√©cifit√© d‚Äô√™tre normale. De ce fait, \\(\\Delta r \\sim N(0, \\sigma^2 \\Delta t)\\). En faisant l‚Äôapproximation de la variation du prix, il est possible d‚Äôapprocher une VaR par la sensibilit√© :\n\\[\n\\begin{aligned}\nDuration &= - \\frac{d B_t}{d r} \\times \\frac{1}{B_t} \\\\\n\\frac{d B_t}{B_t} &\\approx - Duration \\times  \\Delta r \\\\\n\\end{aligned}\n\\]\n\n\nShow the code\n# Objectif : √©crire une fonction qui calcule la VaR avec l'approche par duration\nfrom scipy.stats import norm\n\ndef sensitive_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h,dt=1,alpha=0.99) :\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    Duration = sensivity_to_rate(t,c,T,r,lambda_,R,N,dt)\n\n    return -(mu + Duration * sigma * np.sqrt(h) * norm.ppf(1 - alpha))\n\n#--------------------------------------\n# Param√®tres du mod√®le de taux\n#---------------------------------------\n\nmu = 0\nh = 1/12\nsigma = 0.01\n\n#--------------------------------------\n# Param√®tres de la valorisation du bond\n#---------------------------------------\n\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nT=10\n\nVaR = sensitive_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h=h,dt=1,alpha=0.99)\nprint(f\"VaR estim√© par l'approche par la sensibilit√© : {VaR:.4%}\")\n\n\nVaR estim√© par l'approche par la sensibilit√© : 5.8049%\n\n\nPour cette estimation, nous avions suppos√© que la volatilit√© \\(\\sigma = 1\\%\\). Cependant, cette hypoth√®se pourrait s‚Äô√©carter de la r√©alit√©. Pour ce faire, nous allons extraire les donn√©es de taux d‚Äôint√©r√™t et calculer la volatilit√© empirique. L‚ÄôESTR est un taux un jour collat√©ralis√©,c‚Äôest donc quasiment sans risque. Nous allons donc utiliser ce taux pour calculer la volatilit√© empirique. Pour cela, nous allons utiliser l‚Äôhistorique des taux d‚Äôint√©r√™t de l‚ÄôESTR sur une p√©riode de 5 ans, i.e.¬†10/03/2025 - 12/03/2020, disponible sur ce lien.\nPour ce faire, nous utilisons la formule suivante :\n\\[\n\\sigma = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (r_i - \\bar{r})^2}\n\\]\nNous constatons que la volatilit√© empirique sur les 5 ans est de 25.45%. Cette valeur est tr√®s √©lev√©e, ce qui signifie que les taux d‚Äôint√©r√™t ont connu des variations importantes sur cette p√©riode. Cela peut √™tre du √† la trop grande p√©riode utilis√©e pour la calibration de la volatilit√©. Pour ce faire, nous avons restreint la p√©riode √† 1 an, i.e.¬†10/03/2025 - 10/03/2024. La volatilit√© empirique sur cette p√©riode est de 1.04%. Cette valeur est plus coh√©rente avec les taux d‚Äôint√©r√™t sans risque.\n\n\nShow the code\nimport pandas as pd\nestr_df = pd.read_excel(\"estr.xlsx\", skiprows=6)\n#date as date\nestr_df[\"Date\"] = pd.to_datetime(estr_df[\"Date\"], format=\"%Y-%m-%d\")\nestr_df = estr_df.set_index(\"Date\")\nestr_df = estr_df.sort_index()\n\nestr_df.head()\n\n\n\n\n\n\n\n\n\nPX_LAST\nCHG_PCT_1D\n\n\nDate\n\n\n\n\n\n\n2020-03-12\n-0.540\n0.5525\n\n\n2020-03-13\n-0.541\n-0.1852\n\n\n2020-03-16\n-0.536\n0.9242\n\n\n2020-03-17\n-0.531\n0.9328\n\n\n2020-03-18\n-0.529\n0.3766\n\n\n\n\n\n\n\n\n\nShow the code\nvol_est = np.std(estr_df[\"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 5 ans: {vol_est:.4f}\")\n\n# volatilit√© sur 1 an\nvol_est= np.std(estr_df.loc[\"2024-03-10\":\"2025-03-10\", \"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 1 an : {vol_est:.4f}\")\n\n\nVolatilit√© estim√©e sur 5 ans: 0.2545\nVolatilit√© estim√©e sur 1 an : 0.0104\n\n\n\n\nV.2. Approche avec un repricing\nCette approche consiste √† revaloriser, sous l‚Äôhypoth√®se de normalit√© des taux, l‚Äôobligation √† l‚Äôinstant \\(t\\) pour un taux \\(r + \\Delta r\\) et de calculer la perte maximale possible. De ce fait, elle est plus pr√©cise que l‚Äôapproche par la sensibilit√©.\nPuisque dans le cas d‚Äôune obligation, ce dont on veut se pr√©munir c‚Äôest de la hausse des taux (puisqu‚Äôelle fait baisser le taux d‚Äôint√©r√™t). De ce fait, la VaR est donn√©e par :\n\\[\n\\text{VaR} =  - \\frac{B_t(r + \\Delta r) - B_t }{B_t},\n\\]\no√π \\(\\Delta r\\) est la variation des taux d‚Äôint√©r√™t.\nPar d√©finition, la VaR estim√©e sera plus basse que l‚Äôautre approche en raison de la convexit√© de l‚Äô√©volution du prix de l‚Äôobligation en fonction des taux d‚Äôint√©r√™t. La Var par l‚Äôapproche de la sensibilit√©, quant √† elle, suppose une lin√©arit√© de l‚Äô√©volution du prix de l‚Äôobligation en fonction des taux d‚Äôint√©r√™t.\n\n\nShow the code\ndef repricing_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h,dt=1,alpha=0.99) :\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    delta_r = mu + sigma * np.sqrt(h) * norm.ppf(alpha)\n    P_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    P_t_shocked = pricing_bond(t,c,T,r+delta_r,lambda_,R,N,dt)\n\n    VAR = -( (P_t_shocked - P_t)/P_t)\n\n    return VAR, delta_r\n\n\n#--------------------------------------\n# Param√®tres du mod√®le de taux\n#---------------------------------------\nmu = 0\nh = 1/12\nsigma = 0.01\n\n#--------------------------------------\n# Param√®tres de la valorisation du bond\n#---------------------------------------\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nT=10\n\nVaR, delta_r = repricing_VaR(mu,sigma,t,c,T,r,lambda_,R,N,h=h,dt=1,alpha=0.99)\n\nprint(f\"VaR estim√© par l'approche par le r√©ajustement : {VaR:.4%}\")\nprint(f\"Choc de taux : {delta_r:.2%}\")\n\n\nVaR estim√© par l'approche par le r√©ajustement : 5.6272%\nChoc de taux : 0.67%"
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#vi.-focus-risque-de-cr√©dit-contrepartie",
    "href": "3A/gestion_actifs/TP-3.html#vi.-focus-risque-de-cr√©dit-contrepartie",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "VI. Focus risque de cr√©dit & contrepartie",
    "text": "VI. Focus risque de cr√©dit & contrepartie\nLe risque de cr√©dit est le risque que l‚Äô√©metteur d‚Äôune obligation ne soit pas en mesure de respecter ses engagements financiers, entra√Ænant ainsi des pertes pour les investisseurs. Ce risque est particuli√®rement pertinent dans le contexte des obligations, car il peut affecter la valeur de l‚Äôobligation et la capacit√© de l‚Äôinvestisseur √† r√©cup√©rer son capital. Le risque de contrepartie, quant √† lui, est le risque que la contrepartie d‚Äôune transaction financi√®re ne respecte pas ses obligations contractuelles. Dans le cas des obligations, cela peut se traduire par le non-paiement des coupons ou du capital √† l‚Äô√©ch√©ance. Ce risque est particuli√®rement important dans les transactions de gr√© √† gr√© (OTC), o√π les parties ne sont pas soumises aux m√™mes exigences r√©glementaires que celles des march√©s organis√©s.\n\nVI.1. Comment estimer l‚Äôintensit√© de d√©faut ?\nPr√©cedemment, nous avons valoriser l‚Äôobligation de la mani√®re suivante :\n\\[\n\\begin{aligned}\nB_t &= C_t + N_t + R_t \\\\\n&= N \\left[ \\sum_{i=1}^{n}  c \\times e^{-(r + \\lambda) \\times (T_i\n-t)} \\mathbb{1}_{T_i \\geq t} + e^{-(r+\\lambda)(T-t)} \\mathbb{1}_{T \\geq t} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\mathbb{1}_{T \\geq t} \\right]\n\\end{aligned}\n\\]\net nous avons signfi√© la condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est \\(c \\approx r + \\lambda\\). Cependant, ce n‚Äôest pas exact.\nSoit un coupon pay√© en continu, la valorisation de l‚Äôobligation est donn√©e par :\n\\[\n\\begin{aligned}\nB_t &= N \\left[ \\int_{t}^{T} c e^{-(r + \\lambda) \\times (u-t)} du + e^{-(r+\\lambda)(T-t)} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\right]\\\\\n&= N \\left[ \\frac{c}{r + \\lambda} \\left(1 - e^{-(r + \\lambda)(T-t)} \\right) + e^{-(r+\\lambda)(T-t)} +  \\lambda R \\times \\frac{1 - e^{-(r+\\lambda)(T-t)}}{r+\\lambda} \\right]\\\\\nB_t = N &\\Leftrightarrow c = r + \\lambda (1 - R) \\\\\n& c - r = \\lambda (1 - R)\n\\end{aligned}\n\\]\nDe ce fait, en extrayant \\(c\\), la condition dans laquelle l‚Äôobligation √©mise vaut 100% du nominal, i.e.¬†au pair, est $c = r + (1 - R) $.\n\nüí° \\(s = c-r\\) est la prime de cr√©dit ou encore le spread de cr√©dit. C‚Äôest la prime que l‚Äôinvestisseur demande pour le risque de cr√©dit. Si \\(c &gt; r\\), l‚Äôobligation est √©mise √† un prix sup√©rieur √† 100% du nominal. Si \\(c &lt; r\\), l‚Äôobligation est √©mise √† un prix inf√©rieur √† 100% du nominal. Ce spread permet de faire la relation entre la PD exprim√© par \\(\\lambda\\) et la LGD exprim√© par \\(1 -R\\). Cette information est plus facile √† avoir que l‚Äôintensit√© de d√©faut car le spread est cot√© sur le march√© √† travers les CDS. Par d√©finition, on en d√©duit facilement que plus cettre prime est √©lev√©, plus l‚Äôemetteur est risqu√©.\n\n\nTake away : Les notions de duration et de spread sont tr√®s importants dans la mod√©lisation du risque de taux\n\n\ncas de l‚Äôargentine, pays risqu√©\n\n\nShow the code\ns = 1031/10000 \nR = 0.4\nlambda_ = s / (1 - R)\nt=0\n\nprint(f\"Spread de cr√©dit : {s:.2%}\")\nprint(f\"Recouvrement : {R:.2%}\")\nprint(f\"Intensit√© de d√©faut : {lambda_:.2%}\")\nPS = [np.exp( - lambda_ * (T - t)) for T in range(1, 21)]\n\nplt.plot(range(1,21),PS)\nplt.title(\"Probabilit√© de survie en fonction de la maturit√©\")\nplt.xlabel(\"T\")\nplt.ylabel(\"Probabilit√© de survie\")\nplt.grid()\n\n\nSpread de cr√©dit : 10.31%\nRecouvrement : 40.00%\nIntensit√© de d√©faut : 17.18%\n\n\n\n\n\n\n\n\n\n\n\ncas de la France\n\n\nShow the code\n# Cas de la France\ns = 32.4/10000 \nR = 0.4\nlambda_ = s / (1 - R)\nt=0\n\nprint(f\"Spread de cr√©dit : {s:.2%}\")\nprint(f\"Recouvrement : {R:.2%}\")\nprint(f\"Intensit√© de d√©faut : {lambda_:.2%}\")\nPS = [np.exp( - lambda_ * (T - t)) for T in range(1, 21)]\n\nplt.plot(range(1,21),PS)\nplt.title(\"Probabilit√© de survie en fonction de la maturit√©\")\nplt.xlabel(\"T\")\nplt.ylabel(\"Probabilit√© de survie\")\nplt.grid()\n\n\nSpread de cr√©dit : 0.32%\nRecouvrement : 40.00%\nIntensit√© de d√©faut : 0.54%\n\n\n\n\n\n\n\n\n\nEn comparant les deux pays, l‚ÄôArgentine et la France. On sait que l‚Äôargentine est un pays plus risqu√© que la France. Cela se voit √©galement au niveau des spreads de cr√©dit. En effet, le spread de l‚ÄôArgentine est plus √©lev√© que celui de la France. Cela signifie que les investisseurs demandent une prime de risque plus √©lev√©e pour investir dans des obligations argentines que dans des obligations fran√ßaises. De plus, en regardant la probabilit√© de survie des deux pays, on constate que la probabilit√© de survie de l‚ÄôArgentine est plus faible que celle de la France. Cela signifie que les investisseurs consid√®rent que l‚ÄôArgentine est plus susceptible de faire d√©faut que la France.\n\n\n\nVI.2 Sensibilit√© cr√©dit\n\n\nShow the code\ndef sensivity_to_credit(t,c,T,r,lambda_,R,N,dt=1,dlambda_= 0.01/100):\n    \"\"\"\n    Fonction qui calcule la sensibilit√© d'une obligation √† un taux d'int√©r√™t.\n    \"\"\"\n    B_t = pricing_bond(t,c,T,r,lambda_,R,N,dt)\n    B_t_plus = pricing_bond(t,c,T,r,lambda_ + dlambda_,R,N,dt)\n    \n    sensivity = -((B_t_plus - B_t)/dlambda_) * (1/B_t) * (1/ (1-R))\n\n    return sensivity\n\n\n\n\nShow the code\nt=0\nlambda_ = 1/100\nT = 10\nc =3/100\nR = 40/100\nr = 2/100\nN=1\n\nsensivity_to_credit(t,c,T,r,lambda_,R,N,dlambda_=0.01/100)\n\n\nnp.float64(8.821190868023761)\n\n\n\n\nShow the code\nt=0\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\n\ndirty_prices = []\ngrid_values_T = np.arange(1,20,1)\nfor T in grid_values_T:\n    B_t_dirty = sensivity_to_credit(t,c,T,r,lambda_,R,N)\n    dirty_prices.append(B_t_dirty)\n\nplt.plot(grid_values_T,dirty_prices, label=\"Dirty prices\")\nplt.title(\"Sensibilit√© cr√©dit en fonction de la maturit√©\")\nplt.legend()\nplt.grid()\nplt.xlabel(\"T\")\nplt.ylabel(\"Sensibilit√©\")\n\n\nText(0, 0.5, 'Sensibilit√©')\n\n\n\n\n\n\n\n\n\nSoit le mod√®le normale pour le taux :\n\\[\ndr_t = \\theta (\\mu - r_t) dt + \\sigma_r dW_t\n\\]\net un mod√®le log normale pour le spread de cr√©dit, puisqu‚Äôil ne peut √™tre n√©gatif : \\[\n\\frac{ds_t}{s_t} = \\sigma_s dZ_t\n\\]\nCes deux mod√®les sont li√©s par \\(dW_t dZ_t = \\rho dt\\). On peut exprimer cette relation par \\(Z_t = \\rho W_t + \\sqrt{1 - \\rho^2} V_t\\), o√π \\(V_t\\) est un mouvement brownien standard ind√©pendant de \\(W_t\\). Cette corr√©lation est positive. &gt; üí° si les taux d‚Äôint√©r√™t montent, le risque de cr√©dit augmente puisque les entreprises ont plus de mal √† rembourser leur dette. De ce fait, le spread de cr√©dit augmente.\nSupposons qu‚Äôon cherche √† calculer une VaR d‚Äôhorizon \\(h\\). Pour cela, il faudra faire des simulations de Monte Carlo pour les deux mod√®les.\n\\[\n\\begin{aligned}\nr_{t+h} &= r_t + \\theta (\\mu - r_t) h + \\sigma_r \\sqrt{h} W_t \\\\\ns_{t+h} &= s_t \\exp(\\sigma_s \\sqrt{h} Z_t) \\\\\nou \\quad s_{t+h} &= s_t (1 + \\sigma_s \\sqrt{h} Z_t) \\quad (\\text{par DL})\n\\end{aligned}\n\\]\nPosons les param√®tres suivants :\n\n\\(\\theta = 0.1\\) : le coefficient de vitesse de r√©version\n\\(\\mu = 0.02\\) : le taux d‚Äôint√©r√™t moyen\n\\(\\sigma_r = 0.01\\) : la volatilit√© du taux d‚Äôint√©r√™t\n\\(\\sigma_s = 0.4\\) : la volatilit√© du spread de cr√©dit\n\\(\\rho = 0.4\\) : la corr√©lation entre les deux mouvements browniens\n\\(r = 0.02\\) : le taux d‚Äôint√©r√™t initial\n\\(h = 1/12\\) : l‚Äôhorizon de calcul de la VaR\n\\(c = 3\\%\\) : le coupon annuel\n\\(R = 40\\%\\) : le taux de recouvrement\n\\(N = 1\\) : le nominal de l‚Äôobligation\n\\(T = 10\\) : l‚Äô√©ch√©ance de l‚Äôobligation\n\n\n\nShow the code\nimport numpy as np\n\ndef MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1):\n    # print(\"Parameters\")\n    # print(f\"coupon : {c:.2%}\")\n    # print(f\"maturity : {T} years\")\n    # print(f\"risk free rate : {r:.2%}\")\n    # print(f\"credit spread : {lambda_:.2%}\")\n    # print(f\"recovery rate : {R:.2%}\")\n    # print(f\"nominal : {N}\")\n\n    prices = []\n    \n    P_0 = pricing_bond(t=0,c=c,T=T,r=r,lambda_=lambda_,R=R,N=N,dt=dt)\n    # print(f\"P0 : {P_0}\")\n\n    s_0 = lambda_ * (1 - R)\n    r_0 = r\n\n    for t in range(N_MC):\n        # Generate correlated Brownian motions\n        W1 = np.random.normal()\n        W2 = rho * W1 + np.sqrt(1 - rho) * np.random.normal()\n\n        # Euler discretization \n        r_h = r_0 + sigma_r * np.sqrt(h) * W1\n        s_h = s_0 * ( 1 + sigma_s * np.sqrt(h) * W2)\n        lambda_h = s_h / (1 - R)\n        # print(\"=\"*30)\n        # print(\"Parameters\")\n        # print(f\"coupon : {c:.2%}\")\n        # print(f\"maturity : {T} years\")\n        # print(f\"risk free rate : {r_h:.2%}\")\n        # print(f\"credit spread : {lambda_h:.2%}\")\n        # print(f\"recovery rate : {R:.2%}\")\n        # print(f\"nominal : {N}\")\n\n        P_h = pricing_bond(t=h,c=c,T=T,r=r_h,lambda_ = lambda_h,R=R,N=N,dt=dt)\n        # print(f\"Price : {P_h}\")\n        variation = (P_h - P_0)/P_0\n        prices.append(variation)\n\n\n    VaR = np.quantile(prices, 1 - alpha)\n\n    return VaR\n\nsigma_r = 0.01\nsigma_s = 0.4\nrho = 0.40\n\nlambda_ = 1/100\nr = 2/100\nc =3/100\nR = 40/100\nN=1\nh = 1/12\nT= 10\n\nVaR_monte_carlo = MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1)\nprint(f\"VaR estim√© par Monte Carlo : {VaR_monte_carlo:.4%}\")\n\n\nVaR estim√© par Monte Carlo : -5.9903%\n\n\nLorsque $ $ (le coefficient de corr√©lation entre les taux d‚Äôint√©r√™t et le spread de cr√©dit) est positif, une hausse des taux d‚Äôint√©r√™t entra√Æne √©galement une hausse du spread de cr√©dit. Cette dynamique amplifie le risque global, car les pertes dues √† la hausse des taux s‚Äôajoutent aux pertes induites par l‚Äô√©largissement du spread de cr√©dit. Ainsi, la Value at Risk (VaR) est plus √©lev√©e, refl√©tant l‚Äôaccumulation des risques li√©s aux deux facteurs.\nEn revanche, lorsque $ $ est n√©gatif, une hausse des taux d‚Äôint√©r√™t tend √† r√©duire le spread de cr√©dit, et inversement. Il se cr√©e alors un effet de compensation : les pertes g√©n√©r√©es par l‚Äô√©volution des taux sont partiellement absorb√©es par les gains r√©sultant de la contraction du spread (ou inversement). C‚Äôest le principe de diversification. Dans ce cas, la VaR est plus faible, car les effets du taux d‚Äôint√©r√™t et du spread de cr√©dit s‚Äôannulent en partie, r√©duisant ainsi l‚Äôampleur des pertes potentielles.\n\n\nShow the code\n# VaR en fonction de rho\nfrom tqdm import tqdm \n\nrhos = np.linspace(-1,1,100)\nVaRs = [MC_VaR(c,T,r,lambda_,R,N,h,sigma_r, sigma_s, rho, alpha=0.99, N_MC=1000,dt=1) for rho in tqdm(rhos)]\n\nplt.plot(rhos,VaRs)\nplt.title(\"VaR en fonction de la corr√©lation\")\nplt.xlabel(\"rho\")\nplt.ylabel(\"VaR\")\nplt.grid()\n\n\n  0%|          | 0/100 [00:00&lt;?, ?it/s] 10%|‚ñà         | 10/100 [00:00&lt;00:00, 94.97it/s] 20%|‚ñà‚ñà        | 20/100 [00:00&lt;00:00, 95.02it/s] 30%|‚ñà‚ñà‚ñà       | 30/100 [00:00&lt;00:00, 95.28it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:00&lt;00:00, 96.09it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:00&lt;00:00, 95.79it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:00&lt;00:00, 95.88it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [00:00&lt;00:00, 95.85it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:00&lt;00:00, 96.04it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:00&lt;00:00, 95.75it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01&lt;00:00, 95.46it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01&lt;00:00, 95.62it/s]\n\n\n\n\n\n\n\n\n\n\nEstimation de la volatilit√© du spread de cr√©dit\nPour la volatilit√© du spread, nous avons suppos√© que \\(\\sigma_s = 40\\%\\). Nous allons maintenant estimer cette volatilit√© √† partir des donn√©es de march√©, en utilisant l‚Äôhistorique des spreads de cr√©dit sur une p√©riode de 5 ans, i.e.¬†11/03/2025 - 11/03/2020, disponible sur ce lien.\nLa volatilit√© du spread est une volatilit√© annualis√©e. De ce fait, la formulation de la volatilit√© empirique, lorsque la fr√©quence est quotidienne, est la suivante :\n\\[\n\\sigma_s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (s_i - \\bar{s})^2} \\times \\sqrt{255}\n\\]\n\n\nShow the code\ncds_df = pd.read_excel(\"cds.xlsx\", skiprows=6)\n#date as date\ncds_df[\"Date\"] = pd.to_datetime(cds_df[\"Date\"], format=\"%Y-%m-%d\")\ncds_df = cds_df.set_index(\"Date\")\ncds_df = cds_df.sort_index()\n\ncds_df.head()\n\n\n\n\n\n\n\n\n\nPX_LAST\n\n\nDate\n\n\n\n\n\n2020-03-11\n30.772\n\n\n2020-03-12\n36.977\n\n\n2020-03-13\n37.869\n\n\n2020-03-16\n44.432\n\n\n2020-03-17\n49.923\n\n\n\n\n\n\n\n\n\nShow the code\nvol_est = np.std(cds_df[\"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 5 ans: {vol_est*np.sqrt(255):.4f}\")\n\n# volatilit√© sur 1 an\nvol_est= np.std(cds_df.loc[\"2024-03-11\":\"2025-03-11\", \"PX_LAST\"].pct_change())\nprint(f\"Volatilit√© estim√©e sur 1 an : {vol_est*np.sqrt(255):.4f}\")\n\n\nVolatilit√© estim√©e sur 5 ans: 0.4512\nVolatilit√© estim√©e sur 1 an : 0.5181"
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#risque-de-mod√®le",
    "href": "3A/gestion_actifs/TP-3.html#risque-de-mod√®le",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "Risque de mod√®le",
    "text": "Risque de mod√®le\nLe risque de mod√®le est le risque associ√© √† une mauvaise utilisation d‚Äôun mod√®le dans le processus de prise de d√©cision. Il peut √™tre caus√© par :\n\nune incertitude sur les param√®tres,\nune qualit√© insuffisante des donn√©es,\nou une inad√©quation structurelle du mod√®le par rapport √† la r√©alit√© observ√©e.\n\nCe risque peut conduire √† des erreurs de pr√©vision, √† des d√©cisions inappropri√©es ou √† des pertes financi√®res. Il est donc essentiel de bien comprendre les hypoth√®ses et limites des mod√®les employ√©s.\nC‚Äôest une activit√© o√π l‚Äôintervention humaine reste indispensable : il est n√©cessaire de proc√©der √† des sanity checks, du backtesting, et de challenger les mod√®les √† l‚Äôaide de versions plus simples (d√©g√©n√©r√©es) ou plus riches, afin d‚Äôen comparer les r√©sultats et de mieux cerner leurs faiblesses.\nPour r√©duire le risque de mod√®le, il est courant de mettre en place une fonction ind√©pendante de validation, g√©n√©ralement assur√©e par les √©quipes MRM (Model Risk Management). Ces derni√®res sont charg√©es :\n\nd‚Äôauditer les mod√®les,\nde mener des tests de robustesse,\net de recommander des provisions en cas de risque de sur√©valuation des actifs.\n\nLes comp√©tences cl√©s pour cette activit√© incluent : - les math√©matiques financi√®res, - la connaissance des march√©s financiers et des mod√®les de risque, - la programmation et l‚Äôutilisation d‚Äôoutils quantitatifs, - ainsi que des aptitudes √† la communication, au travail en √©quipe et au raisonnement critique."
  },
  {
    "objectID": "3A/gestion_actifs/TP-3.html#risque-climatique",
    "href": "3A/gestion_actifs/TP-3.html#risque-climatique",
    "title": "Asset management : Risque de valorisation ( valorisation d‚Äôobligation)",
    "section": "Risque climatique",
    "text": "Risque climatique\nLe risque climatique d√©signe les impacts potentiels des changements climatiques et des politiques environnementales sur les entreprises et les march√©s financiers. Contrairement aux risques classiques, les trajectoires de r√©f√©rence sont d√©finies par des organismes scientifiques comme le GIEC, ce qui limite l‚Äôappropriation directe des mod√®les par les institutions financi√®res. Il s‚Äôagit donc d‚Äôun domaine o√π le risque de mod√®le est indirect.\nLe r√©seau NGFS (Network for Greening the Financial System) a √©t√© mis en place pour aider les r√©gulateurs et les banques centrales √† mieux int√©grer les risques climatiques dans leurs cadres prudentiels.\n√Ä l‚Äôheure actuelle, il n‚Äôexiste pas de consensus clair sur la mani√®re d‚Äôint√©grer pleinement le risque climatique dans les mod√®les financiers ; il s‚Äôagit plut√¥t de tentatives progressives d‚Äôadaptation. Le NGFS propose plusieurs sc√©narios climatiques, parmi lesquels :\n\nCurrent Policies : continuit√© des politiques actuelles sans nouvel engagement.\nNDC (Nationally Determined Contributions) : politiques actuelles + engagements annonc√©s par les √âtats.\nDisorderly Transition (1.5¬∞C) : les engagements sont mis en ≈ìuvre avec retard ou de fa√ßon d√©sorganis√©e.\nNet Zero / 2¬∞C : sc√©nario optimis√© pour limiter le r√©chauffement √† 2¬∞C ‚Äî le cadre le plus ambitieux et le plus stable.\n\nOn distingue g√©n√©ralement deux types de risques climatiques :\n\n1. Risque physique\nCe risque est li√© aux cons√©quences directes des changements climatiques sur les infrastructures et l‚Äôenvironnement : - Risques aigus : √©v√©nements extr√™mes (inondations, s√©cheresses, temp√™tes, etc.). - Risques chroniques : √©volutions lentes (√©l√©vation du niveau des mers, hausse des temp√©ratures, etc.).\nCes risques peuvent entra√Æner : - des pertes mat√©rielles, - des interruptions d‚Äôactivit√©, - des co√ªts de r√©paration et d‚Äôadaptation.\n\n\n2. Risque de transition\nCe risque est associ√© aux mesures prises pour r√©duire les √©missions de gaz √† effet de serre et passer √† une √©conomie bas carbone. Il peut provoquer : - des pertes de valeur sur certains actifs, - des co√ªts de transition √©lev√©s, - des bouleversements sectoriels et technologiques.\nIl comprend deux composantes : - le risque politique (durcissement r√©glementaire, interdictions, fiscalit√© verte, etc.), - et le risque/opportunit√© technologique (√©mergence de nouvelles technologies, changement dans les pr√©f√©rences de consommation, etc.).\nOn peut formuler ce risque comme une relation :\n\\[\n\\text{Risque de transition} = \\text{Risque politique} - \\text{Opportunit√©s technologiques}\n\\]\nAinsi, une entreprise bien positionn√©e sur les technologies vertes peut compenser tout ou partie du risque politique subi.\n\nLe risque physique est plus √©lev√© dans les sc√©narios Current Policies, NDC, et Disorderly Transition, car ils impliquent une action climatique insuffisante ou retard√©e.\n√Ä l‚Äôinverse, le risque de transition est plus important dans les sc√©narios ambitieux comme Net Zero, o√π les ajustements politiques et √©conomiques sont rapides et profonds."
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html",
    "href": "3A/proc_stochastique/modele_bs.html",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "Le mod√®le de Black-Scholes est un mod√®le math√©matique qui permet de d√©terminer le prix d‚Äôune option √† partir de plusieurs param√®tres. Il est bas√© sur l‚Äôhypoth√®se que le prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique :\n\\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t\n\\]\nAvec \\(S_t\\) le prix de l‚Äôactif, \\(\\mu\\) le taux de rendement moyen, \\(\\sigma\\) la volatilit√© et \\(W_t\\) un mouvement brownien.\nDe ce fait, le prix d‚Äôune option europ√©enne peut √™tre calcul√© par la formule de Black-Scholes :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\nAvec \\(C\\) le prix de l‚Äôoption, \\(S_t\\) le prix de l‚Äôactif sous-jacent, \\(K\\) le prix d‚Äôexercice de l‚Äôoption, \\(T\\) la maturit√© de l‚Äôoption, \\(r\\) le taux d‚Äôint√©r√™t sans risque, \\(\\sigma\\) la volatilit√© de l‚Äôactif, \\(N\\) la fonction de r√©partition de la loi normale centr√©e r√©duite, et :\n\\[\nd_1 = \\frac{1}{\\sigma \\sqrt{T}} \\left( \\ln \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T \\right)\n\\]\n\\[\nd_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\n\n# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "href": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le mod√®le √† volatilit√© stochastique de Taylor est un mod√®le de volatilit√© stochastique qui est utilis√© pour mod√©liser la volatilit√© des actifs financiers. Le mod√®le est d√©fini par l‚Äô√©quation suivante :\n\\[\n\\begin{aligned}\nr_t &= \\exp(x_t/2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim \\text{N}(0,1) \\\\\nx_t &= \\mu + \\phi \\cdot x_{t-1} + \\sigma_t \\cdot \\eta_t\n\\end{aligned}\n\\]\no√π \\(r_t\\) est le rendement de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(\\mu\\) est la moyenne de la volatilit√©, \\(\\phi\\) est le coefficient d‚Äôautor√©gression, \\(\\sigma_t\\) est l‚Äô√©cart-type de la volatilit√© √† l‚Äôinstant \\(t\\), \\(\\eta_t\\) est un bruit blanc gaussien, et \\(\\varepsilon_t\\) est un bruit blanc gaussien.\nPour extraire la volatilit√©, nous utilisons le filtre de Kalman sur le logarithme des rendements au carr√© \\(y_t = \\log(r_t^2)\\), afin de lin√©ariser le mod√®le. Le mod√®le lin√©aris√© est d√©fini par l‚Äô√©quation suivante :\n\\[\ny_t = x_t + \\varepsilon_t\n\\]\no√π \\(y_t\\) est le logarithme des rendements au carr√© √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), et \\(\\varepsilon_t\\) est un bruit blanc de loi log-\\(\\chi^2\\).\nDe ce fait, nous pouvons utiliser le filtre de Kalman pour estimer la volatilit√© de l‚Äôactif financier en utilisant les rendements observ√©s. En effet, le filtre de Kalman est un algorithme r√©cursif qui permet d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique √† partir d‚Äôobservations bruit√©es. Il s‚Äôapplique √† des mod√®les lin√©aires dont le bruit est gaussien. Dans notre cas, nous avons lin√©aris√© le mod√®le pour qu‚Äôil soit compatible avec le filtre de Kalman. Cependant, le bruit n‚Äôest pas gaussien, mais log-\\(\\chi^2\\). L‚Äôobjectif de ce notebook est d‚Äôobserver le comportement du filtre de Kalman o√π le bruit n‚Äôest pas gaussien.\nNous poss√©dons d√©j√† d‚Äôun fichier avec les rendements de l‚Äôactif financier et la vraie volatilit√© simul√©s avec les param√®tres suivants \\(\\mu = -0.8\\), \\(\\phi = 0.9\\), \\(\\sigma = 0.09\\). Nous allons donc utiliser ces donn√©es pour estimer la volatilit√© de l‚Äôactif financier en utilisant le filtre de Kalman. N√©anmoins, le code est √©galement fourni pour simuler les donn√©es si vous souhaitez tester le filtre de Kalman sur d‚Äôautres param√®tres.\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\n\n# r_t = exp(x_t/2)*eps_t (eps_t iid N(0,1))\n# x_t = mu + phi * x_{t-1} + sigma_t * eta_t  (eta_t iid N(0,1))\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Param√®tres\nn = 252\nmu = -0.8\nphi = 0.9\nsigma_squared = 0.09\n\n# Simulation\n# np.random.seed(0)\n\n# x = np.zeros(n)\n# r = np.zeros(n)\n\n# for t in range(0, n):\n#     if t == 0:\n#         x[t] = np.random.normal(loc= mu/(1-phi), scale=np.sqrt(sigma_squared/(1-phi**2))) # Densit√© de transition stationnaire de x_t\n#     else:\n#         x[t] = mu + phi * x[t-1] + np.sqrt(sigma_squared) * np.random.normal(loc=0, scale=1)\n#     r[t] = np.exp(x[t]/2) * np.random.normal(loc=0, scale=1)\n\ndata  = pd.read_csv('true_sv_taylor.csv')\nr = data['r']\nx = data['x']\n\n\n# Affichage des trajectoires\nimport matplotlib.pyplot as plt\n\nplt.plot(r, color=\"black\")\nplt.title(\"Trajectoire des rendements\")\nplt.show()\n\nplt.plot(x, color='red')\nplt.title(\"Trajectoire de log-volatilit√©\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Construction du mod√®le log-SV (mod√®le de Taylor)\n\n# Pour appliquer le filtre de Kalman, il faut que les bruits soient centr√©s.\nmu_r_squared = -1.27 # car log(eps**2) suit une log chi-deux\ny = np.log(r**2) - mu_r_squared\n\n\n\nLe filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Dans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html",
    "href": "3A/proc_stochastique/pj_courbe_tx.html",
    "title": "Mod√®les de courbe de taux",
    "section": "",
    "text": "Que ce soit pour les banques, les assureurs ou les fonds de pension, la courbe de taux z√©ro-coupon constitue la brique de base pour la valorisation de nombreux instruments financiers, de la d√©termination du prix des obligations aux produits d√©riv√©s plus complexes tels que les swaps de taux, les caplets, les swaptions ou encore les produits structur√©s de taux. De ce fait, la construction fiable, coh√©rente et r√©guli√®rement mise √† jour d‚Äôune courbe de taux z√©ro-coupon repr√©sente un enjeu majeur.\nDans ce contexte, ce projet propose une m√©thodologie compl√®te de reconstitution de la courbe de taux z√©ro-coupon implicite, √©labor√©e √† partir des cotations de march√© disponibles sur diff√©rents segments : Money Market (march√© mon√©taire), Futures et Swaps. L‚Äôapproche adopt√©e repose sur une combinaison de bootstrapping, permettant d‚Äôextraire les taux z√©ro-coupon pour chaque maturit√© observable, et d‚Äôinterpolations par spline cubique, afin d‚Äôassurer la lissit√© et la continuit√© de la courbe sur l‚Äôensemble de l‚Äô√©ch√©ancier, y compris sur les maturit√©s non directement observ√©es.\nLa courbe obtenue sert ensuite de r√©f√©rence pour la valorisation de produits d√©riv√©s de taux, en particulier les caplets et les swaptions, via le mod√®le classique de Black, largement utilis√© sur les march√©s. Toutefois, afin de mieux capturer la dynamique temporelle des taux d‚Äôint√©r√™t et de prendre en compte la structure temporelle de la volatilit√© implicite, la seconde partie du projet repose sur la calibration d‚Äôun mod√®le de Hull-White, un mod√®le de taux affine avec retour √† la moyenne. La calibration est r√©alis√©e √† partir des cotations de caplets at-the-money (ATM), √† l‚Äôaide d‚Äôune proc√©dure de recherche num√©rique par dichotomie.\nLe projet met √©galement en √©vidence la sensibilit√© de la courbe de taux forward et des prix d‚Äôoptions aux param√®tres de march√©, notamment la volatilit√© et le param√®tre de mean reversion du mod√®le Hull-White. Cette analyse de sensibilit√© illustre comment la structure de la courbe de taux et son √©volution future sont influenc√©es par les hypoth√®ses de mod√©lisation, ce qui est particuli√®rement crucial pour les desks de trading, les gestionnaires d‚Äôactifs ou les √©quipes de gestion actif-passif (ALM).\nEnfin, le projet se prolonge par une extension appliqu√©e aux produits structur√©s : la valorisation de caplets √† barri√®re d√©sactivante (knock-out caplets), qui n√©cessite une approche par simulation Monte-Carlo. Cette extension illustre comment la dynamique du taux court, simul√©e sous la mesure forward neutre, peut √™tre exploit√©e pour √©valuer des produits de plus en plus complexes, r√©pondant √† des besoins sp√©cifiques d‚Äôinvestisseurs ou de gestionnaires de risques.\n\n\n\n\n\n\nImportant\n\n\n\nLe rapport de ce projet est disponible ici."
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#introduction",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#introduction",
    "title": "Mod√®les de courbe de taux",
    "section": "",
    "text": "Que ce soit pour les banques, les assureurs ou les fonds de pension, la courbe de taux z√©ro-coupon constitue la brique de base pour la valorisation de nombreux instruments financiers, de la d√©termination du prix des obligations aux produits d√©riv√©s plus complexes tels que les swaps de taux, les caplets, les swaptions ou encore les produits structur√©s de taux. De ce fait, la construction fiable, coh√©rente et r√©guli√®rement mise √† jour d‚Äôune courbe de taux z√©ro-coupon repr√©sente un enjeu majeur.\nDans ce contexte, ce projet propose une m√©thodologie compl√®te de reconstitution de la courbe de taux z√©ro-coupon implicite, √©labor√©e √† partir des cotations de march√© disponibles sur diff√©rents segments : Money Market (march√© mon√©taire), Futures et Swaps. L‚Äôapproche adopt√©e repose sur une combinaison de bootstrapping, permettant d‚Äôextraire les taux z√©ro-coupon pour chaque maturit√© observable, et d‚Äôinterpolations par spline cubique, afin d‚Äôassurer la lissit√© et la continuit√© de la courbe sur l‚Äôensemble de l‚Äô√©ch√©ancier, y compris sur les maturit√©s non directement observ√©es.\nLa courbe obtenue sert ensuite de r√©f√©rence pour la valorisation de produits d√©riv√©s de taux, en particulier les caplets et les swaptions, via le mod√®le classique de Black, largement utilis√© sur les march√©s. Toutefois, afin de mieux capturer la dynamique temporelle des taux d‚Äôint√©r√™t et de prendre en compte la structure temporelle de la volatilit√© implicite, la seconde partie du projet repose sur la calibration d‚Äôun mod√®le de Hull-White, un mod√®le de taux affine avec retour √† la moyenne. La calibration est r√©alis√©e √† partir des cotations de caplets at-the-money (ATM), √† l‚Äôaide d‚Äôune proc√©dure de recherche num√©rique par dichotomie.\nLe projet met √©galement en √©vidence la sensibilit√© de la courbe de taux forward et des prix d‚Äôoptions aux param√®tres de march√©, notamment la volatilit√© et le param√®tre de mean reversion du mod√®le Hull-White. Cette analyse de sensibilit√© illustre comment la structure de la courbe de taux et son √©volution future sont influenc√©es par les hypoth√®ses de mod√©lisation, ce qui est particuli√®rement crucial pour les desks de trading, les gestionnaires d‚Äôactifs ou les √©quipes de gestion actif-passif (ALM).\nEnfin, le projet se prolonge par une extension appliqu√©e aux produits structur√©s : la valorisation de caplets √† barri√®re d√©sactivante (knock-out caplets), qui n√©cessite une approche par simulation Monte-Carlo. Cette extension illustre comment la dynamique du taux court, simul√©e sous la mesure forward neutre, peut √™tre exploit√©e pour √©valuer des produits de plus en plus complexes, r√©pondant √† des besoins sp√©cifiques d‚Äôinvestisseurs ou de gestionnaires de risques.\n\n\n\n\n\n\nImportant\n\n\n\nLe rapport de ce projet est disponible ici."
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#i.-reconstitution-de-la-courbe-de-taux",
    "title": "Mod√®les de courbe de taux",
    "section": "I. Reconstitution de la courbe de taux",
    "text": "I. Reconstitution de la courbe de taux\n\nI.1. Formules de valorisation des taux de march√©\nLa courbe interbancaire est une courbe de taux qui repr√©sente les taux d‚Äôint√©r√™t auxquels les banques se pr√™tent de l‚Äôargent entre elles. Elle est utilis√©e pour d√©terminer les taux d‚Äôint√©r√™t des pr√™ts et des emprunts √† court terme. Elle est construite sur le court terme (maturit√©&lt;6M) √† partir des taux du march√©s mon√©taire (Money Market) bas√©s sur les d√©pots non garantis entre banques. Sur le moyen terme (6m - 3y) elle est construite √† partir des contrats futures, i.e.¬†des forwards sur un march√© OTC (Over The Counter) et sur le long terme (&gt;3y) elle est construite √† partir des contrats de swap euribor (Euro Interbank Offered Rate) 3M ou 6M.\nCi dessous, nous disposons de ces donn√©es de taux de march√© cot√©s sur le march√© interbancaire. Nous allons essayer de reconstituer la courbe de taux zero coupon implicite, qui ne cote pas directement sur le march√©. Le fichier de donn√©es contient trois variables : - Type d‚Äôinstruments (Money Market, Futures, Swap) - Maturit√© (en ann√©es) - Taux d‚Äôint√©r√™t\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_excel('data/Data_tx.xlsx', sheet_name='tx_marche')\ndata.columns = ['type', 'T', 'tx']\ndata\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n5\nFUT\n1.50\n0.977094\n\n\n6\nFUT\n1.75\n0.974981\n\n\n7\nFUT\n2.00\n0.972911\n\n\n8\nFUT\n2.25\n0.970984\n\n\n9\nFUT\n2.50\n0.969711\n\n\n10\nFUT\n2.75\n0.968436\n\n\n11\nSWAP\n3.00\n0.026112\n\n\n12\nSWAP\n4.00\n0.028117\n\n\n13\nSWAP\n5.00\n0.029680\n\n\n14\nSWAP\n6.00\n0.031107\n\n\n15\nSWAP\n7.00\n0.032313\n\n\n16\nSWAP\n8.00\n0.033382\n\n\n17\nSWAP\n9.00\n0.034385\n\n\n18\nSWAP\n10.00\n0.035312\n\n\n19\nSWAP\n11.00\n0.036197\n\n\n20\nSWAP\n12.00\n0.037003\n\n\n21\nSWAP\n13.00\n0.037668\n\n\n22\nSWAP\n14.00\n0.038201\n\n\n23\nSWAP\n15.00\n0.038624\n\n\n24\nSWAP\n20.00\n0.039380\n\n\n25\nSWAP\n25.00\n0.038501\n\n\n26\nSWAP\n30.00\n0.037668\n\n\n\n\n\n\n\nEn l‚Äôabsence d‚Äôoppotunit√© d‚Äôarbitrage, les valorisations des instruments de march√© s‚Äôexpriment en fonction des taux z√©ro coupon implicites suivantes :\n\nSur le segment Money Market, on cote en taux mon√©taires :\n\\[\n  L_t(T,T+\\delta) = \\frac{1}{\\delta} \\left( \\frac{B(t,T)}{B(t,T+\\delta)} - 1 \\right),\n  \\]\navec t le temps courant, T la maturit√© et \\(\\delta\\) la p√©riode de capitalisation. Dans notre cas, t=T=0 et \\(\\delta\\) varie en fonction de la maturit√©.\nSur le segment Future, on c√¥te en 1 - tx forward :\n\\[\n  future(T, T+\\delta) = 1 - L_t(T,T+\\delta).\n  \\] Dans notre cas, t=0, T= maturit√© - 3m et \\(\\delta = 3m\\).\nSur le segment Swap, on c√¥te en taux swap :\n\\[\n  Swap(t, T_0, T_n) = B(t, T_0)‚àíB(t, T_n)‚àíK √ólvl(t),\n  \\]\navec K le taux fixe du swap qui √©galise la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de march√© √† la maturit√© \\(T_n\\). Dans notre cas, la date de d√©part est le spot, i.e.¬†\\(T_0=0\\) et \\(T_n\\) est la maturit√© du swap, et t=0 (vu d‚Äôaujourd‚Äôhui).\n\nDe ce fait, le donn√©es ne sont pas homog√®nes en taux du fait de la diff√©rente cotations des instruments. Nous allons donc les transformer en taux mon√©taires pour les homog√©n√©iser.\nRemarques pr√©liminaires : - En zone EURO, les swaps standards c√¥t√©s sur le march√© ont une fr√©quence de paiement semestrielle pour la patte variable et annuelle pour la patte fixe. Ainsi pour le calcul du level du swap, \\(\\delta=1\\) et on ajoute progressivement les taux de march√©. - Pour simplifier les calculs, nous supposerons que les dates de d√©part des taux mon√©taires et des taux de swap sont spot (i.e.¬†T0 = 0 et non 1 ou 2 jours).\nMethode de bootstrapping & stripping :\nPour extraire les taux z√©ro coupon implicites, nous allons utiliser la m√©thode de bootstrapping. Cette m√©thode consiste √† calculer les taux z√©ro coupon implicites √† partir des taux de march√©. Pour cela, nous allons utiliser les formules des taux mon√©taires pr√©sent√©es ci-dessus, qui sont vu comme des fonctions de taux z√©ro coupon implicites.\nComme les taux de swap ne sont pas n√©cessairement disponibles pour toutes les maturit√©s annuelles, il faut interpoler les taux interm√©diaires. Cela permettra de simplifier la m√©thode de bootstrapping. Nous allons utiliser une interpolation par spline cubic afin d‚Äôavoir des taux swap par an. Une interpolation par spline permet d‚Äôavoir des bonnes propri√©t√©s en terme de d√©rivabilit√© et de continuit√© de la courbe de taux.\nIl s‚Äôagira donc de construire une nouvelle courbe de taux de march√© discr√®te avec des cotations annuelles de taux swap √† l‚Äôaide d‚Äôune m√©thode d‚Äôinterpolation par spline, en plus des autres instruments. Par la suite, on supposera que cette nouvelle courbe est la courbe de march√© de r√©f√©rence, i.e.¬†la courbe utilis√©e pour impliciter les taux z√©ro coupon.\nEnfin, nous allons faire du stripping afin de reconstituer une courbe de taux zero coupon implicite plus lisse √† l‚Äôaide de diff√©rentes m√©thodes d‚Äôinterpolations (lin√©aire, spline, etc).\n\n\nI.2. Construction de la courbe des taux z√©ro-coupon\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\n\ndef interpolate_and_update_df(data, col_x, col_y, kind='cubic', start=3, end=30, step=1):\n    \"\"\"\n    Interpole les taux SWAP et met √† jour le DataFrame avec les nouvelles valeurs interpol√©es.\n\n    Param√®tres :\n    - data : DataFrame d'origine contenant une colonne 'type' avec 'SWAP', 'T' et 'tx'.\n    - kind : Type d'interpolation (par d√©faut 'cubic', peut √™tre 'linear', 'quadratic', etc.).\n    - start : Valeur minimale de T pour l'interpolation (par d√©faut 3).\n    - end : Valeur maximale de T pour l'interpolation (par d√©faut 31).\n    - step : Pas d'incr√©mentation pour la grille interpol√©e (par d√©faut 1).\n\n    Retourne :\n    - new_df : DataFrame mis √† jour avec les taux SWAP interpol√©s.\n    \"\"\"\n\n    data = data.copy()\n    x, y = data[col_x].values, data[col_y].values\n\n    f = interp1d(x, y, kind=kind)\n    xnew = np.arange(start, end+step, step)\n    tx_new = f(xnew)\n    df = pd.DataFrame({col_x: xnew, col_y: tx_new})\n\n    return df\n\ndf_interp = interpolate_and_update_df(data, 'T', 'tx') \ndf_interp['type'] = 'SWAP'\ndf_interp.head()\n\n\n\n\n\n\n\n\nT\ntx\ntype\n\n\n\n\n0\n3\n0.026112\nSWAP\n\n\n1\n4\n0.028117\nSWAP\n\n\n2\n5\n0.029680\nSWAP\n\n\n3\n6\n0.031107\nSWAP\n\n\n4\n7\n0.032313\nSWAP\n\n\n\n\n\n\n\n\n# Le rajoiuter dans le df\nnew_df = pd.concat([data[data[\"type\"] != \"SWAP\"], df_interp], ignore_index=True)\nnew_df.head()\n\n\n\n\n\n\n\n\ntype\nT\ntx\n\n\n\n\n0\nMM\n0.25\n0.030698\n\n\n1\nMM\n0.50\n0.026191\n\n\n2\nMM\n0.75\n0.023958\n\n\n3\nMM\n1.00\n0.022979\n\n\n4\nFUT\n1.25\n0.978691\n\n\n\n\n\n\n\n\n# Affichage d'une courbe homog√®ne de taux de march√© en fonction de la maturit√©\nnew_df['tx_h'] = new_df.apply(lambda x: 1 - x['tx'] if x['type'] == 'FUT' else x['tx'], axis=1)\n\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], marker='o', linestyle='-', color='b')\nplt.xlabel('Maturit√©')\nplt.ylabel('Taux de march√©')\nplt.title('Courbe des taux de march√©')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nExtraction du taux zero coupon sur le segment Money Market\nLes taux z√©ro-coupon continus sont d√©finis par la formule suivante :\n\\[\nr(t,T) = -\\frac{1}{T-t} \\ln B(t,T),\n\\]\no√π B(t,T) est le facteur d‚Äôactualisation (\\(B(t,T) = exp(-r(t,T) \\times  T)\\) ), i.e.¬†le prix d‚Äôune obligation z√©ro-coupon de maturit√© T √† la date t.\nIls sont la brique de base pour la valorisation des produits d√©riv√©s et des obligations. De ce fait, nous allons essayer de reconstituer la courbe des taux z√©ro-coupon implicite √† partir de la courbe des taux de march√© √† l‚Äôaide de la m√©thode du bootstrapping. Cette m√©thode consiste √† calculer les taux z√©ro-coupon implicites √† partir des taux de march√© en utilisant la formule suivante selon le segment Money Market :\n\\[\nL_0(0,\\delta) = \\frac{1}{\\delta} \\left( \\frac{1}{B(0,\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,\\delta) = \\frac{1}{1 + \\delta L_0(0,\\delta)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Money Market\nmm = new_df[new_df['type'] == 'MM']\n\nmm.loc[:, 'B'] = 1 / (1 + mm['tx'] * mm['T'])\nmm.loc[:, 'R'] = - np.log(mm['B']) / mm['T']\n\ndf_ZC = mm\n\n\n\nExtraction du taux zero coupon sur le segment Future\nSur le segment Future, on a :\n\\[\nFuture = 1 - L_0(T,T+\\delta) = 1 - \\frac{1}{\\delta} \\left( \\frac{B(0,T)}{B(0,T+\\delta)} - 1 \\right)\n\\]\nDe ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,T+\\delta) = \\frac{B(0,T)}{1 + \\delta (1- Future)}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Futures\n\nfut = new_df[new_df['type'] == 'FUT']\n\n# concat √† mm\ndf_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\nmm_len = len(mm)\n\nfor i in range(mm_len, len(df_ZC)):\n    df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, 'tx'])* 0.25)\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, 'T']\n\n\n\nExtraction du taux zero coupon sur le segment swap\nPour le segment swap payeur, on a :\n\\[\nSwap(t, T_0, T_n) = B(t, T_0)‚àíB(t, T_n)‚àíK √ólvl(t) = 0,\n\\]\navec K le taux fixe du swap qui fait que la PV du swap vaut 0 et \\(lvl(t)=\\sum_{i=1}^{n} \\delta_i B(t, T_i)\\) le taux de march√© √† la maturit√© \\(T_n\\). De ce fait, le facteur d‚Äôactualisation est : \\[\nB(0,T_n) = \\frac{1 - K \\sum_{i=1}^{n-1} \\delta_i B(0,T_i)}{1 + K}\n\\]\n\n# Extraction des facteurs d'actualisation pour les Swaps\nswap = new_df[new_df['type'] == 'SWAP']\nfut_len = len(fut)\n\ndf_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\nfor i in range(mm_len+fut_len, len(df_ZC)):\n    T_n = df_ZC.loc[i, 'T']  # R√©cup√®re la valeur de T actuelle\n    mask = (df_ZC['T'] &lt; T_n) & (df_ZC['T'] % 1 == 0)  # S√©lectionne uniquement les T entiers &lt; T_n\n    df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, 'tx'] * sum(df_ZC.loc[mask, 'B'].fillna(0)))/(1+df_ZC.loc[i, 'tx'])\n    df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC['T'], df_ZC['R'], label='Courbe de taux z√©ro coupon', marker='o')\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de march√©', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux z√©ro coupon discr√©tis√©e')\n\nText(0.5, 1.0, 'Courbe de taux z√©ro coupon discr√©tis√©e')\n\n\n\n\n\n\n\n\n\nComme on peut le constater, le mode d‚Äôinterpolation a un impact significatif sur le calcul des taux de march√© car il affecte la forme de la courbe des taux et donc la valorisation des instruments financiers.\n\n\n\nI.3. Construction de la courbe des taux forward\nA partir de la courbe des taux z√©ro-coupon issue de la m√©thode de bootstrapping, nous souhaitons tracer la courbe des taux forwards de tenor 3M en fonction de la maturit√© √† l‚Äôaide des m√©thodes d‚Äôinterpolation lin√©aire et par spline, avec une discr√©tisation de 0.1 an.\nPour tracer la courbe taux forward, on utilisera la formule suivante pour calculer les taux forward :\n\\[\nL_0(T,T+\\delta) = \\frac{1}{\\delta} (\\frac{B(0,T)}{B(0,T+\\delta)} - 1)\n\\] avec \\(\\delta = 0.25\\).\nPour le segment swap, il s‚Äôagira d‚Äôinterpoler les taux z√©ro-coupon implicites pour avoir des tx forwards 3M. # changer la discretisatio √† 1an ce qui est diff√©rent du t√©nor.\n\ndf_ZC = pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC], ignore_index=True)\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\n\ndef compute_forward_rates(R, T_range, tau):\n    \"\"\" Calcule les taux forward pour chaque maturit√© \"\"\"\n    fwd_rates = []\n    T_values = []\n    for i in range(len(T_range)-1):\n        T = T_range[i]\n        T_tau = T + tau\n        if T_tau &gt;= max(T_range):\n            break  # √âviter d'extrapoler au-del√† des donn√©es disponibles\n        B_T = np.exp(-R[i] * T)\n        R_T_tau = np.interp(T_tau, T_range, R) \n        B_T_tau = np.exp(-R_T_tau * T_tau)\n\n        # Formule du taux forward instantan√©\n        fwd_rate = (B_T / B_T_tau - 1) / tau\n        fwd_rates.append(fwd_rate)\n        T_values.append(T)\n\n    return pd.DataFrame({\"T\": T_values, \"tx_fwd\": fwd_rates})\n\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation lin√©aire')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation lin√©aire')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nLorsqu‚Äôon utilise une interpolation lin√©aire, on obtient une courbe plus discontinue. La courbe a une structure en marches d‚Äôescalier. Il y a des sauts brusques lorsque l‚Äôon passe d‚Äôun intervalle √† un autre. En effet, par nature, l‚Äôinterpolation lin√©aire qui ne prend pas en compte les points interm√©diaires. En interpolant avec une fonction spline, on obtient une courbe plus lisse et continue.Elle est plus coh√©rente avec l‚Äô√©volution naturelle des taux d‚Äôint√©r√™t. En effet, la fonction spline est une fonction polynomiale qui passe par tous les points de la courbe. Elle est plus flexible et permet de mieux capturer les variations des taux d‚Äôint√©r√™t.\nNous sommes int√©ress√©s √† ce qui pourrait se passer lorsque nous shiftons le taux de swap 5Y de 10 points de base. Cela permet de d√©terminer la sensibilit√© de la courbe des taux forward aux variations des taux de swap et donc donner des indications sur comment hedger ce risque.\nNous allons donc calculer le taux forward 3M pour les deux courbes de taux forward et comparer les r√©sultats.\n\nchoc = 10/10000\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx_h']+choc if x['T'] == 5 else x['tx_h'], axis=1)\n\n# plot\nplt.figure(figsize=(8, 5))\nplt.plot(new_df[\"T\"], new_df[\"tx_s\"], label='Taux de march√© shift√©s', color=\"r\")\nplt.plot(new_df[\"T\"], new_df[\"tx_h\"], label='Taux de march√©', color=\"b\")\nplt.xlabel('Maturit√©')\nplt.ylabel('Taux de march√©')\nplt.title('Courbe des taux de march√©')\nplt.legend()\nplt.grid()\nplt.show()\n\nnew_df[\"tx_s\"] = new_df.apply(lambda x: x['tx']+choc if x['T'] == 5 else x['tx'], axis=1)\n\n\n\n\n\n\n\n\n\n# Extraction des facteurs d'actualisation pour les Money Market\nimport numpy as np\nimport pandas as pd\n\ndef compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx\"):\n    \"\"\"\n    Calcule les facteurs d'actualisation (B) et les taux z√©ro-coupon (R) \n    √† partir des taux du march√© pour les instruments MM, FUT et SWAP.\n\n    Param√®tres :\n    - new_df : DataFrame contenant les taux du march√© avec les colonnes sp√©cifi√©es.\n    - col_T : Nom de la colonne contenant les maturit√©s (ex: \"T\").\n    - col_tx : Nom de la colonne contenant les taux du march√© (ex: \"tx\").\n\n    Retourne :\n    - df_ZC : DataFrame contenant les facteurs d'actualisation et les taux z√©ro-coupon.\n    \"\"\"\n\n    # --- Extraction des donn√©es du march√© mon√©taire (MM) ---\n    mm = new_df[new_df['type'] == 'MM'].copy()\n    mm.loc[:, 'B'] = 1 / (1 + mm[col_tx] * mm[col_T])\n    mm.loc[:, 'R'] = - np.log(mm['B']) / mm[col_T]\n\n    df_ZC = mm.copy()\n\n    # --- Extraction des donn√©es Futures (FUT) ---\n    fut = new_df[new_df['type'] == 'FUT'].copy()\n    df_ZC = pd.concat([df_ZC, fut], ignore_index=True)\n\n    mm_len = len(mm)\n\n    # --- Calcul des facteurs d'actualisation pour les Futures ---\n    for i in range(mm_len, len(df_ZC)):\n        df_ZC.loc[i, 'B'] = df_ZC.loc[i-1, 'B'] / (1 + (1 - df_ZC.loc[i, col_tx]) * 0.25)\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / df_ZC.loc[i, col_T]\n\n    # --- Extraction des donn√©es Swaps (SWAP) ---\n    swap = new_df[new_df['type'] == 'SWAP'].copy()\n    fut_len = len(fut)\n    df_ZC = pd.concat([df_ZC, swap], ignore_index=True)\n\n    # --- Calcul des facteurs d'actualisation pour les Swaps ---\n    for i in range(mm_len + fut_len, len(df_ZC)):\n        T_n = df_ZC.loc[i, col_T]  # Maturit√© actuelle\n        mask = (df_ZC[col_T] &lt; T_n) & (df_ZC[col_T] % 1 == 0)  # S√©lection des T entiers &lt; T_n\n\n        sum_B = sum(df_ZC.loc[mask, 'B'].fillna(0))\n        df_ZC.loc[i, 'B'] = (1 - df_ZC.loc[i, col_tx] * sum_B) / (1 + df_ZC.loc[i, col_tx])\n        df_ZC.loc[i, 'R'] = - np.log(df_ZC.loc[i, 'B']) / T_n\n\n    return df_ZC\n\n\ndf_ZC_s = compute_discount_factors(new_df, col_T=\"T\", col_tx=\"tx_s\")\ndf_ZC_s=pd.concat([pd.DataFrame({\"T\": [0], \"B\": [1], \"R\": [0]}), df_ZC_s], ignore_index=True)\n\nplt.figure(figsize=(8, 5))\nplt.plot(df_ZC_s.loc[1:,'T'], df_ZC_s.loc[1:,'R'], label='Courbe de taux z√©ro coupon shift√©', marker='o')\nplt.plot(df_ZC.loc[1:,\"T\"], df_ZC.loc[1:,\"R\"], label='Courbe de taux z√©ro coupon non shift√©', marker='o')\nplt.xlabel('T')\nplt.ylabel('R')\nplt.legend()\nplt.title('Courbe de taux z√©ro coupon discr√©tis√©e')\n\nText(0.5, 1.0, 'Courbe de taux z√©ro coupon discr√©tis√©e')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='linear', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\n\nnew_df_ZC= interpolate_and_update_df(df_ZC_s, 'T', 'R', kind='cubic', start=0, end=30, step=0.1)\n\ntau = 0.25  # 3 mois = 0.25 an\nfwd_rates = compute_forward_rates(new_df_ZC['R'], new_df_ZC['T'], tau)\n\nplt.figure(figsize=(8, 5))\nplt.plot(fwd_rates[\"T\"],fwd_rates[\"tx_fwd\"], label='Taux forward')\nplt.xlabel('T')\nplt.ylabel('Taux forward')\nplt.legend()\nplt.title('Courbe des taux forward avec interpolation par spline')\n\nText(0.5, 1.0, 'Courbe des taux forward avec interpolation par spline')\n\n\n\n\n\n\n\n\n\nEn shiftant le taux de swap 5Y, la courbe de forward baisse brusquement pour T=5Y. Il y a une d√©formation locale de la courbe des taux forward. Cela signifie que la courbe des taux forward est sensible aux variations des taux de swap. En effet, les taux swap sont des instruments financiers qui permettent de se couvrir contre les variations des taux d‚Äôint√©r√™t."
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#ii.-valorisation-de-swaptions-et-de-caplets",
    "title": "Mod√®les de courbe de taux",
    "section": "II. Valorisation de swaptions et de caplets",
    "text": "II. Valorisation de swaptions et de caplets\n\nPour coter les caplets/floorlets et swaptions, le mod√®le de Black est souvent utilis√©. Ce mod√®le est bas√© sur l‚Äôhypoth√®se que les taux d‚Äôint√©r√™t sont log-normalement distribu√©s. Il permet de calculer le prix d‚Äôun caplet/floorlet et d‚Äôun swaption en fonction des taux d‚Äôint√©r√™t et de la volatilit√© implicite.\nL‚ÄôEDS (Equation Diff√©rentielle Stochastique) de Black est donn√©e par :\n\n\\[\ndL(t) = \\sigma L(t) dW(t)\n\\]\navec \\(L(t)\\) le taux, \\(\\sigma\\) la volatilit√© du taux et \\(W(t)\\) un mouvement brownien. En utilisant le changement de num√©raire, ce taux est une martingale sous la mesure du numeraire (probabilit√© risque neutre). Cela permet de calculer le prix d‚Äôun caplet/floorlet ou d‚Äôune swaption.\nCaplets et Floorlets\n\nPour un caplet, le prix est donn√© par la formule suivante :\n\n\\[\nCaplet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ L_i(t) \\phi(d) - K \\phi (d - \\sigma_i \\sqrt{T_{i-1}-t} )\\right]\n\\]\navec \\(d = \\frac{1}{\\sigma \\sqrt{T_{i-1}-t}} \\left( \\ln \\left( \\frac{L_i(t)}{K} \\right) + \\frac{\\sigma^2(T_{i-1}-t)}{2} \\right)\\), \\(L_i(t)\\) le taux forward 3M √† la date t, \\(K\\) le strike du caplet, \\(N\\) le nominal, \\(\\delta_i\\) la p√©riode de capitalisation, \\(B(t,T_i)\\) le facteur d‚Äôactualisation √† la maturit√© \\(T_i\\), \\(\\sigma_i\\) la volatilit√© du taux forward 3M √† la maturit√© \\(T_i\\) et \\(\\phi\\) la fonction de r√©partition de la loi normale standard.\nPour un floorlet, le prix est donn√© par la formule suivante :\n\\[\nFloorlet(t,T_{i-1},T_i) = N \\delta_i B(t,T_i) \\left[ K \\phi (d - \\sigma_i \\sqrt(T_{i-1}-t) ) - L_i(t) \\phi(d) \\right]\n\\]\nSwaptions\nPour un swaption donneur, le prix est donn√© par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ F_S(t) \\Phi(d) - K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) \\right]\n\\]\navec \\(F_S(t)\\) le taux swap √† la date t, \\(K\\) le strike du swaption, \\(N\\) le nominal, \\(\\delta\\) la p√©riode de capitalisation, \\(B(t,T_j)\\) le facteur d‚Äôactualisation √† la maturit√© \\(T_j\\), \\(\\sigma_S\\) la volatilit√© du taux swap et \\(\\Phi\\) la fonction de r√©partition de la loi normale standard.\nd est donn√© par la formule suivante :\n\\[\nd = \\frac{1}{\\sigma_S \\sqrt{T_0 - t}} \\left( \\ln \\left( \\frac{F_S(t)}{K} \\right) + \\frac{\\sigma_S^2(T_0 - t)}{2} \\right)\n\\]\nPour un swaption receveur, le prix est donn√© par la formule suivante :\n\\[\n\\text{Swaption}_t = \\left( \\sum_{j=1}^{n} N \\delta B(t, T_j) \\right) \\left[ K \\Phi(d - \\sigma_S \\sqrt{T_0 - t}) - F_S(t) \\Phi(d) \\right]\n\\]\nIl s‚Äôagit, √† partir des cotations d√©crites dans le tableau ci-dessous et de la courbe des taux z√©ro-coupon construite pr√©c√©demment, calculer les prix de march√© de caplets sur euribor12M, ce qui implique une p√©riode de capitalisation annuelle, de maturit√© T = 5Y, i.e.¬†pay√© √† 6Y, et de strikes K associ√©s au tableau. Nous souhaitons ainsi calculer : - Le prix des caplets Caplet(t, 5Y, 6Y) pour les strikes du tableau ci-dessous. - Le prix des swaptions Swaption(t, 5Y, 6Y) pour les strikes du tableau ci-dessous.\n\nvol_data = pd.read_excel('data/Data_tx.xlsx', sheet_name='vol')\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\n\n\n\n\n0\n-100\n0.311859\n0.311859\n\n\n1\n-50\n0.283274\n0.283274\n\n\n2\n-25\n0.265921\n0.265921\n\n\n3\n0\n0.250000\n0.250000\n\n\n4\n25\n0.243451\n0.243451\n\n\n5\n50\n0.249019\n0.249019\n\n\n6\n100\n0.271828\n0.271828\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet'):\n    \"\"\"\n    Calcule la valeur d'un caplet selon le mod√®le de Black.\n\n    Param√®tres :\n    - N : Notional\n    - delta_i : P√©riode du caplet \n    - B_t_Ti : Facteur d'actualisation B(t, Ti)\n    - L_i_t : Taux forward Li(t)\n    - K : Strike du caplet\n    - sigma_i : Volatilit√© implicite\n    - Ti_1 : Date de d√©but de la p√©riode\n    - Ti : Date de fin de la p√©riode\n    - t : Temps actuel\n\n    Retourne :\n    - Valeur du caplet\n    \"\"\"\n    Ti_1 = Ti - delta_i\n    d1 = (np.log(L_i_t / K) + 0.5 * sigma_i**2 * (Ti_1 - t)) / (sigma_i * np.sqrt(Ti_1 - t))\n    d2 = sigma_i * np.sqrt(Ti_1 - t) - d1\n    if option_type == 'caplet':\n        price = N * delta_i * B_t_Ti * (L_i_t * norm.cdf(d1) - K * norm.cdf(-d2))\n    elif option_type == 'floorlet':\n        price = N * delta_i * B_t_Ti * (K * norm.cdf(d2) - L_i_t * norm.cdf(-d1))\n    return price\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # P√©riode du caplet\n    delta_i = 1 \n\n    # Maturit√© du caplet\n    T=5 \n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == T, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == T+1, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    # Volatilit√© implicite\n    sigma_i = vol_data.loc[i, \"Vols Caplets\"]  \n\n    # Date de d√©but de la p√©riode\n    Ti = T + delta_i\n\n    # Temps actuel\n    t = 0  \n\n    caplet_price = price_oplet(N, delta_i, B_t_Ti, L_i_t, K, sigma_i, Ti, t, option_type='caplet')\n    vol_data.loc[i, 'Caplet Price MKT'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n\n\n\n\n\n\n\n\n# pricer les swaptions"
  },
  {
    "objectID": "3A/proc_stochastique/pj_courbe_tx.html#iii.-mod√®le-de-hull-white",
    "href": "3A/proc_stochastique/pj_courbe_tx.html#iii.-mod√®le-de-hull-white",
    "title": "Mod√®les de courbe de taux",
    "section": "III. Mod√®le de Hull-White",
    "text": "III. Mod√®le de Hull-White\n\nIII.1 Du mod√®le HJM vers le mod√®le Hull&White\nPour une maturit√© \\(T\\) fix√©e, Heath, Jarrow et Morton ont suppos√© que le taux forward instantan√© √©volue selon la dynamique suivante :\n\\[\ndf(t, T) = \\alpha(t, T)\\, dt + \\sigma(t, T)\\, dW_t \\quad (1)\n\\]\nLa dynamique (1) ne se place pas forc√©ment dans un cadre sans opportunit√© d‚Äôarbitrage. Les auteurs ont montr√© que le processus \\(\\alpha\\) ne pouvait pas √™tre choisi arbitrairement et que, pour qu‚Äôil existe une unique mesure martingale √©quivalente, \\(\\alpha\\) devait √™tre li√© √† la volatilit√© du z√©ro coupon.\nSupposons donc l‚Äôexistence d‚Äôune unique mesure martingale √©quivalente \\(\\mathbb{Q}\\) (mesure risque-neutre) dont le num√©raire est l‚Äôactif sans risque.\nOn suppose que le prix du z√©ro coupon (payant une unit√© de devise en date \\(T\\)) √©volue sous \\(\\mathbb{Q}\\) selon l‚ÄôEDS :\n\\[\n\\frac{dB(t, T)}{B(t, T)} = r_t\\, dt + \\Gamma(t, T)\\, dW_t^Q \\quad (2)\n\\]\nPar d√©finition, on sait que $ B(t, T) = e^{-_t^T f(t,s), ds}$ et que $ f(t, T) = -_T (B(t, T)). $\nEn appliquant le lemme d‚ÄôIt√¥, on obtient : $ df(t, T) = (t, T), _T (t, T), dt - _T (t, T), dW_t^Q. $\nEn posant $ -_T (t, T) = (t, T), $ nous obtenons :\n\\[\ndf(t, T) = \\gamma(t, T) \\int_t^T \\gamma(t, u)\\, du \\, dt + \\gamma(t, T)\\, dW_t^Q \\quad (3)\n\\]\nApr√®s int√©gration, on retrouve finalement :\n\\[\nf(t, T) = f(0, T) + \\int_0^t \\gamma(s, T) \\left( \\int_s^T \\gamma(s, u)\\, du \\right) ds + \\int_0^t \\gamma(s, T)\\, dW_s^Q \\quad (4)\n\\]\n\n\nIII.2 Hypoth√®ses du mod√®le Hull&White\nOn suppose que le mod√®le HJM est gaussien, lin√©aire et calibrable. Ces hypoth√®ses permettent d‚Äô√©crire :\n\\[\n\\gamma(t, T) = \\sigma(t)\\, e^{-\\lambda (T-t)}  \\quad \\text{ et} \\quad \\Gamma(t, T) = \\frac{\\sigma(t)}{\\lambda}\\Bigl(e^{-\\lambda (T-t)} - 1\\Bigr)\n\\]\no√π la fonction de volatilit√© instantan√©e \\(\\sigma(t)\\) est constante par morceaux.\n\n\nIII.3 Construction de la formule z√©ro-coupon\nDans le cadre du mod√®le Hull&White, la dynamique du taux court instantan√© \\(r_t\\) s‚Äô√©crit :\n\\[\ndr_t = \\left[\\lambda\\bigl(f(0,t) - r_t\\bigr) + \\partial_t f(0,t) + \\int_0^t \\sigma^2(s)\\, e^{-2\\lambda (t-s)} ds \\right] dt + \\sigma(t)\\, dW_t^Q \\quad (5)\n\\]\nOn introduit alors une nouvelle variable d‚Äô√©tat : $ X_t = r_t - f(0,t)$\nLa dynamique de \\(X_t\\) devient :\n\\[\ndX_t = \\left[\\varphi(t) - \\lambda X_t\\right] dt + \\sigma(t)\\, dW_t^Q \\quad (8)\n\\]\navec $ (t) = _0^t ^2(s), e^{-2(t-s)} ds. $\nLa formule du prix du z√©ro coupon s‚Äôexprime alors comme une fonction d√©terministe de \\(X_t\\) :\n\\[\nB(t, T) = \\frac{B(0,T)}{B(0,t)} \\exp\\!\\Biggl\\{ -\\frac{1}{2\\beta^2(t,T)} \\varphi(t) - \\beta(t,T) X_t \\Biggr\\} \\quad (9)\n\\]\no√π $ (t, T) = . $\n\nA quelle cat√©gorie de mod√®le appartient le mod√®le Hull&White? Justifier.\n\nLe mod√®le Hull & White est un mod√®le √† structure √† terme aÔ¨Éne, i.e.¬†un mod√®le de taux d‚Äôint√©r√™t pour lequel le taux z√©ro-coupon continu R(t, T ) est une fonction aÔ¨Éne du taux court r (t).\nIl ressemble √† un processus d‚ÄôOrnstein-Uhlenbeck ou mean reversing process, qui est un processus gaussien d√©finit de la mani√®re suivante :\n\\[\ndY_t = - \\theta \\left[Y_t - \\mu \\right] dt + \\sigma dW_t,\n\\]\no√π \\(\\theta, \\mu, \\sigma\\) sont des param√®tres d√©terministes et \\(W_t\\) est le processus de Wiener.\nDans notre cas, on a \\(\\theta = \\lambda\\), \\(\\mu = \\frac{\\phi(t)}{\\lambda}\\) et \\(\\sigma = \\sigma(t)\\). De ce fait, la moyenne et la variance d√©pend du temps et le param√®tre de vitesse de retour √† la moyenne est constant.\n\nD√©terminer la loi du processus \\(X_t|X_s\\)?\n\nSous la probabilit√© risque neute \\(\\mathbb{Q}\\), le processus \\(X_t\\) s‚Äôecrit :\n\\[\ndX_t = \\left(\\phi(t)- \\lambda X_t \\right) dt + \\sigma(t) dW_t^Q,\n\\]\navec \\(\\phi(t) = \\int_0^t \\sigma^2(s) e^{-2\\lambda(t-s)} ds\\).\nPosons \\(K_t = e^{\\lambda t} X_t  = f(X_t, t)\\implies X_t = e^{-\\lambda t} K_t\\), par la formule d‚ÄôIt√¥, on a :\n\\[\\begin{aligned}\ndf(X_t, t) &= e^{\\lambda t} dX_t + de^{\\lambda t} X_t \\\\\n&= e^{\\lambda t} \\left( \\phi(t) - \\lambda X_t \\right) dt + e^{\\lambda t} \\sigma(t) dW_t^Q + e^{\\lambda t} X_t dt \\\\\n&= e^{\\lambda t}\\phi(t) dt + e^{\\lambda t}\\sigma(t)dW_t^Q \\\\\n&\\implies f(X_t, t) = K_t = e^{- \\lambda t}K_s  + \\int_s^t e^{-\\lambda(t-u)}\\phi(u) du + \\int_s^t e^{-\\lambda(t-u)}\\sigma(u)dW_u^Q \\\\\n&\\Leftrightarrow X_t = X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\phi(u)  du + \\int_s^t e^{-\\lambda (t-u)} \\sigma(u) dW_u^T\n\\end{aligned}\\]\nDe ce fait, on en d√©duit que $X_t|X_s ( X_s e^{-(t-s)} + _s^t e^{-(t-u)} (u) du, _s^t e^{-2 (t-u)} (u)^2 d ) $.\n\n\nIII.4 Dynamique des taux forwards\nOn note ensuite \\(L_i(t)\\) le taux LIBOR forward √† la date \\(t\\) qui fixe en \\(T_i\\) et paie en \\(T_{i+1}\\). Sous l‚Äôhypoth√®se d‚Äôabsence d‚Äôopportunit√© d‚Äôarbitrage, ce taux s‚Äôexprime √† partir de la courbe de taux : √é \\[\nL_i(t) = \\frac{1}{\\delta_i}\\left(\\frac{B(t, T_i)}{B(t, T_{i+1})} - 1\\right)  = \\frac{1}{\\delta_i}\\left(Z_t- 1\\right) ,\n\\]\nPour connaitre la dynamique des taux forwards, on applique le lemme d‚ÄôIt√¥ au processus :\n\\[\nZ_t = \\frac{B(t, T_i)}{B(t, T_{i+1})}.\n\\]\n\nRappel du lemme d‚ÄôIt√¥ : Consid√©rons deux actifs \\(X\\) et \\(Y\\) et posons \\(Z = \\frac{X}{Y}\\) (la valeur de \\(X\\) exprim√©e en num√©raire \\(Y\\)). Le lemme d‚ÄôIt√¥ nous donne l‚Äô√©volution de \\(Z\\) par :\n\\[\n\\frac{dZ}{Z} = \\left(\\frac{dX}{X} - \\frac{dY}{Y}\\right) - \\left\\langle \\frac{dX}{X} - \\frac{dY}{Y},\\, \\frac{dY}{Y}\\right\\rangle.\n\\]\n\nEn appliquant le lemme d‚ÄôIt√¥ √† \\(Z_t\\), on obtient :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= \\frac{dB(t, T_i)}{B(t, T_i)} - \\frac{dB(t, T_{i+1})}{B(t, T_{i+1})} \\\\\n&- \\frac{1}{\\cancel{B(t,T_{i+1})^2}} \\cancel{B(t,T_{i+1})^2} \\, \\Gamma(t,T_i,T_{i+1})^2 \\, dt \\\\\n&-  \\frac{-1}{\\cancel{B(t,T_i)B(t,T_{i+1})}} \\cancel{B(t,T_i)B(t,T_{i+1})}\\,\\Gamma(t,T_i)\\Gamma(t,T_{i+1})\\,dt\\\\\n&= \\Gamma(t,T_i,T_{i+1})\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i)\\right)dt + \\left(\\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\right)dW_t^Q\\\\\n&\\implies \\frac{dZ_t}{Z_t} = \\mu(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\n\\end{aligned}\\]\navec\n\\[\\begin{cases}\n\\sigma(t,T_i,T_{i+1}) = \\Gamma(t,T_i) - \\Gamma(t,T_{i+1})\\\\\n\\mu(t,T_i,T_{i+1}) = \\Gamma(t,T_i)\\left(\\Gamma(t,T_{i+1}) - \\Gamma(t,T_i) \\right) = - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\n\\end{cases}\\]\nDe ce fait, on a :\n\\[\\begin{aligned}\n\\frac{dZ_t}{Z_t} &= - \\Gamma(t,T_i) \\sigma(t,T_i,T_{i+1})\\,dt + \\sigma(t,T_i,T_{i+1})\\,dW_t^Q\\\\\n&= \\sigma(t,T_i,T_{i+1}) \\underbrace{\\left( - \\Gamma(t,T_i)\\,dt + dW_t^Q \\right)}_{d\\tilde{W_t}}\\\\\n&= \\sigma(t,T_i,T_{i+1}) d\\tilde{W_t}\n\\end{aligned}\\]\no√π \\(d\\tilde{W_t}\\) est un mouvement brownien selon le th√©or√®me de Girsanov.\nLa diffusion de \\(Z_t\\) est une loi log-normale, sans drift sous la probabilit√© risque forward. De ce fait, il suit le mod√®le de Black pour la valorisation des options.\nOn peut √©crire ainsi la dynamique du taux forward \\(L_i(t)\\) sous la probabilit√© risque forward :\n\\[\\begin{aligned}\ndL_i(t) &= \\frac{Z_t}{\\delta_i} \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}  \\\\\n&= (L_i(t) + \\frac{1}{\\delta_i}) \\sigma(t,T_i,T_{i+1})  d\\tilde{W_t}\n\\end{aligned}\\]\n\n\nIII.5 Valorisation des instruments de calibration\n\nPayoff d‚Äôun caplet vanille :\nLe payoff d‚Äôun caplet sur le taux LIBOR \\(L_i(T_i)\\), de maturit√© \\(T_i\\), avec paiement en \\(T_{i+1}\\) et de strike \\(K\\) est donn√© par :\n\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr).\n\\]\n\nFormule de valorisation dans le cadre du mod√®le H&W :\nIl peut √™tre d√©montr√© que la formule de valorisation de ce caplet s‚Äôexprime de la mani√®re suivante :\n\n\\[\nC\\Bigl( Z_t,\\, \\tilde{K},\\, T_i,\\, \\sigma_i^*,\\, B(t, T_{i+1}) \\Bigr) \\quad (11)\n\\]\navec :\n\n$ Z_t = $,\n$ (_i^*)^2 = t^{T_i} ( (s, T_i) - (s, T{i+1}) )^2 ds = ^2(T_i, T_{i+1}), (T_i) $,\n$ = 1 + _i K $,\n$ C() $ d√©signe le prix d‚Äôun Call selon le cadre Black, en fonction du forward, du strike, de la maturit√©, de la volatilit√© et du facteur d‚Äôactualisation.\n\nEn effet, on peut r√©√©crire le payoff d‚Äôun caplet sous la forme d‚Äôun Call sur \\(Z_t\\) qui suit un mod√®le de Black :\n\\[\\begin{aligned}\n\\text{Payoff} &= \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t}{\\delta_i} - 1 - K; 0 \\Bigr) \\\\\n&= \\delta_i\\, \\max\\Bigl( \\frac{Z_t - 1 - \\delta_i K}{\\delta_i}; 0 \\Bigr) \\\\\n&= \\max\\Bigl(Z_t - 1 - \\delta_i K; 0 \\Bigr) \\\\\n\\text{Payoff} &= \\max\\Bigl( Z_t - \\tilde{K},\\; 0 \\Bigr) \\\\\n\\end{aligned}\\]\n\n\nIII.6. Calibration du mod√®le\nLe mod√®le de Hull White permet d‚Äôavoir une formule ferm√©e pour le prix des caplets. De fait, puisqu‚Äôon a calcul√© les prix de march√© de caplets sur euribor12M, ce qui implique une p√©riode de capitalisation annuelle, de maturit√© T = 5Y, nous pouvons desormais calibrer le param√®tre de volatilit√© \\(\\sigma_i^*\\) avec la m√©thode de dichotomie et aussi extraire de mani√®re analytique la volatilit√© instantan√©e \\(\\sigma(t)\\) du mod√®le Hull&White, qu‚Äôon supposera constante, i.e.¬†\\(\\sigma(t) = \\sigma\\).\nOn pose √©galement, pour la calibration, \\(\\lambda = 5\\%\\).\nPour extraire la volatilit√© spot, nous utiliserons uniquement le prix de marche ATM.\n\ncaplet_price_MKT = vol_data.loc[3, 'Caplet Price MKT']\nprint(f\"Le prix de march√© caplet sur euribor 12M de maturit√© T=5Y est de {caplet_price_MKT:.4%}\")\n\nLe prix de march√© caplet sur euribor 12M de maturit√© T=5Y est de 0.7137%\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW'):\n    if model == \"HW\" :\n        Z_t = delta_i * L_i_t + 1\n        K = 1 + delta_i * K\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=Z_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n    else:\n        return price_oplet(N=N, delta_i=delta_i, B_t_Ti=B_t_Ti, L_i_t=L_i_t, K=K, sigma_i=sigma_i, Ti=Ti, t=t, option_type=option_type)\n\n\ndef Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT,option_type='caplet', model=\"HW\",tol=1e-6, sigma_low=1/10000, sigma_high=1):\n    \"\"\"\n    Extrait la volatilit√© implicite sigma en utilisant la m√©thode de dichotomie.\n    \"\"\"\n    fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n    fmax = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_high,option_type=option_type, model=model)\n    price = caplet_price_MKT\n    if fmin&gt;price :\n        return sigma_low\n    elif fmax&lt;price :\n        return sigma_high\n    else:\n        while sigma_high-sigma_low&gt;tol:\n            sigma_mid = (sigma_low + sigma_high) / 2\n            fmin = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_low, option_type=option_type, model=model)\n            fmid = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_mid,option_type=option_type, model=model)\n            if ((fmin - price) * (fmid - price) &gt; 0) : # jette la moiti√© de gauche\n                sigma_low = sigma_mid\n            else: # jette la moiti√© de droite\n                sigma_high = sigma_mid\n        sigma_mid = (sigma_low + sigma_high) / 2\n        return sigma_mid\n    \n\n# Notional\nN = 1  \n\n# P√©riode du caplet\ndelta_i = 1 \n\n# Maturit√© du caplet\nTi=6\n\n# Facteur d'actualisation B(t, Ti)\nB_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n# Taux forward Li(t)\nL_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n# Strike du caplet\nK = L_i_t + vol_data.loc[3, 'Strike en bps et en rel. / fwd']/10000\n\nlambda_ = 5/100\n\nsigma_i = Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t,caplet_price_MKT, option_type='caplet',model=\"HW\")\nprint(\"Volatilit√© implicite (en %) :\", sigma_i*100) \n\nVolatilit√© implicite (en %) : 0.9271045541763303\n\n\n\nbeta_Ti_Ti_1 =  ((1 - np.exp(- lambda_ * delta_i))/lambda_)**2\nTi_1 = Ti - delta_i\nphi = (1 - np.exp(-2*lambda_*(Ti_1-t)))/(2*lambda_)\nsigma = np.sqrt((sigma_i**2 * Ti_1)/ (beta_Ti_Ti_1 * phi))\n\nprint(\"Volatilit√© instantan√©e (en %) :\", sigma*100) \n\nVolatilit√© instantan√©e (en %) : 1.071446257025021\n\n\n\nCette volatilit√© spot nous permet de valoriiser les caplets pour des strikes diff√©rents de l‚ÄôATM √† l‚Äôaide de la formule de valorisation ferm√©e du mod√®le Hull et White.\n\n\nfor i in vol_data.index:\n    # Notional\n    N = 1  \n\n    # P√©riode du caplet\n    delta_i = 1 \n\n    # Maturit√© du caplet\n    Ti=6\n\n    # Facteur d'actualisation B(t, Ti)\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n\n    # Taux forward Li(t)\n    L_i_t = (1/delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0]/df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n\n    # Strike du caplet\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd']/10000\n\n    sigma_i = sigma_i\n\n    lambda_ = 5/100\n\n    caplet_price = price_oplet_HW(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, sigma_i, option_type='caplet', model='HW')\n    vol_data.loc[i, 'Caplet Price HW'] = caplet_price\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n\n\n\n\n\n\n\nL‚Äôune des faiblesses du mod√®le de Hull et White est le fait qu‚Äôil n‚Äôarrive pas √† capter le smile de volatilit√©. En effet, la volatilit√© implicite extraite est un skew. Pour constater ce ph√©nom√®ne, nous inverserons la formule de Black pour les caplets et nous en d√©duirons la volatilit√© implicite pour chaque strike. Nous utiliserons toujours la m√©thode de dichotomie pour trouver la volatilit√© implicite.\n\nfor i in vol_data.index:\n    N = 1\n    delta_i = 1\n    Ti = 6\n    B_t_Ti = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]\n    L_i_t = (1 / delta_i) * ((df_ZC.loc[df_ZC['T'] == Ti-1, 'B'].values[0] / df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0]) - 1)\n    K = L_i_t + vol_data.loc[i, 'Strike en bps et en rel. / fwd'] / 10000\n    lambda_ = 5 / 100\n    caplet_price_MKT = vol_data.loc[i, 'Caplet Price HW']\n\n    sigma_extracted =  Dichotomie(N, delta_i, B_t_Ti, L_i_t, K, Ti, t, caplet_price_MKT, option_type='caplet', model='Black')\n    vol_data.loc[i, 'Sigma_HW'] = sigma_extracted\n\nvol_data\n\n\n\n\n\n\n\n\nStrike en bps et en rel. / fwd\nVols Caplets\nVols Swaptions\nCaplet Price MKT\nCaplet Price HW\nSigma_HW\n\n\n\n\n0\n-100\n0.311859\n0.311859\n0.012511\n0.012015\n0.288669\n\n\n1\n-50\n0.283274\n0.283274\n0.009788\n0.009389\n0.267386\n\n\n2\n-25\n0.265921\n0.265921\n0.008419\n0.008215\n0.258287\n\n\n3\n0\n0.250000\n0.250000\n0.007137\n0.007137\n0.250009\n\n\n4\n25\n0.243451\n0.243451\n0.006185\n0.006156\n0.242433\n\n\n5\n50\n0.249019\n0.249019\n0.005661\n0.005269\n0.235466\n\n\n6\n100\n0.271828\n0.271828\n0.005167\n0.003771\n0.223060\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Sigma_HW\"], label='Volatilit√© extraite', marker='o')\nplt.plot(vol_data[\"Strike en bps et en rel. / fwd\"], vol_data[\"Vols Caplets\"], label='Volatilit√© de march√©', marker='o')\nplt.legend()\nplt.title('Comparaison de la volatilit√© extraite par le mod√®le HW et la volatilit√© de march√©')\nplt.xlabel('Strike en bps et en rel. / fwd ')\nplt.ylabel('Volatilit√©')\n\nText(0, 0.5, 'Volatilit√©')\n\n\n\n\n\n\n\n\n\n\nATM  = L_i_t\n\n\n\n3.7 Valorisation d‚Äôun produit structur√©\n\nRemarques pr√©liminaires :\n\nNous garderons dans un premier temps la calibration ATM effectu√©e avec \\(\\lambda = 5\\%\\).\nPour la partie Monte-Carlo, nous admettrons que l‚ÄôEDS pour le processus \\(X_t\\) sous la probabilit√© forward neutre \\(Q^T\\) associ√©e au num√©raire \\(B(t,T)\\) s‚Äô√©crit comme :\n\n\\[\ndX_t = \\left[\\phi(t) + \\sigma(t)\\Gamma(t,T) - \\lambda X_t\\right] dt + \\sigma(t) dW_t^T \\tag{12}\n\\]\nO√π \\(W_t^T\\) est un brownien sous \\(Q^T\\).\n\nNous souhaitons valoriser un caplet de strike \\(K\\), de dates de fixing \\(T_i = 5Y\\) et de paiement \\(T_{i+1} = 6Y\\) et de barri√®re d√©sactivante \\(B\\) (avec \\(B &gt; K\\)).\n\n√âcrire le payoff de l‚Äôoption et tracer la fonction de payoff en fonction de \\(L_i(T_i)\\). Cette option est-elle plus ou moins ch√®re qu‚Äôun simple caplet de strike \\(K\\) ?\n\nLe payoff de l‚Äôoption est donn√© par :\n\\[\n\\text{Payoff} = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &lt; B}\n\\]\n\n\n\nimage-2.png\n\n\nIl est possible de d√©composer le payoff √† partir d‚Äôoptions vanilles et digitales :\n\\[\n\\text{Payoff} = C_K - CB - (B-K) \\times D_B\n\\]\n\nUne option digitale est une option qui paie 1 si le sous-jacent est au-dessus d‚Äôun certain seuil et 0 sinon. De ce fait, le payoff de l‚Äôoption est donn√© par :\n\\[\nD_B = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{L_i(T_i) &gt; K}\n\\]\n\n\n\nimage.png\n\n\nUne option vanille est un contrat financier standardis√© qui donne le droit, mais non l‚Äôobligation, d‚Äôacheter (call) ou de vendre (put) un actif sous-jacent √† un prix fix√© (strike) √† une date donn√©e (maturit√©). De ce fait, dans le cas d‚Äôun call, le payoff de l‚Äôoption est donn√© par :\n\\[\nC_K = \\delta_i\\, \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr)\n\\]\n\n\n\nCapture d‚ÄôeÃÅcran 2025-03-05 aÃÄ 00.17.09.png\n\n\n\n\nRappeler les principes du pricing par m√©thode de Monte-Carlo.\n\nLa m√©thode de Monte-Carlo est une m√©thode num√©rique qui permet de pricer des produits financiers complexes lorsque les formules ferm√©es ne sont pas disponibles. Elle consiste √† simuler un grand nombre \\(N\\) de trajectoires du processus stochastique et √† calculer la moyenne empiriques des payoffs actualis√©s pour obtenir le prix de l‚Äôoption.\n\nRappeler comment on simule une loi gaussienne √† partir d‚Äôune loi uniforme.\n\nPour simuler une variable al√©atoire suivant une loi gaussienne standard \\(\\mathcal{N}(0,1)\\) √† partir d‚Äôune variable uniforme \\(U\\) sur \\([0,1]\\), on applique l‚Äôinverse de la fonction de r√©partition de la loi gaussienne standard (aussi appel√©e la fonction quantile de la loi normale) :\n\\[\nX = F^{-1}(U)\n\\]\no√π \\(F\\) est la fonction de r√©partition de la loi normale standard.\n\nOn consid√®re un caplet sur euribor12M √† barri√®re d√©sactivante de strike $ K = ATM - 100 bps$, de barri√®re \\(B = ATM + 100 bps\\) et de maturit√© \\(T_i = 5Y\\). Pour valoriser cette option, nous allons utiliser une m√©thode num√©rique de type Monte-Carlo. Pour cela, il est necessaire de connaire la loi de X_t sachant X_s. En nous aidant de la question pr√©c√©dente, on peut d√©duire que la loi de \\(X_t|X_s\\) est une loi normale de param√®tres :\n\n\\[\nX_t|X_s \\sim \\mathcal{N}\\left( X_s e^{-\\lambda (t-s)} + \\int_s^t e^{-\\lambda (t-u)} \\left( \\phi(u) - \\sigma \\Gamma(u,T) \\right) du, \\quad \\int_s^t e^{-2 \\lambda (t-u)} \\sigma^2 d \\right)\n\\]\nPour valoriser cette option, nous pouvons directement utiliser la loi de \\(X_5|X_0\\) pour simuler les trajectoires du taux court et calculer le payoff de l‚Äôoption ou diffuser progressivement le taux court en utilisant la loi de \\(X_t|X_s\\) pour chaque pas de temps. Ensuite, il s‚Äôagira de calculer le payoff de l‚Äôoption √† chaque date, en faire la moyenne et l‚Äôactualiser pour obtenir le prix de l‚Äôoption.\n\nM√©thode 1 : Simulation de la loi de \\(X_5|X_0\\)\n\nimport numpy as np\nfrom scipy.integrate import quad\n\n# Fonction phi(t) - variance cumul√©e\ndef phi(t, sigma, lambda_):\n    return (sigma**2 / (2 * lambda_)) * (1 - np.exp(-2 * lambda_ * t))\n\n# Fonction beta(t, T)\ndef beta(t, T, lambda_):\n    return (1 - np.exp(-lambda_ * (T - t))) / lambda_\n\n# Fonction gamma(t, T)\ndef gamma(t, T, sigma, lambda_):\n    return (sigma / lambda_) * (np.exp(-lambda_ * (T - t)) - 1)\n\n# Fonction B(t, T)\ndef B_t_T(t, T, B0_T, B0_t, X_t, sigma, lambda_):\n    beta_t_T = beta(t, T, lambda_)**2\n    phi_t = phi(t, sigma, lambda_)\n    exponent = -0.5 * beta_t_T * phi_t - beta_t_T * X_t\n    return (B0_T / B0_t) * np.exp(exponent)\n\n# Fonction d'int√©gration avec param√®tres suppl√©mentaires\ndef integrand_mean(u, t, Xs, s, sigma, lambda_, T):\n    # t = borne sup\n    # s = borne inf\n    # T = maturit√©\n    Xt = Xs * np.exp(-lambda_ * (t - s))\n    exp_part = np.exp(-lambda_ * (t - u))\n    return Xt + exp_part * (phi(u, sigma, lambda_) + sigma * gamma(u, T, sigma, lambda_))\n\n# Param√®tres\nT = Ti_1  = 5\nXs = X0 = 0  \ns = 0\nt = Ti_1 \n\n# Calcul de la moyenne conditionnelle\nmean_5_given_0, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\nprint(f\"Moyenne conditionnelle de X_5 | X_0 : {mean_5_given_0:.6f}\")\n\n# Calcul de la variance conditionnelle (ind√©pendant de gamma ici)\n\ndef compute_variance(sigma, lambda_, t, s):\n    return (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nvar_5_given_0 = compute_variance(sigma, lambda_, t, s)\n# var_5_given_0 = (sigma**2) * (1 - np.exp(-2 * lambda_ * (t - s))) / (2 * lambda_)\nprint(f\"Variance conditionnelle de X_5 | X_0 : {var_5_given_0:.6f}\")\n\nMoyenne conditionnelle de X_5 | X_0 : 0.000000\nVariance conditionnelle de X_5 | X_0 : 0.000452\n\n\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003048\n\n\n\n\nM√©thode 2 : Methode de diffusion\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[i-1]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003032\n\n\nNous constatons qu‚Äôavec les deux m√©thodes, nous obtenons des prix d‚Äôoptions similaires (diff√©rent de 0.2bps). Cela confirme que les deux m√©thodes convergent vers le m√™me r√©sultat.\nEn d√©g√©n√©rant le produit en faisant tendre la barri√®re √† \\(+\\infty\\), nous constatons que le prix de l‚Äôoption call est √©gal au prix de march√© du forward. En d√©g√©n√©rant le produit en faisant tendre la barri√®re √† 0, nous constatons que le prix de l‚Äôoption call est √©gal √† 0.\nCela est coh√©rent car lorsque la barri√®re est tr√®s √©lev√©e, le produit est √©quivalent √† un forward et lorsque la barri√®re est nulle, le produit est √©quivalent √† un call classique. La fonction que nous avons impl√©ment√© est donc coh√©rente et bien impl√©ment√©e.\n\nTi_1 = 5\nTi = 6\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n    # Moyenne\n    mu_X = mean_5_given_0\n\n    # Ecart-type\n    sigma_X = np.sqrt(var_5_given_0)\n\n    # X_5|X_0\n    X = np.random.normal(mu_X, sigma_X)  \n\n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_i_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n    # D√©finition du strike\n    strike = ATM # - bp\n\n    # Barri√®re\n    B = np.inf#ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.007094\n\n\nNous rendons la barri√®re ‚Äòbermud√©enne‚Äô en √©tendant la condition de d√©sactivation aux dates 1Y, 2Y, 3Y, 4Y et 5Y. De ce fait, le payoff de cette option s‚Äô√©crit :\n\\[\n\\text{Payoff} = \\delta_i \\max\\Bigl( L_i(T_i) - K,\\; 0 \\Bigr) \\mathbb{1}_{max_{i=1,\\dots,5}(L_i(T_i) &lt; B)}\n\\]\n\n# Option bermudienne\n\n# Simulation Monte Carlo\nn_simulations = 10000\n\npayoffs = np.zeros(n_simulations)\nX0 = 0\nTi = 6\nTi_1 = 5\nX = np.zeros(Ti)\nX[0] = X0\n\nfor sim in range(n_simulations):\n    phi_ = phi(Ti_1, sigma, lambda_)\n    gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n    L_i_t = np.zeros(Ti_1)\n    for i in range(1,Ti):\n        t = i\n        s = i-1\n        T = 5 \n        Xs = X[s]\n\n        # Calcul de la moyenne conditionnelle\n        mu_X, _ = quad(integrand_mean, s, t, args=(t, Xs, s, sigma, lambda_, T))\n        sigma_X = np.sqrt(compute_variance(sigma, lambda_, t, s))\n        X[i] = np.random.normal(mu_X, sigma_X)\n\n        # Calcul du prix B(5,6) selon Hull-White\n        Bi_t = df_ZC.loc[df_ZC['T'] == t, 'B'].values[0] # B(0,6)\n        Bi_s = df_ZC.loc[df_ZC['T'] == s, 'B'].values[0] # B(0,5)\n\n        B_s_t = B_t_T(s, t, Bi_t, Bi_s, X[i], sigma, lambda_)\n        B_s_s = B_t_T(s, s, Bi_s, Bi_s, X[i], sigma, lambda_)\n\n        # Calcul du taux forward L_i_t\n        L_i_t[i-1] = (1 / (t-s)) * ((B_s_s / B_s_t) - 1)\n    \n    # Calcul du prix B(5,6) selon Hull-White\n    B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n    B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n    B_5_6 = B_t_T(5, 6, B0_6, B0_5, X[Ti_1], sigma, lambda_)\n    B_5_5 = B_t_T(5, 5, B0_5, B0_5, X[Ti_1], sigma, lambda_)\n\n    # Calcul du taux forward L_i_t\n    L_t = ((B_5_5 / B_5_6) - 1)\n    bp = 100/10000\n\n\n    # D√©finition du strike\n    strike = ATM - bp\n\n    # Barri√®re\n    B = ATM + bp\n\n    # Payoff de l'option\n    payoff = np.maximum(L_t - strike, 0) * (np.max(L_i_t) &lt; B)\n    payoffs[sim] = payoff\n\n# Prix de l'option call\ncall_price = B0_6 * np.mean(payoffs)\nprint(f\"Prix du call : {call_price:.6f}\")\n\nPrix du call : 0.003071\n\n\nL‚Äôun des param√®tres important du mod√®le de Hull et White est la mean reversion \\(\\lambda\\), qui caract√©rise la force de rappel √† la moyenne du processus. Ce param√®tre a un impact positive sur la valorisation de l‚Äôoption, comme nous pouvons l‚Äôobserver dans la figure ci dessous .\n\n# Liste des lambda √† tester\nlambdas = np.linspace(0.01, 1, 15)  # Exemple de grille de lambda\nresults = []\n\nTi_1 = 5\nTi = 6\n# Boucle principale sur les lambdas\nfor lambda_ in lambdas:\n    n_touched = 0\n    payoffs = np.zeros(n_simulations)\n\n    for sim in range(n_simulations):\n        phi_ = phi(Ti_1, sigma, lambda_)\n        gamma_ = gamma(Ti_1,Ti, sigma, lambda_)\n\n        # Moyenne\n        mu_X = mean_5_given_0\n\n        # Ecart-type\n        sigma_X = np.sqrt(var_5_given_0)\n\n        # X_5|X_0\n        X = np.random.normal(mu_X, sigma_X)  \n\n        # Calcul du prix B(5,6) selon Hull-White\n        B0_6 = df_ZC.loc[df_ZC['T'] == Ti, 'B'].values[0] # B(0,6)\n        B0_5 = df_ZC.loc[df_ZC['T'] == Ti_1, 'B'].values[0] # B(0,5)\n\n        B_5_6 = B_t_T(5, 6, B0_6, B0_5, X, sigma, lambda_)\n        B_5_5 = B_t_T(5, 5, B0_5, B0_5, X, sigma, lambda_)\n\n        L_i_t = ((B_5_5 / B_5_6) - 1)\n        bp = 100 / 10000\n        strike = ATM - bp\n        B = ATM + bp\n\n        payoff = np.maximum(L_i_t - strike, 0) * (L_i_t &lt; B)\n        payoffs[sim] = payoff\n\n        # V√©rification de la barri√®re\n        if np.any(L_i_t&gt;= B):\n            n_touched += 1\n\n    prob_toucher_barriere = n_touched / n_simulations\n\n    call_price = B0_6 * np.mean(payoffs)\n    results.append((lambda_, call_price,prob_toucher_barriere))\n\n\n# Optionnel : Graphique de la sensibilit√©\nimport matplotlib.pyplot as plt\n\nlambdas, prices, probabilities = zip(*results)\nplt.plot(lambdas, prices, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Prix de l\\'option')\nplt.title('Sensibilit√© du prix √† la mean reversion (Œª)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nDe plus, plus le param√®tre de mean reversion \\(\\lambda\\) est √©lev√©, plus la probabilit√© de toucher la barri√®re est faible. Inversement, une faible mean reversion laisse plus de libert√© au processus pour explorer des valeurs extr√™mes, augmentant ainsi la probabilit√© de franchir la barri√®re.\n\nplt.plot(lambdas, probabilities, marker='o')\nplt.xlabel('Lambda (Mean Reversion)')\nplt.ylabel('Probabilit√© de toucher la barri√®re')\nplt.title('Probabilit√© de toucher la barri√®re en fonction de la mean reversion (Œª)')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "3A/risque_def.html",
    "href": "3A/risque_def.html",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "",
    "text": "En finance, le risque peut √™tre d√©fini comme la survenance d‚Äôun √©v√©nement incertain qui peut avoir des cons√©quences n√©gatives sur le bilan, ou le compte de r√©sultat d‚Äôune banque. Par exemple, une fraude aura un impact n√©gative sur la r√©putation d‚Äôune banque ce qui peut entrainer des pertes importants ayant un impact n√©gatif sur le r√©sultat net de celle-ci. En √©conomie, le risque est un √©v√©nement probabilisable tandis que l‚Äôincertitude est non probabilisable.\nNous pouvons caract√©riser 3 grands types de risques √©tablis par le comit√© de B√¢le qui veille au renforcement et √† la stabilit√© du syst√®me financier. (rang√©s par ordre d‚Äôimportance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la cons√©quence. Toutefois, certains risque sont difficiles √† distinguer. Ils se trouvent √† la fronti√®re entre le risque de march√©, de cr√©dit et le risque op√©rationnel.\nIl est important de noter que le but d‚Äôune banque n‚Äôest pas de prendre le moins de risque, mais d‚Äôatteindre une rentabilit√© maximale pour un risque donn√©. La th√©orie financi√®re nous apprend que seul le risque est r√©mun√©r√©. La banque proc√®de donc √† une arbitrage entre risque et rentabilit√©. C‚Äôest pourquoi la gestion des risques est un √©l√©ment cl√© de la strat√©gie de d√©cision de la banque. La mesure du risque intervient pour calculer les fonds propres n√©cessaires pour assurer chaque op√©ration financi√®re. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "3A/risque_def.html#les-mesures-de-risque",
    "href": "3A/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al.¬†ont d√©fini les propri√©t√©s que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  coh√©rente. Une mesure de risque est une fonction qui permet de quantifier le risque d‚Äôun portefeuille. Elle est coh√©rente si elle satisfait les propri√©t√©s suivantes :\n\nsous-additivit√© : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomog√©n√©it√© positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivit√© signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal √† la somme des risques des actifs qui le composent. Ce ph√©nom√®ne est appel√© effet de diversification. En effet, la diversification permet de r√©duire le risque d‚Äôun portefeuille en investissant dans des actifs non corr√©l√©s. Ainsi, en agr√©geant deux porte-feuilles, il n‚Äôy a pas de cr√©ation de risque suppl√©mentaire.\nL‚Äôhomog√©n√©it√© positive signifie que le risque d‚Äôun portefeuille est proportionnel √† la taille du portefeuille. Cette propri√©t√© ignore les probl√®mes de liquidit√©.\nL‚Äôinvariance par translation signifie que l‚Äôaddition au portefeuille initiale un montant s√ªr r√©mun√©r√© au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d‚Äôimmobiliser des fonds propres √©gaux √† la mesure du risque.\nLa monotonie signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal au risque d‚Äôun autre portefeuille si la distribution de probabilit√© de la perte potentielle du premier portefeuille est inf√©rieure ou √©gale √† celle du deuxi√®me portefeuille. Cel√† traduit l‚Äôordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d‚Äôun portefeuille √† un instant t donn√©, le risque est la variation n√©gative de ce portefeuille dans le futur. Le risque se caract√©risait donc par une perte relativfe (par rapport √† la valeur initiale du portefeuille √† un instant t). Pendant tr√®s longtemps, les banques utilisaient la volatilit√© (√©cart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup √©volu√©e et celle qui est la plus r√©pandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d‚Äôun portefeuille √† un horizon \\(h\\) donn√© et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\no√π F est la distribution de probabilit√© de la perte potentielle du portefeuille.\nPar exemple, une VaR √† \\(\\alpha=1\\%\\) de 1 million d‚Äôeuros signifie que la probabilit√© que la banque perde plus de 1 million d‚Äôeuros est √©gale √† 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d‚Äôeuros sur une p√©riode donn√©e (C‚Äôest la perte maximale encourue par la banque avec un intervalle de confiance √† 99%). Nous allons pr√©f√©rer la deuxi√®me formulation de l‚Äôinterpr√©tation.\nDeux √©l√©ments sont n√©cessaires pour d√©terminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance √† un indicateur de tol√©rance pour le risque. Une couverture √† 99% est beaucoup plus exigente et donc plus co√ªteuse qu‚Äôune couverture √† 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait d√©finir l‚Äôhorizon h. Par exemple, une couverture √† 1 jour est moins co√ªteuse qu‚Äôune couverture √† 1 mois. C‚Äôest la combinaison de ces deux √©l√©ments qui d√©termine le degr√© de la couverture qui peut √™tre exprim√© en temps de retour 1 \\(t¬∞\\)qui est la dur√©e moyenne entre deux d√©passements de la VaR. Il permet de caract√©riser la raret√© d‚Äôun √©v√®nement (dont la probabilit√© d‚Äôoccurence est petite)\n\\[t¬∞= \\frac{h}{1-\\alpha}\\]\nLorsqu‚Äôon entend parler de gestion de risque d√©cennal, cel√† revient √† consid√©rer une valeur en risque (VaR) journali√®re (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non coh√©rente car elle ne respecte pas la propri√©t√© de sous-additivit√©. De nombreux professionnels recommanderaient alors l‚Äôutilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque coh√©rente. La CVAR est l‚Äôesp√©rance de la perte au del√† de la VaR. Toutefois, la VaR reste une mesure de risque tr√®s utilis√©e en pratique, qui ne respecte pas la propri√©t√© de sous-additivit√© que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilit√©s sont principalement localis√©es dans les quantiles extr√™mes.\n\n\n\nD‚Äôautres mesures de risque\nD‚Äôautres mesures peuvent √™tre d√©finis comme celle de la perte exceptionnelle (Unexpected Loss - UL) d√©finie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] o√π L est la distribution de la perte potentielle.\nIl s‚Äôagit l√† de la diff√©rence entre la VaR et la perte moyenne (expected loss - EL). Il y a √©galement le regret esp√©r√© d√©fini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donn√© repr√©sentant le montant de la perte tol√©rable par l‚Äôinstitut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l‚Äôinstitut financier. Lorsque H est endog√®ne, c‚Äôest-√†-dire d√©pendant de la distribution de la perte potentielle, et √©gale √† la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l‚Äôesp√©rance de la perte au del√† de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au del√† d‚Äôun certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au del√† de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c‚Äôest la semi variance."
  },
  {
    "objectID": "3A/risque_def.html#footnotes",
    "href": "3A/risque_def.html#footnotes",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\np√©riode de retour doit √™tre interpr√©t√©e comme la probabilit√© statistique qu‚Äôun √©v√®nement se produise‚Ü©Ô∏é"
  },
  {
    "objectID": "3A/value-at-risk/var_application.html",
    "href": "3A/value-at-risk/var_application.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous int√©resser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d‚Äôestimer les pertes maximales potentielles d‚Äôun portefeuille d‚Äôactifs financiers sur un horizon de temps donn√©, √† un certain niveau de confiance. Elle est largement utilis√©e par les institutions financi√®res pour √©valuer et g√©rer les risques de march√©, de cr√©dit et de liquidit√©.\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-analytique",
    "href": "3A/value-at-risk/var_application.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est bas√©e sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR √† horizon 1 jour. La VaR √† horizon 1 jour est d√©finie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] o√π \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) √† \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l‚Äôhypoth√®se nulle est que la population √©tudi√©e suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\nnp.float64(5.464960138585188e-41)\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l‚Äôhypoth√®se de la distribution normale de nos rendements. Cel√† est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(8, 6))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR √† horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR √† horizon 1 jour est de 0.0326\n\n\nLa VaR √† horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l‚Äô√©chantillon test les rendements avec la VaR √† horizon 1 jour. Si le rendement est inf√©rieur √† l‚Äôoppos√© de la VaR gaussienne, alors la VaR est viol√©e et cel√† correspond √† une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d‚Äôexceptions que nous comptabilisons sur nos donn√©es test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 30 et en d√©duisons que le taux d‚Äôexception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.12\n\n\nPour tester la pertinence de la VaR calcul√©e, il faudrait id√©alement que le taux d‚Äôexception soit inf√©rieur √† 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\nnp.float64(0.2789593041824687)\n\n\nLa pvaleur de ce test est 3.70%, cel√† est inf√©rieur √† 5% donc nous rejetons l‚Äôhypoth√®se nulle selon laquelle le taux d‚Äôexception est √©gale √† 0.01 au risque 5% de se tromper. Cel√† nous indique que la VaR gaussienne n‚Äôest pas performante. Ceci n‚Äôest pas surprenant √©tant donn√© que nous faisons une hypoth√®se sur la distribution des rendements qui n‚Äôest pas v√©rifi√©e."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-historique",
    "href": "3A/value-at-risk/var_application.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est bas√©e sur les rendements historiques. Elle est d√©finie comme l‚Äôoppos√© du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsid√©rons les mouvements de prix quotidiens pour l‚Äôindice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 sc√©narios ou cas qui serviront de guide pour les performances futures de l‚Äôindice, c‚Äôest-√†-dire que les 6513 derniers jours seront repr√©sentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour √† 99% correspond au 1er percentile de la distribution de probabilit√© des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique √† horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique √† horizon 1 jour est de 0.0396\n\n\nNous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que la VaR gaussienne. Le taux d‚Äôexception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 14 et en d√©duisons que le taux d‚Äôexception est 0.64%. Ce taux d‚Äôexception est statistiquement sup√©rieur √† 1% (car la pvaleur est d‚Äôenviron 0.95). Ainsi, la VaR historique est performante pour la p√©riode consid√©r√©e.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\nnp.float64(0.985516774434753)"
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "href": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est bas√©e sur la simulation de trajectoires de rendements. Nous allons simuler jusqu‚Äô√† 10000 sc√©narios de rendements et calculer la VaR √† horizon 1 jour en posant une hypoth√®se de normalit√© sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise √† partir de 3000 sc√©narios. Nous utiliserons donc 3000 simulations de rendements. Nous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\nnp.float64(0.04525343549844288)\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que les deux autres VaRs. En effet, le taux d‚Äôexception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inf√©rieur √† 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\nnp.float64(0.9994215138745117)"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html",
    "href": "3A/value-at-risk/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque r√©glementaire correspond √† la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s‚Äôagit dans cette section de d√©velopper la notion de VaR pour des portefeuilles lin√©aires et non lin√©aires."
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#le-backtesting",
    "href": "3A/value-at-risk/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contr√¥le de la qualit√© de la VaR pour un horizon de 1 jour. Il permet de v√©rifier si la VaR est bien calibr√©e. Pour cela, on compare la VaR calcul√©e avec la perte r√©elle. Si la VaR est bien calibr√©e, la perte r√©elle ne doit pas d√©passer la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d‚Äôexception pour valider le mod√®le. Notons PnL le profit and loss du portefeuille et VaR la valeur √† risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsid√©rons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inf√©rieur √† l‚Äôoppos√© de la VaR avec probabilit√© \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une p√©riode ouvr√© comptant n jours, la probabilit√© d‚Äôavoir \\(i\\) exceptions est donn√©e par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir plus de \\(k\\) exceptions est donn√©e par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir au plus de \\(i\\) exceptions est donn√©e par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilit√© d‚Äôexception n‚Äôexc√®de pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d‚Äôexceptions empirique est sup√©rieur √† celui attendu, le mod√®le est rejet√© :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donn√©e par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d‚Äôexceptions observ√©e lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donn√©e par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) o√π \\(t\\) est la valeur observ√©e de la statistique de test et \\(\\phi\\) est la fonction de r√©partition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#cas-g√©n√©ral",
    "href": "3A/value-at-risk/var_def.html#cas-g√©n√©ral",
    "title": "La VaR",
    "section": "2.2 Cas g√©n√©ral",
    "text": "2.2 Cas g√©n√©ral\nDans cette approche, nous consid√©rons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en d√©duisons donc que la VaR est calcul√© comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction d√©croissante de l‚Äôesp√©rance de PnL et une fonction croissante de la volatilit√© du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de pr√©voir l‚Äôesp√©rance du PnL futur.\n\n2.2.1 Exemple\nNous consid√©rons une position courte de 1 million de dollars sur le contrat √† terme S&P 500. Nous estimons que la volatilit√© annualis√©e \\(\\sigma_{\\text{SPX}}\\) est √©gale √† 35%.\nLa perte du portefeuille est √©gale √† \\(L(w) = N \\times R_{\\text{SPX}}\\) o√π \\(N\\) est le montant de l‚Äôexposition (‚àí1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l‚Äôindice S&P 500. Nous d√©duisons que la volatilit√© de la perte annualis√©e est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur √† risque pour une p√©riode de d√©tention d‚Äôun an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l‚Äôinvestisseur sur un 1an s‚Äô√©l√®ve √† 815 500‚Ç¨ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre p√©riode de d√©tention, nous utilisons la r√®gle de la racine carr√© pour convertir la volatilit√© pour une fr√©quence donn√© \\(f_1\\) en une autre volatilit√© pour une autre fr√©quence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les r√©sultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calcul√© sur 1 jour, pour l‚Äôavoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "href": "3A/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Mod√®les lin√©aires de facteurs",
    "text": "2.3 Mod√®les lin√©aires de facteurs\nNous consid√©rons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est lin√©aire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La premi√®re id√©e est de choisir les facteurs comme √©tant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du march√© et chaque actif poss√®de son propre facteur de risque.\nLe probl√®me est que les prix sont loin d‚Äô√™tre stationnaires, ce qui nous am√®ne √† devoir affronter certains probl√®mes pour mod√©liser la distribution \\(F_t\\). Une autre id√©e est de r√©crire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] o√π \\(R_i(t;h)\\) est le retour de l‚Äôactif entre \\(t\\) et \\(t+h\\).\nNous d√©duisons que le PnL al√©atoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\no√π \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l‚Äôexposition nominale)dans l‚Äôactif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s‚Äôensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous d√©duisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l‚Äô√âquation (2.6), l‚Äôexpression de la valeur √† risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d‚Äôestimer la matrice de covariance des retours des actifs pour calculer la valeur √† risque. Cela explique la popularit√© de ce mod√®le, surtout lorsque le P&L du portefeuille est une fonction lin√©aire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsid√©rons l‚Äôexemple des entreprises d‚ÄôApple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l‚Äô√©cart type estim√© des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corr√©lation crois√©e est √©gale √† 12,0787 %. Il s‚Äôensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous d√©duisons que la valeur √† risque quotidienne √† 99% est de 41,21 $. Nous obtenons une figure inf√©rieure √† celle de la valeur √† risque historique, qui √©tait de 47,39 $. Nous expliquons ce r√©sultat par le fait que la distribution gaussienne sous-estime la probabilit√© des √©v√©nements extr√™mes et n‚Äôest donc pas adapt√©e √† des calculs pr√©cis de risque dans des situations de march√© volatiles.\n\n\n2.3.2 Exemple de portefeuille lin√©aire d‚Äôactifs\nConsid√©rons un portefeuille lin√©aire compos√© de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne √©gaux √† : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilit√©s journali√®res sont √©gales √† 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244‚Ç¨, 135‚Ç¨,315‚Ç¨. La matrice de corr√©lation est donn√©e par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journ√©e est donc de 18.42‚Ç¨ avec un risque 1% de se tromper.\n\n2.3.2.1 Impl√©mentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les √©carts-types et les corr√©lations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "href": "3A/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Mod√®les factoriels de risque",
    "text": "2.4 Mod√®les factoriels de risque\nNous supposons que la valeur du portefeuille d√©pend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille √† \\(t+h\\) d√©pend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\no√π g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit lin√©aire par rapport aux facteurs de risque, ainsi donc le retour des actifs √† l‚Äôhorizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] o√π \\(B\\) est la matrice des sensibilit√©s du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque √† l‚Äôhorizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables al√©atoires gaussiennes ind√©pendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable al√©atoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calcul√© comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette m√©thode repose sur 3 hypoth√®ses : l‚Äôind√©pendance temporelle des variations de la valeur du portefeuille, la normalit√© des facteurs et la relation lin√©aire entre les facteurs et la valeur du portefeuille. En g√©n√©ral, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons √† partir des donn√©es historiques des facteurs et \\(B\\) est le vecteur des sensibilit√©s du portefeuille aux facteurs de risque. La seuil difficult√© de cette m√©thode est l‚Äôestimation de la matrice de variance covariance.\n\n2.4.1 Exemple d‚Äôun portefeuille obligataire sans risque de cr√©dit\nNous consid√©rons une exposition sur une obligation am√©ricaine √† $t=$31 d√©cembre 2014. Le nominal de l‚Äôobligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont √©gaux √† 5, \\(t_m&gt;t\\). La maturit√© r√©siduelle est de cinq ans et les dates de fixation sont √† la fin de d√©cembre (\\(n_C=5\\). Le nombre d‚Äôobligations d√©tenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d‚Äôune obligation z√©ro coupon (montant qu‚Äôun investisseur serait pr√™t √† payer aujourd‚Äôhui pour recevoir un paiement fixe √† une date future : combien me rapport un euro √† maturit√© \\(T\\) aujourd‚Äôhui?) au temps \\(t\\) pour l‚Äô√©ch√©ance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) o√π \\(R_t(T)\\) est le taux de rendement z√©ro coupon.\nLa valeur de l‚Äôobligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en d√©duit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\no√π \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l‚Äôobligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l‚Äôobligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en d√©duisons que le prix de l‚Äôobligation est de \\(P(t)=115,47 \\$\\) et l‚Äôexposition totale est de 1 154 706 $. En utilisant la p√©riode historique de l‚Äôann√©e 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d‚Äôint√©r√™t √† coupon z√©ro sachant que l‚Äô√©cart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), o√π \\(h\\) correspond √† un jour de bourse. Pour la matrice de corr√©lation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en d√©duisons que la valeur √† risque √† 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur √† risque de 4970$ pour une p√©riode de d√©tention d‚Äôun jour.\n\n2.4.1.1 Impl√©mentation en R\n\n# D√©finition des √©carts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html",
    "href": "3A/value-at-risk/var_garch.html",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 et du TP-2 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap (TP1). Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, nous avons utilis√© des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold (TP2). Jusqu‚Äô√† maintenant, on consid√©rait que la s√©rie est iid. Cependant, dans la r√©alit√©, les s√©ries financi√®res sont souvent caract√©ris√©es par une d√©pendance temporelle et une volatilit√© conditionnelle.\nDans le cadre du TP3, il s‚Äôagira de prendre en compte la d√©pendance temporelle et la volatilit√© conditionnelle dans les s√©ries temporelles financi√®res. Pour ce faire, nous utiliserons un mod√®le de VAR dynamique avec le mod√®le GARCH.\nLe mod√®le GARCH (Generalized Autoregressive Conditional Heteroskedasticity) est un mod√®le de volatilit√© conditionnelle qui permet de mod√©liser la volatilit√© des rendements financiers. Il a √©t√© introduit par Bollerslev en 1986. Le mod√®le GARCH est une extension du mod√®le ARCH (Autoregressive Conditional Heteroskedasticity) introduit par Engle en 1982. Le mod√®le GARCH est d√©fini par les √©quations suivantes:\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\sum \\alpha_i \\epsilon_{t-i}^2 + \\sum \\beta_i \\sigma_{t-i}^2\n\\]\nDans ce mod√®le \\(\\mu_t\\) est un param√®tre de tendance moyenne √† identifier, \\(\\epsilon_t\\) est le r√©sidu, \\(\\sigma_t^2\\) est la variance conditionnelle, \\(z_t\\) est un bruit blanc, \\(\\omega\\) est un param√®tre de constante, \\(\\alpha_i\\) et \\(\\beta_i\\) sont les param√®tres du mod√®le GARCH √† identifier.\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "href": "3A/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "I. Impl√©mentation de la VaR dynamique",
    "text": "I. Impl√©mentation de la VaR dynamique\n\nI.1. Pertinence du mod√®le AR(1)-GARCH(1,1)\nLe mod√®le AR(1)-GARCH(1,1) est le mod√®le qui, en pratique, est utilis√© pour r√©aliser la VaR dynamique. Cependant, il n‚Äôest pas tout le temps adapt√© aux donn√©es financi√®res. Dans ce TP, nous allons commencer par tester l‚Äô√©ligibilit√© de ce mod√®le dans le cadre des donn√©es que nous poss√©dons.\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, label='Train')\nplt.title('CAC 40 Log Returns')\nplt.show()\n\n\n\n\n\n\n\n\n\n## ACF et PACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(data_train, ax=plt.gca(), lags=40)\nplt.subplot(222)\nplot_pacf(data_train, ax=plt.gca(), lags=40)\nplt.show()\n\n\n\n\n\n\n\n\nDans la s√©rie temporelle que nous poss√©dons, nous constatons que la s√©rie peut √™tre mod√©liser par un AR(1). Pour un test plus rigoureux de cette hypoth√®se, nous allons utiliser la m√©thode de Lljung Box afin de d√©terminer le meilleur mod√®le qui puisse mod√©liser la s√©rie. Ainsi, pour un ordre pmax = 2 et qmax=2, nous allons : 1. Estimer les param√®tres du mod√®le ARMA(p,q) pour chaque combinaison de p et q 2. Calculer la statistique de Ljung Box pour chaque combinaison de p et q afin d‚Äôexaminer si les r√©sidus d‚Äôun mod√®le sont du bruit blanc 3. Filtrer les mod√®les pour lesquels les r√©sidus sont du bruit blanc 4. Choisir le meilleur mod√®le en utilisant le crit√®re d‚ÄôAkaike\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Param√®tres du mod√®le\np_max = 2\nq_max = 2\nbest_aic = np.inf\nbest_order = (0, 0, 0)\n\n# Chargement de la s√©rie temporelle (remplacer par la vraie s√©rie data_unindex)\n# Exemple fictif avec des donn√©es al√©atoires\nnp.random.seed(42)\ndata_unindex = data_train.copy()\ndata_unindex.reset_index(drop=True, inplace=True)\n\n# Cr√©ation de la matrice pour stocker les AIC\naic_matrix = pd.DataFrame(np.nan, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\nbb_test = pd.DataFrame(0, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\n# Boucle pour estimer les mod√®les et stocker les AIC\nfor p in range(p_max + 1):\n    for q in range(q_max + 1):\n        try:\n            model = ARIMA(data_unindex, order=(p, 0, q))\n            out = model.fit()\n            aic_matrix.loc[f\"p={p}\", f\"q={q}\"] = out.aic  # Stockage de l'AIC\n            \n            # Test de la blancheur des r√©sidus\n            ljung_box_result = acorr_ljungbox(out.resid, lags=[1], return_df=True)\n            p_value = ljung_box_result['lb_pvalue'].iloc[0]\n\n            if p_value &gt; 0.05:\n                bb_test.loc[f\"p={p}\", f\"q={q}\"] = 1\n            \n            # Mise √† jour du meilleur mod√®le\n            if out.aic &lt; best_aic :\n                best_aic = out.aic\n                best_order = (p, 0, q)\n                \n        except Exception as e:\n            print(f\"Erreur avec (p={p}, q={q}): {e}\")\n\nprint(f\"Meilleur mod√®le ARIMA: {best_order} avec AIC={best_aic}\")\n\nprint(\"=\"*30)\nprint(\"Matrice des AIC:\")\nprint(aic_matrix)\nprint(\"=\"*30)\nprint(\"Matrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\")\nprint(bb_test)\n\nMeilleur mod√®le ARIMA: (0, 0, 0) avec AIC=-20100.176479566246\n==============================\nMatrice des AIC:\n              q=0           q=1           q=2\np=0 -20100.176480 -20098.205891 -20097.679059\np=1 -20098.227385 -20099.862840 -20097.046957\np=2 -20097.887027 -20098.545030 -20094.033191\n==============================\nMatrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\n     q=0  q=1  q=2\np=0    1    1    1\np=1    1    1    1\np=2    1    1    1\n\n\n\np = 1\nq = 0\n\nAR1 = ARIMA(data_unindex, order=(p, 0, q))\nprint(AR1.fit().summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nEn utilisant la m√©thode √©nonc√©e plus haut, nous constatons que le mod√®le ARMA(0,0) est le meilleur mod√®le. En effet, c‚Äôest le mod√®le avec le crit√®re d‚ÄôAkaike le plus faible. Cela porte √† croire que la tendance moyenne de la s√©rie est constante. Nous allons tout de m√™me utiliser un mod√®le AR(1) pour la mod√©liser. En effet, c‚Äôest le deuxi√®me mod√®le avec un AIC faible.\nDans la s√©rie des r√©sidus, nous constatons des clusters de volatilit√© ce qui est signe d‚Äôune volatilit√© conditionnelle, et donc de la pr√©sence d‚Äôun GARCH. De plus, dans la s√©rie des r√©sidus du log-rendement, nous constatons une faible autocorr√©lation, ce qui les fait ressembler √† du bruit blanc. Toutefois, lorsque l‚Äôon examine ces r√©sidus au carr√©, la s√©rie temporelle pr√©sente g√©n√©ralement une forte autocorr√©lation, mise en √©vidence par la pr√©sence de grappes de volatilit√©. Cela sugg√®re que les rendements repr√©sentent un processus h√©t√©rosc√©dastique, ce qui rend le mod√®le GARCH particuli√®rement pertinent dans le cadre de notre √©tude.\n\nAR1_resid = AR1.fit().resid\nplt.figure(figsize=(10, 5))\nplt.plot(AR1_resid)\nplt.title(\"R√©sidus du mod√®le AR(1)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(AR1_resid, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1)\")\nplt.subplot(222)\nplot_acf(AR1_resid**2, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1) au carr√©\")\nplt.show()\n\n\n\n\n\n\n\n\nMotiv√©s par les commentaires de (Franke, H√§rdle et Hafner 2004) sugg√©rant que, dans les applications pratiques, les mod√®les GARCH avec des ordres plus petits d√©crivent souvent suffisamment les donn√©es et que dans la plupart des cas GARCH(1,1) est ad√©quat, nous avons consid√©r√© quatre combinaisons diff√©rentes de p=0, 1 et q=1, 2 pour chaque p√©riode afin d‚Äôentra√Æner le mod√®le GARCH, en supposant que les r√©sidus standardis√©s suivent une distribution normale.\n\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\n\ndef find_garch(p_min, p_max, q_min, q_max, data, dist=\"normal\"):\n    \"\"\"\n    Trouve le meilleur mod√®le GARCH(p, q) en minimisant l'AIC.\n\n    Param√®tres :\n    - p_min, p_max : Bornes pour p (ordre de l'AR dans la variance)\n    - q_min, q_max : Bornes pour q (ordre de MA dans la variance)\n    - data : S√©rie temporelle utilis√©e pour l'estimation\n    - dist : Distribution des erreurs (\"normal\", \"t\", \"ged\", etc.)\n\n    Retour :\n    - DataFrame contenant les valeurs de AIC pour chaque combinaison (p, q)\n    - Meilleur mod√®le GARCH trouv√© en fonction du crit√®re AIC\n    \"\"\"\n    \n    best_aic = np.inf\n    best_order = (0, 0, 0)\n    \n    results = []\n\n    for p in range(p_min, p_max + 1):\n        for q in range(q_min, q_max + 1):\n            try:\n                # Sp√©cification du mod√®le GARCH(p, q)\n                garch_spec = arch_model(data, vol='Garch', p=p, q=q, mean='Zero', dist=dist)\n                out = garch_spec.fit(disp=\"off\")\n                \n                # Calcul de l'AIC\n                current_aic = out.aic * len(data)\n\n                # Mettre √† jour le meilleur mod√®le si un plus petit AIC est trouv√©\n                if current_aic &lt; best_aic:\n                    best_aic = current_aic\n                    best_order = (p, 0, q)\n                \n                # Ajouter les r√©sultats dans la liste\n                results.append({'p': p, 'q': q, 'aic': current_aic, 'relative_gap': np.nan})\n            \n            except Exception as e:\n                print(f\"Erreur pour (p={p}, q={q}): {e}\")\n                continue\n    \n    # Convertir en DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Calculer l'√©cart relatif par rapport au meilleur AIC\n    results_df['relative_gap'] = (results_df['aic'] - best_aic) * 100 / best_aic\n    \n    return results_df, best_order\n\nresults_df, best_garch_order = find_garch(p_min=1, p_max=2, q_min=0, q_max=2, data=data_unindex, dist=\"normal\")\n\nprint(f\"Meilleur mod√®le GARCH: {best_garch_order} avec AIC={best_aic}\")\nprint(\"=\"*30)\nprint(\"R√©sultats pour les mod√®les test√©s:\")\nresults_df.sort_values(by='relative_gap', ascending=False)\n\nMeilleur mod√®le GARCH: (1, 0, 1) avec AIC=-20100.176479566246\n==============================\nR√©sultats pour les mod√®les test√©s:\n\n\n\n\n\n\n\n\n\np\nq\naic\nrelative_gap\n\n\n\n\n1\n1\n1\n-7.493455e+07\n-0.000000\n\n\n4\n2\n1\n-7.491663e+07\n-0.023918\n\n\n2\n1\n2\n-7.486519e+07\n-0.092567\n\n\n5\n2\n2\n-7.483014e+07\n-0.139333\n\n\n3\n2\n0\n-7.189514e+07\n-4.056091\n\n\n0\n1\n0\n-7.073946e+07\n-5.598342\n\n\n\n\n\n\n\nEn utilisant le crit√®re AIC pour s√©lectionner le meilleur mod√®le, nous avons conclu que GARCH(1,1) est effectivement le meilleur mod√®le.\n\ngarch11 = arch_model(data_unindex, vol='Garch', p=1, q=1, mean='Zero', dist='normal')\nprint(\"=\"*78)\nprint(\"R√©sum√© du mod√®le GARCH(1,1)\")\nprint(\"=\"*78)\nprint(garch11.fit(disp=\"off\").summary())\n\n==============================================================================\nR√©sum√© du mod√®le GARCH(1,1)\n==============================================================================\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\n\n\n\ncond_resid =garch11.fit(disp=\"off\").conditional_volatility # Volatilit√© conditionnelle =&gt; sigma_t\nresid = garch11.fit(disp=\"off\").resid # r√©sidus du mod√®le =&gt; eps_t\nresid_std = garch11.fit(disp=\"off\").std_resid  # r√©sidus studentis√©s =&gt; eta_t\n\n# jarque bera test\n\nfrom scipy.stats import jarque_bera\n\njb_test = jarque_bera(resid_std)\nprint(\"H0: Les r√©sidus studentis√©s suivent une loi normale\")\nprint(f\"Test de Jarque-Bera sur les r√©sidus studentis√©s: JB={jb_test[0]}, p-value={jb_test[1]}\")\n# reject the null hypothesis of normality for the distribution of the residuals, \n# as a rule of thumb, which implies that the data to be fitted is not\n# normally distributed\n\nH0: Les r√©sidus studentis√©s suivent une loi normale\nTest de Jarque-Bera sur les r√©sidus studentis√©s: JB=848.8557767883675, p-value=4.71313744144075e-185\n\n\n\n### y revenir\n\n### coeff &lt;1\n\n\n# Test d'homosc√©dasticit√©\n# Ljung-Box test sur r√©sidus\nlb_test_resid = acorr_ljungbox(resid_std, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur r√©sidus:\\n\", lb_test_resid)\n\n# Ljung-Box test sur carr√©s des r√©sidus\nlb_test_resid_sq = acorr_ljungbox(resid_std**2, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur carr√©s des r√©sidus:\\n\", lb_test_resid_sq)\n\nLjung-Box Test sur r√©sidus:\n      lb_stat  lb_pvalue\n1   0.028087   0.866904\n2   0.572026   0.751253\n3   0.690100   0.875530\n4   1.235029   0.872298\n5   2.199461   0.820914\n6   2.491801   0.869384\n7   2.828137   0.900433\n8   2.941010   0.938005\n9   3.793237   0.924486\n10  4.644433   0.913631\n11  4.727144   0.943657\n12  6.763448   0.872842\nLjung-Box Test sur carr√©s des r√©sidus:\n       lb_stat  lb_pvalue\n1    0.280711   0.596235\n2    0.339634   0.843819\n3    6.670837   0.083163\n4    7.395445   0.116409\n5    8.091586   0.151260\n6    8.233789   0.221471\n7    8.724987   0.273009\n8    9.386238   0.310768\n9    9.938908   0.355454\n10  11.579309   0.314198\n11  13.394501   0.268325\n12  13.845698   0.310670\n\n\n\n# LM test pour les effets ARCH\nfrom statsmodels.stats.diagnostic import het_arch\n\nlm_test = het_arch(resid_std)\nprint('LM Test Statistique: %.3f, p-value: %.3f' % (lm_test[0], lm_test[1]))\n\nLM Test Statistique: 12.218, p-value: 0.271\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(resid_std, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus studentis√©s\")\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\nplt.subplot(222)\nplot_pacf(resid_std, lags=40, ax=plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\nLe mod√®le AR(1)-GARCH(1,1) estim√© est le suivant :\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\no√π \\(\\mu_t = 0.0001 - 0.0037 r_{t-1}\\)\n\\[\n\\epsilon_t = \\sigma_t \\eta_t\n\\]\n\\[\n\\sigma_t^2 = 3.89 \\times 10^{-6} + 0.10 \\times \\epsilon_{t-i}^2 + 0.88 \\times \\sigma_{t-i}^2\n\\]\navec \\(\\eta_t\\) un bruit blanc suppos√©e gaussien.\nDans ce cas, nous rencontrons des probl√®mes au niveau de la significativit√© du coefficient AR(1). En effet, il aurait √©t√© plus judicieux de ne pas mod√©liser la tendance moyenne du rendement et la supposer constante. De plus, au niveau du GARCH(1,1), les r√©sidus sont bien des bruits blancs homosc√©dastiques (test de lljung box et test LM). Cependant, nous avons suppos√© que \\(\\eta_t\\) est un bruit blanc gaussien. Cela n‚Äôest pas v√©rifi√©. Il aurait √©t√© judicieux de tester d‚Äôautres distributions telles que Students‚Äôs t (‚Äôt‚Äô, ‚Äòstudentst‚Äô), Skewed Student‚Äôs t (‚Äòskewstudent‚Äô, ‚Äòskewt‚Äô) ou encore Generalized Error Distribution (GED).\n**Test de Lagrange Multiplier (LM) pour l'effet ARCH**\n\nLe test de Lagrange Multiplier (LM) pour l'effet ARCH est un outil statistique qui v√©rifie la pr√©sence d'effets ARCH (AutoRegressive Conditional Heteroskedasticity) dans une s√©rie temporelle.\n\nL'effet ARCH se manifeste lorsque la variance d'une erreur est une fonction de ses erreurs pass√©es. Cette propri√©t√© est courante dans les s√©ries temporelles financi√®res, o√π de grandes variations des rendements sont souvent suivies par de grandes variations et vice versa.\n\nLe test de LM v√©rifie l'hypoth√®se nulle que les erreurs sont homosc√©dastiques (variance constante). Si la p-value du test est inf√©rieure √† un seuil pr√©d√©fini (g√©n√©ralement 0,05), l'hypoth√®se nulle est rejet√©e, indiquant la pr√©sence d'effets ARCH.\n\n# Cr√©ation de la figure avec des sous-graphiques align√©s verticalement\nplt.figure(figsize=(10, 12))\n\n# Premier graphique : CAC 40\nplt.subplot(311)\nplt.plot(resid) \nplt.title(\"R√©sidus du mod√®le AR(1)\")\n\n# Deuxi√®me graphique : R√©sidus du mod√®le AR(1)\nplt.subplot(312)\nplt.plot(cond_resid)\nplt.title(\"Volatile conditionnelle du mod√®le GARCH(1,1)\")\n\n# Troisi√®me graphique : R√©sidus studentis√©s du mod√®le GARCH(1,1)\nplt.subplot(313)\nplt.plot(resid_std, label='R√©sidus studentis√©s du mod√®le GARCH(1,1)')\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\n\n# Affichage des graphiques\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2. Dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\)\n\\[\nr_t = \\mu_t + \\sigma_t \\times \\epsilon_t\n\\quad\n\\text{avec} \\quad\n\\begin{cases}\n    \\mu_t = \\mu + \\varphi r_{t-1} \\\\\n    \\sigma_t^2 = \\omega + a (r_{t-1} - \\mu_{t-1})^2 + b \\sigma_{t-1}^2\n\\end{cases}\n\\]\nPour avoir la dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\), nous allons utiliser les donn√©es historiques de la s√©rie temporelle ainsi que les estimations des param√®tres \\(\\Theta = (\\mu, \\varphi, \\omega, a, b)\\) du mod√®le AR(1)-GARCH(1,1) que nous avons estim√© pr√©c√©demment par maximum de vraisemblance.\nPour \\(t=1\\), nous allons initialiser \\(\\mu_1\\) par la moyenne \\(\\hat{\\mu}\\) et \\(\\sigma_1\\) par la variance √† long terme \\(\\frac{\\omega}{1 - a - b}\\).\n\nprint(AR1.fit().summary())\n\n# tester arima avec arch_model ou arch\nmu = AR1.fit().params[0]\nprint(f\"Param√®tre mu: {mu}\")\nphi = AR1.fit().params[1]\nprint(f\"Param√®tre phi: {phi}\")\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nParam√®tre mu: 0.00014959052741773347\nParam√®tre phi: -0.003743634042716415\n\n\n\nprint(garch11.fit(disp=\"off\").summary())\nomega = garch11.fit(disp=\"off\").params[0]\nprint(f\"Param√®tre omega: {omega}\")\na = garch11.fit(disp=\"off\").params[1]\nprint(f\"Param√®tre alpha: {a}\")\nb = garch11.fit(disp=\"off\").params[2]\nprint(f\"Param√®tre beta: {b}\")\n\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\nParam√®tre omega: 3.892997741815931e-06\nParam√®tre alpha: 0.1\nParam√®tre beta: 0.88\n\n\n\nT_train = len(data_train)\nT_test = len(data_test)\n\nT = T_train + T_test\n\n# Initialisation des s√©ries\nr = pd.concat([data_train, data_test], axis=0)\nmu_t = np.zeros(T)    # Composante moyenne\nsigma2 = np.zeros(T)  # Variance conditionnelle\n\n# Conditions initiales\nmu_t[0] = mu\nsigma2[0] = omega / (1 - a - b)  # Variance de long terme\n\n# Simulation du mod√®le\nfor t in range(1, T):\n    mu_t[t] = mu + phi * r[t-1]  # Partie moyenne\n    sigma2[t] = omega + a * (r[t-1] - mu_t[t-1])**2 + b * sigma2[t-1]  # Variance conditionnelle\n\n# Affichage des r√©sultats\nfig, ax = plt.subplots(3, 1, figsize=(10, 12))\n\nax[0].plot(r, color=\"blue\")\nax[0].set_title(\"Rendements $r_t$\")\nax[0].legend()\n\nax[1].plot(mu_t, color=\"green\")\nax[1].set_title(\"Composante moyenne $\\mu_t$\")\nax[1].legend()\n\nax[2].plot(np.sqrt(sigma2), color=\"red\")\nax[2].set_title(\"Volatilit√© conditionnelle $\\sigma_t$\")\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEn analysant la dynamique de \\(\\mu_t\\), nous constatons que la tendance moyenne est tr√®s semblable √† la s√©rie des log-rendements. Cela est d√ª au fait que le mod√®le AR(1) n‚Äôest pas pertinent pour mod√©liser la s√©rie. En effet, la s√©rie des log-rendements ressemble d√©j√† √† un bruit blanc. Par ailleurs, nous observons de fortes p√©riodes de volatilit√© dans la s√©rie des log-rendements pendant les p√©riodes de crises, i.e.¬†2008-2009 qui correspond √† la crise des subprimes et 2020 qui correspond √† la crise du Covid-19. Le mod√®le GARCH semble bien capturer ces p√©riodes de volatilit√© dans la volatilit√© conditionnelle calibr√©e.\n\n\nI.3. Estimation de la VaR\n\n# VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1 - alpha))\n\n# VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n# Loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n    \n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n    \n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(resid_std)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0], \n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n    \n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\n#### A FAIRE VAR POT et BM\n\nfrom scipy.stats import genextreme as gev\n\nimport numpy as np\nimport pandas as pd\nneg_resid = -resid_std\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nextremes = get_extremes(neg_resid, block_size=21, min_last_block=0.6)\nparams_gev = gev.fit(extremes)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de GEV sont : \")\nprint(\"-\"*15)\nprint(f\"Shape (xi) = {params_gev[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gev[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gev[2]:.2f}\")\nprint(\"=\"*80)\n\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\n\nu = 0.03\nexcess_values = [value - u for value in neg_resid if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*80)\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\nprint(\"=\"*80)\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.42506987856855155\nSigma :  0.8686238872541445\nGamma :  -0.6074089740677895\nNu :  5.607559653340765\n================================================================================\n================================================================================\nLes param√®tres estim√©s de la loi de GEV sont : \n---------------\nShape (xi) = -0.01\nLocalisation (mu) = 1.64\nEchelle (sigma) = 0.72\n================================================================================\n================================================================================\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = -0.04\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.80\n================================================================================\n\n\n\nalpha = 0.99\n\nvar_hist_train = historical_var(resid_std, alpha=alpha)\nvar_gauss_train = gaussian_var(resid_std, alpha=alpha)\nvar_sstd_train = sstd_var(alpha, params_sstd)\nvar_BM_train,_ = BM_var(0.99, 21, *params_gev)\nvar_POT_train,_ = POT_var(neg_resid, alpha, u,*params_gpd)\n\n# in a df\nvar = pd.DataFrame({\n    'Historique': [var_hist_train],\n    'Gaussienne': [var_gauss_train],\n    'Skew Student': [var_sstd_train],\n    'Block Maxima': [var_BM_train],\n    'Peak Over Threshold': [var_POT_train]\n})\n\nprint(\"=\"*80)\nprint(\"Value at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\")\nprint(round(100*var,2))\nprint(\"=\"*80)\n\n================================================================================\nValue at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\n   Historique  Gaussienne  Skew Student  Block Maxima  Peak Over Threshold\n0      264.12      229.76        280.29        268.68               284.98\n================================================================================\n\n\n\na. VaR historique dynamique\n\nvar_t = np.zeros(T_test)    # Composante moyenne\nnb_exp = 0\nfor t in range(T_test):\n    var_t[t] = - (mu_t[t+T_train] + np.sqrt(sigma2[t+T_train])*var_hist_train)\n    nb_exp += (r[t+T_train] &lt; var_t[t]).astype(int)\n    \nvar_t = pd.Series(var_t, index=data_test.index)\nprint(f\"Nombre d'exceptions = {nb_exp} sur {T_test} jours\")\n\nNombre d'exceptions = 4 sur 586 jours\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"blue\", label='Train')\nplt.plot(data_test, color=\"orange\", label='Test')\nplt.plot(var_t, color=\"red\",label='VaR dynamique')\nplt.axvline(x=data_test.index[0], color='black', linestyle='--')\nplt.legend()\nplt.title('S√©rie des log-rendements et VaR dynamique')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_test, color=\"orange\")\nplt.plot(var_t, color=\"red\")\nplt.title('Zoom sur la VaR dynamique')\n\nText(0.5, 1.0, 'Zoom sur la VaR dynamique')\n\n\n\n\n\n\n\n\n\n\n# backtest √† faire (optionnel)"
  },
  {
    "objectID": "autres/ISR.html",
    "href": "autres/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL‚ÄôISR pr√©sente de multiples avantages tant pour les investisseurs que pour les entreprises engag√©es dans cette d√©marche. Ces avantages refl√®tent l‚Äô√©volution des attentes soci√©tales et la reconnaissance croissante de l‚Äôimportance de la durabilit√© et de l‚Äô√©thique dans le monde des affaires. J‚Äôen ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement am√©liorer la r√©putation d‚Äôune entreprise. Elle t√©moigne de son engagement envers des pratiques durables et √©thiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financi√®re : De nombreuses √©tudes d√©montrent que les entreprises avec une notation ESG √©lev√©e tendent de meilleures performances financi√®rement sur le long terme. Cela s‚Äôexplique par une meilleure anticipation des risques, une gestion plus efficace et une capacit√© √† saisir les opportunit√©s de march√© li√©es √† la durabilit√©.\nMeilleure gestion des risques : L‚Äôadoption de pratiques ESG solides permet aux entreprises de mieux identifier et g√©rer les risques, qu‚Äôils soient climatiques, sociaux ou de march√©.\nMeilleure attractivit√© pour les investisseurs : En d√©montrant un engagement clair envers la durabilit√© et l‚Äô√©thique, les entreprises attirent davantage d‚Äôinvestisseurs conscients de l‚Äôimportance des crit√®res ESG. Cette attractivit√© accrue peut se traduire par un acc√®s facilit√© au capital et √† de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution √† la r√©duction de certains risques financiers : En investissant dans des entreprise int√©grant les crit√®res ESG dans leur processus de d√©cision, les investisseurs contribuent indirectement √† une meilleure identification et anticipation les risques li√©s au changement climatique, aux probl√©matiques sociales, et aux d√©fis de gouvernance, ce qui contribue √† une meilleure protection de leur capital sur le long terme.\nImpact positif sur la soci√©t√© : L‚ÄôISR permet aux investisseurs de contribuer activement √† une √©conomie plus durable et √©quitable. En choisissant d‚Äôinvestir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des mod√®les √©conomiques respectueux de l‚Äôenvironnement et du bien-√™tre social.\n\nEn somme, l‚ÄôISR offre une perspective d‚Äôinvestissement qui va au-del√† des retours financiers imm√©diats pour embrasser des b√©n√©fices √† long terme, tant sur le plan √©conomique que social et environnemental."
  },
  {
    "objectID": "autres/ISR.html#comment-investir",
    "href": "autres/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire √† un fonds ISR ?\nSouscrire √† un fonds1 ISR (Investissement Socialement Responsable) est une d√©marche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la soci√©t√© et l‚Äôenvironnement[@comment]. En consultant son conseiller financier ou son √©tablissement bancaire, il est possible de placer son argent dans une vari√©t√© de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d‚Äô√âpargne en Actions (PEA) : (sous r√©server de s‚Äôassurer que le fonds ISR choisi est bien √©ligible au PEA) Offre la possibilit√© de placer son √©pargne en actions de soci√©t√©s europ√©ennes.\nLes compte-titres ordinaires(CTO) : permet d‚Äôinvestir en bourse sur les march√©s financiers fran√ßais et/ou √©trangers et dans tout type de valeurs mobili√®res (OPC2, actions, obligations, mon√©taire, warrants, trackers‚Ä¶).\nL‚Äô√©pargne salariale ou les plans d‚Äô√©pargne d‚Äôentreprise (PEE) : un produit d‚Äô√©pargne collectif qui permet aux salari√©s d‚Äôune entreprise de se constituer un portefeuille de valeurs mobili√®res qui peuvent proposer des fonds ISR.\nEnfin, certains produits d‚Äô√©pargne retraite individuelle, comme le Plan d‚ÄôEpargne Retraite (PER).\n\nCes v√©hicules d‚Äôinvestissement permettent aux particuliers de contribuer √† une √©conomie plus durable tout en recherchant une performance financi√®re. Il est recommand√© de se rapprocher d‚Äôun conseiller pour d√©terminer le produit le mieux adapt√© √† ses objectifs financiers et √† ses valeurs √©thiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) n√©cessite une approche combinant analyses personnelle, financi√®re et extra-financi√®re, cette derni√®re se concentrant sur les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance)[@comment2021].\nPour choisir efficacement une entreprise ISR, il est crucial de r√©aliser une analyse √† triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR¬†qui vous convient le mieux. Elles vous aideront √† identifier le type de placement √† privil√©gier et d√©finir par exemple des fonds th√©matiques, d‚Äôexclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financi√®re : Elle permet d‚Äô√©valuer la performance √©conomique de l‚Äôentreprise, sa sant√© financi√®re, sa capacit√© √† g√©n√©rer des profits et √† maintenir une croissance durable. Cette analyse est indispensable pour s‚Äôassurer que l‚Äôentreprise est non seulement responsable, mais aussi viable et performante √† long terme.\nAnalyse extra-financi√®re (ESG) : Cette analyse compl√®te l‚Äô√©valuation financi√®re en examinant comment l‚Äôentreprise aborde les d√©fis et saisit les opportunit√©s li√©es aux crit√®res environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds int√©grant les crit√®res ESG comme les fonds labellis√©s ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du minist√®re de l‚Äô√©conomie et des finances fran√ßais qui garantit que le fonds d‚Äôinvestissement respecte des crit√®res ESG stricts4 dans ses choix d‚Äôinvestissement. Il assure √©galement que le fonds investit dans des entreprises qui adh√®rent √† ces principes, offrant ainsi une couche suppl√©mentaire de confiance pour les investisseurs soucieux de l‚Äôimpact de leurs placements.\nCe label est attribu√© aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,class√©es en 6 cat√©gories, qui constituent les 6 piliers du r√©f√©rentiel [@crit√®resa]."
  },
  {
    "objectID": "autres/ISR.html#footnotes",
    "href": "autres/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appel√© fonds de placement. Il s‚Äôagit d‚Äôune soci√©t√© d‚Äôordre public ou priv√© qui investit du capital pour soutenir des projets souvent innovants.‚Ü©Ô∏é\norganismes de placement collectif‚Ü©Ô∏é\nLes fonds d‚Äôinvestissement d‚Äôexclusion normatifs font r√©f√©rence aux fonds faisant l‚Äôobjet de plusieurs controverses.‚Ü©Ô∏é\ncf liste des fonds labellis√©s [@listede].‚Ü©Ô∏é"
  },
  {
    "objectID": "autres/spearman-pearson.html",
    "href": "autres/spearman-pearson.html",
    "title": "Corr√©lation de Spearman vs corr√©lation de Pearson",
    "section": "",
    "text": "Corr√©lation de Spearman vs corr√©lation de Pearson\nLa corr√©lation de Spearman est une mesure de corr√©lation non param√©trique qui permet de mesurer la relation monotone entre deux variables. Elle est souvent utilis√©e pour mesurer la d√©pendance entre les variables al√©atoires. La corr√©lation de Spearman est bas√©e sur les rangs des observations et est moins sensible aux valeurs extr√™mes que la corr√©lation de Pearson. Elle est donc plus robuste et plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires.\nLa corr√©lation de Pearson, quant √† elle, ne permet de comparer les d√©pendances lin√©aires des variables. De plus, elle ne permet de comparer les corr√©lation que lorsque les variables al√©atoires sont normales. En effet, soit (\\(X_1,X_2,X_3\\)), si \\(\\rho(X_1,X_2) &gt; \\rho(X_1,X_3)\\), cela ne veut dire que la corr√©lation entre \\(X_1\\) et \\(X_2\\) est plus forte que celle entre \\(X_1\\) et \\(X_3\\) que si ces variables sont gaussiennes.\nNous verrons dans la suite de ce document que la corr√©lation de Spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires, car elle est une mesure de concordance. De ce fait, elle est d√©finit par une copule. Il n‚Äôen est pas ainsi pour la corr√©lation de Pearson.\n\nQu‚Äôest ce qu‚Äôune mesure de concordance ?\nUne mesure de concordance est une mesure qui permet de quantifier la relation entre deux variables al√©atoires. Cinq propri√©t√©s sont g√©n√©ralement attribu√©es √† une mesure de concordance :\n\nSym√©trie : la mesure de concordance entre X et Y est la m√™me que celle entre Y et X.\nNormalisation : la mesure de concordance est comprise entre -1 et 1.\n\\(\\delta(X_1, X_2) = 1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(U) \\right)\\)\n\\(\\delta(X_1, X_2) = -1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(1-U) \\right)\\)\n\\(\\delta(f(X_1), X_2) = \\delta(X_1, X_2)\\) si \\(f\\) est croissante; \\(\\delta(f(X_1), X_2) = -\\delta(X_1, X_2)\\) si \\(f\\) est d√©croissante.\n\nEn raison des propri√©t√©s 3 et 4, la corr√©lation de pearson n‚Äôest pas une mesure de concordance lorsque les variables al√©atoires ne sont pas gaussiennes. En effet, la corr√©lation de pearson ne v√©rifie pas la propri√©t√© 3. C‚Äôest pourquoi, la corr√©lation de spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires. Une mesure encore plus appropri√©e est la copule. La corr√©lation de spearman, quant √† elle, est une mesure de concordance.\n\n\nExemple :\nSoit un vecteur gaussien X = (\\(X_1, X_2, X_3\\)) suivant une loi N(0,\\(\\Sigma\\)) avec :\n\\[\n\\Sigma = \\begin{pmatrix}\n1 & 0.4 & 0.2 \\\\\n0.4 & 1 & -0.8 \\\\\n0.2 & -0.8 & 1\n\\end{pmatrix}\n\\]\n\n\nShow the code\nm &lt;- 3\nn &lt;- 2000\nsigma &lt;- matrix(c(1,0.4,0.2,0.4,1,-0.8,0.2,-0.8,1), nrow=3, ncol=3)\nX &lt;- mvrnorm(n,mu=rep(0,m),Sigma=sigma)\npairs.panels(X,method=\"spearman\")\n\n\n\n\n\n\n\n\n\nOn cr√©e ensuite un vecteur Z constitu√© des fonctions de r√©partition des √©l√©ments de X. On constate que la corr√©lation de Spearman entre les √©l√©ments de Z ne change pas. Cel√† s‚Äôexplique par le fait que nous appliquons une fonction croissante √† chaque √©l√©ment de X. Cependant, la distribution de Z est diff√©rente de celle de X. En effet, les composantes de Z suivent une loi uniforme \\(\\mathcal{U}(0,1)\\).\n\n\nShow the code\nZ &lt;- pnorm(X, 0,1)\npar(mfrow=c(1,2))\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\n\n\nShow the code\npairs.panels(Z,method=\"pearson\")\n\n\n\n\n\n\n\n\n\nA partir de Z, nous construisons un vecteur W tel que les composantes de W suivent des lois marginales diff√©rentes, resp \\(\\beta(0,1), \\gamma(2,1), \\beta(2,1)\\). La corr√©lation de Spearman entre les √©l√©ments de W ne change pas bien que la distribution des marginales ait chang√©, puisque nous appliquons une fonction croissante √† chaque √©l√©ment de Z.\n\n\nShow the code\nw1 &lt;- qbeta(Z[,1], 2, 1)\nw2 &lt;- qgamma(Z[,2], 2, 1)\nw3 &lt;- qbeta(Z[,3], 2,1)\nW &lt;- cbind(w1,w2,w3)\n\npar(mfrow=c(1,2))\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\n\n\nShow the code\npairs.panels(W,method=\"pearson\")\n\n\n\n\n\n\n\n\n\nOn en conclut que la structure de d√©pendance d‚Äôun vecteur de variables al√©atoire peut √™tre isol√©e, caract√©ris√©ee, et mod√©lis√©e ind√©pendamment des lois marginales/distributions univari√©es des composantes du vecteur al√©atoire. Le concept de copule permet de mod√©liser cette structure de d√©pendance.\nSi l‚Äôon utilise la correlation de Pearson, on constate que la corr√©lation entre les √©l√©ments de W et Z change. En effet, la corr√©lation de Pearson est une mesure de corr√©lation lin√©aire et ne permet pas de comparer, xdans tous les cas, les d√©pendances entre les variables al√©atoires.\n\n\nShow the code\npairs.panels(X,method=\"pearson\")\n\n\n\n\n\n\n\n\n\nShow the code\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\n\n\nShow the code\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Gestion des risques",
    "section": "",
    "text": "Note\n\n\n\nCompte tenu de ma sp√©cialisation en gestion des risques, les contenus que je pr√©vois de partager dans cette section d√©di√©e √† la 3√®me ann√©e (3A) porteront principalement sur les enseignements sp√©cifiques √† cette sp√©cialisation. Je m‚Äôefforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en esp√©rant qu‚Äôils serviront de guide pr√©cieux pour ceux qui suivront une voie similaire.\n\n\n\n\n\nConstruction du bilan d‚Äôentreprise\nReglementation prudentielle TO REWRITE\n\n\n\n\n\nD√©finition du risque financier\nValue-at-risk (VaR) :\n\nD√©finition de la VaR\nImpl√©mentation de la VaR sur python\nTP1 : M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\nTP2 : M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\nTP3 : M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\nEstimation de la CreditVaR √† l‚Äôaide de la th√©orie des copules\n\n\n\n\n\n\nCalibration avec le mod√®le de black-scholes\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre de Kalman\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre particulaire bootstrap\nMod√®le de Heston :\n\nEstimation de la volatilit√© avec le filtre boostrap\nComparaison du filtre bootstrap au filtre particulaire auxiliaire (APF)\n\nCourbes de taux (Construction d‚Äôune courbe ZC, Mod√®le Hull - White)\n\n\n\n\n\nBond, Forward, Future, Swaps : Qu‚Äôest ce que c‚Äôest?\n\nAsset management\n\nRisque de march√©\nRisque de liquidit√©\nRisque de valorisation, de cr√©dit etc.\n\nAsset pricing\n\nPricing d‚Äôoptions vanilles TO DO\nPricing de taux, swap, d‚Äôobligations TO DO\nTracking error TO DO\nConstruction de portefeuille markowitz TO DO\n\n\n\n\n\nInvestissement socialement responsable\nFinance durable\nCorr√©lation de spearman vs Corr√©lation de pearson\n\n\n\n\nProjets\n\nProjet de s√©ries temporelles\nProjet de scoring TO DO\nProjet de th√©orie de valeurs extr√™mes 1\nProjet de th√©orie de valeurs extr√™mes 2\n\nApplications d√©ploy√©es\n\nStock price prediction : CAC40\nAsset pricing and management"
  },
  {
    "objectID": "index_gdr.html#assets",
    "href": "index_gdr.html#assets",
    "title": "Gestion des risques",
    "section": "",
    "text": "Bond, Forward, Future, Swaps : Qu‚Äôest ce que c‚Äôest?\n\nAsset management\n\nRisque de march√©\nRisque de liquidit√©\nRisque de valorisation, de cr√©dit etc.\n\nAsset pricing\n\nPricing d‚Äôoptions vanilles TO DO\nPricing de taux, swap, d‚Äôobligations TO DO\nTracking error TO DO\nConstruction de portefeuille markowitz TO DO"
  },
  {
    "objectID": "index_gdr.html#projets-et-applications-d√©ploy√©es",
    "href": "index_gdr.html#projets-et-applications-d√©ploy√©es",
    "title": "Gestion des risques",
    "section": "",
    "text": "Projets\n\nProjet de s√©ries temporelles\nProjet de scoring TO DO\nProjet de th√©orie de valeurs extr√™mes 1\nProjet de th√©orie de valeurs extr√™mes 2\n\nApplications d√©ploy√©es\n\nStock price prediction : CAC40\nAsset pricing and management"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Universit√© Paris Cit√© ‚Äì Paris\nMaster 2 ‚Äì Mod√©lisation Al√©atoire (M2MO), Ex-DEA Laure Elie | Sept.¬†2025 ‚Äì Sept.¬†2026\nSp√©cialisation : Finance quantitative, probabilit√©s avanc√©es, statistiques, et mod√©lisation stochastique\nENSAI ‚Äì √âcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation ‚Äì Rennes | Sept.¬†2022 ‚Äì Sept.¬†2025\nDipl√¥me d‚Äôing√©nieur en statistiques (grade de master)\nSp√©cialisation : Gestion des risques et ing√©nierie financi√®re"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Soci√©t√© G√©n√©rale ‚Äì RISQ/MDL/MOD\nStagiaire analyste quantitatif ‚Äì IFRS 9 | Avril 2025 - Aout 2025\nINSERM\nStagiaire recherche ‚Äì Mod√®les de cure mixtes | Mai 2024 - Aout 2024"
  },
  {
    "objectID": "about.html#experiences",
    "href": "about.html#experiences",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Soci√©t√© G√©n√©rale ‚Äì RISQ/MDL/MOD\nStagiaire analyste quantitatif ‚Äì IFRS 9 | Avril 2025 - Aout 2025\nINSERM\nStagiaire recherche ‚Äì Mixture cure models | Mai 2024 - Aout 2024\n\n\n\n\n\n\nNote\n\n\n\nCe site web, con√ßu et g√©n√©r√© avec Quarto, s‚Äôadresse principalement aux √©tudiants de l‚ÄôENSAI et du M2MO et a pour vocation d‚Äôoffrir un soutien √† ceux qui, comme moi, ont √©t√© confront√©s √† des d√©fis acad√©miques au cours de leur formation. Il ne pr√©tend en aucun cas se substituer √† l‚Äôenseignement dispens√© par nos professeurs, dont la rigueur et l‚Äôexpertise sont essentielles. Son objectif est plut√¥t de compl√©ter leur travail en partageant mes exp√©riences personnelles et les projets que j‚Äôai r√©alis√©s."
  },
  {
    "objectID": "about.html#formation",
    "href": "about.html#formation",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Universit√© Paris Cit√© ‚Äì Paris\nMaster 2 ‚Äì Mod√©lisation Al√©atoire (M2MO), Ex-DEA Laure Elie | Sept.¬†2025 ‚Äì Sept.¬†2026\nSp√©cialisation : Finance quantitative, probabilit√©s avanc√©es, statistiques, et mod√©lisation stochastique\nENSAI ‚Äì √âcole Nationale de la Statistique et de l‚ÄôAnalyse de l‚ÄôInformation ‚Äì Rennes\nDipl√¥me d‚Äôing√©nieur en statistiques (grade de master) | Sept.¬†2022 ‚Äì Sept.¬†2025\nSp√©cialisation : Gestion des risques et ing√©nierie financi√®re"
  },
  {
    "objectID": "index_stat.html#th√©orie-des-valeurs-extr√™mes",
    "href": "index_stat.html#th√©orie-des-valeurs-extr√™mes",
    "title": "Statistiques",
    "section": "",
    "text": "R√©sum√© de la th√©orie des valeurs extr√™mes"
  },
  {
    "objectID": "index_stat.html#micro√©conom√©trie-appliqu√©e",
    "href": "index_stat.html#micro√©conom√©trie-appliqu√©e",
    "title": "Statistiques",
    "section": "",
    "text": "R√©sum√© des m√©thodes de micro√©conom√©trie appliqu√©e enseign√©es √† la promo 2024-2025 ENSAI"
  },
  {
    "objectID": "index_stat.html#apprentissage-supervis√©",
    "href": "index_stat.html#apprentissage-supervis√©",
    "title": "Statistiques",
    "section": "",
    "text": "R√©gression lin√©aire\nKernel Trick & SVM\nGradient Boosting\nFeature Selection\nR√©gression Ridge & Lasso\nCART & Random Forest"
  },
  {
    "objectID": "index_gdr.html#analyse-financi√®re",
    "href": "index_gdr.html#analyse-financi√®re",
    "title": "Gestion des risques",
    "section": "",
    "text": "Construction du bilan d‚Äôentreprise\nReglementation prudentielle TO REWRITE"
  },
  {
    "objectID": "index_gdr.html#risques-financiers",
    "href": "index_gdr.html#risques-financiers",
    "title": "Gestion des risques",
    "section": "",
    "text": "D√©finition du risque financier\nValue-at-risk (VaR) :\n\nD√©finition de la VaR\nImpl√©mentation de la VaR sur python\nTP1 : M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\nTP2 : M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\nTP3 : M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\nEstimation de la CreditVaR √† l‚Äôaide de la th√©orie des copules"
  },
  {
    "objectID": "index_gdr.html#calibrations-de-processus-stochastique",
    "href": "index_gdr.html#calibrations-de-processus-stochastique",
    "title": "Gestion des risques",
    "section": "",
    "text": "Calibration avec le mod√®le de black-scholes\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre de Kalman\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre particulaire bootstrap\nMod√®le de Heston :\n\nEstimation de la volatilit√© avec le filtre boostrap\nComparaison du filtre bootstrap au filtre particulaire auxiliaire (APF)\n\nCourbes de taux (Construction d‚Äôune courbe ZC, Mod√®le Hull - White)"
  },
  {
    "objectID": "index_gdr.html#autres-sujets",
    "href": "index_gdr.html#autres-sujets",
    "title": "Gestion des risques",
    "section": "",
    "text": "Investissement socialement responsable\nFinance durable\nCorr√©lation de spearman vs Corr√©lation de pearson"
  }
]