[
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Compte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire.\n\n\n\n\nConstruction du bilan d’entreprise (bilan_entreprise.qmd)\n\n\n\n\n\nReglementation prudentielle (reglementation_prudentielle.qmd)\n\n\n\n\n\nDéfinition du risque financier (risque_def.qmd)\nValue-at-risk (VaR) :\n\nDéfinition de la VaR (var_def.qmd)\nImplémentation de la VaR sur python (var_application.qmd)\n\n\n\n\n\n\nInvestissement socialement responsable (ISR.qmd)"
  },
  {
    "objectID": "index_gdr.html#analyse-financière",
    "href": "index_gdr.html#analyse-financière",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Construction du bilan d’entreprise (bilan_entreprise.qmd)"
  },
  {
    "objectID": "index_gdr.html#reglementation-prudentielle",
    "href": "index_gdr.html#reglementation-prudentielle",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Reglementation prudentielle (reglementation_prudentielle.qmd)"
  },
  {
    "objectID": "index_gdr.html#risques-financiers",
    "href": "index_gdr.html#risques-financiers",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Définition du risque financier (risque_def.qmd)\nValue-at-risk (VaR) :\n\nDéfinition de la VaR (var_def.qmd)\nImplémentation de la VaR sur python (var_application.qmd)"
  },
  {
    "objectID": "index_gdr.html#autres-sujets",
    "href": "index_gdr.html#autres-sujets",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Investissement socialement responsable (ISR.qmd)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A propos de moi",
    "section": "",
    "text": "Cheryl KOUADIO\n\n\n\n\nEtudiante gestion des risques financiers\n\n\nÉcole Nationale de la Statistique et de l’analyse de l’information\n\n\n\nEmail : cheryl.s.kouadio@gmail.com\n\n\nCV (Français/English) : cv_cheryl_kouadio_fr.pdf/cv_cheryl_kouadio_en.pdf\n\n\n\n\nIntérêts\n\nJe m’intéresse particulièrement à la modélisation des risques financiers (risques de marché et de crédit) ainsi qu’aux normes et réglementations qui régissent le système bancaire (Bâle III, etc.), à l’audit des modèles quantitatifs et à la validation des méthodes statistiques appliquées à la gestion des risques. Je suis également passionné par l’évolution des approches statistiques, ce qui m’amène à effectuer une veille constante des nouvelles méthodes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3A/risque_def.html",
    "href": "3A/risque_def.html",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "",
    "text": "En finance, le risque peut être défini comme la survenance d’un événement incertain qui peut avoir des conséquences négatives sur le bilan, ou le compte de résultat d’une banque. Par exemple, une fraude aura un impact négative sur la réputation d’une banque ce qui peut entrainer des pertes importants ayant un impact négatif sur le résultat net de celle-ci. En économie, le risque est un événement probabilisable tandis que l’incertitude est non probabilisable.\nNous pouvons caractériser 3 grands types de risques établis par le comité de Bâle qui veille au renforcement et à la stabilité du système financier. (rangés par ordre d’importance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la conséquence. Toutefois, certains risque sont difficiles à distinguer. Ils se trouvent à la frontière entre le risque de marché, de crédit et le risque opérationnel.\nIl est important de noter que le but d’une banque n’est pas de prendre le moins de risque, mais d’atteindre une rentabilité maximale pour un risque donné. La théorie financière nous apprend que seul le risque est rémunéré. La banque procède donc à une arbitrage entre risque et rentabilité. C’est pourquoi la gestion des risques est un élément clé de la stratégie de décision de la banque. La mesure du risque intervient pour calculer les fonds propres nécessaires pour assurer chaque opération financière. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "3A/risque_def.html#les-mesures-de-risque",
    "href": "3A/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al. ont défini les propriétés que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  cohérente. Une mesure de risque est une fonction qui permet de quantifier le risque d’un portefeuille. Elle est cohérente si elle satisfait les propriétés suivantes :\n\nsous-additivité : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomogénéité positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivité signifie que le risque d’un portefeuille est inférieur ou égal à la somme des risques des actifs qui le composent. Ce phénomène est appelé effet de diversification. En effet, la diversification permet de réduire le risque d’un portefeuille en investissant dans des actifs non corrélés. Ainsi, en agrégeant deux porte-feuilles, il n’y a pas de création de risque supplémentaire.\nL’homogénéité positive signifie que le risque d’un portefeuille est proportionnel à la taille du portefeuille. Cette propriété ignore les problèmes de liquidité.\nL’invariance par translation signifie que l’addition au portefeuille initiale un montant sûr rémunéré au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d’immobiliser des fonds propres égaux à la mesure du risque.\nLa monotonie signifie que le risque d’un portefeuille est inférieur ou égal au risque d’un autre portefeuille si la distribution de probabilité de la perte potentielle du premier portefeuille est inférieure ou égale à celle du deuxième portefeuille. Celà traduit l’ordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d’un portefeuille à un instant t donné, le risque est la variation négative de ce portefeuille dans le futur. Le risque se caractérisait donc par une perte relativfe (par rapport à la valeur initiale du portefeuille à un instant t). Pendant très longtemps, les banques utilisaient la volatilité (écart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup évoluée et celle qui est la plus répandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d’un portefeuille à un horizon \\(h\\) donné et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\noù F est la distribution de probabilité de la perte potentielle du portefeuille.\nPar exemple, une VaR à \\(\\alpha=1\\%\\) de 1 million d’euros signifie que la probabilité que la banque perde plus de 1 million d’euros est égale à 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d’euros sur une période donnée (C’est la perte maximale encourue par la banque avec un intervalle de confiance à 99%). Nous allons préférer la deuxième formulation de l’interprétation.\nDeux éléments sont nécessaires pour déterminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance à un indicateur de tolérance pour le risque. Une couverture à 99% est beaucoup plus exigente et donc plus coûteuse qu’une couverture à 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait définir l’horizon h. Par exemple, une couverture à 1 jour est moins coûteuse qu’une couverture à 1 mois. C’est la combinaison de ces deux éléments qui détermine le degré de la couverture qui peut être exprimé en temps de retour 1 \\(t°\\)qui est la durée moyenne entre deux dépassements de la VaR. Il permet de caractériser la rareté d’un évènement (dont la probabilité d’occurence est petite)\n\\[t°= \\frac{h}{1-\\alpha}\\]\nLorsqu’on entend parler de gestion de risque décennal, celà revient à considérer une valeur en risque (VaR) journalière (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non cohérente car elle ne respecte pas la propriété de sous-additivité. De nombreux professionnels recommanderaient alors l’utilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque cohérente. La CVAR est l’espérance de la perte au delà de la VaR. Toutefois, la VaR reste une mesure de risque très utilisée en pratique, qui ne respecte pas la propriété de sous-additivité que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilités sont principalement localisées dans les quantiles extrêmes.\n\n\n\nD’autres mesures de risque\nD’autres mesures peuvent être définis comme celle de la perte exceptionnelle (Unexpected Loss - UL) définie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] où L est la distribution de la perte potentielle.\nIl s’agit là de la différence entre la VaR et la perte moyenne (expected loss - EL). Il y a également le regret espéré défini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donné représentant le montant de la perte tolérable par l’institut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l’institut financier. Lorsque H est endogène, c’est-à-dire dépendant de la distribution de la perte potentielle, et égale à la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l’espérance de la perte au delà de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au delà d’un certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au delà de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c’est la semi variance."
  },
  {
    "objectID": "3A/risque_def.html#footnotes",
    "href": "3A/risque_def.html#footnotes",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npériode de retour doit être interprétée comme la probabilité statistique qu’un évènement se produise↩︎"
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "",
    "text": "L’analyse financière constitue l’ensemble des outils permettant de donner un avis objectif d’une organisation (entreprises, fondations, etc.) sur la santé finanière et les risques financiers auxquels elle sera confrontée. Il s’agit de determiner quels sont les critères d’une santé financière, qu’est le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le gère-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financière d’une entreprise. Il s’agit du bilan et du compte de résultat. Ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise. Comprendre comment ils fonctionnent permet de mieux appréhender la situation financière d’une banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un état des lieux de la situation patrimoniale de l’entreprise à un moment donné. Il est composé de deux parties : l’actif et le passif. L’actif ou l’emploi regroupe l’ensemble des biens et des droits de l’entreprise tandis que le passif regroupe l’ensemble des ressources de l’entreprise (d’où vient l’argent et où peut-on s’en procurer). Le bilan est équilibré en valeur nette, c’est-à-dire que l’actif est égal au passif.\nLe compte de résultats, quant à lui, est un document qui permet de faire un état des lieux des performances de l’entreprise sur une période donnée (il résume les bénéfices ou pertes générées). Il est composé du détail des produits et des charges de l’entreprise. Les produits sont les éléments qui génèrent des revenus pour l’entreprise tandis que les charges sont les éléments qui génèrent des dépenses pour l’entreprise. Le compte de résultat alimente par ailleurs la partie “résultat de l’exercice” du bilan comptable.\nLe coeur de l’entreprise à analyser comme ressources supplémentaires dans le compte de résultat est l’ensembles des charges financières & exceptionnelles ainsi que l’ensemble des produits d’exploitation et financiers. Ces éléments clés permettent de déterminer la rentabilité de l’entreprise. En effet, si les charges sont supérieures aux produits, l’entreprise est en perte. Si les produits sont supérieurs aux charges, l’entreprise est en bénéfice.\nIl est important de noter que ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d’analyser la situation financière d’une entreprise, il faut donc le remodeler en un bilan “fonctionnel” pour pouvoir l’analyser. Le bilan fonctionnel est un document qui permet de faire un état des lieux de la situation financière de l’entreprise en fonction de son activité, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d’investissement à long terme\nEmplois stables\n\nactifs immobilisés en valeur brute\n\nCycle de financement à long terme\nRessources stables\n\nCapitaux propres,\nEmprunts à long terme,\nAmortissements et dépréciation,\nProvisions pour risques\n\n\n\nCycle d’exploitation\nEmplois d’exploitation\n\nStocks et encours\nCréances\n\nCycle d’exploitation\nRessources d’exploitation\n\nDettes circulantes\n\n\n\nTrésorerie active\n\nDisponibilités\n\nTrésorerie passive\n\nDécouverts bancaires\n\n\n\n\nLes ressources stables font référence aux ressources saines du bilan etfont face aux emplois stables. La trésorerie passive fait référence aux découverts bancaires. Il est important de souligner qu’une trésorerie passive est perçue négativement dans le bilan fonctionnel. En effet, une trésorerie passive signifie que l’entreprise a des dettes à court terme qui ne sont pas couvertes par des actifs à court terme d’où la nécessité d’avoir des découverts bancaires.\nNb : La provision pour le risque peuve être considérée comme une ressource stable ou une ressource d’exploitation en fonction de l’entreprise. Tout dépend de la longevité des provisions.\n\nEquilibre financier\nNous dirons qu’il y a équilibre financier lorsque :\n\nLes emplois stables soient entièrement financés par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a necéssité d’un fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) dépend du cycle d’exploitation (entre autre, la rapidité de rotation des stocks et des créances). Il doit couvrir les besoins de financement du cycle d’exploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, créances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la trésorerie (\\(\\text{Trésorerie}=FDR-BFR\\)). Si la trésorerie est positive, il y a équilibre financier. Celà signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d’exploitation. Lorsqu’il est négatif, il faut trouver des ressources pour financer le cycle d’exploitation. Si la trésorerie est nulle, il y a équilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-même le cycle d’exploitation de l’entreprise. C’est ce qu’on appelle le crédit fournisseur. Il est important de noter que le crédit fournisseur est une source de financement gratuite pour l’entreprise. C’est le cas des E-commerce où les acteurs encaissent leurs clients avant même d’acheter les stocks auprès des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transformé en ressources en fonds de roulement, celà est une situation très favorable pour l’entreprise et est appelée “crédit inter-entreprises”."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Analyse du compte de résultat",
    "text": "Analyse du compte de résultat\nNous pouvons faire les mêmes critiques faites au bilan comptable sur le compte de résultat. En effet, le compte de résultat est conçu de sorte à fournir des informations au seul détenteur du capital, à savoir les actionnaires. Il fait apparaitre uniquement le bénéfice ou la perte. C’est un document d’intérêt pour l’Etat pour déterminer si un pays est en croissance ou en récession. Pour en faire un vrai diagnostic financier, il faut le découper en sous-soldes appelés “soldes intermédiaires de gestion” (SIG). Les SIG permettent de déterminer la rentabilité de l’entreprise, sa capacité d’autofinancement, sa capacité de remboursement, sa capacité de financement, etc.\nIl existe 9 soldes intermédiaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajoutée\nL’excédent brut d’exploitation (EBE)\nLe résultat d’exploitation\nLe résultat courant avant impôt\nLe résultat exceptionnel\nLe résultat net\nLa plus ou moins value de cession\n\nSelon la théorie de prise de décisions, il y a deux grands types de décisions : des décisions qui permettent de créer de la riches (Marge co., production et valeur ajoutée) et des décisions qui permettent de distribuer/dépenser de la richesse (EBE, résultat d’exploitation, résultat courant avant impôt, résultat exceptionnel, résultat net et plus ou moins value de cession). Lorsqu’on dépense la riches, il faudrait qu’elle soit bien dépensée.\n\nSoldes de création de richesse\nLes soldes qui contribuent à la création de richesse sont la marge commerciale, la production et la valeur ajoutée :\n\nLa marge commerciale est la différence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C’est un solde des entreprises commerciales (par exemple, les supermarchés). Pour une entreprise qui n’ont pas de marchandises, le marge commerciale est nulle.\nLa production de l’exercice est la somme des produits vendus(\\(\\pm\\) les produits stockées) et des produits immobilisées par l’entreprise (certaines entreprises peuvent se vendre des produits à elles-mêmes). C’est un solde des entreprises industrielles.\nLa valeur ajoutée est la richesse créée par l’entreprise. C’est la somme des marges commerciales, de la production de l’exercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services extérieurs).\n\nLa valeur ajouté est un indicateur très suivi par l’Etat pour déterminer le produit intérieur brut (PIB) afin de déterminer si un pays est en croissance ou en récession. Par ailleurs, la valeur ajoutée divisée par le nombre de salariés permet de déterminer le niveau de technicité de l’entreprise. Plus la valeur ajoutée par salarié est élevée, plus l’entreprise est techniquement avancée.\n\n\nLa richesse dédiée à l’activité économique\nIl existe 5 tiers à qui l’entreprise redistribue la VA (rangée par ordre de priorité) :\n\nLe personnel (à travers les salaires),\nL’Etat (à travers les impôts),\nLe capital technique (via les amortissements),\nLes banques (via les intérêts),\nLes actionnaires ou les associés (via le bénéfice comptable)\n\nLes soldes qui permettent de financer l’activité économique (Etat, personnel, capital technique) sont l’excédent brut d’exploitation et le résultat d’exploitation :\n\nLe solde EBE rémunère le personnel et l’Etat. Il représente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d’entreprise pour l’Etat. Un EBE positif signifie que l’entreprise est capable de rémunérer le personnel et l’Etat, et donc de financer l’emploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajoutée} - \\text{Impôts, tâxes et versements assimilés} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de résultat d’exploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de rémunérer le capital technique (machines etc.) et appartient à tout ceux qui dépendent du capital financier et mesure les performances industrielles et commerciales de l’entreprise.\n\n\\[\\begin{align*}\n\\text{Résultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse dédiée à l’activité financière\nLes soldes qui permettent de financer l’activité financière (banques, actionnaires) sont le résultat courant avant impôt, le résultat exceptionnel, le résultat net et la plus ou moins value de cession :\n\nLe résultat courant avant impôt est le solde qui permet de rémunérer les banques. Il est un indicateur de la capacité de l’entreprise à rembourser ses dettes et est un témoin de l’incidence de la politique financière de l’entreprise sur son résultat. Il faut distinguer les intérêts à long terme et ceux de court terme. Plus ceux ci sont liés à des dettes de court terme (ex. : découverts), on peut dire que l’entreprise est en difficulté financière tandis que l’endettement à long terme est un signe de bonne santé financière, car il est voulu plutôt que subi. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat courant avant impôt} &= \\text{Résultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financières}\n\\end{align*}\\]\n\nLe résultat exceptionnel est le solde qui est le moins analysé car il est souvent lié à des évènements exceptionnels (ex. : vente d’un bien immobilier). Il est calculé comme étant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe résultat net est le solde qui permet de rémunérer les actionnaires. C’est le solde en bas du compte de résultat. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat net} &= \\text{Résultat courant avant impôt} + \\text{Résultat exceptionnel} \\\\\n&- \\text{participations des salariés} - \\text{Impôts sur les bénéfices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu’une entreprise vend une immobilisation. Ce ratio permet de déterminer si l’entreprise a vendu une immobilisation à un prix supérieur ou inférieur à sa valeur comptable. Celà constitue un temoin d’alerte sur la santé de l’entreprise et permet de déterminer si l’entreprise est en difficulté financière (car rien ne l’oblige à vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "La capacité d’autofinancement",
    "text": "La capacité d’autofinancement\nLa capacité d’autofinancement (CAF) est un indicateur qui permet de déterminer si l’entreprise est capable de financer ses investissements sans recourir à des financements extérieurs. Elle regroupe la capacité à dégager de la liquidité. Il n’y a pas de correspondance entre la trésorerie et le bénéfice. En effet, une entreprise peut être en bénéfice mais en difficulté financière. Pour la calculer, il faut éliminer les sommes non encaissanles et non décaissables (ex. : Dotations, provision, reprise sur amortissements, les écritures exceptionnelles).\nPour passer du bénéfice à la CAF, on ne conserve que les éléments qui sont encaissables et décaissables et est calculée comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges décaissables (intérêt bancaire, impôt sur bénéfice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu versé par l’entreprise à ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas à elle toute seule de déterminer l’autofinancement de l’entreprise. Dans le cadre légal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu’à 95% du bénéfice comptable imposé par l’Etat et garder 5% à l’entreprise. C’est ce qu’on appelle le “dividende légal”. Au delà de 10%, les actionnaires peuvent retirer jusqu’à 100% du bénéfice comptable. C’est ce qu’on appelle le “dividende statutaire”.\nAinsi, l’autofinancement est la somme qui reste de la CAF après le dividende légal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaissés par les actionnaires détermine la politique d’autofinancement de l’entreprise.\nL’autofinancement est essentiel pour l’entreprise car il permet de:\n\nrembourser les emprunts,\naméliorer la trésorerie,\ncouvrir les risque de l’entreprise (provisions pour risque),\nfinancer l’exploitation (stocks & créances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html",
    "href": "3A/Apprentisage-stat/Tp1.html",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It’s the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let’s start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10² and 10⁴ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : −λ(X^T X + λId) (−1) θ\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]\n\n\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don’t have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It’s the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let’s start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10² and 10⁴ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : −λ(X^T X + λId) (−1) θ\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "If we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don’t have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "title": "Ridge regression vs. Lasso regression",
    "section": "1. Ridge regression",
    "text": "1. Ridge regression\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter \\(\\lambda\\) by cross-validation. We will use the function RidgeCV from the sklearn library to perform the cross-validation. Since we didn’t center the covariables, we will set the parameter fit_intercept to True in order to include an intercept in the model.\nBy default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter λ. This is called regularization path. We can do this by fitting the model for different values of λ and store the coefficients.\n\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'λ = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter \\(\\lambda\\) wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients.\n In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. \n\na. Check on the intercept value of the model using lambda found by cross validation\n\nprint(f'Intercept value : {ridge_cv.intercept_}')\n\nIntercept value : 152.28937186306015\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "title": "Ridge regression vs. Lasso regression",
    "section": "2. Lasso regression",
    "text": "2. Lasso regression\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation. It is important to perform a cross-validation in order to choose the best value of the regularization parameter \\(\\lambda\\) as we have demonstrated in the first activity. For the lasso regression, we will use the function LassoCV from the sklearn library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n\n\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'λ = {lasso_cv.alpha_:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter \\(\\lambda\\) given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of \\(\\lambda\\), we can select more or less variables.\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n\na. Check on the intercept value of the model using lambda found by cross validation\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n\nIntercept value : 151.95282341561403"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "3. Quality of the models (ridge regression vs lasso regression)",
    "text": "3. Quality of the models (ridge regression vs lasso regression)\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\\[\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2\\]\nwhere \\(\\hat g(x)\\) is the predicted value of the output variable y given the input variable x, \\(\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j\\).\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html",
    "href": "2A/App_sup/reg_lin.html",
    "title": "La régression linéaire",
    "section": "",
    "text": "La régression linéaire est une méthode d’apprentissage supervisé qui vise à évaluer, lorsqu’il existe, la relation linéaire entre une variable d’intérêt et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de données constitué de n échantillons iid (indépendant et identiquement distribué), le modèle de regression linéaire s’écrit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\noù \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l’erreur, l’information que les autres variables explicatives ne donnent pas.\nL’hypothèse fondamentale de la régression linéaire est l’existence d’une relation linéaire entre la variable cible et les variables explicatives. Pour s’assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire (à l’aide de visualisation ou de tests- spearman, pearson, etc.)\nL’hypothèse de rang plein est la seconde plus grande hypothèse, elle stipule que les variables explicatives ne soient pas corrélées entre elles. Cette condition est nécessaire pour garantir l’unicité des estimations des paramètres du modèle et ainsi l’identifiabilité du modèle étudié\nPar ailleurs pour que les estimations des paramètres du modèle linéaire soient fiables, les erreurs du modèle, représentées par \\(\\xi_i\\), doivent répondre à plusieurs critères :\n\nErreurs centrées : La moyenne attendue des erreurs doit être nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\nHomoscédasticité : La variance des erreurs doit être constante pour toutes les observations, exprimée par \\(V[\\xi_i] = \\sigma^2\\). Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\nDécorrélation des erreurs : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d’erreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d’observer une hypothèse supplémentaire sur la loi des erreurs. En effet, les erreurs sont souvent supposées suivre une loi normale, c’est à dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Celà nous permet de faire des inférences sur les paramètres du modèle et de construire des intervalles de confiance.\n\n\nToutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus.\n\n\n\nDans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "href": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "title": "La régression linéaire",
    "section": "",
    "text": "Toutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "title": "La régression linéaire",
    "section": "",
    "text": "Dans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#simulation-des-données",
    "href": "2A/App_sup/reg_lin.html#simulation-des-données",
    "title": "La régression linéaire",
    "section": "1. Simulation des données",
    "text": "1. Simulation des données\nPour évaluer l’intérêt de la regréssion linéaire, nous allons simuler un échantillon de taille n=200, où la variable cible Y est une fonction linéaire de la variable explicative X. La vraie relation est donnée par \\(Y = 2 + 3X + \\epsilon\\), où \\(\\epsilon \\sim N(0, 1.6)\\). De fait le modèle linéaire est adéquat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un modèle linéaire simple à nos données, nous obtenons une estimation des paramètres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du modèle suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "title": "La régression linéaire",
    "section": "2. Evaluation du modèle",
    "text": "2. Evaluation du modèle\n\n2.1. Hypothèses sur les erreurs et l’existence d’une relation linéaire\nPour évaluer la qualité du modèle, nous allons tracer les résidus studentisés en fonction des valeurs ajustées. Les résidus studentisés sont les résidus divisés par l’écart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajustées\", ylab=\"Résidus studentisés\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression, est bien adaptée aux données, et donc que l’hypothèse de linéarité est acceptable.\nSi l’on observait une forme de trompette, celà reviendrait à soulever une question sur l’hétéroscédascité des résidus, tandis qu’une forme de banane revèle plutôt une relation de non-linéarité.\nLorsque le nuage de point n’a pas de structure particulière, a priori l’hypothèse d’homoscédascticité n’est pas remise en question, comme cela semble être le cas ici. Attention : ces principes peuvent parfois être mis en défaut et il vaut toujours mieux réaliser plusieurs contrôles différents.\nPour vérifier l’hypothèse d’homoscédasticité, nouspouvons également utiliser le test de Breusch-Pagan. Ce test est basé sur la régression des carrés des résidus sur les variables explicatives. Si le test est significatif, l’hypothèse d’homoscédasticité est rejetée.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a nécessité le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l’hypothèse de non corrélation des résidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est basé sur l’autocorrélation des résidus. Si le test est significatif, l’hypothèse de non corrélation des résidus est rejetée.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l’hypothèse de normalité des résidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est basé sur la comparaison des résidus avec une loi normale. Si le test est significatif, l’hypothèse de normalité des résidus est rejetée.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l’hypothèse de normalité des résidus n’est pas rejetée.\n\n\n2.2. Qualité du modèle\nPour évaluer la qualité du modèle, nous allons calculer le coefficient de détermination \\(R^2\\) et le \\(R^2\\) ajusté.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajusté de 0.97. Cela signifie que 97% de la variance de la variable cible est expliquée par le modèle. Notre modèle de régression linéaire est bien ajusté à nos données."
  },
  {
    "objectID": "3A/ISR.html",
    "href": "3A/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "3A/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "3A/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "3A/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "3A/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL’ISR présente de multiples avantages tant pour les investisseurs que pour les entreprises engagées dans cette démarche. Ces avantages reflètent l’évolution des attentes sociétales et la reconnaissance croissante de l’importance de la durabilité et de l’éthique dans le monde des affaires. J’en ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement améliorer la réputation d’une entreprise. Elle témoigne de son engagement envers des pratiques durables et éthiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financière : De nombreuses études démontrent que les entreprises avec une notation ESG élevée tendent de meilleures performances financièrement sur le long terme. Cela s’explique par une meilleure anticipation des risques, une gestion plus efficace et une capacité à saisir les opportunités de marché liées à la durabilité.\nMeilleure gestion des risques : L’adoption de pratiques ESG solides permet aux entreprises de mieux identifier et gérer les risques, qu’ils soient climatiques, sociaux ou de marché.\nMeilleure attractivité pour les investisseurs : En démontrant un engagement clair envers la durabilité et l’éthique, les entreprises attirent davantage d’investisseurs conscients de l’importance des critères ESG. Cette attractivité accrue peut se traduire par un accès facilité au capital et à de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution à la réduction de certains risques financiers : En investissant dans des entreprise intégrant les critères ESG dans leur processus de décision, les investisseurs contribuent indirectement à une meilleure identification et anticipation les risques liés au changement climatique, aux problématiques sociales, et aux défis de gouvernance, ce qui contribue à une meilleure protection de leur capital sur le long terme.\nImpact positif sur la société : L’ISR permet aux investisseurs de contribuer activement à une économie plus durable et équitable. En choisissant d’investir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des modèles économiques respectueux de l’environnement et du bien-être social.\n\nEn somme, l’ISR offre une perspective d’investissement qui va au-delà des retours financiers immédiats pour embrasser des bénéfices à long terme, tant sur le plan économique que social et environnemental."
  },
  {
    "objectID": "3A/ISR.html#comment-investir",
    "href": "3A/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire à un fonds ISR ?\nSouscrire à un fonds1 ISR (Investissement Socialement Responsable) est une démarche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la société et l’environnement[@comment]. En consultant son conseiller financier ou son établissement bancaire, il est possible de placer son argent dans une variété de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d’Épargne en Actions (PEA) : (sous réserver de s’assurer que le fonds ISR choisi est bien éligible au PEA) Offre la possibilité de placer son épargne en actions de sociétés européennes.\nLes compte-titres ordinaires(CTO) : permet d’investir en bourse sur les marchés financiers français et/ou étrangers et dans tout type de valeurs mobilières (OPC2, actions, obligations, monétaire, warrants, trackers…).\nL’épargne salariale ou les plans d’épargne d’entreprise (PEE) : un produit d’épargne collectif qui permet aux salariés d’une entreprise de se constituer un portefeuille de valeurs mobilières qui peuvent proposer des fonds ISR.\nEnfin, certains produits d’épargne retraite individuelle, comme le Plan d’Epargne Retraite (PER).\n\nCes véhicules d’investissement permettent aux particuliers de contribuer à une économie plus durable tout en recherchant une performance financière. Il est recommandé de se rapprocher d’un conseiller pour déterminer le produit le mieux adapté à ses objectifs financiers et à ses valeurs éthiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) nécessite une approche combinant analyses personnelle, financière et extra-financière, cette dernière se concentrant sur les critères ESG (Environnementaux, Sociaux et de Gouvernance)[@comment2021].\nPour choisir efficacement une entreprise ISR, il est crucial de réaliser une analyse à triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR qui vous convient le mieux. Elles vous aideront à identifier le type de placement à privilégier et définir par exemple des fonds thématiques, d’exclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financière : Elle permet d’évaluer la performance économique de l’entreprise, sa santé financière, sa capacité à générer des profits et à maintenir une croissance durable. Cette analyse est indispensable pour s’assurer que l’entreprise est non seulement responsable, mais aussi viable et performante à long terme.\nAnalyse extra-financière (ESG) : Cette analyse complète l’évaluation financière en examinant comment l’entreprise aborde les défis et saisit les opportunités liées aux critères environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds intégrant les critères ESG comme les fonds labellisés ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du ministère de l’économie et des finances français qui garantit que le fonds d’investissement respecte des critères ESG stricts4 dans ses choix d’investissement. Il assure également que le fonds investit dans des entreprises qui adhèrent à ces principes, offrant ainsi une couche supplémentaire de confiance pour les investisseurs soucieux de l’impact de leurs placements.\nCe label est attribué aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,classées en 6 catégories, qui constituent les 6 piliers du référentiel [@critèresa]."
  },
  {
    "objectID": "3A/ISR.html#footnotes",
    "href": "3A/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appelé fonds de placement. Il s’agit d’une société d’ordre public ou privé qui investit du capital pour soutenir des projets souvent innovants.↩︎\norganismes de placement collectif↩︎\nLes fonds d’investissement d’exclusion normatifs font référence aux fonds faisant l’objet de plusieurs controverses.↩︎\ncf liste des fonds labellisés [@listede].↩︎"
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La réglementation prudentielle",
    "section": "",
    "text": "La réglementation prudentielle a été initiée par le développement des marchés financiers et des chocs alimentés par diverses crises financières. Face à ce constat, les autorités de contrôle bancaire ainsi que les autorités de marché ont pris des décisions pour réguler les marchés. C’est notamment le rôle qu’occupe le Comité de Bâle ou la Commission bancaire, qui ont pour objectif de renforcer la stabilité des marchés financiers. En France, l’ACPR (Autorité de Contrôle Prudentiel et de Résolution) et la Banque de France sont membres du Comité de Bâle et participent à ses travaux et décisions.\nIl existe par ailleurs plusieurs textes réglementaires ou documents relatifs au risque de marché. Parmi ces textes, on peut citer le document de référence pour calculer le ratio de solvabilité de la Commission bancaire, intitulé “Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL”, actualisé tous les ans par l’ACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "title": "La réglementation prudentielle",
    "section": "Approche standard de mesure du risque de marché",
    "text": "Approche standard de mesure du risque de marché\nL’approche standard de mesure du risque de marché consiste à calculer les exigences en fonds propres pour chaque catégorie de risque, à savoir :\n\nle risque de taux (général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque lié aux titres de propriété(général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque de change calculé sur l’ensemble des opérations appartenant aussi bien au portefeuille de négociation ou non;\nle risque sur matières premières calculé sur l’ensemble des opérations du portefeuille de négociation ou non;\nles risques opérationnels calculés sur les options associées à chachune des catégories de risque citées ci-dessus.\n\nPar la suite, il s’agit de les additionner de manière arithmétique. Par exemple, pour les titres de propriété, l’exigence de fonds propres est la somme de l’exigence de fonds propres pour le risque général et l’exigence de fonds propres pour le risque spécifique.\nPour le calcul des exigences de fonds propres au titre des risques de marché, il faut tout d’abord déterminer les positions nettes. Les positions de titrisation logées dans le portefeuille de négociation sont traitées comme tout instrument de dette au titre du risque de taux.\nPour le risque spécifique, l’exigence en fonds propres sera la somme des positions nettes multipliées par un coefficient de pondération (2%, 4%, 8% ou 12%) choisi en fonction de la liquidité et la diversification de la position. Pour le risque général, l’exigence en fonds propres est la somme des positions nettes globales (pour chaque marché national) multipliées par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "href": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "title": "La réglementation prudentielle",
    "section": "Approche modèle interne",
    "text": "Approche modèle interne\nL’approche modèle interne est une méthode de calcul des exigences en fonds propres pour le risque de marché qui permet aux établissements de calculer leurs propres exigences. L’exigence en fonds propres est généralement un calcul de la VaR. Cette approche est soumise à des conditions strictes et à une validation par l’ACPR.\nConcernant l’utilisation conjointe des modèles internes et de l’approche standard, la position de la commission prête une attention particulière à la permanence des méthodes ainsi qu’à leur évolution. L’objectif est de s’orienter vers un modèle global qui tient compte de l’ensemble des risques de marché.\n\nAinsi, un établissement commençant à utiliser des modèles pour une ou plusieurs catégories de facteurs de risque doit en principe étendre progressivement ce système à tous ses risques à la méthodologie standardisée (à moins que la Commission Bancaire ne lui ait retiré son agrément pour ses modèles).\n\nPour une banque, la construction d’un modèle interne doit permettre de fournir une mesure plus économique du risque de marché. Au titre de l’article 363 du CRR (Règlement sur les exigences de fonds propres), l’autorité compétente autorise les établissements assujettis à utiliser leurs modèles internes pour calculer les exigences de fonds propres pour risques de marché, après avoir vérifié qu’ils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3ème partie du CRR [@journal]. L’autorisation d’utiliser des modèles internes accordée par les autorités compétentes est requise pour chaque catégorie de risques (risque général et spécifique liés aux actions et titres de créance, risque de change et risque sur matières premières), et elle n’est accordée que si le modèle interne couvre une part importante des positions d’une certaine catégorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives à la mesure du risque (articles 367) mais aussi d’ordre général (article 365).\n\nExigences générales\nLe calcul de la valeur en risque visée à l’article 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprimé en centiles et unilatéral, de 99 %;\nune période de détention de dix jours;\nune période effective d’observation historique d’au moins un an, à moins qu’une période d’observation plus courte ne soit justifiée par une augmentation significative de la volatilité des prix;\ndes mises à jour au moins mensuelles des séries de données.\n\nL’établissement peut utiliser des mesures de la valeur en risque calculées sur la base de périodes de détention inférieures à dix jours, qu’il porte à dix jours selon une méthode appropriée qu’il revoit régulièrement.\nChaque établissement doit également calculer, au moins hebdomadairement, une “valeur en risque en situation de tensions” (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit être calculée conformément aux mêmes exigences que la VaR standard énoncées plus haut (intervalle de confiance de 99% etc.). Cependant, les données d’entrée du modèle de Stressed VaR doivent être calibrées par rapport à une période historique de tensions financières significatives d’au moins 12 mois, pertinente pour le portefeuille de l’établissement. Le choix de cette période de tensions historiques fait l’objet d’un examen au moins annuel par l’établissement, qui en communique les résultats aux autorités compétentes. L’objectif est de s’assurer que la Stressed VaR reflète de manière adéquate les risques auxquels l’établissement serait exposé en période de crise financière.\nPour résumer, les établissements doivent calculer la perte potentielle quotidiennement pour une période de détention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent également calculer une Stressed VaR au moins une fois par semaine, en utilisant des données historiques de périodes de tensions financières significatives.\nNotons \\(VaR(t)\\) la valeur en risque à la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions à la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) à la date t pour le risque de marché sont calculées comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\noù \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu’on vera plus tard.\nDans des périodes normales, l’exigence en fonds propres sera donc la somme d’un multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n’est que dans les périodes de crises financières que l’exigence en fonds propres correspond à la VaR ou à la sVaR du jour précédent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est égal à la somme du chiffre 3, au minimum, et d’un cumulateur compris entre 0 et 1 conformément au tableau 1. Ce cumulateur dépend du nombre de dépassements, sur les 250 derniers jours ouvrés, mis en évidence par les contrôles a posteriori de la mesure de la valeur en risque, au sens de l’article 365, paragraphe 1, effectués par l’établissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.dépassements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque spécifique, tout modèle interne utilisé pour calculer les exigences de fonds propres et tout modèle interne utilisé pour la négociation en corrélation satisfont aux exigences supplémentaires suivantes:\n\nle modèle interne explique la variation historique des prix à l’inté rieur du portefeuille;\nil reflète la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement défavorable;\nil est validé par des contrôles a posteriori(backtesting) visant à établir si le risque spécifique a été correctement pris en compte. Si l’établissement effectue ces contrôles a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de manière cohérente;\nil tient compte du risque de base lié à la signature et, en particulier, il est sensible aux différences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d’événement.\n\nLe risque spécifique vise à tenir compte du risque de contrepartie lié à l’emetteur de l’instrument.\nPour en savoir plus, reportez au règlement (UE) No 575/2013 du parlement européen du journal officiel de l’Union Européenne, appelé aussi règlement CRR. (voir aussi la notice 2020 relative aux « Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV»)."
  },
  {
    "objectID": "3A/var_def.html",
    "href": "3A/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque réglementaire correspond à la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s’agit dans cette section de développer la notion de VaR pour des portefeuilles linéaires et non linéaires."
  },
  {
    "objectID": "3A/var_def.html#le-backtesting",
    "href": "3A/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contrôle de la qualité de la VaR pour un horizon de 1 jour. Il permet de vérifier si la VaR est bien calibrée. Pour cela, on compare la VaR calculée avec la perte réelle. Si la VaR est bien calibrée, la perte réelle ne doit pas dépasser la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d’exception pour valider le modèle. Notons PnL le profit and loss du portefeuille et VaR la valeur à risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsidérons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inférieur à l’opposé de la VaR avec probabilité \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une période ouvré comptant n jours, la probabilité d’avoir \\(i\\) exceptions est donnée par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilité d’avoir plus de \\(k\\) exceptions est donnée par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilité d’avoir au plus de \\(i\\) exceptions est donnée par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilité d’exception n’excède pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d’exceptions empirique est supérieur à celui attendu, le modèle est rejeté :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donnée par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d’exceptions observée lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donnée par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) où \\(t\\) est la valeur observée de la statistique de test et \\(\\phi\\) est la fonction de répartition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "3A/var_def.html#cas-général",
    "href": "3A/var_def.html#cas-général",
    "title": "La VaR",
    "section": "2.2 Cas général",
    "text": "2.2 Cas général\nDans cette approche, nous considérons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en déduisons donc que la VaR est calculé comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction décroissante de l’espérance de PnL et une fonction croissante de la volatilité du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de prévoir l’espérance du PnL futur.\n\n2.2.1 Exemple\nNous considérons une position courte de 1 million de dollars sur le contrat à terme S&P 500. Nous estimons que la volatilité annualisée \\(\\sigma_{\\text{SPX}}\\) est égale à 35%.\nLa perte du portefeuille est égale à \\(L(w) = N \\times R_{\\text{SPX}}\\) où \\(N\\) est le montant de l’exposition (−1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l’indice S&P 500. Nous déduisons que la volatilité de la perte annualisée est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur à risque pour une période de détention d’un an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l’investisseur sur un 1an s’élève à 815 500€ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre période de détention, nous utilisons la règle de la racine carré pour convertir la volatilité pour une fréquence donné \\(f_1\\) en une autre volatilité pour une autre fréquence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les résultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calculé sur 1 jour, pour l’avoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "3A/var_def.html#modèles-linéaires-de-facteurs",
    "href": "3A/var_def.html#modèles-linéaires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Modèles linéaires de facteurs",
    "text": "2.3 Modèles linéaires de facteurs\nNous considérons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est linéaire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La première idée est de choisir les facteurs comme étant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du marché et chaque actif possède son propre facteur de risque.\nLe problème est que les prix sont loin d’être stationnaires, ce qui nous amène à devoir affronter certains problèmes pour modéliser la distribution \\(F_t\\). Une autre idée est de récrire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] où \\(R_i(t;h)\\) est le retour de l’actif entre \\(t\\) et \\(t+h\\).\nNous déduisons que le PnL aléatoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\noù \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l’exposition nominale)dans l’actif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s’ensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous déduisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l’Équation (2.6), l’expression de la valeur à risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d’estimer la matrice de covariance des retours des actifs pour calculer la valeur à risque. Cela explique la popularité de ce modèle, surtout lorsque le P&L du portefeuille est une fonction linéaire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsidérons l’exemple des entreprises d’Apple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l’écart type estimé des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corrélation croisée est égale à 12,0787 %. Il s’ensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous déduisons que la valeur à risque quotidienne à 99% est de 41,21 $. Nous obtenons une figure inférieure à celle de la valeur à risque historique, qui était de 47,39 $. Nous expliquons ce résultat par le fait que la distribution gaussienne sous-estime la probabilité des événements extrêmes et n’est donc pas adaptée à des calculs précis de risque dans des situations de marché volatiles.\n\n\n2.3.2 Exemple de portefeuille linéaire d’actifs\nConsidérons un portefeuille linéaire composé de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne égaux à : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilités journalières sont égales à 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244€, 135€,315€. La matrice de corrélation est donnée par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journée est donc de 18.42€ avec un risque 1% de se tromper.\n\n2.3.2.1 Implémentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les écarts-types et les corrélations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "3A/var_def.html#modèles-factoriels-de-risque",
    "href": "3A/var_def.html#modèles-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Modèles factoriels de risque",
    "text": "2.4 Modèles factoriels de risque\nNous supposons que la valeur du portefeuille dépend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille à \\(t+h\\) dépend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\noù g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit linéaire par rapport aux facteurs de risque, ainsi donc le retour des actifs à l’horizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] où \\(B\\) est la matrice des sensibilités du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque à l’horizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables aléatoires gaussiennes indépendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable aléatoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calculé comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette méthode repose sur 3 hypothèses : l’indépendance temporelle des variations de la valeur du portefeuille, la normalité des facteurs et la relation linéaire entre les facteurs et la valeur du portefeuille. En général, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons à partir des données historiques des facteurs et \\(B\\) est le vecteur des sensibilités du portefeuille aux facteurs de risque. La seuil difficulté de cette méthode est l’estimation de la matrice de variance covariance.\n\n2.4.1 Exemple d’un portefeuille obligataire sans risque de crédit\nNous considérons une exposition sur une obligation américaine à $t=$31 décembre 2014. Le nominal de l’obligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont égaux à 5, \\(t_m&gt;t\\). La maturité résiduelle est de cinq ans et les dates de fixation sont à la fin de décembre (\\(n_C=5\\). Le nombre d’obligations détenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d’une obligation zéro coupon (montant qu’un investisseur serait prêt à payer aujourd’hui pour recevoir un paiement fixe à une date future : combien me rapport un euro à maturité \\(T\\) aujourd’hui?) au temps \\(t\\) pour l’échéance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) où \\(R_t(T)\\) est le taux de rendement zéro coupon.\nLa valeur de l’obligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en déduit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\noù \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l’obligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l’obligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en déduisons que le prix de l’obligation est de \\(P(t)=115,47 \\$\\) et l’exposition totale est de 1 154 706 $. En utilisant la période historique de l’année 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d’intérêt à coupon zéro sachant que l’écart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), où \\(h\\) correspond à un jour de bourse. Pour la matrice de corrélation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en déduisons que la valeur à risque à 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur à risque de 4970$ pour une période de détention d’un jour.\n\n2.4.1.1 Implémentation en R\n\n# Définition des écarts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Ce site web a été conçu et généré à l’aide de Quarto, dans le but principal d’offrir un soutien aux étudiants de l’ENSAI qui pourraient se trouver face à des défis similaires à ceux que j’ai rencontrés au cours de mon parcours académique, notamment durant mes 2ème et 3ème années d’étude. L’intention derrière la création de ce site n’est pas de remplacer les enseignements prodigués par nos estimés professeurs. Au contraire, il vise à compléter leur travail remarquable en partageant mes expériences personnelles et les projets que j’ai réalisés. L’objectif est de fournir une ressource supplémentaire qui peut aider les étudiants à naviguer dans leurs propres projets et défis académiques."
  },
  {
    "objectID": "index.html#régressions",
    "href": "index.html#régressions",
    "title": "Cheryl KOUADIO",
    "section": "Régressions",
    "text": "Régressions\n\nRegression linéaire (regression_lin.qmd)\nRegression ridge et lasso (lasso_ridge.qmd)"
  },
  {
    "objectID": "index.html#classifications",
    "href": "index.html#classifications",
    "title": "Cheryl KOUADIO",
    "section": "Classifications",
    "text": "Classifications\n\nApplications des arbres & forêts aléatoires sur un jeu de données (arbres_&_forets_aleatoires.html)"
  }
]