[
  {
    "objectID": "index_stat.html",
    "href": "index_stat.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Th√©orie des valeurs extr√™mes\n\n\nR√©sum√© de la th√©orie des valeurs extr√™mes\n\n\n\n\nMicro√©conom√©trie appliqu√©e\n\n\nR√©sum√© des m√©thodes de micro√©conom√©trie appliqu√©e enseign√©es √† la promo 2024-2025 ENSAI\n\n\n\n\nApprentissage supervis√©\n\n\nR√©gression lin√©aire\nKernel Trick and Support Vector Machines (SVM)\nGradient boosting\nFeature selection\nR√©gression ridge et lasso\nCART & Random Forest\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\n\n\n37 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\n\n\n17 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\n\n\n20 min\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le mod√®le d‚ÄôHeston\n\n\n9 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGestion de risques de portefeuille\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d‚Äô√©coulement/ de liquidation de portefeuille\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire\n\n\n15 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman\n\n\n8 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration du mod√®le Black-Scholes\n\n\n8 min\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFeatures selection\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\n\n7 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Trick and SVM\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRidge regression vs.¬†Lasso regression\n\n\n12 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nSep 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa r√©glementation prudentielle\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu‚Äôest ce que c‚Äôest ?\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa r√©gression lin√©aire\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise\n\n\n27 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "autres/finance_durable.html",
    "href": "autres/finance_durable.html",
    "title": "Investissement responsable",
    "section": "",
    "text": "Le cadre de la finance durable\nLa finance durable vise √† concilier rentabilit√© financi√®re et prise en compte des enjeux environnementaux, sociaux et de gouvernance (ESG). Elle repose sur trois grands objectifs :\n\nR√©orienter les flux de capitaux vers des investissements responsables.\nG√©rer les risques financiers li√©s aux crit√®res ESG (changement climatique, violations des droits humains, mauvaise gouvernance).\nFavoriser la transparence et une vision √† long terme gr√¢ce aux nouvelles r√©gulations.\n\nPour structurer cette transition, plusieurs outils ont √©t√© mis en place :\n\nLes crit√®res ESG, qui permettent d‚Äô√©valuer la durabilit√© d‚Äôun investissement.\nLa Taxonomie Europ√©enne, qui d√©finit quelles activit√©s √©conomiques sont r√©ellement durables. Il a 6 objectifs principaux.\nLes labels ISR (pour des investissements prenant en compte les crit√®res ESG), Greenfin (pour des projects plus verts, excluant les √©nergies fossiles) et Finansol (une finance solidaire pour prendre en compte l‚Äôaspect ¬´social¬ª), qui garantissent des investissements align√©s avec la finance responsable.\nLes obligations r√©glementaires (SFDR - R√®glement, CSRD - Directive), qui imposent un reporting extra-financier aux entreprises et investisseurs.\n\nCependant, des d√©fis subsistent, comme le greenwashing, le manque d‚Äôuniformisation des notations ESG et la crainte d‚Äôun impact n√©gatif sur la performance financi√®re. Malgr√© cela, la finance durable est devenue un levier incontournable pour assurer une √©conomie plus r√©siliante et √©thique. La finance durable n‚Äôest plus une option mais une n√©cessit√© pour r√©pondre aux d√©fis climatiques et sociaux tout en maintenant la stabilit√© du syst√®me financier mondial.\n\n\nLa finance durable appliqu√©e √† l‚Äôinvestissement\nLa finance durable ne se limite pas √† des principes th√©oriques ; elle se traduit par des strat√©gies d‚Äôinvestissement concr√®tes visant √† int√©grer les crit√®res ESG dans la gestion des actifs financiers. Pour cela, plusieurs approches existent.\n\nLes strat√©gies d‚Äôinvestissement responsable\n\nL‚ÄôExclusion : √âliminer certains secteurs ou entreprises jug√©s non responsables (ex : tabac, √©nergies fossiles, armement).\nLe Best-in-Class : S√©lectionner les entreprises ayant les meilleures pratiques ESG dans chaque secteur, sans exclure de domaines sp√©cifiques.\nL‚ÄôEngagement actionnarial : Influer sur les entreprises en tant qu‚Äôactionnaire via le vote en assembl√©e g√©n√©rale et le dialogue.\nL‚ÄôImpact Investing : Financer directement des entreprises ou projets ayant un impact environnemental ou social positif mesurable. Les strat√©gies les plus actives et efficaces sont l‚ÄôEngagement Actionnarial et l‚ÄôImpact Investing, car elles permettent d‚Äôinfluencer directement les pratiques ESG et de cr√©er un impact mesurable sur l‚Äô√©conomie.\n\n\n\nL‚Äôimportance des donn√©es ESG\nL‚Äôinvestissement responsable repose sur des donn√©es ESG fiables pour √©valuer la performance extra-financi√®re des entreprises. Cependant, plusieurs limites persistent : - Manque de standardisation : Chaque fournisseur (MSCI, Sustainalytics, Moody‚Äôs ESG) utilise des m√©thodologies diff√©rentes, rendant les comparaisons complexes. - Risque de biais et greenwashing : Certaines entreprises embellissent leurs rapports ESG pour obtenir de meilleures notes. - Absence de transparence : Les investisseurs doivent analyser minutieusement les sources de donn√©es avant de prendre des d√©cisions.\nDes solutions existent pour am√©liorer la fiabilit√© des notations ESG : - Harmonisation des standards via des r√©gulations comme la directive CSRD. - Audit ind√©pendant des donn√©es ESG pour √©viter les manipulations. - Utilisation de l‚ÄôIA et du Big Data pour analyser des sources plus diversifi√©es et d√©tecter les incoh√©rences.\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôint√©gration de l‚ÄôESG est une approche dynamique et √©volutive, qui d√©pend du niveau d‚Äôengagement souhait√© par l‚Äôinvestisseur.L‚ÄôEngagement actionnarial et l‚ÄôImpact Investing sont les strat√©gies les plus efficaces, car elles permettent d‚Äôinfluencer l‚Äô√©conomie de mani√®re proactive. La fiabilit√© des donn√©es ESG doit encore √™tre renforc√©e pour garantir une transparence totale dans la finance durable.\n\n\n\n\n\nPrise en compte du risque climatique dans la gestion d‚Äôactifs\nLe changement climatique est devenu un facteur cl√© dans la gestion d‚Äôactifs. Les investisseurs doivent int√©grer ces risques pour prot√©ger leurs portefeuilles, se conformer aux r√©gulations et capter les opportunit√©s de la transition √©nerg√©tique.\n\nImpact du risque climatique sur les actifs financiers\nDeux types de risques √† prendre en compte :\n\nRisque physique ‚Üí Catastrophes naturelles (ouragans, inondations, s√©cheresses) qui endommagent les infrastructures et r√©duisent la valeur des actifs.\nRisque de transition ‚Üí R√©gulations environnementales et √©volutions du march√© qui d√©valorisent les industries polluantes (p√©trole, charbon, transport intensif, etc.). Exemple : L‚Äôimmobilier en zone inondable peut perdre de la valeur, et les entreprises p√©troli√®res risquent de voir leurs co√ªts augmenter avec la taxe carbone.\n\n\n\nMesure et gestion des risques climatiques en finance\nIndicateurs utilis√©s : - Score ESG ‚Üí √âvaluation des crit√®res environnementaux, sociaux et de gouvernance. - √âmissions de GES (Scope 1, 2, 3) ‚Üí Mesure l‚Äôempreinte carbone d‚Äôune entreprise ou d‚Äôun portefeuille. - Alignement 2¬∞C ‚Üí V√©rifie si un investissement est compatible avec les objectifs climatiques de l‚ÄôAccord de Paris. - Stress-tests climatiques ‚Üí Simulent l‚Äôimpact des sc√©narios climatiques sur les portefeuilles d‚Äôinvestissement.\n\n\nOutils de gestion des risques :\n\nDiversification des actifs ‚Üí R√©duire l‚Äôexposition aux secteurs vuln√©rables au climat.\nExclusion des industries polluantes ‚Üí √âviter les investissements dans le charbon et le p√©trole.\nInvestissement dans des actifs verts (Green Bonds, infrastructures bas carbone) ‚Üí Financer la transition √©nerg√©tique.\nEngagement actionnarial ‚Üí Influer sur les entreprises en votant en assembl√©e g√©n√©rale pour exiger des strat√©gies bas carbone.\n\n\n\nRaisons pour int√©grer les sc√©narios climatiques en gestion de portefeuille ?\n\n√âviter les pertes financi√®res ‚Üí Anticiper l‚Äôimpact du climat sur les entreprises et secteurs sensibles.\nSe conformer aux r√©gulations ‚Üí Respecter les normes SFDR, CSRD et Taxonomie Europ√©enne.\nCapter les opportunit√©s d‚Äôinvestissement ‚Üí Financer des entreprises et projets align√©s avec la transition √©nerg√©tique.\n\nExemple : Un investisseur qui anticipe les r√©glementations sur les √©nergies fossiles pourra transf√©rer ses capitaux vers les √©nergies renouvelables, √©vitant ainsi des pertes et profitant de la croissance du secteur.\n\n\n\n\n\n\nNote\n\n\n\nLe risque climatique est un enjeu central en gestion d‚Äôactifs. Il affecte la valorisation des entreprises, les r√©gulations financi√®res et les d√©cisions d‚Äôinvestissement. Les investisseurs doivent int√©grer ces risques dans leur gestion de portefeuille, en utilisant des indicateurs ESG, des stress-tests climatiques et des strat√©gies de diversification. Ne pas prendre en compte ces sc√©narios expose √† des actifs d√©valoris√©s et √† des pertes financi√®res √† long terme.\n\n\n\n\n\n\n\n\nTake away\n\n\n\nL‚Äôinvestissement responsable int√®gre les crit√®res ESG (Environnement, Social, Gouvernance) dans la gestion d‚Äôactifs afin de concilier performance financi√®re et impact durable. Il repose sur diff√©rentes strat√©gies : exclusion des secteurs controvers√©s, Best-in-Class, Engagement actionnarial et Impact Investing. Les r√©gulations, comme la Taxonomie Europ√©enne, la SFDR et la CSRD, imposent plus de transparence aux entreprises et investisseurs pour lutter contre le greenwashing et favoriser la transition vers une √©conomie bas carbone. Le risque climatique, divis√© en risques physiques (catastrophes naturelles) et risques de transition (r√©gulations et √©volutions de march√©), influence fortement la valorisation des actifs. Les outils comme l‚Äôalignement 2¬∞C et les stress-tests climatiques aident √† √©valuer l‚Äôexposition des portefeuilles. Enfin, les Green Bonds et autres financements durables permettent de soutenir la transition √©nerg√©tique et d‚Äôorienter les capitaux vers des projets √† fort impact environnemental positif. L‚Äôinvestissement responsable est donc un levier cl√© pour une finance plus durable et r√©siliente.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Cheryl KOUADIO\n\n\nEtudiante en gestion des risques financiers\n\n\nEmail : cheryl.s.kouadio@gmail.com\n\n\nCV (Fran√ßais/English) : cv_cheryl_kouadio_fr.pdf / cv_cheryl_kouadio_en.pdf\n\n\nGithub :  github.com/cheryl-kdio\n\n\nLinkedin : /in/cheryl-kouadio-251815206 \n\n\nMedium :  medium.com/@cheryl.s.kouadio\n\n\n\n\n\nA propos de moi\n\nJe suis Cheryl Kouadio, √©tudiante en gestion des risques bancaires √† l‚ÄôENSAI. Je me sp√©cialise dans la mod√©lisation des risques financiers, en particulier les risques de march√© et de cr√©dit, ainsi que dans l‚Äô√©tude des normes financi√®res, comptables et r√©glementaires encadrant le syst√®me bancaire (B√¢le III, IFRS 9, etc.).\nCe site web, con√ßu et g√©n√©r√© avec Quarto, s‚Äôadresse principalement aux √©tudiants de l‚ÄôENSAI et a pour vocation d‚Äôoffrir un soutien √† ceux qui, comme moi, ont √©t√© confront√©s √† des d√©fis acad√©miques au cours de leur formation, notamment en deuxi√®me et troisi√®me ann√©e. Il ne pr√©tend en aucun cas se substituer √† l‚Äôenseignement dispens√© par nos professeurs, dont la rigueur et l‚Äôexpertise sont essentielles. Son objectif est plut√¥t de compl√©ter leur travail en partageant mes exp√©riences personnelles et les projets que j‚Äôai r√©alis√©s.\nAu-del√† de son utilit√© pour les √©tudiants, ce site constitue √©galement une ressource pr√©cieuse pour les professionnels souhaitant revisiter certains concepts cl√©s en gestion des risques ou m√™me en statistiques. Il propose des exemples concrets et des pistes de r√©flexion adapt√©es aux probl√©matiques actuelles, afin d‚Äôaider chacun √† mieux appr√©hender ses projets et √† relever ses propres d√©fis acad√©miques et professionnels.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html",
    "href": "3A/value-at-risk/var_evt.html",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes]",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap. Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, on peut utiliser des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold.\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\nneg_data_train = -data_train\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']\nneg_data_test = -data_test"
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "href": "3A/value-at-risk/var_evt.html#i.-impl√©mentation-de-la-var-avec-la-th√©orie-des-valeurs-extr√™mes",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes]",
    "section": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes",
    "text": "I. Impl√©mentation de la VaR avec la th√©orie des valeurs extr√™mes\n\nI.1. VaR TVE : Approche Maxima par bloc\nL‚Äôapproche des Block Maxima (BM) est une m√©thode mod√©lise les maxima des rendements sur des blocs de taille fixe \\(s\\) en utilisant la distribution GEV. Le seuil de confiance \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nNous allons dans ce projet une taille de blocs \\(s = 21\\) jours ouvr√©s comme ce qui souvent utilis√© en pratique. De ce fait, nous parvenons √† construire 239 blocs de taille 21 et un bloc de taille De fait, la Value-at-Risk sur un horizon 1 et pour un niveau de confiance $ _{}$ est :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV (\\(\\hat \\mu, \\hat \\sigma, \\hat \\xi\\)) estim√©e.\n\n\nI.1.1. Construction de l‚Äô√©chantillon de maxima sur data_train\n\nimport numpy as np\nimport pandas as pd\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\nextremes = get_extremes(neg_data_train, block_size=21, min_last_block=0.6)\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"grey\")\nplt.plot(-extremes,\".\", color=\"red\") # \nplt.title(\"Series des rendements du CAC 40 avec les pertes extr√™mes\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rendements\")\nplt.show()\n\n\n\n\n\n\n\n\nPour avoir une id√©e de la distribution GEV de la serie des pertes maximales de rendements du CAC 40 pour \\(s=21\\), nous utilisons un Gumbel plot qui est un outil graphique pour juger de l‚Äôhypoth√®se \\(\\xi=0\\), i.e.¬†la distribution GEV se r√©duit √† la distribution de Gumbel.\nPour le construire, nous devons suivre les √©tapes suivantes :\n\ncalculer l‚Äôabscisse avec la s√©rie des maximas ordon√©es \\(R_{(1)} \\leq R_{(2)} \\leq \\ldots \\leq R_{(n)}\\).\ncalculer l‚Äôordonn√©e de la mani√®re suivante :\n\n\\[\n- log(-log(\\frac{i - 0.5}{k})), \\quad i = 1, \\ldots, k.\n\\]\nLorsque la distribution adapt√©e est celle de Gumbel alors le Gumbel plot est lin√©aire. Dans notre cas, nous constatons une courbure ce qui nous pousse √† conclure qu‚Äôune distribution Gumbel n‚Äôest pas adapt√©e dans la mod√©lisation des maxima des pertes de rendements du CAC 40. Une distribution Fr√©chet ou de Weibull serait plus adapt√©e.\n\nquantiles_theoriques_gumbel = []\nk=len(extremes)\nfor i in range(1,len(extremes)+1):\n    val = -np.log(-np.log((i-0.5)/k))\n    quantiles_theoriques_gumbel.append(val)\n\n# Tracer le Gumbel plot\nplt.scatter(quantiles_theoriques_gumbel, np.sort(extremes), marker='o')\nplt.title('Rendements CAC 40 - Gumbel plot')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.1.2. Estimation des param√®tres de la loi de GEV\nEn estimant les param√®tres de la loi GEV, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = -0.15, \\mu=0.02, \\sigma=0.01\\). Nous constatons par ailleurs que le param√®tre de forme \\(\\xi\\) est n√©gatif ce qui est coh√©rent avec notre observation pr√©c√©dente.\n\nfrom scipy.stats import genextreme as gev\n\nparams_gev = gev.fit(extremes)\n\nshape, loc, scale = params_gev\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV\")\nprint(\"=\"*50)\nprint(f\"Shape (xi) = {shape:.2f}\")\nprint(f\"Loc (mu) =  {loc:.2f}\")\nprint(f\"Scale (sigma) = {scale:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParam√®tres estim√©s de la distribution GEV\n==================================================\nShape (xi) = -0.15\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\nPour accorder plus de poids √† cette observation, nous avons calcul√© un intervalle de confiance profil√© √† 95% pour le param√®tre de forme \\(\\xi\\). Pour ce faire, nous avons suivi les √©tapes suivantes : 1. Estimation des param√®tres par maximum de vraisemblance : Nous avons estim√© \\(\\hat{\\xi}\\), \\(\\hat{\\mu}\\) et \\(\\hat{\\sigma}\\) en maximisant la log-vraisemblance de la loi GEV.\n\nConstruction du profil de vraisemblance : Nous avons fix√© \\(\\xi\\) √† diff√©rentes valeurs autour de \\(\\hat{\\xi}\\) et, pour chacune, r√©estim√© \\(\\mu\\) et \\(\\sigma\\) afin d‚Äôobtenir une log-vraisemblance profil√©e.\nSeuil bas√© sur le test du rapport de vraisemblance : Le seuil critique est d√©termin√© par la statistique $ ^2(1) $ :\n\\[\n\\mathcal{L}_{\\max} - \\frac{\\chi^2_{0.95, 1}}{2}\n\\]\nD√©termination des bornes de l‚ÄôIC : L‚Äôintervalle est form√© par les valeurs de \\(\\xi\\) pour lesquelles la log-vraisemblance reste au-dessus de ce seuil.\n\nCette approche permet une meilleure prise en compte de l‚Äôincertitude en √©vitant les approximations asymptotiques classiques. La mod√©lisation des maxima des pertes de rendements du CAC 40 par une distribution de Weibull serait plus adapt√©e.\nNous obtenons ainsi un intervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\). Comme 0 n‚Äôappartient pas √† cet intervalle, nous pouvons rejeter l‚Äôhypoth√®se \\(\\xi=0\\). De ce fait, la distribution de Weibull est plus adapt√©e pour mod√©liser les maxima des pertes de rendements du CAC 40 car \\(\\xi\\) est n√©gatif.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\n# Fonction de log-vraisemblance\ndef gev_neg_log_likelihood(params, shape_fixed, data):\n    \"\"\"\n    Calcule la log-vraisemblance n√©gative de la distribution GEV\n    en fixant le param√®tre 'shape'.\n    \"\"\"\n    loc, scale = params\n    if scale &lt;= 0:  # Contrainte pour √©viter des valeurs invalides\n        return np.inf\n    return -np.sum(gev.logpdf(data, shape_fixed, loc=loc, scale=scale))\n\n# Log-vraisemblance maximale\nlog_likelihood_max = -gev_neg_log_likelihood([loc, scale], shape, extremes)\n\n# Calcul des IC profil√©s pour le param√®tre shape\nshape_grid = np.linspace(shape - 0.4, shape + 0.4, 50)  # Plage autour de la valeur estim√©e\nprofile_likelihood = []\n\nfor s in shape_grid:\n    # R√©optimiser loc et scale en fixant shape\n    result = minimize(\n        gev_neg_log_likelihood,\n        x0=[loc, scale],  # Initial guess for loc and scale\n        args=(s, extremes),  # Fixer 'shape' √† la valeur actuelle\n        bounds=[(None, None), (1e-5, None)],  # Contraintes sur loc et scale\n        method='L-BFGS-B'\n    )\n    if result.success:\n        profile_likelihood.append(-result.fun)\n    else:\n        profile_likelihood.append(np.nan)\n\n# Calcul du seuil pour les IC\nchi2_threshold = log_likelihood_max - chi2.ppf(0.95, 1) / 2\n\n# D√©terminer les bornes des IC\nprofile_likelihood = np.array(profile_likelihood)\nvalid_points = np.where(profile_likelihood &gt;= chi2_threshold)[0]\nif len(valid_points) &gt; 0:\n    lower_bound = shape_grid[valid_points[0]]\n    upper_bound = shape_grid[valid_points[-1]]\n    print(f\"IC profil√© pour shape: [{lower_bound:.3f}, {upper_bound:.3f}]\")\nelse:\n    print(\"Impossible de d√©terminer des IC profil√©s avec les param√®tres actuels.\")\n\n# Trac√© du profil de log-vraisemblance\nplt.plot(shape_grid, profile_likelihood, label=\"Log-likelihood\")\nplt.axhline(chi2_threshold, color='red', linestyle='--', label=\"95% Confidence threshold\")\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Profile log-likelihood\")\nplt.title(\"Profile log-likelihood for shape parameter\")\nplt.legend()\nplt.show()\n\nIC profil√© pour shape: [-0.287, -0.042]\n\n\n\n\n\n\n\n\n\n\na. Validation ex-ante\nOn remarque la loi GEV estim√©e par une weibull semble coller √† la distribution des rendements extr√™mes du CAC 40. De plus, en utilisant un QQ-plot, nous constatons que les quantiles th√©oriques de la GEV-Weibull et empiriques sembelnt align√©s sauf pour les quantiles √©l√©v√©s o√π l‚Äôon constate un d√©crochage.\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\nplt.title(\"Ajustement de la distribution GEV\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.001,1, 0.001)\nquantiles_empiriques_TVE = np.quantile(extremes, niveaux_quantiles) \nquantiles_theoriques_GEV = gev.ppf(niveaux_quantiles, shape, loc = loc, scale = scale)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV\")\nplt.xlabel('Quantiles th√©oriques (Loi GEV)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nb. Calcul de la VaR TVE par MB\nPour calculer la VaR TVE pour un horizon de 1jour par MB, nous utilisons la formule suivante :\n\\[\n\\text{VaR}_h(\\alpha_{\\text{VaR}}) = G^{-1}_{(\\hat \\mu, \\hat \\sigma, \\hat \\xi)}(\\alpha_{\\text{GEV}}),\n\\]\no√π G est la fonction de r√©partition de la GEV\\((\\hat \\mu, \\hat \\sigma, \\hat \\xi)\\) estim√©e, et \\(\\alpha_{\\text{GEV}}\\) est ajust√© pour correspondre √† l‚Äôhorizon temporel de la VaR via la relation :\n\\[\n\\frac{1}{1-\\alpha_{\\text{VaR}}} = s \\times \\frac{1}{1-\\alpha_{\\text{GEV}}}.\n\\]\nPour convertir la VaR √† horizon 1jour en VaR √† horizon T jours, la m√©thode de scaling soul√®ve quelques questions, car elle repose essentiellement sur la normalit√© et l‚Äôind√©pendance des rendements ce qui n‚Äôest pas le cas en pratique. De ce fait, nous utiliserons la m√©thode alternative reposant sur la th√©orie des valeurs extr√™mes.\ny revenir\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape, loc, scale)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 3.3274%\nLa VaR TVE pour h=10j et alpha=0.99 est : 20.5167%\n\n\n\n\n\nI.1.3. Estimation des param√®tres de la loi de EV\nBien que l‚Äôintervalle de confiance √† 95% pour le param√®tre de forme \\(\\xi\\) de \\([-0.284, -0.039]\\) ne contienne pas 0, nous avons tout de m√™me estim√© les param√®tres de la loi EV pour comparer les r√©sultats avec ceux de la loi GEV. En estimant tout de m√™me les param√®tres de la loi EV, nous obtenons les param√®tres suivants : \\(\\mu=0.02, \\sigma=0.01, \\xi=0\\).\nNous constatons que la loi EV ne semble pas mal s‚Äôadapter √† la distribution des rendements extr√™mes du CAC 40.\n\nfrom scipy.stats import gumbel_r\n\nparams_gumbel = gumbel_r.fit(extremes)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*50)\nprint(\"Param√®tres estim√©s de la distribution GEV GUMBEL\")\nprint(\"=\"*50)\nprint(f\"Loc (mu) =  {params_gumbel[0]:.2f}\")\nprint(f\"Scale (sigma) = {params_gumbel[1]:.2f}\")\nprint(\"=\"*50)\n\n==================================================\nParam√®tres estim√©s de la distribution GEV GUMBEL\n==================================================\nLoc (mu) =  0.02\nScale (sigma) = 0.01\n==================================================\n\n\n\nplt.figure(figsize=(10, 5))\nplt.hist(extremes, bins=30, density=True, label='Maxima observ√©es')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GEV ajust√©e\np_gev = gev.pdf(x, *params_gev)\nplt.plot(x, p_gev, 'k', linewidth=2, label='GEV ajust√©e')\n\n# Densit√© Gumbel ajust√©e\np_gumbel = gumbel_r.pdf(x, *params_gumbel)\nplt.plot(x, p_gumbel, 'r', linewidth=2, label='Gumbel ajust√©e')\ntitle = \"Comparaison GEV vs Gumbel\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nquantiles_theoriques_Gumb = gumbel_r.ppf(niveaux_quantiles, *params_gumbel)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(quantiles_theoriques_GEV, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_GEV, quantiles_theoriques_GEV, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi GEV Weibull\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(quantiles_theoriques_Gumb, quantiles_empiriques_TVE)\nplt.plot(quantiles_theoriques_Gumb, quantiles_theoriques_Gumb, color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title(\"QQ Plot d'une mod√©lisation par loi Gumbel\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nalpha = 0.99\nvar_BM_train,alpha_bm = BM_var(0.99, 21, shape=0, loc=params_gumbel[0], scale=params_gumbel[1])\n\nprint(f\"La VaR TVE Gumbel pour h=1j et alpha={alpha} est : {var_BM_train:.4%}\")\nprint(f\"La VaR TVE Gumbel pour h=10j et alpha={alpha} est : {(10**alpha_bm)*var_BM_train:.4%}\")\n\nLa VaR TVE Gumbel pour h=1j et alpha=0.99 est : 3.3513%\nLa VaR TVE Gumbel pour h=10j et alpha=0.99 est : 20.6638%\n\n\nDe plus, les r√©sultats en terme de VaR sont tr√®s proches entre les deux mod√®les."
  },
  {
    "objectID": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "href": "3A/value-at-risk/var_evt.html#i.2.-var-tve-approche-peak-over-threshold",
    "title": "TP2:M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes]",
    "section": "I.2. VaR TVE : Approche Peak over threshold",
    "text": "I.2. VaR TVE : Approche Peak over threshold\n\nI.2.1. Choix du seuil u\nCette m√©thode est bas√©e sur la mod√©lisation de la distribution des exc√®s au-dessus d‚Äôun seuil √©lev√© de log-rendement n√©gatif (\\(u\\)), seuil d√©termin√© de mani√®re subjective √† partir de l‚Äôanalyse du mean residual life plot, en ajustant une distribution de Pareto g√©n√©ralis√©e (GPD). Dans le mean residual life plot, si les exc√®s au-del√† de ùíñ suivent une loi GPD, alors le mean-excess plot a un comportement lin√©aire. On cherche alors la valeur du seuil $$ pour laquelle le mean-excess plot est lin√©aire. Nous ne privil√©gions pas les seuils \\(u\\) √©lev√©s puisque la moyenne est faite sur peu d‚Äôobservations.\nNous allons choisir un seuil \\(u = 0.03\\) pour lequel le mean residual life plot est lin√©aire. Nous allons ensuite ajuster une distribution GPD pour les exc√®s au-dessus de ce seuil.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, genpareto\n\ndef mean_residual_life_plot(data, tlim=None, pscale=False, nt=100, conf=0.95, return_values=False):\n    \"\"\"\n    Trace le Mean Residual Life (MRL) Plot pour identifier un seuil optimal pour une GPD.\n\n    Param√®tres :\n    - data : array-like, donn√©es d'entr√©e.\n    - tlim : tuple (min, max), limites des seuils (si None, calcul√© automatiquement).\n    - pscale : bool, si True, utilise des quantiles au lieu de valeurs absolues.\n    - nt : int, nombre de seuils √† consid√©rer.\n    - conf : float, niveau de confiance pour l'intervalle (ex: 0.95 pour 95%).\n\n    Retourne :\n    - Un graphique MRL avec l'intervalle de confiance.\n    \"\"\"\n\n    # Trier et filtrer les donn√©es\n    data = np.sort(data[~np.isnan(data)])\n    nn = len(data)\n    if nn &lt;= 5:\n        raise ValueError(\"Les donn√©es contiennent trop peu de valeurs valides.\")\n\n    # D√©finition des seuils\n    if tlim is None:\n        tlim = (data[0], data[nn - 5])  # √âvite les 4 plus grandes valeurs\n\n    if np.all(data &lt;= tlim[1]):\n        raise ValueError(\"La borne sup√©rieure du seuil est trop √©lev√©e.\")\n\n    if pscale:\n        # Travailler en quantiles au lieu de valeurs absolues\n        tlim = (np.mean(data &lt;= tlim[0]), np.mean(data &lt;= tlim[1]))\n        pvec = np.linspace(tlim[0], tlim[1], nt)\n        thresholds = np.quantile(data, pvec)\n    else:\n        thresholds = np.linspace(tlim[0], tlim[1], nt)\n\n    # Initialiser les r√©sultats\n    mean_excess = np.zeros(nt)\n    lower_conf = np.zeros(nt)\n    upper_conf = np.zeros(nt)\n\n    # Calcul du Mean Excess et de l'IC\n    for i, u in enumerate(thresholds):\n        exceedances = data[data &gt; u] - u  # Exc√®s au-dessus du seuil\n        if len(exceedances) == 0:\n            mean_excess[i] = np.nan\n            lower_conf[i] = np.nan\n            upper_conf[i] = np.nan\n            continue\n        \n        mean_excess[i] = np.mean(exceedances)\n        std_dev = np.std(exceedances, ddof=1)\n        margin = norm.ppf((1 + conf) / 2) * std_dev / np.sqrt(len(exceedances))\n        \n        lower_conf[i] = mean_excess[i] - margin\n        upper_conf[i] = mean_excess[i] + margin\n\n    # Trac√© du Mean Residual Life Plot\n    plt.figure(figsize=(8, 5))\n    plt.plot(thresholds, mean_excess, label=\"Mean Excess\", color='blue')\n    plt.fill_between(thresholds, lower_conf, upper_conf, color='blue', alpha=0.2, label=f\"{conf*100:.0f}% Confidence Interval\")\n    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    plt.xlabel(\"Threshold\" if not pscale else \"Threshold Probability\")\n    plt.ylabel(\"Mean Excess\")\n    plt.title(\"Mean Residual Life Plot\")\n    plt.legend()\n    plt.show()\n    if return_values:\n        return thresholds, mean_excess, lower_conf, upper_conf\n\nmean_residual_life_plot(neg_data_train, tlim=[0,0.08])\n\n# regarder quantile √† 5%\n\n\n\n\n\n\n\n\n\n\nI.2.2. Estimation des param√®tres de la loi GPD\nEn estimant les param√®tres de la loi GPD, nous utilisons la m√©thode du maximum de vraisemblance. Les param√®tres estim√©s par maximisation de la fonction de vraisemblance sont les suivants \\(\\xi = 1.33, \\mu \\approx 0.00, \\sigma=0.01\\). De ce fait, la distribution de Pareto g√©n√©ralis√©e est adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\).\n\nu = 0.03\nexcess_values = [value - u for value in neg_data_train if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\n\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = 1.33\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.01\n\n\n\n\nI.2.3. Validation ex-ante\nEn comparant la distribution GPD estim√©e et la distribution empirique des exc√®s, nous constatons que la distribution ne semble pas correspondre. De plus, le QQ-plot estim√© indique que les quantiles th√©oriques de la loi GPD sont beaucoup plus grands que les quantiles empiriques observ√©s dans notre distribution des exc√®s. Nous concluons que la distribution GPD n‚Äôest pas adapt√©e pour mod√©liser les exc√®s au-dessus du seuil \\(u = 0.03\\). Cela peut √™tre d√ª √† un mauvais choix du seuil \\(u\\), une analyse plus aprofondie aurait √©t√© n√©cessaire pour choisir un seuil plus adapt√©.\n\nplt.figure(figsize=(10, 5))\nplt.hist(excess_values, bins=30, density=True, label='Donn√©es observ√©es des exc√®s')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\n\n# Densit√© GPD ajust√©e\np_gpd = genpareto.pdf(x, *params_gpd)\nplt.plot(x, p_gpd, 'r', linewidth=2, label='GPD ajust√©e')\n\ntitle = \"Distribution GPD\"\nplt.title(title)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nniveaux_quantiles = np.arange(0.01, 1, 0.01)\nquantiles_empiriques_POT = np.quantile(excess_values, niveaux_quantiles)\nquantiles_theoriques_GDP = genpareto.ppf(niveaux_quantiles, *params_gpd)\n\nplt.figure(figsize=(10, 5))\n\nplt.scatter(quantiles_theoriques_GDP, quantiles_empiriques_POT)\nplt.title(\"QQ Plot d'une mod√©lisation par loi GPD\")\nplt.xlabel('Quantiles th√©oriques')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2.3. Calcul de la VaR POT par POT\nLa Value-at-Risk sur un horizon 1 jour et pour un niveau de confiance \\({\\alpha}\\) est alors obtenue par la formule :\n\\[\n\\text{VaR}_h(\\alpha) = \\hat{H}_{(\\hat{\\sigma}, \\hat{\\xi}) }(\\alpha_{\\text{POT}})^{-1} + u,\n\\]\no√π \\(\\hat{H}(\\hat{\\sigma}, \\hat{\\xi})\\) est la fonction de r√©partition de la GPD(\\(\\hat{\\sigma},\\hat{\\xi}\\)) estim√©e, \\(\\alpha_{\\text{POT}}\\) est le quantile ajust√©, n√©cessaire pour adapter le calcul de la VaR dans le cadre de la distribution GPD.\nComme on ne se concentre que sur l‚Äô√©chantillon des exc√®s dans cette mod√©lisation, l‚Äôestimation de la VaR √† partir de la GPD ne doit pas se faire au niveau \\(\\alpha\\), mais √† un niveau ajust√© \\(\\alpha_{\\text{POT}}\\) d√©fini par la relation suivante :\n\\[\n1 - \\alpha_{\\text{POT}} = \\frac{n}{N_u} \\times (1 - \\alpha),\n\\]\no√π \\(n\\) repr√©sente le nombre total d‚Äôobservations, \\(N_u\\) correspond au nombre d‚Äôexc√®s au-del√† du seuil \\(u\\),\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\nalpha = 0.99\nvar_POT_train,alpha_pot = POT_var(neg_data_train, alpha, u,*params_gpd)\n\nprint(f\"La VaR TVE pour h=1j et alpha={alpha} est : {var_POT_train:.4%}\")\nprint(f\"La VaR TVE pour h=10j et alpha={alpha} est : {(10**alpha_pot)*var_POT_train:.4%}\")\n\nLa VaR TVE pour h=1j et alpha=0.99 est : 4.3634%\nLa VaR TVE pour h=10j et alpha=0.99 est : 17.3572%"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html",
    "href": "3A/value-at-risk/var_classiques.html",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "",
    "text": "Ce TP est fait dans le but de mod√©liser la Value at Risk qui est une mesure de risque financier. La Value at Risk (VaR) est une mesure essentielle du risque de march√© qui estime la perte potentielle maximale d‚Äôun portefeuille sur un horizon h donn√©, avec un certain niveau de confiance \\(\\alpha\\). La VaR est utilis√©e par les institutions financi√®res et les gestionnaires de risques pour √©valuer l‚Äôexposition aux pertes extr√™mes et ajuster leurs strat√©gies d‚Äôinvestissement. Elle est d√©finie comme suit :\n\\[VaR_{\\alpha}(h) = inf \\{ l \\in \\mathbb{R} | P(PnL \\geq -l) \\geq 1 - \\alpha \\}\\]\nLe PnL repr√©sente le profit and loss, c‚Äôest-√†-dire la variation de la valeur du portefeuille. Dans notre cas, nous utilserons les rendements logarithmiques des actifs financiers, qui est stationnaire, pour calculer la VaR. Les rendements logarithmiques sont calcul√©s comme suit :\n\\[r_t = log(\\frac{P_t}{P_{t-1}}) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}.\\]\nCette mesure est pr√©f√©r√©e aux rendements simples car sa d√©composition en termes additifs permet de mieux mod√©liser les variations de prix des actifs financiers. De plus, en supposant la distribution identique et ind√©pendante des rendements, on peut facilement d√©terminer sa loi de probabilit√© et calculer la VaR.\nDans ce projet, nous explorerons plusieurs m√©thodes pour calculer la VaR, en tenant compte de la nature des rendements financiers et des hypoth√®ses sous-jacentes :\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tqdm import tqdm\nfrom scipy.stats import bootstrap\n\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\ndata.head()\n\n\n\n\n\n\n\nPrice\nClose\nHigh\nLow\nOpen\nVolume\nlog_return\n\n\nTicker\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n^FCHI\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n1990-03-02\n1860.0\n1860.0\n1831.0\n1831.0\n0\n0.015168\n\n\n1990-03-05\n1874.0\n1874.0\n1862.0\n1866.0\n0\n0.007499\n\n\n1990-03-06\n1872.0\n1875.0\n1866.0\n1869.0\n0\n-0.001068\n\n\n1990-03-07\n1880.0\n1881.0\n1874.0\n1874.0\n0\n0.004264\n\n\n1990-03-08\n1917.0\n1923.0\n1891.0\n1891.0\n0\n0.019490"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "href": "3A/value-at-risk/var_classiques.html#ii.1.-statistiques-descriptives",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.1. Statistiques descriptives",
    "text": "II.1. Statistiques descriptives\nNous constatons que dans la p√©riode d‚Äôentrainement est plus volatile (std=1.39%). Cela s‚Äôexplique par le fait que la p√©riode d‚Äôentrainement prend en compte deux crises majeures : la crise des subprimes et la crise du Covid-19. Cela peut √©galement se voir √† travers les clusters de volatilit√© observables √† la suite de ces crises. Dans la p√©riode de test, aucun √©v√®nement majeur n‚Äôest observ√©, avec une plus faible volatilit√© observ√©e (std=0.08%).\nOn s‚Äôattend √† ce que la VaR entrain√©e sur la p√©riode d‚Äôentrainement performe tr√®s bien sur la p√©riode de test, mais on s‚Äôattend √©galement √† ce que la VaR entrain√©e sur la p√©riode de test performe moins bien sur la p√©riode d‚Äôentrainement.\n\nplt.figure(figsize=(10, 4))\nplt.plot(data_train, label='Train', color='grey')\nplt.plot(data_test, label='Test', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Donn√©es d'entrainement et de test\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_train.describe()\n\ncount    3523.000000\nmean        0.000153\nstd         0.013953\nmin        -0.130983\n25%        -0.006099\n50%         0.000580\n75%         0.006855\nmax         0.096169\nName: log_return, dtype: float64\n\n\n\ndata_test.describe()\n\ncount    586.000000\nmean       0.000292\nstd        0.008947\nmin       -0.036484\n25%       -0.004763\n50%        0.000642\n75%        0.005612\nmax        0.041504\nName: log_return, dtype: float64"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "href": "3A/value-at-risk/var_classiques.html#ii.2.-var-non-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.2. VaR non param√©trique",
    "text": "II.2. VaR non param√©trique\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de scaling. 2. Approche bootstrap : On tire al√©atoirement des √©chantillons de rendements pass√©s avec remise, puis on prend le quantile empirique d‚Äôordre \\(\\alpha\\) pour calculer la VaR de chaque √©chantillon. La VaR finale est la moyenne des VaR obtenues.\n\nII.2.1. Historique\n\n# Objectif : impl√©menter une fonction calculant la VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1- alpha))\n\n\n# Calcul de la VaR historique sur l'√©chantillon d'entrainement pour h=1j et alpha=0.99\nalpha = 0.99\nvar_hist_train = historical_var(data_train, alpha=alpha)\nprint(f\"La VaR historique pour h=1j et alpha=0.99 est : {var_hist_train:.4%}\")\n\nLa VaR historique pour h=1j et alpha=0.99 est : 4.0850%\n\n\nOn constate que la VaR historique pour une horizon de 1 jour et un niveau de confiance de 99% est de -4,09%. De ce fait, la perte maximale que l‚Äôon peut subir avec un niveau de confiance de 99% sur un jour est de 4,09%. Autrement dit, il y a 1 chance sur 100 que la perte soit sup√©rieure √† 4,09%. Cette perte peut se produire 2 √† 3 ans fois en une ann√©e (252 jours de trading).\n\n\nII.2.2. Bootstrap\nPour l‚Äôimpl√©mentation de la VaR bootstrap, nous faisons le choix de faire un tirage de taille n=la taille de la s√©rie des rendements, avec remise. Ce choix est fait pour des raisons de simplicit√©. En ce qui concerne le choix du nombre d‚Äô√©chantillons, nous allons observer l‚Äô√©volution de de l‚Äôestimation de la VaR en fonction du nombre d‚Äô√©chantillons. Nous limiterons √† des √©chantillons compris entre 1000 et 10000, pour des raisons de temps computationnels, en ayant conscience que plus le nombre d‚Äô√©chantillons est grand, plus l‚Äôestimation de la VaR sera pr√©cise.\n\n# Objectif : impl√©menter une fonction calculant la VaR bootstrap et un IC\n\ndef bootstrap_var(data, alpha=0.99, M=1000, seuil=0.05):\n    \"\"\"\n    Calcul de la VaR bootstrap\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance de la VaR\n    n : le nombre de simulations\n    seuil : le seuil de l'intervalle de confiance\n    \"\"\"\n    # set seed\n    np.random.seed(42)\n\n    # Initialisation du vecteur des VaR\n    var = np.zeros(M)\n\n    # Calcul de la VaR bootstrap\n    for i in range(M):\n        sample = np.random.choice(data, size=len(data), replace=True)\n        var[i] = -np.percentile(sample, 100*(1- alpha))\n\n    # Calcul de l'intervalle de confiance\n    lower = np.percentile(var, 100*(1-seuil)/2)\n    upper = np.percentile(var, 100*(seuil + (1-seuil)/2))\n\n    return np.mean(var), lower, upper\n\n\n# Observer la variation de la VaR en fonction de M\nM_values = np.arange(1000, 10000, 10)\nvar_bs_values = []\n\nfor M in tqdm(M_values):\n    var_bs_train, _, _ = bootstrap_var(data_train, alpha=alpha, M=M)\n    var_bs_values.append(var_bs_train)\n\n  0%|          | 0/900 [00:00&lt;?, ?it/s]  0%|          | 2/900 [00:00&lt;01:04, 14.01it/s]  0%|          | 4/900 [00:00&lt;01:04, 13.90it/s]  1%|          | 6/900 [00:00&lt;01:04, 13.86it/s]  1%|          | 8/900 [00:00&lt;01:05, 13.68it/s]  1%|          | 10/900 [00:00&lt;01:05, 13.50it/s]  1%|‚ñè         | 12/900 [00:00&lt;01:06, 13.32it/s]  2%|‚ñè         | 14/900 [00:01&lt;01:07, 13.12it/s]  2%|‚ñè         | 16/900 [00:01&lt;01:08, 12.93it/s]  2%|‚ñè         | 18/900 [00:01&lt;01:09, 12.72it/s]  2%|‚ñè         | 20/900 [00:01&lt;01:10, 12.55it/s]  2%|‚ñè         | 22/900 [00:01&lt;01:10, 12.37it/s]  3%|‚ñé         | 24/900 [00:01&lt;01:11, 12.19it/s]  3%|‚ñé         | 26/900 [00:02&lt;01:12, 12.01it/s]  3%|‚ñé         | 28/900 [00:02&lt;01:13, 11.83it/s]  3%|‚ñé         | 30/900 [00:02&lt;01:14, 11.65it/s]  4%|‚ñé         | 32/900 [00:02&lt;01:15, 11.44it/s]  4%|‚ñç         | 34/900 [00:02&lt;01:16, 11.28it/s]  4%|‚ñç         | 36/900 [00:02&lt;01:17, 11.12it/s]  4%|‚ñç         | 38/900 [00:03&lt;01:18, 10.95it/s]  4%|‚ñç         | 40/900 [00:03&lt;01:20, 10.74it/s]  5%|‚ñç         | 42/900 [00:03&lt;01:20, 10.61it/s]  5%|‚ñç         | 44/900 [00:03&lt;01:21, 10.47it/s]  5%|‚ñå         | 46/900 [00:03&lt;01:23, 10.29it/s]  5%|‚ñå         | 48/900 [00:04&lt;01:23, 10.16it/s]  6%|‚ñå         | 50/900 [00:04&lt;01:24, 10.03it/s]  6%|‚ñå         | 52/900 [00:04&lt;01:25,  9.91it/s]  6%|‚ñå         | 53/900 [00:04&lt;01:26,  9.84it/s]  6%|‚ñå         | 54/900 [00:04&lt;01:26,  9.76it/s]  6%|‚ñå         | 55/900 [00:04&lt;01:27,  9.67it/s]  6%|‚ñå         | 56/900 [00:04&lt;01:28,  9.57it/s]  6%|‚ñã         | 57/900 [00:05&lt;01:28,  9.48it/s]  6%|‚ñã         | 58/900 [00:05&lt;01:29,  9.40it/s]  7%|‚ñã         | 59/900 [00:05&lt;01:30,  9.29it/s]  7%|‚ñã         | 60/900 [00:05&lt;01:31,  9.22it/s]  7%|‚ñã         | 61/900 [00:05&lt;01:31,  9.17it/s]  7%|‚ñã         | 62/900 [00:05&lt;01:32,  9.11it/s]  7%|‚ñã         | 63/900 [00:05&lt;01:32,  9.05it/s]  7%|‚ñã         | 64/900 [00:05&lt;01:32,  8.99it/s]  7%|‚ñã         | 65/900 [00:05&lt;01:33,  8.93it/s]  7%|‚ñã         | 66/900 [00:06&lt;01:34,  8.79it/s]  7%|‚ñã         | 67/900 [00:06&lt;01:35,  8.74it/s]  8%|‚ñä         | 68/900 [00:06&lt;01:35,  8.72it/s]  8%|‚ñä         | 69/900 [00:06&lt;01:35,  8.69it/s]  8%|‚ñä         | 70/900 [00:06&lt;01:35,  8.65it/s]  8%|‚ñä         | 71/900 [00:06&lt;01:36,  8.59it/s]  8%|‚ñä         | 72/900 [00:06&lt;01:36,  8.55it/s]  8%|‚ñä         | 73/900 [00:06&lt;01:37,  8.50it/s]  8%|‚ñä         | 74/900 [00:07&lt;01:37,  8.47it/s]  8%|‚ñä         | 75/900 [00:07&lt;01:37,  8.42it/s]  8%|‚ñä         | 76/900 [00:07&lt;01:38,  8.35it/s]  9%|‚ñä         | 77/900 [00:07&lt;01:38,  8.31it/s]  9%|‚ñä         | 78/900 [00:07&lt;01:39,  8.27it/s]  9%|‚ñâ         | 79/900 [00:07&lt;01:39,  8.23it/s]  9%|‚ñâ         | 80/900 [00:07&lt;01:40,  8.18it/s]  9%|‚ñâ         | 81/900 [00:07&lt;01:40,  8.14it/s]  9%|‚ñâ         | 82/900 [00:07&lt;01:41,  8.10it/s]  9%|‚ñâ         | 83/900 [00:08&lt;01:41,  8.06it/s]  9%|‚ñâ         | 84/900 [00:08&lt;01:41,  8.02it/s]  9%|‚ñâ         | 85/900 [00:08&lt;01:42,  7.95it/s] 10%|‚ñâ         | 86/900 [00:08&lt;01:42,  7.91it/s] 10%|‚ñâ         | 87/900 [00:08&lt;01:43,  7.87it/s] 10%|‚ñâ         | 88/900 [00:08&lt;01:43,  7.83it/s] 10%|‚ñâ         | 89/900 [00:08&lt;01:44,  7.79it/s] 10%|‚ñà         | 90/900 [00:09&lt;01:44,  7.75it/s] 10%|‚ñà         | 91/900 [00:09&lt;01:44,  7.71it/s] 10%|‚ñà         | 92/900 [00:09&lt;01:45,  7.67it/s] 10%|‚ñà         | 93/900 [00:09&lt;01:45,  7.62it/s] 10%|‚ñà         | 94/900 [00:09&lt;01:46,  7.57it/s] 11%|‚ñà         | 95/900 [00:09&lt;01:46,  7.52it/s] 11%|‚ñà         | 96/900 [00:09&lt;01:47,  7.49it/s] 11%|‚ñà         | 97/900 [00:09&lt;01:47,  7.44it/s] 11%|‚ñà         | 98/900 [00:10&lt;01:48,  7.39it/s] 11%|‚ñà         | 99/900 [00:10&lt;01:49,  7.34it/s] 11%|‚ñà         | 100/900 [00:10&lt;01:49,  7.32it/s] 11%|‚ñà         | 101/900 [00:10&lt;01:49,  7.30it/s] 11%|‚ñà‚ñè        | 102/900 [00:10&lt;01:51,  7.17it/s] 11%|‚ñà‚ñè        | 103/900 [00:10&lt;01:51,  7.15it/s] 12%|‚ñà‚ñè        | 104/900 [00:10&lt;01:51,  7.14it/s] 12%|‚ñà‚ñè        | 105/900 [00:11&lt;01:51,  7.10it/s] 12%|‚ñà‚ñè        | 106/900 [00:11&lt;01:52,  7.08it/s] 12%|‚ñà‚ñè        | 107/900 [00:11&lt;01:52,  7.03it/s] 12%|‚ñà‚ñè        | 108/900 [00:11&lt;01:53,  7.01it/s] 12%|‚ñà‚ñè        | 109/900 [00:11&lt;01:53,  6.99it/s] 12%|‚ñà‚ñè        | 110/900 [00:11&lt;01:53,  6.96it/s] 12%|‚ñà‚ñè        | 111/900 [00:11&lt;01:53,  6.94it/s] 12%|‚ñà‚ñè        | 112/900 [00:12&lt;01:54,  6.89it/s] 13%|‚ñà‚ñé        | 113/900 [00:12&lt;01:54,  6.86it/s] 13%|‚ñà‚ñé        | 114/900 [00:12&lt;01:54,  6.84it/s] 13%|‚ñà‚ñé        | 115/900 [00:12&lt;01:55,  6.81it/s] 13%|‚ñà‚ñé        | 116/900 [00:12&lt;01:55,  6.78it/s] 13%|‚ñà‚ñé        | 117/900 [00:12&lt;01:55,  6.75it/s] 13%|‚ñà‚ñé        | 118/900 [00:12&lt;01:56,  6.73it/s] 13%|‚ñà‚ñé        | 119/900 [00:13&lt;01:56,  6.70it/s] 13%|‚ñà‚ñé        | 120/900 [00:13&lt;01:57,  6.66it/s] 13%|‚ñà‚ñé        | 121/900 [00:13&lt;01:57,  6.64it/s] 14%|‚ñà‚ñé        | 122/900 [00:13&lt;01:57,  6.61it/s] 14%|‚ñà‚ñé        | 123/900 [00:13&lt;01:58,  6.58it/s] 14%|‚ñà‚ñç        | 124/900 [00:13&lt;01:58,  6.53it/s] 14%|‚ñà‚ñç        | 125/900 [00:14&lt;01:59,  6.51it/s] 14%|‚ñà‚ñç        | 126/900 [00:14&lt;01:59,  6.48it/s] 14%|‚ñà‚ñç        | 127/900 [00:14&lt;01:59,  6.46it/s] 14%|‚ñà‚ñç        | 128/900 [00:14&lt;02:00,  6.43it/s] 14%|‚ñà‚ñç        | 129/900 [00:14&lt;02:00,  6.38it/s] 14%|‚ñà‚ñç        | 130/900 [00:14&lt;02:01,  6.36it/s] 15%|‚ñà‚ñç        | 131/900 [00:15&lt;02:01,  6.34it/s] 15%|‚ñà‚ñç        | 132/900 [00:15&lt;02:01,  6.31it/s] 15%|‚ñà‚ñç        | 133/900 [00:15&lt;02:02,  6.29it/s] 15%|‚ñà‚ñç        | 134/900 [00:15&lt;02:02,  6.26it/s] 15%|‚ñà‚ñå        | 135/900 [00:15&lt;02:02,  6.23it/s] 15%|‚ñà‚ñå        | 136/900 [00:15&lt;02:03,  6.20it/s] 15%|‚ñà‚ñå        | 137/900 [00:15&lt;02:03,  6.17it/s] 15%|‚ñà‚ñå        | 138/900 [00:16&lt;02:04,  6.13it/s] 15%|‚ñà‚ñå        | 139/900 [00:16&lt;02:04,  6.11it/s] 16%|‚ñà‚ñå        | 140/900 [00:16&lt;02:04,  6.09it/s] 16%|‚ñà‚ñå        | 141/900 [00:16&lt;02:04,  6.08it/s] 16%|‚ñà‚ñå        | 142/900 [00:16&lt;02:05,  6.05it/s] 16%|‚ñà‚ñå        | 143/900 [00:16&lt;02:05,  6.03it/s] 16%|‚ñà‚ñå        | 144/900 [00:17&lt;02:05,  6.00it/s] 16%|‚ñà‚ñå        | 145/900 [00:17&lt;02:06,  5.98it/s] 16%|‚ñà‚ñå        | 146/900 [00:17&lt;02:06,  5.95it/s] 16%|‚ñà‚ñã        | 147/900 [00:17&lt;02:06,  5.93it/s] 16%|‚ñà‚ñã        | 148/900 [00:17&lt;02:07,  5.91it/s] 17%|‚ñà‚ñã        | 149/900 [00:17&lt;02:07,  5.88it/s] 17%|‚ñà‚ñã        | 150/900 [00:18&lt;02:07,  5.86it/s] 17%|‚ñà‚ñã        | 151/900 [00:18&lt;02:08,  5.84it/s] 17%|‚ñà‚ñã        | 152/900 [00:18&lt;02:08,  5.82it/s] 17%|‚ñà‚ñã        | 153/900 [00:18&lt;02:08,  5.80it/s] 17%|‚ñà‚ñã        | 154/900 [00:18&lt;02:09,  5.78it/s] 17%|‚ñà‚ñã        | 155/900 [00:19&lt;02:09,  5.75it/s] 17%|‚ñà‚ñã        | 156/900 [00:19&lt;02:09,  5.73it/s] 17%|‚ñà‚ñã        | 157/900 [00:19&lt;02:10,  5.71it/s] 18%|‚ñà‚ñä        | 158/900 [00:19&lt;02:10,  5.68it/s] 18%|‚ñà‚ñä        | 159/900 [00:19&lt;02:10,  5.66it/s] 18%|‚ñà‚ñä        | 160/900 [00:19&lt;02:11,  5.62it/s] 18%|‚ñà‚ñä        | 161/900 [00:20&lt;02:12,  5.59it/s] 18%|‚ñà‚ñä        | 162/900 [00:20&lt;02:12,  5.57it/s] 18%|‚ñà‚ñä        | 163/900 [00:20&lt;02:13,  5.54it/s] 18%|‚ñà‚ñä        | 164/900 [00:20&lt;02:13,  5.52it/s] 18%|‚ñà‚ñä        | 165/900 [00:20&lt;02:13,  5.50it/s] 18%|‚ñà‚ñä        | 166/900 [00:21&lt;02:13,  5.49it/s] 19%|‚ñà‚ñä        | 167/900 [00:21&lt;02:15,  5.42it/s] 19%|‚ñà‚ñä        | 168/900 [00:21&lt;02:15,  5.41it/s] 19%|‚ñà‚ñâ        | 169/900 [00:21&lt;02:15,  5.40it/s] 19%|‚ñà‚ñâ        | 170/900 [00:21&lt;02:15,  5.38it/s] 19%|‚ñà‚ñâ        | 171/900 [00:21&lt;02:15,  5.37it/s] 19%|‚ñà‚ñâ        | 172/900 [00:22&lt;02:16,  5.34it/s] 19%|‚ñà‚ñâ        | 173/900 [00:22&lt;02:16,  5.33it/s] 19%|‚ñà‚ñâ        | 174/900 [00:22&lt;02:16,  5.31it/s] 19%|‚ñà‚ñâ        | 175/900 [00:22&lt;02:16,  5.29it/s] 20%|‚ñà‚ñâ        | 176/900 [00:22&lt;02:16,  5.28it/s] 20%|‚ñà‚ñâ        | 177/900 [00:23&lt;02:17,  5.27it/s] 20%|‚ñà‚ñâ        | 178/900 [00:23&lt;02:17,  5.26it/s] 20%|‚ñà‚ñâ        | 179/900 [00:23&lt;02:17,  5.24it/s] 20%|‚ñà‚ñà        | 180/900 [00:23&lt;02:17,  5.22it/s] 20%|‚ñà‚ñà        | 181/900 [00:23&lt;02:18,  5.20it/s] 20%|‚ñà‚ñà        | 182/900 [00:24&lt;02:18,  5.19it/s] 20%|‚ñà‚ñà        | 183/900 [00:24&lt;02:18,  5.17it/s] 20%|‚ñà‚ñà        | 184/900 [00:24&lt;02:18,  5.15it/s] 21%|‚ñà‚ñà        | 185/900 [00:24&lt;02:19,  5.14it/s] 21%|‚ñà‚ñà        | 186/900 [00:24&lt;02:19,  5.12it/s] 21%|‚ñà‚ñà        | 187/900 [00:25&lt;02:19,  5.09it/s] 21%|‚ñà‚ñà        | 188/900 [00:25&lt;02:20,  5.06it/s] 21%|‚ñà‚ñà        | 189/900 [00:25&lt;02:20,  5.05it/s] 21%|‚ñà‚ñà        | 190/900 [00:25&lt;02:20,  5.04it/s] 21%|‚ñà‚ñà        | 191/900 [00:25&lt;02:21,  5.02it/s] 21%|‚ñà‚ñà‚ñè       | 192/900 [00:26&lt;02:21,  5.00it/s] 21%|‚ñà‚ñà‚ñè       | 193/900 [00:26&lt;02:21,  4.99it/s] 22%|‚ñà‚ñà‚ñè       | 194/900 [00:26&lt;02:22,  4.97it/s] 22%|‚ñà‚ñà‚ñè       | 195/900 [00:26&lt;02:23,  4.92it/s] 22%|‚ñà‚ñà‚ñè       | 196/900 [00:26&lt;02:24,  4.87it/s] 22%|‚ñà‚ñà‚ñè       | 197/900 [00:27&lt;02:24,  4.87it/s] 22%|‚ñà‚ñà‚ñè       | 198/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 199/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 200/900 [00:27&lt;02:26,  4.78it/s] 22%|‚ñà‚ñà‚ñè       | 201/900 [00:27&lt;02:25,  4.79it/s] 22%|‚ñà‚ñà‚ñè       | 202/900 [00:28&lt;02:25,  4.79it/s] 23%|‚ñà‚ñà‚ñé       | 203/900 [00:28&lt;02:26,  4.77it/s] 23%|‚ñà‚ñà‚ñé       | 204/900 [00:28&lt;02:25,  4.77it/s] 23%|‚ñà‚ñà‚ñé       | 205/900 [00:28&lt;02:26,  4.75it/s] 23%|‚ñà‚ñà‚ñé       | 206/900 [00:28&lt;02:26,  4.73it/s] 23%|‚ñà‚ñà‚ñé       | 207/900 [00:29&lt;02:26,  4.73it/s] 23%|‚ñà‚ñà‚ñé       | 208/900 [00:29&lt;02:26,  4.72it/s] 23%|‚ñà‚ñà‚ñé       | 209/900 [00:29&lt;02:26,  4.72it/s] 23%|‚ñà‚ñà‚ñé       | 210/900 [00:29&lt;02:26,  4.71it/s] 23%|‚ñà‚ñà‚ñé       | 211/900 [00:30&lt;02:26,  4.69it/s] 24%|‚ñà‚ñà‚ñé       | 212/900 [00:30&lt;02:27,  4.67it/s] 24%|‚ñà‚ñà‚ñé       | 213/900 [00:30&lt;02:27,  4.66it/s] 24%|‚ñà‚ñà‚ñç       | 214/900 [00:30&lt;02:27,  4.65it/s] 24%|‚ñà‚ñà‚ñç       | 215/900 [00:30&lt;02:27,  4.63it/s] 24%|‚ñà‚ñà‚ñç       | 216/900 [00:31&lt;02:28,  4.62it/s] 24%|‚ñà‚ñà‚ñç       | 217/900 [00:31&lt;02:28,  4.60it/s] 24%|‚ñà‚ñà‚ñç       | 218/900 [00:31&lt;02:28,  4.59it/s] 24%|‚ñà‚ñà‚ñç       | 219/900 [00:31&lt;02:28,  4.57it/s] 24%|‚ñà‚ñà‚ñç       | 220/900 [00:31&lt;02:29,  4.56it/s] 25%|‚ñà‚ñà‚ñç       | 221/900 [00:32&lt;02:29,  4.55it/s] 25%|‚ñà‚ñà‚ñç       | 222/900 [00:32&lt;02:29,  4.52it/s] 25%|‚ñà‚ñà‚ñç       | 223/900 [00:32&lt;02:29,  4.52it/s] 25%|‚ñà‚ñà‚ñç       | 224/900 [00:32&lt;02:30,  4.49it/s] 25%|‚ñà‚ñà‚ñå       | 225/900 [00:33&lt;02:30,  4.48it/s] 25%|‚ñà‚ñà‚ñå       | 226/900 [00:33&lt;02:30,  4.47it/s] 25%|‚ñà‚ñà‚ñå       | 227/900 [00:33&lt;02:30,  4.47it/s] 25%|‚ñà‚ñà‚ñå       | 228/900 [00:33&lt;02:30,  4.45it/s] 25%|‚ñà‚ñà‚ñå       | 229/900 [00:34&lt;02:31,  4.44it/s] 26%|‚ñà‚ñà‚ñå       | 230/900 [00:34&lt;02:31,  4.43it/s] 26%|‚ñà‚ñà‚ñå       | 231/900 [00:34&lt;02:31,  4.42it/s] 26%|‚ñà‚ñà‚ñå       | 232/900 [00:34&lt;02:32,  4.39it/s] 26%|‚ñà‚ñà‚ñå       | 233/900 [00:34&lt;02:32,  4.38it/s] 26%|‚ñà‚ñà‚ñå       | 234/900 [00:35&lt;02:32,  4.37it/s] 26%|‚ñà‚ñà‚ñå       | 235/900 [00:35&lt;02:32,  4.36it/s] 26%|‚ñà‚ñà‚ñå       | 236/900 [00:35&lt;02:33,  4.33it/s] 26%|‚ñà‚ñà‚ñã       | 237/900 [00:35&lt;02:33,  4.33it/s] 26%|‚ñà‚ñà‚ñã       | 238/900 [00:36&lt;02:33,  4.32it/s] 27%|‚ñà‚ñà‚ñã       | 239/900 [00:36&lt;02:33,  4.30it/s] 27%|‚ñà‚ñà‚ñã       | 240/900 [00:36&lt;02:33,  4.29it/s] 27%|‚ñà‚ñà‚ñã       | 241/900 [00:36&lt;02:33,  4.28it/s] 27%|‚ñà‚ñà‚ñã       | 242/900 [00:37&lt;02:33,  4.27it/s] 27%|‚ñà‚ñà‚ñã       | 243/900 [00:37&lt;02:34,  4.26it/s] 27%|‚ñà‚ñà‚ñã       | 244/900 [00:37&lt;02:34,  4.25it/s] 27%|‚ñà‚ñà‚ñã       | 245/900 [00:37&lt;02:34,  4.23it/s] 27%|‚ñà‚ñà‚ñã       | 246/900 [00:37&lt;02:35,  4.22it/s] 27%|‚ñà‚ñà‚ñã       | 247/900 [00:38&lt;02:35,  4.21it/s] 28%|‚ñà‚ñà‚ñä       | 248/900 [00:38&lt;02:35,  4.20it/s] 28%|‚ñà‚ñà‚ñä       | 249/900 [00:38&lt;02:35,  4.19it/s] 28%|‚ñà‚ñà‚ñä       | 250/900 [00:38&lt;02:35,  4.18it/s] 28%|‚ñà‚ñà‚ñä       | 251/900 [00:39&lt;02:36,  4.15it/s] 28%|‚ñà‚ñà‚ñä       | 252/900 [00:39&lt;02:36,  4.15it/s] 28%|‚ñà‚ñà‚ñä       | 253/900 [00:39&lt;02:36,  4.14it/s] 28%|‚ñà‚ñà‚ñä       | 254/900 [00:39&lt;02:36,  4.13it/s] 28%|‚ñà‚ñà‚ñä       | 255/900 [00:40&lt;02:37,  4.11it/s] 28%|‚ñà‚ñà‚ñä       | 256/900 [00:40&lt;02:37,  4.09it/s] 29%|‚ñà‚ñà‚ñä       | 257/900 [00:40&lt;02:38,  4.07it/s] 29%|‚ñà‚ñà‚ñä       | 258/900 [00:40&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 259/900 [00:41&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 260/900 [00:41&lt;02:38,  4.05it/s] 29%|‚ñà‚ñà‚ñâ       | 261/900 [00:41&lt;02:38,  4.04it/s] 29%|‚ñà‚ñà‚ñâ       | 262/900 [00:41&lt;02:38,  4.04it/s] 29%|‚ñà‚ñà‚ñâ       | 263/900 [00:42&lt;02:38,  4.03it/s] 29%|‚ñà‚ñà‚ñâ       | 264/900 [00:42&lt;02:39,  4.00it/s] 29%|‚ñà‚ñà‚ñâ       | 265/900 [00:42&lt;02:38,  4.00it/s] 30%|‚ñà‚ñà‚ñâ       | 266/900 [00:42&lt;02:39,  3.97it/s] 30%|‚ñà‚ñà‚ñâ       | 267/900 [00:43&lt;02:39,  3.97it/s] 30%|‚ñà‚ñà‚ñâ       | 268/900 [00:43&lt;02:39,  3.96it/s] 30%|‚ñà‚ñà‚ñâ       | 269/900 [00:43&lt;02:39,  3.95it/s] 30%|‚ñà‚ñà‚ñà       | 270/900 [00:43&lt;02:39,  3.95it/s] 30%|‚ñà‚ñà‚ñà       | 271/900 [00:44&lt;02:39,  3.94it/s] 30%|‚ñà‚ñà‚ñà       | 272/900 [00:44&lt;02:39,  3.93it/s] 30%|‚ñà‚ñà‚ñà       | 273/900 [00:44&lt;02:40,  3.91it/s] 30%|‚ñà‚ñà‚ñà       | 274/900 [00:44&lt;02:41,  3.89it/s] 31%|‚ñà‚ñà‚ñà       | 275/900 [00:45&lt;02:40,  3.89it/s] 31%|‚ñà‚ñà‚ñà       | 276/900 [00:45&lt;02:40,  3.88it/s] 31%|‚ñà‚ñà‚ñà       | 277/900 [00:45&lt;02:41,  3.87it/s] 31%|‚ñà‚ñà‚ñà       | 278/900 [00:45&lt;02:40,  3.86it/s] 31%|‚ñà‚ñà‚ñà       | 279/900 [00:46&lt;02:41,  3.85it/s] 31%|‚ñà‚ñà‚ñà       | 280/900 [00:46&lt;02:41,  3.83it/s] 31%|‚ñà‚ñà‚ñà       | 281/900 [00:46&lt;02:41,  3.82it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 282/900 [00:47&lt;02:41,  3.82it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 283/900 [00:47&lt;02:41,  3.81it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 284/900 [00:47&lt;02:42,  3.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 285/900 [00:47&lt;02:42,  3.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 286/900 [00:48&lt;02:42,  3.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 287/900 [00:48&lt;02:42,  3.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 288/900 [00:48&lt;02:42,  3.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 289/900 [00:48&lt;02:43,  3.74it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 290/900 [00:49&lt;02:43,  3.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 291/900 [00:49&lt;02:45,  3.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 292/900 [00:49&lt;02:45,  3.68it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 293/900 [00:49&lt;02:45,  3.67it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 294/900 [00:50&lt;02:45,  3.67it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 295/900 [00:50&lt;02:45,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 296/900 [00:50&lt;02:45,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 297/900 [00:51&lt;02:44,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 298/900 [00:51&lt;02:44,  3.66it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 299/900 [00:51&lt;02:44,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 300/900 [00:51&lt;02:44,  3.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 301/900 [00:52&lt;02:45,  3.63it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 302/900 [00:52&lt;02:44,  3.63it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 303/900 [00:52&lt;02:44,  3.62it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 304/900 [00:53&lt;02:45,  3.60it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 305/900 [00:53&lt;02:46,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 306/900 [00:53&lt;02:45,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 307/900 [00:53&lt;02:45,  3.58it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 308/900 [00:54&lt;02:46,  3.56it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 309/900 [00:54&lt;02:46,  3.55it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 310/900 [00:54&lt;02:46,  3.55it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 311/900 [00:55&lt;02:46,  3.54it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 312/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 313/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 314/900 [00:55&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 315/900 [00:56&lt;02:46,  3.52it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 316/900 [00:56&lt;02:46,  3.51it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 317/900 [00:56&lt;02:46,  3.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 318/900 [00:57&lt;02:46,  3.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 319/900 [00:57&lt;02:46,  3.49it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 320/900 [00:57&lt;02:46,  3.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 321/900 [00:57&lt;02:46,  3.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 322/900 [00:58&lt;02:46,  3.47it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 323/900 [00:58&lt;02:46,  3.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 324/900 [00:58&lt;02:46,  3.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 325/900 [00:59&lt;02:46,  3.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 326/900 [00:59&lt;02:46,  3.44it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 327/900 [00:59&lt;02:47,  3.41it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 328/900 [00:59&lt;02:47,  3.41it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 329/900 [01:00&lt;02:47,  3.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 330/900 [01:00&lt;02:47,  3.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 331/900 [01:00&lt;02:47,  3.39it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 332/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 333/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 334/900 [01:01&lt;02:48,  3.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 335/900 [01:01&lt;02:48,  3.36it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 336/900 [01:02&lt;02:48,  3.36it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 337/900 [01:02&lt;02:48,  3.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 338/900 [01:02&lt;02:49,  3.33it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 339/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 340/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 341/900 [01:03&lt;02:48,  3.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 342/900 [01:04&lt;02:48,  3.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 343/900 [01:04&lt;02:48,  3.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 344/900 [01:04&lt;02:48,  3.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 345/900 [01:05&lt;02:48,  3.29it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 346/900 [01:05&lt;02:48,  3.29it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 347/900 [01:05&lt;02:48,  3.28it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 348/900 [01:05&lt;02:48,  3.27it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 349/900 [01:06&lt;02:48,  3.26it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 350/900 [01:06&lt;02:49,  3.25it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 351/900 [01:06&lt;02:49,  3.25it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 352/900 [01:07&lt;02:49,  3.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 353/900 [01:07&lt;02:49,  3.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 354/900 [01:07&lt;02:52,  3.16it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 355/900 [01:08&lt;02:51,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 356/900 [01:08&lt;02:51,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 357/900 [01:08&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 358/900 [01:09&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 359/900 [01:09&lt;02:50,  3.18it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 360/900 [01:09&lt;02:50,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 361/900 [01:10&lt;02:50,  3.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 362/900 [01:10&lt;02:50,  3.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 363/900 [01:10&lt;02:50,  3.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 364/900 [01:10&lt;02:50,  3.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 365/900 [01:11&lt;02:50,  3.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 366/900 [01:11&lt;02:50,  3.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 367/900 [01:11&lt;02:50,  3.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 368/900 [01:12&lt;02:50,  3.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 369/900 [01:12&lt;02:50,  3.11it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 370/900 [01:12&lt;02:50,  3.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 371/900 [01:13&lt;02:50,  3.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 372/900 [01:13&lt;02:51,  3.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 373/900 [01:13&lt;02:51,  3.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 374/900 [01:14&lt;02:51,  3.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 375/900 [01:14&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 376/900 [01:14&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 377/900 [01:15&lt;02:51,  3.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 378/900 [01:15&lt;02:51,  3.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 379/900 [01:15&lt;02:51,  3.04it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 380/900 [01:16&lt;02:51,  3.03it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 381/900 [01:16&lt;02:52,  3.02it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 382/900 [01:16&lt;02:51,  3.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 383/900 [01:17&lt;02:51,  3.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 384/900 [01:17&lt;02:51,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 385/900 [01:17&lt;02:51,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 386/900 [01:18&lt;02:50,  3.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 387/900 [01:18&lt;02:50,  3.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 388/900 [01:18&lt;02:50,  3.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 389/900 [01:19&lt;02:51,  2.99it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 390/900 [01:19&lt;02:50,  2.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 391/900 [01:19&lt;02:51,  2.97it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 392/900 [01:20&lt;02:51,  2.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 393/900 [01:20&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 394/900 [01:20&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 395/900 [01:21&lt;02:51,  2.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 396/900 [01:21&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 397/900 [01:21&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 398/900 [01:22&lt;02:51,  2.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 399/900 [01:22&lt;02:51,  2.92it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 400/900 [01:22&lt;02:52,  2.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 401/900 [01:23&lt;02:51,  2.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 402/900 [01:23&lt;02:51,  2.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 403/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 404/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 405/900 [01:24&lt;02:51,  2.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 406/900 [01:25&lt;02:51,  2.88it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 407/900 [01:25&lt;02:55,  2.81it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 408/900 [01:25&lt;03:01,  2.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 409/900 [01:26&lt;02:58,  2.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 410/900 [01:26&lt;02:55,  2.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 411/900 [01:26&lt;02:56,  2.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 412/900 [01:27&lt;02:57,  2.75it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 413/900 [01:27&lt;02:55,  2.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 414/900 [01:27&lt;02:53,  2.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 415/900 [01:28&lt;02:53,  2.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 416/900 [01:28&lt;02:52,  2.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 417/900 [01:29&lt;02:51,  2.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 418/900 [01:29&lt;02:51,  2.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 419/900 [01:29&lt;02:51,  2.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 420/900 [01:30&lt;02:51,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 421/900 [01:30&lt;02:50,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 422/900 [01:30&lt;02:50,  2.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 423/900 [01:31&lt;02:51,  2.79it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 424/900 [01:31&lt;02:51,  2.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 425/900 [01:31&lt;02:51,  2.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 426/900 [01:32&lt;02:51,  2.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 427/900 [01:32&lt;02:51,  2.76it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 428/900 [01:32&lt;02:51,  2.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 429/900 [01:33&lt;02:51,  2.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 430/900 [01:33&lt;02:55,  2.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 431/900 [01:34&lt;02:54,  2.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 432/900 [01:34&lt;02:52,  2.71it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 433/900 [01:34&lt;02:51,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 434/900 [01:35&lt;02:51,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 435/900 [01:35&lt;02:50,  2.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 436/900 [01:35&lt;02:50,  2.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 437/900 [01:36&lt;02:51,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 438/900 [01:36&lt;02:50,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 439/900 [01:37&lt;02:50,  2.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 440/900 [01:37&lt;02:50,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 441/900 [01:37&lt;02:49,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 442/900 [01:38&lt;02:49,  2.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 443/900 [01:38&lt;02:49,  2.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 444/900 [01:38&lt;02:49,  2.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 445/900 [01:39&lt;02:49,  2.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 446/900 [01:39&lt;02:49,  2.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 447/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 448/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 449/900 [01:40&lt;02:49,  2.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 450/900 [01:41&lt;02:49,  2.65it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 451/900 [01:41&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 452/900 [01:41&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 453/900 [01:42&lt;02:50,  2.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 454/900 [01:42&lt;02:49,  2.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 455/900 [01:43&lt;02:49,  2.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 456/900 [01:43&lt;02:49,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 457/900 [01:43&lt;02:49,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 458/900 [01:44&lt;02:48,  2.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 459/900 [01:44&lt;02:48,  2.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 460/900 [01:45&lt;02:48,  2.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 461/900 [01:45&lt;02:48,  2.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 462/900 [01:45&lt;02:48,  2.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 463/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 464/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 465/900 [01:46&lt;02:48,  2.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 466/900 [01:47&lt;02:48,  2.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 467/900 [01:47&lt;02:47,  2.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 468/900 [01:48&lt;02:47,  2.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 469/900 [01:48&lt;02:47,  2.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 470/900 [01:48&lt;02:47,  2.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 471/900 [01:49&lt;02:47,  2.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 472/900 [01:49&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 473/900 [01:50&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 474/900 [01:50&lt;02:47,  2.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 475/900 [01:50&lt;02:47,  2.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 476/900 [01:51&lt;02:47,  2.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 477/900 [01:51&lt;02:47,  2.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 478/900 [01:52&lt;02:46,  2.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 479/900 [01:52&lt;02:47,  2.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 480/900 [01:52&lt;02:47,  2.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 481/900 [01:53&lt;02:46,  2.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 482/900 [01:53&lt;02:46,  2.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 483/900 [01:54&lt;02:46,  2.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 484/900 [01:54&lt;02:46,  2.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 485/900 [01:54&lt;02:47,  2.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 486/900 [01:55&lt;02:46,  2.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 487/900 [01:55&lt;02:46,  2.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 488/900 [01:56&lt;02:47,  2.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 489/900 [01:56&lt;02:46,  2.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 490/900 [01:56&lt;02:46,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 491/900 [01:57&lt;02:45,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 492/900 [01:57&lt;02:45,  2.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 493/900 [01:58&lt;02:45,  2.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 494/900 [01:58&lt;02:45,  2.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 495/900 [01:58&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 496/900 [01:59&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 497/900 [01:59&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 498/900 [02:00&lt;02:45,  2.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 499/900 [02:00&lt;02:45,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 500/900 [02:01&lt;02:45,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 501/900 [02:01&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 502/900 [02:01&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 503/900 [02:02&lt;02:44,  2.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 504/900 [02:02&lt;02:44,  2.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 505/900 [02:03&lt;02:43,  2.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 506/900 [02:03&lt;02:44,  2.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 507/900 [02:03&lt;02:44,  2.39it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 508/900 [02:04&lt;02:44,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 509/900 [02:04&lt;02:43,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 510/900 [02:05&lt;02:43,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 511/900 [02:05&lt;02:42,  2.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 512/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 513/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 514/900 [02:06&lt;02:42,  2.38it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 515/900 [02:07&lt;02:42,  2.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 516/900 [02:07&lt;02:41,  2.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 517/900 [02:08&lt;02:41,  2.37it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 518/900 [02:08&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 519/900 [02:08&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 520/900 [02:09&lt;02:41,  2.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 521/900 [02:09&lt;02:41,  2.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 522/900 [02:10&lt;02:41,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 523/900 [02:10&lt;02:41,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 524/900 [02:11&lt;02:40,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 525/900 [02:11&lt;02:40,  2.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 526/900 [02:11&lt;02:40,  2.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 527/900 [02:12&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 528/900 [02:12&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 529/900 [02:13&lt;02:40,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 530/900 [02:13&lt;02:39,  2.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 531/900 [02:14&lt;02:40,  2.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 532/900 [02:14&lt;02:39,  2.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 533/900 [02:15&lt;02:40,  2.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 534/900 [02:15&lt;02:39,  2.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 535/900 [02:15&lt;02:39,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 536/900 [02:16&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 537/900 [02:16&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 538/900 [02:17&lt;02:38,  2.29it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 539/900 [02:17&lt;02:38,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 540/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 541/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 542/900 [02:18&lt;02:37,  2.28it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 543/900 [02:19&lt;02:37,  2.27it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 544/900 [02:19&lt;02:36,  2.27it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 545/900 [02:20&lt;02:36,  2.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 546/900 [02:20&lt;02:37,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 547/900 [02:21&lt;02:36,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 548/900 [02:21&lt;02:36,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 549/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 550/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 551/900 [02:22&lt;02:35,  2.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 552/900 [02:23&lt;02:35,  2.24it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 553/900 [02:23&lt;02:35,  2.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 554/900 [02:24&lt;02:35,  2.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 555/900 [02:24&lt;02:35,  2.22it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 556/900 [02:25&lt;02:35,  2.21it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 557/900 [02:25&lt;02:41,  2.12it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 558/900 [02:26&lt;02:39,  2.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 559/900 [02:26&lt;02:37,  2.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 560/900 [02:27&lt;02:39,  2.13it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 561/900 [02:27&lt;02:37,  2.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 562/900 [02:28&lt;02:35,  2.17it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 563/900 [02:28&lt;02:34,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 564/900 [02:28&lt;02:33,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 565/900 [02:29&lt;02:33,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 566/900 [02:29&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 567/900 [02:30&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 568/900 [02:30&lt;02:32,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 569/900 [02:31&lt;02:31,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 570/900 [02:31&lt;02:31,  2.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 571/900 [02:32&lt;02:31,  2.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 572/900 [02:32&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 573/900 [02:33&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 574/900 [02:33&lt;02:30,  2.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 575/900 [02:34&lt;02:30,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 576/900 [02:34&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 577/900 [02:34&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 578/900 [02:35&lt;02:29,  2.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 579/900 [02:35&lt;02:29,  2.15it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 580/900 [02:36&lt;02:29,  2.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 581/900 [02:36&lt;02:29,  2.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 582/900 [02:37&lt;02:29,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 583/900 [02:37&lt;02:28,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 584/900 [02:38&lt;02:28,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 585/900 [02:38&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 586/900 [02:39&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 587/900 [02:39&lt;02:27,  2.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 588/900 [02:40&lt;02:26,  2.12it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 589/900 [02:40&lt;02:26,  2.12it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 590/900 [02:41&lt;02:26,  2.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 591/900 [02:41&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 592/900 [02:42&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 593/900 [02:42&lt;02:26,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 594/900 [02:42&lt;02:25,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 595/900 [02:43&lt;02:25,  2.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 596/900 [02:43&lt;02:25,  2.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 597/900 [02:44&lt;02:25,  2.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 598/900 [02:44&lt;02:25,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 599/900 [02:45&lt;02:24,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 600/900 [02:45&lt;02:24,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 601/900 [02:46&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 602/900 [02:46&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 603/900 [02:47&lt;02:23,  2.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 604/900 [02:47&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 605/900 [02:48&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 606/900 [02:48&lt;02:22,  2.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 607/900 [02:49&lt;02:22,  2.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 608/900 [02:49&lt;02:21,  2.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 609/900 [02:50&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 610/900 [02:50&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 611/900 [02:51&lt;02:21,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 612/900 [02:51&lt;02:20,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 613/900 [02:52&lt;02:20,  2.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 614/900 [02:52&lt;02:20,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 615/900 [02:53&lt;02:19,  2.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 616/900 [02:53&lt;02:19,  2.04it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 617/900 [02:54&lt;02:19,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 618/900 [02:54&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 619/900 [02:55&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 620/900 [02:55&lt;02:18,  2.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 621/900 [02:56&lt;02:18,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 622/900 [02:56&lt;02:17,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 623/900 [02:57&lt;02:17,  2.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 624/900 [02:57&lt;02:16,  2.01it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 625/900 [02:58&lt;02:16,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 626/900 [02:58&lt;02:16,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 627/900 [02:59&lt;02:15,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 628/900 [02:59&lt;02:15,  2.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 629/900 [03:00&lt;02:15,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 630/900 [03:00&lt;02:15,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 631/900 [03:01&lt;02:14,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 632/900 [03:01&lt;02:14,  2.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 633/900 [03:02&lt;02:13,  1.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 634/900 [03:02&lt;02:13,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 635/900 [03:03&lt;02:13,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 636/900 [03:03&lt;02:12,  1.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 637/900 [03:04&lt;02:12,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 638/900 [03:04&lt;02:12,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 639/900 [03:05&lt;02:11,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 640/900 [03:05&lt;02:11,  1.98it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 641/900 [03:06&lt;02:11,  1.97it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 642/900 [03:06&lt;02:11,  1.96it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 643/900 [03:07&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 644/900 [03:07&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 645/900 [03:08&lt;02:10,  1.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 646/900 [03:08&lt;02:10,  1.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 647/900 [03:09&lt;02:09,  1.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 648/900 [03:09&lt;02:09,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 649/900 [03:10&lt;02:09,  1.93it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 650/900 [03:10&lt;02:09,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 651/900 [03:11&lt;02:08,  1.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 652/900 [03:11&lt;02:08,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 653/900 [03:12&lt;02:07,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 654/900 [03:12&lt;02:07,  1.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 655/900 [03:13&lt;02:09,  1.89it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 656/900 [03:13&lt;02:08,  1.90it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 657/900 [03:14&lt;02:07,  1.90it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 658/900 [03:15&lt;02:06,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 659/900 [03:15&lt;02:06,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 660/900 [03:16&lt;02:05,  1.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 661/900 [03:16&lt;02:04,  1.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 662/900 [03:17&lt;02:04,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 663/900 [03:17&lt;02:03,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 664/900 [03:18&lt;02:03,  1.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 665/900 [03:18&lt;02:04,  1.89it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 666/900 [03:19&lt;02:08,  1.82it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 667/900 [03:19&lt;02:06,  1.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 668/900 [03:20&lt;02:06,  1.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 669/900 [03:20&lt;02:05,  1.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 670/900 [03:21&lt;02:04,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 671/900 [03:21&lt;02:03,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 672/900 [03:22&lt;02:03,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 673/900 [03:23&lt;02:02,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 674/900 [03:23&lt;02:01,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 675/900 [03:24&lt;02:00,  1.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 676/900 [03:24&lt;02:01,  1.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 677/900 [03:25&lt;02:01,  1.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 678/900 [03:25&lt;02:00,  1.83it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 679/900 [03:26&lt;02:01,  1.82it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 680/900 [03:26&lt;02:00,  1.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 681/900 [03:27&lt;01:59,  1.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 682/900 [03:28&lt;02:05,  1.73it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 683/900 [03:28&lt;02:03,  1.75it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 684/900 [03:29&lt;02:02,  1.76it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 685/900 [03:29&lt;02:01,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 686/900 [03:30&lt;02:01,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 687/900 [03:30&lt;02:00,  1.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 688/900 [03:31&lt;02:01,  1.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 689/900 [03:32&lt;02:00,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 690/900 [03:32&lt;01:59,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 691/900 [03:33&lt;01:58,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 692/900 [03:33&lt;01:57,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 693/900 [03:34&lt;01:57,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 694/900 [03:34&lt;01:56,  1.76it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 695/900 [03:35&lt;01:57,  1.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 696/900 [03:35&lt;01:55,  1.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 697/900 [03:36&lt;01:54,  1.78it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 698/900 [03:37&lt;01:52,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 699/900 [03:37&lt;01:51,  1.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/900 [03:38&lt;01:51,  1.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 701/900 [03:38&lt;01:50,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 702/900 [03:39&lt;01:50,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 703/900 [03:39&lt;01:49,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 704/900 [03:40&lt;01:49,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 705/900 [03:40&lt;01:48,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 706/900 [03:41&lt;01:48,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 707/900 [03:42&lt;01:47,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 708/900 [03:42&lt;01:47,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 709/900 [03:43&lt;01:46,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 710/900 [03:43&lt;01:46,  1.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 711/900 [03:44&lt;01:46,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 712/900 [03:44&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 713/900 [03:45&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 714/900 [03:46&lt;01:45,  1.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 715/900 [03:46&lt;01:44,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 716/900 [03:47&lt;01:45,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 717/900 [03:47&lt;01:44,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 718/900 [03:48&lt;01:43,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 719/900 [03:48&lt;01:42,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 720/900 [03:49&lt;01:42,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 721/900 [03:50&lt;01:41,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 722/900 [03:50&lt;01:41,  1.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 723/900 [03:51&lt;01:40,  1.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 724/900 [03:51&lt;01:40,  1.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 725/900 [03:52&lt;01:39,  1.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 726/900 [03:52&lt;01:39,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 727/900 [03:53&lt;01:38,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 728/900 [03:54&lt;01:38,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 729/900 [03:54&lt;01:37,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 730/900 [03:55&lt;01:37,  1.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 731/900 [03:55&lt;01:39,  1.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 732/900 [03:56&lt;01:39,  1.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 733/900 [03:57&lt;01:38,  1.69it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 734/900 [03:57&lt;01:39,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 735/900 [03:58&lt;01:37,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 736/900 [03:58&lt;01:37,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 737/900 [03:59&lt;01:37,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 738/900 [04:00&lt;01:36,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 739/900 [04:00&lt;01:36,  1.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 740/900 [04:01&lt;01:35,  1.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 741/900 [04:01&lt;01:35,  1.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 742/900 [04:02&lt;01:34,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 743/900 [04:02&lt;01:33,  1.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 744/900 [04:03&lt;01:33,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 745/900 [04:04&lt;01:32,  1.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 746/900 [04:04&lt;01:32,  1.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 747/900 [04:05&lt;01:32,  1.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 748/900 [04:06&lt;01:32,  1.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 749/900 [04:06&lt;01:32,  1.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 750/900 [04:07&lt;01:30,  1.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 751/900 [04:07&lt;01:29,  1.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 752/900 [04:08&lt;01:29,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 753/900 [04:09&lt;01:28,  1.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 754/900 [04:09&lt;01:28,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 755/900 [04:10&lt;01:27,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 756/900 [04:10&lt;01:26,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 757/900 [04:11&lt;01:25,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 758/900 [04:12&lt;01:25,  1.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 759/900 [04:12&lt;01:25,  1.64it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 760/900 [04:13&lt;01:25,  1.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 761/900 [04:13&lt;01:24,  1.65it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 762/900 [04:14&lt;01:23,  1.65it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 763/900 [04:15&lt;01:22,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 764/900 [04:15&lt;01:21,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 765/900 [04:16&lt;01:21,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 766/900 [04:16&lt;01:20,  1.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 767/900 [04:17&lt;01:19,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 768/900 [04:18&lt;01:19,  1.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 769/900 [04:18&lt;01:19,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 770/900 [04:19&lt;01:18,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 771/900 [04:19&lt;01:18,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 772/900 [04:20&lt;01:17,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 773/900 [04:21&lt;01:17,  1.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 774/900 [04:21&lt;01:16,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 775/900 [04:22&lt;01:15,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 776/900 [04:22&lt;01:15,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 777/900 [04:23&lt;01:14,  1.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 778/900 [04:24&lt;01:14,  1.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 779/900 [04:24&lt;01:14,  1.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 780/900 [04:25&lt;01:19,  1.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 781/900 [04:26&lt;01:23,  1.42it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 782/900 [04:27&lt;01:23,  1.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 783/900 [04:27&lt;01:20,  1.45it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 784/900 [04:28&lt;01:17,  1.49it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 785/900 [04:29&lt;01:15,  1.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 786/900 [04:29&lt;01:13,  1.54it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 787/900 [04:30&lt;01:14,  1.51it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 788/900 [04:30&lt;01:13,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 789/900 [04:31&lt;01:11,  1.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 790/900 [04:32&lt;01:11,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 791/900 [04:32&lt;01:12,  1.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 792/900 [04:33&lt;01:10,  1.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 793/900 [04:34&lt;01:09,  1.54it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 794/900 [04:34&lt;01:07,  1.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 795/900 [04:35&lt;01:06,  1.58it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 796/900 [04:36&lt;01:10,  1.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 797/900 [04:37&lt;01:15,  1.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 798/900 [04:37&lt;01:11,  1.43it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 799/900 [04:38&lt;01:08,  1.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 800/900 [04:38&lt;01:05,  1.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 801/900 [04:39&lt;01:04,  1.55it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 802/900 [04:40&lt;01:02,  1.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 803/900 [04:40&lt;01:01,  1.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 804/900 [04:41&lt;01:00,  1.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 805/900 [04:42&lt;01:00,  1.58it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 806/900 [04:42&lt;00:59,  1.59it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 807/900 [04:43&lt;00:58,  1.59it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 808/900 [04:44&lt;00:59,  1.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 809/900 [04:44&lt;00:59,  1.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 810/900 [04:45&lt;00:58,  1.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 811/900 [04:45&lt;00:57,  1.55it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 812/900 [04:46&lt;00:56,  1.55it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 813/900 [04:47&lt;00:55,  1.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 814/900 [04:47&lt;00:54,  1.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 815/900 [04:48&lt;00:54,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 816/900 [04:49&lt;00:54,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 817/900 [04:49&lt;00:53,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 818/900 [04:50&lt;00:52,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 819/900 [04:51&lt;00:52,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 820/900 [04:51&lt;00:51,  1.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 821/900 [04:52&lt;00:50,  1.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 822/900 [04:53&lt;00:50,  1.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 823/900 [04:53&lt;00:49,  1.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 824/900 [04:54&lt;00:48,  1.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 825/900 [04:54&lt;00:48,  1.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 826/900 [04:55&lt;00:47,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 827/900 [04:56&lt;00:47,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 828/900 [04:56&lt;00:47,  1.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 829/900 [04:57&lt;00:46,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 830/900 [04:58&lt;00:45,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 831/900 [04:58&lt;00:44,  1.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 832/900 [04:59&lt;00:44,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 833/900 [05:00&lt;00:43,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 834/900 [05:00&lt;00:44,  1.49it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 835/900 [05:01&lt;00:43,  1.50it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 836/900 [05:02&lt;00:42,  1.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 837/900 [05:02&lt;00:41,  1.53it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 838/900 [05:03&lt;00:40,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 839/900 [05:04&lt;00:39,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 840/900 [05:04&lt;00:38,  1.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 841/900 [05:05&lt;00:38,  1.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 842/900 [05:06&lt;00:37,  1.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 843/900 [05:06&lt;00:37,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 844/900 [05:07&lt;00:36,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 845/900 [05:08&lt;00:35,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 846/900 [05:08&lt;00:35,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 847/900 [05:09&lt;00:34,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 848/900 [05:10&lt;00:34,  1.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 849/900 [05:10&lt;00:33,  1.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 850/900 [05:11&lt;00:32,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 851/900 [05:12&lt;00:32,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 852/900 [05:12&lt;00:31,  1.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 853/900 [05:13&lt;00:31,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 854/900 [05:14&lt;00:30,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 855/900 [05:14&lt;00:29,  1.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 856/900 [05:15&lt;00:29,  1.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 857/900 [05:16&lt;00:28,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 858/900 [05:16&lt;00:28,  1.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 859/900 [05:17&lt;00:27,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 860/900 [05:18&lt;00:26,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 861/900 [05:18&lt;00:25,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 862/900 [05:19&lt;00:25,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 863/900 [05:20&lt;00:24,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 864/900 [05:20&lt;00:24,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 865/900 [05:21&lt;00:23,  1.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 866/900 [05:22&lt;00:22,  1.48it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 867/900 [05:22&lt;00:23,  1.43it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 868/900 [05:23&lt;00:22,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 869/900 [05:24&lt;00:21,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 870/900 [05:24&lt;00:20,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 871/900 [05:25&lt;00:20,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 872/900 [05:26&lt;00:19,  1.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 873/900 [05:26&lt;00:18,  1.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 874/900 [05:27&lt;00:18,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 875/900 [05:28&lt;00:17,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 876/900 [05:29&lt;00:16,  1.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 877/900 [05:29&lt;00:15,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 878/900 [05:30&lt;00:15,  1.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 879/900 [05:31&lt;00:14,  1.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 880/900 [05:31&lt;00:13,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 881/900 [05:32&lt;00:13,  1.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 882/900 [05:33&lt;00:12,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 883/900 [05:33&lt;00:11,  1.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 884/900 [05:34&lt;00:11,  1.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 885/900 [05:35&lt;00:10,  1.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 886/900 [05:36&lt;00:09,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 887/900 [05:36&lt;00:09,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 888/900 [05:37&lt;00:08,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 889/900 [05:38&lt;00:07,  1.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 890/900 [05:38&lt;00:06,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 891/900 [05:39&lt;00:06,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 892/900 [05:40&lt;00:05,  1.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 893/900 [05:40&lt;00:04,  1.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 894/900 [05:41&lt;00:04,  1.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 895/900 [05:42&lt;00:03,  1.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 896/900 [05:42&lt;00:02,  1.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 897/900 [05:43&lt;00:02,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 898/900 [05:44&lt;00:01,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 899/900 [05:45&lt;00:00,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:45&lt;00:00,  1.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [05:45&lt;00:00,  2.60it/s]\n\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(M_values, var_bs_values, marker='o', color='red')\nplt.xlabel('Nombre de simulations')\nplt.ylabel('VaR bootstrap')\nplt.title(\"Variation de la VaR bootstrap en fonction du nombre de simulations\")\n\nText(0.5, 1.0, 'Variation de la VaR bootstrap en fonction du nombre de simulations')\n\n\n\n\n\n\n\n\n\nPour la taille de l‚Äôechantillon bootstrap, nous allons prendre M=8000 √©tant donn√© que la courbe semble se stabiliser √† partir de cette valeur. Avec ce choix, la VaR estim√© est de 4.11%% avec un intervalle de confiance √† 5% de [4.05%, 4.09%]. De plus, en ce qui concerne la VaR historique, nous constatons que l‚Äôestimation est contenu dans l‚Äôintervalle de confiance.\n\nM=8000\nvar_bs_train, lower_ic,upper_ic = bootstrap_var(data_train, alpha=alpha, M=M)\nprint(f\"La VaR bootstrap pour h=1j et alpha=0.99 est : {var_bs_train:.4%}\")\nprint(f\"L'intervalle de confiance est : [{lower_ic:.4%}, {upper_ic:.4%}]\")\n\nLa VaR bootstrap pour h=1j et alpha=0.99 est : 4.1065%\nL'intervalle de confiance est : [4.0536%, 4.0939%]\n\n\n\n\nII.2.3. Backtest\nPour l‚Äôexercice de backtest, il s‚Äôagit de : 1. D√©terminer si la proportion \\(p\\) de violations de la VaR est coh√©rente avec le niveau de confiance, i.e.¬†√©gale √† \\(1-\\alpha\\). Cela permet de v√©rifier si la mesure de risque est bien calibr√©e. Pour cela, nous pouvons avoir recours √† un test de proportion ou un test de ratio de vraisemblance.\n**Unconditional coverage test** :\nSoit I la variable indicatrice de violation de la VaR, i.e. $I=1$ si la perte est sup√©rieure √† la VaR, et $I=0$ sinon. La proportion de violations de la VaR est donn√©e par :\n\n$$p = \\frac{1}{n} \\sum_{i=1}^{n} I_i = \\frac{Z}{n}$$\n\nSous H0, i.e. p=1-$\\alpha$, Z $\\sim$ Binomiale(n, 1-$\\alpha$). En supposant que n est suffisamment grand, on peut approximer Z par une loi normale. Ainsi donc :\n$$\\frac{Z - n (1-\\alpha)}{\\sqrt{\\alpha (1-\\alpha) n}} \\sim \\mathcal{N}(n(1-\\alpha), n\\alpha(1-\\alpha))$$\n\nSous cette hypoth√®se asymptotique, on peut calculer la statistique du ratio de vraisemblance suivant :\n$$LR = -2 ln \\left( \\frac{L(H1)}{L(H0)} \\right) =-2 ln \\left( 1- (1-\\alpha))^{n-e}(1-\\alpha)^e \\right) + 2 ln \\left( (1-\\frac{e}{n})^{n-e} (\\frac{e}{n})^e  \\right)  \\sim \\chi^2(1)$$\n\no√π e est le nombre de violations de la VaR. On rejette H0 si LR &gt; $\\chi^2(1-\\alpha)$.\n\n**Test de proportion** :\n$$\nH_0 : p = p_0 = 1-\\alpha \\\\ H_1 : p &gt; 1-\\alpha\n$$\nOn peut √©galement utiliser un test binomial pour tester si la proportion de violations de la VaR est √©gale √† $1-\\alpha$. On peut calculer la statistique du test suivant :\n$$Z = \\frac{p - p_0}{\\sqrt{p_0 (1-p_0) / n}} \\sim \\mathcal{N}(0,1)$$\n\nOn rejette H0 si Z &gt; $p_0+ \\phi^{-1}(1-\\alpha) \\sqrt{p_0 (1-p_0)/n}$, o√π $\\phi$ est la quantile de la loi normale standard.\n\nD√©terminer si, lorsqu‚Äôil y en a, les violations de VaR √† deux diff√©rents jours sont ind√©pendantes. Cela permet si la mesure de risque est capable de r√©agir aux chocs de march√© affectant la volatilit√© des rendements. Pour cela, nous utilisons un conditional coverage test.\nConditional coverage test : y revenir\n\n\nimport scipy.stats as stats\n\n# Objectif : impl√©menter une fonction calculant le nombre d'exception sur l'√©chantillon test\ndef exceptions(data, var):\n    \"\"\"\n    Calcul du nombre d'exception\n    data : les rendements logarithmiques\n    var : la VaR\n    \"\"\"\n    return np.sum(data &lt; -var)\n\n\n# Objectif : test de proportion binomiale\n\ndef binomial_test(n, p, p0 = 0.01, alpha=0.05):\n    \"\"\"\n    Test de proportion binomiale\n    H0 : p = p0\n    H1 : p &gt; p0\n    n : le nombre d'essais\n    p : la proportion\n    alpha : le niveau de confiance\n    \"\"\"\n\n    z = (p - p0) / np.sqrt(p0 * (1 - p0) / n)\n    #reject_zone = p0 + stats.norm.ppf(1 - alpha) * np.sqrt(p0 * (1 - p0) / n)\n    p_value = 1 - stats.norm.cdf(z)\n    reject = p_value &lt; alpha\n\n    # Calcul des IC\n    lower = p - stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n    upper = p + stats.norm.ppf(1 - alpha) * np.sqrt(p * (1 - p) / n)\n\n    return p_value, reject, lower, upper\n\n# Unconditionnal coverage test ==&gt; to do.\n\n\n# Backtest de la VaR historique\n\nexceptions_test = exceptions(data_test, var_hist_train)\nprint(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {exceptions_test}\")\n\nprint(\"=\"*80)\nn = len(data_test)\np = exceptions_test / n\np_value, reject, lower,upper = binomial_test(n, p)\nprint(f\"H0 : le nombre d'exceptions est inf√©rieur ou √©gale √† {1-alpha:.2%}\")\nprint(f\"IC : [{lower:.2%},{upper:.2%}]\")\nprint(f\"La p-value du test de proportion binomiale est : {p_value:.4f}\")\nprint(f\"Rejet de l'hypoth√®se nulle : {reject}\")\nprint(\"=\"*80)\n\nLe nombre d'exceptions sur l'√©chantillon de test est : 0\n================================================================================\nH0 : le nombre d'exceptions est inf√©rieur ou √©gale √† 1.00%\nIC : [0.00%,0.00%]\nLa p-value du test de proportion binomiale est : 0.9925\nRejet de l'hypoth√®se nulle : False\n================================================================================"
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "href": "3A/value-at-risk/var_classiques.html#ii.3.-var-param√©trique",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.3. VaR param√©trique",
    "text": "II.3. VaR param√©trique\n\nII.3.1. Validation ex-ante\nVisuellement, les donn√©es ne semblent pas suivre une loi normale. En effet, les quantiles th√©oriques d‚Äôune loi normale ne collent pas avec les quantiles empiriques des rendements. Cela peut √™tre d√ª √† la pr√©sence de queues √©paisses observables sur l‚Äôestimation de la densit√© des rendements sur l‚Äô√©chantillon d‚Äôapprentissage, de pics, de clusters de volatilit√© que nous avons observ√©es plus haut. De plus, le skewness est n√©gatif ce qui indique une asym√©trie n√©gative des rendements. Enfin, le kurtosis est sup√©rieur √† 3, ce qui indique une distribution leptokurtique des rendements.\nNous allons tout de m√™me impl√©menter une VaR gaussienne pour voir comment elle se comporte dans le backtest.\n\n# Test visuel d'ad√©quation de la loi normale\n\n# Cr√©er un Q-Q plot\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(data_train, dist=\"norm\", plot=ax)\n\n# Personnalisation du graphique\nax.set_title(\"Q-Q Plot (Normal Distribution)\")\nax.set_xlabel(\"Theoretical Quantiles\")\nax.set_ylabel(\"Sample Quantiles\")\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\n# Densit√© de l'echantillon train et l'√©chantillon de test\n\nplt.figure(figsize=(10, 4))\ndata_train.plot(kind='kde', label='Train', color='red')\nplt.legend(loc='upper left')\nplt.title(\"Densit√© de l'√©chantillon d'entrainement\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Skewness et kurtosis\nprint(\"=\"*80)\nprint(\"Skewness de l'√©chantillon d'entrainement : \", data_train.skew())\nprint(\"Kurtosis de l'√©chantillon d'entrainement : \", data_train.kurt())\nprint(\"=\"*80)\n\n================================================================================\nSkewness de l'√©chantillon d'entrainement :  -0.2981820421484688\nKurtosis de l'√©chantillon d'entrainement :  7.353960005618779\n================================================================================\n\n\n\nfrom scipy.stats import kstest\n\n# Test de Kolmogorov-Smirnov\nks_stat, ks_p_value = kstest(data_train, 'norm')\nprint(\"=\"*80)\nprint(\"H0 : Les donn√©es suivent une loi normale\")\nprint(f\"Statistique de test : {ks_stat:.4f}\")\nprint(f\"P-value : {ks_p_value:.4f}\")\nprint(\"=\"*80)\n\n================================================================================\nH0 : Les donn√©es suivent une loi normale\nStatistique de test : 0.4775\nP-value : 0.0000\n================================================================================\n\n\n\n\nII.3.2. Impl√©mentation de la VaR\n\na. M√©thode scaling\n\n# Objectif : √©crire une fonction qui calcule la VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n\nvar_gauss_train = gaussian_var(data_train, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=1j et alpha={alpha} est : {var_gauss_train:.4%}\")\nprint(f\"La VaR historique pour h=10j et alpha={alpha} est : {np.sqrt(10)*var_gauss_train:.4%}\")\n\nLa VaR gaussienne pour h=1j et alpha=0.99 est : 3.2302%\nLa VaR historique pour h=10j et alpha=0.99 est : 10.2148%\n\n\n\n\nb. M√©thode de diffusion d‚Äôun actif\nPour calculer la VaR gaussienne √† 10 jours par m√©thode de diffusion d‚Äôun actif, nous allons suivre les √©tapes suivantes :\nL‚Äô√©volution du prix d‚Äôun actif suit un processus de type mouvement brownien g√©om√©trique : \\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t &lt;=&gt; S_t = S_{t-1} e^{(\\mu - \\frac{1}{2} \\sigma^2) + \\sigma W_t}\n\\] o√π $ S_t$ est le prix de l‚Äôactif √† l‚Äôinstant$ t$, $ $ est le rendement moyen estim√© (drift), $ $ est la volatilit√© du rendement, $ dW_t$ est un mouvement brownien standard.\nOn peut de ce fait calculer plusieurs trajectoires de rendements de \\(S_0\\) et \\(S_{10}\\), puis calculer la VaR √† partir de la s√©rie des rendements $ r_{10j} = (S_{10} / S_{0}) $ obtenus avec ces trajectoires.\nEn utilisant la m√©thode de scaling et la m√©thode de diffusion, nous obtenons sensiblement la m√™me VaR.\n\nimport numpy as np\n\nmu = np.mean(data_train)\nsigma = np.std(data_train)\n\nreturn_sim = []\nT = 10\nM = 1000\nS0 = train['Close'].iloc[-1]\n\nfor i in range(M):\n    S = np.zeros(T+1)\n    S[0] = S0\n    for t in range(1,T+1):\n        Wt = np.random.normal()\n        S[t] = S[t-1] * np.exp((mu - 0.5 * sigma**2) + sigma * Wt)\n\n    return_sim.append(np.log(S[T]/S0))\n\nvar_gauss_diffusion = gaussian_var(return_sim, alpha=alpha)\nprint(f\"La VaR gaussienne pour h=10j et alpha={alpha} est : {var_gauss_diffusion:.4%}\")\n\nLa VaR gaussienne pour h=10j et alpha=0.99 est : 10.0053%\n\n\n\n\nc.¬†M√©thode EWMA\nLa VaR EWMA est une m√©thode qui permet de calculer la VaR en surpond√©rant les rendements les plus r√©cents. Cela permet de donner plus de poids aux rendements les plus r√©cents, et donc de mieux capturer les changements de volatilit√©. La VaR EWMA est donn√©e par la formule suivante :\n\\[\nVaR_{t+1} = \\mu + \\sigma \\times z_{\\alpha}\n\\]\n\n# VaR la m√©thode EWMA (Exponential Weighting Moving Average)\n\ndef gaussian_var_ewma(data, alpha, lambda_=0.94):\n    \"\"\"\n    Calcul de la VaR gaussienne EWMA\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    lambda_ : le param√®tre de lissage\n    \"\"\"\n\n    weights = np.array([(1-lambda_)*(lambda_**i) for i in range(len(data))])\n    weights = weights / np.sum(weights)\n\n    mu = np.sum(data[::-1] * weights)\n    sigma = np.sqrt(np.sum(weights * (data[::-1] - mu)**2))\n\n    return -(mu + sigma * stats.norm.ppf(1 - alpha)), mu, sigma\n\n# y revenir\n\n\nlambdas = [0.9, 0.95, 0.99]\nalpha = 0.99\nimport scipy.stats as stats\nfor l in lambdas:\n    print(\"=\"*80)\n    print(\"Lambda : \", l)\n    print(\"-\"*15)\n    var_gauss_emwa, mu, sigma = gaussian_var_ewma(data_train, alpha=alpha, lambda_=l)\n    print(f\"La VaR gaussienne EWMA pour h=1j et alpha={alpha} est : {var_gauss_emwa:.4%}\")\n    print(f\"La moyenne est : {mu:.4%}\")\n    print(f\"L'√©cart-type est : {sigma:.4%}\")\n\n    n_exceptions = exceptions(data_test, var_gauss_emwa)\n    print(f\"Le nombre d'exceptions sur l'√©chantillon de test est : {n_exceptions}\")\n\n================================================================================\nLambda :  0.9\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.3751%\nLa moyenne est : 0.1898%\nL'√©cart-type est : 1.1025%\nLe nombre d'exceptions sur l'√©chantillon de test est : 5\n================================================================================\nLambda :  0.95\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 2.8969%\nLa moyenne est : 0.0735%\nL'√©cart-type est : 1.2768%\nLe nombre d'exceptions sur l'√©chantillon de test est : 4\n================================================================================\nLambda :  0.99\n---------------\nLa VaR gaussienne EWMA pour h=1j et alpha=0.99 est : 3.3511%\nLa moyenne est : -0.0321%\nL'√©cart-type est : 1.4267%\nLe nombre d'exceptions sur l'√©chantillon de test est : 1\n\n\nAvec la m√©thode EWMA, nous observons une que la VaR diminue plus \\(\\lambda\\) augmente. Cela est d√ª au fait que plus \\(\\lambda\\) est grand, plus les rendements les plus r√©cents sont surpond√©r√©s, et donc la volatilit√© est plus faible, en raison de la fin de la p√©riode d‚Äôapprentissage."
  },
  {
    "objectID": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "href": "3A/value-at-risk/var_classiques.html#ii.4.-var-skew-student",
    "title": "TP1:M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)",
    "section": "II.4. VaR skew-student",
    "text": "II.4. VaR skew-student\n\nII.4.1. Validation ex-ante\n\n# Ecrire une fonction permettant d‚Äôestimer les param√®tres d‚Äôune loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n\n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n\n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(data_train)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0],\n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.0023251952411471218\nSigma :  0.008823931435233275\nGamma :  -0.23188477391476636\nNu :  2.9618199934116825\n================================================================================\n\n\n\n# Superposition de la densit√© th√©orique et des donn√©es\n\nx_values = np.linspace(min(data_train), max(data_train), 1000)\n\ntheoretical_density = skew_student_pdf(x_values, **params_sstd)\nplt.figure(figsize = (10,4))\nplt.hist(data_train, bins=30, density=True, alpha=0.5, label='Donn√©es empiriques')\nplt.plot(x_values, theoretical_density, label='Densit√© Skew Student', color='red')\n# Densit√© normale\nplt.plot(x_values, stats.norm.pdf(x_values, np.mean(data_train), np.std(data_train)), label='Densit√© normale', color='blue')\nplt.xlabel('Rendements logarithmiques')\nplt.ylabel('Densit√©')\nplt.title(\"Comparaison entre les donn√©es et la densit√© th√©orique d'une loi de Skew Student\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nLe graphique ci-dessus est satisfaisant. La densit√© th√©orique de la skew-student semble bien s‚Äôajuster aux donn√©es. La loi skew-student de param√®tres (\\(\\mu = 0.002, \\sigma = 0.009, \\gamma = -0.23, \\nu = 2.96\\)). Le \\(\\mu\\) est le rendement moyen, \\(\\sigma\\) est l‚Äô√©cart-type, \\(\\gamma\\) est le coefficient d‚Äôasym√©trie et \\(\\nu\\) est le degr√© de libert√©. Le skewness est n√©gatif, ce qui indique une asym√©trie n√©gative des rendements, comme ce qu‚Äôon a observ√© plus haut.\nNous allons appuyer cette validation en utilisant le QQ-plot. La fonction quantile d‚Äôune loi skew-student n‚Äôest pas analytique. Pour ce faire, nous allons construire la fonction de repartition de la skew-student ainsi que la fonction quantile qui est l‚Äôinverse de cette fonction de repartition. Nous allons ensuite comparer les quantiles th√©oriques de la skew-student avec les quantiles empiriques des rendements.\nEn observant le QQ-plot, on constate que les quantiles th√©oriques de la skew-student collent bien avec les quantiles empiriques des rendements. Cela confirme que la skew-student est une bonne approximation de la distribution des rendements. Pour une validation plus rigoureuse, on peut utiliser un test de Kolmogorov-Smirnov pour tester si les rendements suivent une loi skew-student.\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n\n\nniveaux_quantiles = np.arange(0.001, 1, 0.001)\nquantiles_empiriques = np.quantile(data_train, niveaux_quantiles)\nquantiles_theoriques = [skew_student_quantile(alpha,**params_sstd) for alpha in tqdm(niveaux_quantiles)]\n\n  0%|          | 0/999 [00:00&lt;?, ?it/s]  0%|          | 1/999 [00:00&lt;05:35,  2.97it/s]  0%|          | 2/999 [00:00&lt;05:26,  3.05it/s]  0%|          | 3/999 [00:00&lt;05:30,  3.02it/s]  0%|          | 4/999 [00:01&lt;06:18,  2.63it/s]  1%|          | 5/999 [00:01&lt;06:32,  2.53it/s]  1%|          | 6/999 [00:02&lt;06:37,  2.50it/s]  1%|          | 7/999 [00:02&lt;06:34,  2.52it/s]  1%|          | 8/999 [00:03&lt;06:49,  2.42it/s]  1%|          | 9/999 [00:03&lt;06:48,  2.43it/s]  1%|          | 10/999 [00:03&lt;06:58,  2.36it/s]  1%|          | 11/999 [00:04&lt;06:43,  2.45it/s]  1%|          | 12/999 [00:04&lt;06:49,  2.41it/s]  1%|‚ñè         | 13/999 [00:05&lt;06:58,  2.36it/s]  1%|‚ñè         | 14/999 [00:05&lt;07:08,  2.30it/s]  2%|‚ñè         | 15/999 [00:06&lt;06:57,  2.36it/s]  2%|‚ñè         | 16/999 [00:06&lt;06:59,  2.34it/s]  2%|‚ñè         | 17/999 [00:06&lt;06:54,  2.37it/s]  2%|‚ñè         | 18/999 [00:07&lt;06:56,  2.36it/s]  2%|‚ñè         | 19/999 [00:07&lt;06:58,  2.34it/s]  2%|‚ñè         | 20/999 [00:08&lt;06:46,  2.41it/s]  2%|‚ñè         | 21/999 [00:08&lt;06:44,  2.42it/s]  2%|‚ñè         | 22/999 [00:09&lt;06:54,  2.36it/s]  2%|‚ñè         | 23/999 [00:09&lt;06:52,  2.37it/s]  2%|‚ñè         | 24/999 [00:09&lt;06:45,  2.41it/s]  3%|‚ñé         | 25/999 [00:10&lt;06:50,  2.37it/s]  3%|‚ñé         | 26/999 [00:10&lt;07:13,  2.25it/s]  3%|‚ñé         | 27/999 [00:11&lt;07:05,  2.29it/s]  3%|‚ñé         | 28/999 [00:11&lt;06:59,  2.31it/s]  3%|‚ñé         | 29/999 [00:12&lt;07:03,  2.29it/s]  3%|‚ñé         | 30/999 [00:12&lt;06:59,  2.31it/s]  3%|‚ñé         | 31/999 [00:12&lt;06:58,  2.32it/s]  3%|‚ñé         | 32/999 [00:13&lt;07:06,  2.26it/s]  3%|‚ñé         | 33/999 [00:13&lt;07:24,  2.17it/s]  3%|‚ñé         | 34/999 [00:14&lt;07:34,  2.12it/s]  4%|‚ñé         | 35/999 [00:14&lt;07:34,  2.12it/s]  4%|‚ñé         | 36/999 [00:15&lt;07:39,  2.10it/s]  4%|‚ñé         | 37/999 [00:15&lt;07:42,  2.08it/s]  4%|‚ñç         | 38/999 [00:16&lt;07:58,  2.01it/s]  4%|‚ñç         | 39/999 [00:16&lt;08:08,  1.96it/s]  4%|‚ñç         | 40/999 [00:17&lt;07:59,  2.00it/s]  4%|‚ñç         | 41/999 [00:17&lt;07:53,  2.02it/s]  4%|‚ñç         | 42/999 [00:18&lt;07:53,  2.02it/s]  4%|‚ñç         | 43/999 [00:18&lt;07:49,  2.04it/s]  4%|‚ñç         | 44/999 [00:19&lt;07:44,  2.06it/s]  5%|‚ñç         | 45/999 [00:19&lt;07:43,  2.06it/s]  5%|‚ñç         | 46/999 [00:20&lt;07:46,  2.04it/s]  5%|‚ñç         | 47/999 [00:20&lt;07:40,  2.07it/s]  5%|‚ñç         | 48/999 [00:21&lt;07:50,  2.02it/s]  5%|‚ñç         | 49/999 [00:21&lt;08:00,  1.98it/s]  5%|‚ñå         | 50/999 [00:22&lt;07:49,  2.02it/s]  5%|‚ñå         | 51/999 [00:22&lt;07:48,  2.02it/s]  5%|‚ñå         | 52/999 [00:23&lt;08:03,  1.96it/s]  5%|‚ñå         | 53/999 [00:23&lt;08:03,  1.96it/s]  5%|‚ñå         | 54/999 [00:24&lt;07:50,  2.01it/s]  6%|‚ñå         | 55/999 [00:24&lt;07:59,  1.97it/s]  6%|‚ñå         | 56/999 [00:25&lt;07:53,  1.99it/s]  6%|‚ñå         | 57/999 [00:25&lt;07:58,  1.97it/s]  6%|‚ñå         | 58/999 [00:26&lt;08:02,  1.95it/s]  6%|‚ñå         | 59/999 [00:26&lt;07:43,  2.03it/s]  6%|‚ñå         | 60/999 [00:27&lt;07:45,  2.02it/s]  6%|‚ñå         | 61/999 [00:27&lt;08:01,  1.95it/s]  6%|‚ñå         | 62/999 [00:28&lt;08:07,  1.92it/s]  6%|‚ñã         | 63/999 [00:28&lt;08:08,  1.92it/s]  6%|‚ñã         | 64/999 [00:29&lt;08:07,  1.92it/s]  7%|‚ñã         | 65/999 [00:29&lt;07:53,  1.97it/s]  7%|‚ñã         | 66/999 [00:30&lt;07:41,  2.02it/s]  7%|‚ñã         | 67/999 [00:30&lt;07:55,  1.96it/s]  7%|‚ñã         | 68/999 [00:31&lt;07:41,  2.02it/s]  7%|‚ñã         | 69/999 [00:31&lt;07:47,  1.99it/s]  7%|‚ñã         | 70/999 [00:32&lt;07:38,  2.03it/s]  7%|‚ñã         | 71/999 [00:32&lt;07:47,  1.99it/s]  7%|‚ñã         | 72/999 [00:33&lt;07:53,  1.96it/s]  7%|‚ñã         | 73/999 [00:34&lt;07:58,  1.94it/s]  7%|‚ñã         | 74/999 [00:34&lt;07:51,  1.96it/s]  8%|‚ñä         | 75/999 [00:35&lt;07:59,  1.93it/s]  8%|‚ñä         | 76/999 [00:35&lt;08:03,  1.91it/s]  8%|‚ñä         | 77/999 [00:36&lt;07:51,  1.95it/s]  8%|‚ñä         | 78/999 [00:36&lt;08:02,  1.91it/s]  8%|‚ñä         | 79/999 [00:37&lt;08:06,  1.89it/s]  8%|‚ñä         | 80/999 [00:37&lt;08:01,  1.91it/s]  8%|‚ñä         | 81/999 [00:38&lt;07:47,  1.96it/s]  8%|‚ñä         | 82/999 [00:38&lt;07:46,  1.97it/s]  8%|‚ñä         | 83/999 [00:39&lt;07:37,  2.00it/s]  8%|‚ñä         | 84/999 [00:39&lt;07:39,  1.99it/s]  9%|‚ñä         | 85/999 [00:40&lt;07:41,  1.98it/s]  9%|‚ñä         | 86/999 [00:40&lt;07:39,  1.99it/s]  9%|‚ñä         | 87/999 [00:41&lt;07:42,  1.97it/s]  9%|‚ñâ         | 88/999 [00:41&lt;07:31,  2.02it/s]  9%|‚ñâ         | 89/999 [00:42&lt;07:26,  2.04it/s]  9%|‚ñâ         | 90/999 [00:42&lt;07:11,  2.11it/s]  9%|‚ñâ         | 91/999 [00:43&lt;07:16,  2.08it/s]  9%|‚ñâ         | 92/999 [00:43&lt;07:15,  2.08it/s]  9%|‚ñâ         | 93/999 [00:43&lt;06:51,  2.20it/s]  9%|‚ñâ         | 94/999 [00:44&lt;07:07,  2.11it/s] 10%|‚ñâ         | 95/999 [00:44&lt;07:23,  2.04it/s] 10%|‚ñâ         | 96/999 [00:45&lt;07:25,  2.03it/s] 10%|‚ñâ         | 97/999 [00:45&lt;07:35,  1.98it/s] 10%|‚ñâ         | 98/999 [00:46&lt;07:38,  1.96it/s] 10%|‚ñâ         | 99/999 [00:47&lt;07:37,  1.97it/s] 10%|‚ñà         | 100/999 [00:47&lt;07:47,  1.92it/s] 10%|‚ñà         | 101/999 [00:48&lt;07:42,  1.94it/s] 10%|‚ñà         | 102/999 [00:48&lt;07:54,  1.89it/s] 10%|‚ñà         | 103/999 [00:49&lt;07:43,  1.93it/s] 10%|‚ñà         | 104/999 [00:49&lt;07:34,  1.97it/s] 11%|‚ñà         | 105/999 [00:50&lt;07:23,  2.01it/s] 11%|‚ñà         | 106/999 [00:50&lt;07:29,  1.99it/s] 11%|‚ñà         | 107/999 [00:51&lt;07:27,  1.99it/s] 11%|‚ñà         | 108/999 [00:51&lt;07:42,  1.92it/s] 11%|‚ñà         | 109/999 [00:52&lt;07:42,  1.92it/s] 11%|‚ñà         | 110/999 [00:52&lt;07:36,  1.95it/s] 11%|‚ñà         | 111/999 [00:53&lt;07:37,  1.94it/s] 11%|‚ñà         | 112/999 [00:53&lt;07:46,  1.90it/s] 11%|‚ñà‚ñè        | 113/999 [00:54&lt;07:52,  1.88it/s] 11%|‚ñà‚ñè        | 114/999 [00:54&lt;07:46,  1.90it/s] 12%|‚ñà‚ñè        | 115/999 [00:55&lt;07:46,  1.90it/s] 12%|‚ñà‚ñè        | 116/999 [00:55&lt;07:40,  1.92it/s] 12%|‚ñà‚ñè        | 117/999 [00:56&lt;07:42,  1.91it/s] 12%|‚ñà‚ñè        | 118/999 [00:56&lt;07:43,  1.90it/s] 12%|‚ñà‚ñè        | 119/999 [00:57&lt;07:48,  1.88it/s] 12%|‚ñà‚ñè        | 120/999 [00:57&lt;07:39,  1.91it/s] 12%|‚ñà‚ñè        | 121/999 [00:58&lt;07:43,  1.90it/s] 12%|‚ñà‚ñè        | 122/999 [00:59&lt;08:00,  1.83it/s] 12%|‚ñà‚ñè        | 123/999 [00:59&lt;07:57,  1.83it/s] 12%|‚ñà‚ñè        | 124/999 [01:00&lt;08:07,  1.80it/s] 13%|‚ñà‚ñé        | 125/999 [01:00&lt;08:03,  1.81it/s] 13%|‚ñà‚ñé        | 126/999 [01:01&lt;08:09,  1.78it/s] 13%|‚ñà‚ñé        | 127/999 [01:01&lt;08:04,  1.80it/s] 13%|‚ñà‚ñé        | 128/999 [01:02&lt;07:53,  1.84it/s] 13%|‚ñà‚ñé        | 129/999 [01:02&lt;07:47,  1.86it/s] 13%|‚ñà‚ñé        | 130/999 [01:03&lt;07:50,  1.85it/s] 13%|‚ñà‚ñé        | 131/999 [01:04&lt;07:48,  1.85it/s] 13%|‚ñà‚ñé        | 132/999 [01:04&lt;07:37,  1.89it/s] 13%|‚ñà‚ñé        | 133/999 [01:04&lt;07:29,  1.93it/s] 13%|‚ñà‚ñé        | 134/999 [01:05&lt;07:19,  1.97it/s] 14%|‚ñà‚ñé        | 135/999 [01:06&lt;07:23,  1.95it/s] 14%|‚ñà‚ñé        | 136/999 [01:06&lt;07:29,  1.92it/s] 14%|‚ñà‚ñé        | 137/999 [01:07&lt;07:33,  1.90it/s] 14%|‚ñà‚ñç        | 138/999 [01:07&lt;07:24,  1.94it/s] 14%|‚ñà‚ñç        | 139/999 [01:08&lt;07:39,  1.87it/s] 14%|‚ñà‚ñç        | 140/999 [01:08&lt;07:44,  1.85it/s] 14%|‚ñà‚ñç        | 141/999 [01:09&lt;07:38,  1.87it/s] 14%|‚ñà‚ñç        | 142/999 [01:09&lt;07:29,  1.91it/s] 14%|‚ñà‚ñç        | 143/999 [01:10&lt;07:19,  1.95it/s] 14%|‚ñà‚ñç        | 144/999 [01:10&lt;07:12,  1.98it/s] 15%|‚ñà‚ñç        | 145/999 [01:11&lt;07:07,  2.00it/s] 15%|‚ñà‚ñç        | 146/999 [01:11&lt;07:05,  2.00it/s] 15%|‚ñà‚ñç        | 147/999 [01:12&lt;07:16,  1.95it/s] 15%|‚ñà‚ñç        | 148/999 [01:12&lt;07:45,  1.83it/s] 15%|‚ñà‚ñç        | 149/999 [01:13&lt;07:45,  1.83it/s] 15%|‚ñà‚ñå        | 150/999 [01:14&lt;07:54,  1.79it/s] 15%|‚ñà‚ñå        | 151/999 [01:14&lt;07:58,  1.77it/s] 15%|‚ñà‚ñå        | 152/999 [01:15&lt;07:43,  1.83it/s] 15%|‚ñà‚ñå        | 153/999 [01:15&lt;07:46,  1.81it/s] 15%|‚ñà‚ñå        | 154/999 [01:16&lt;07:46,  1.81it/s] 16%|‚ñà‚ñå        | 155/999 [01:16&lt;07:54,  1.78it/s] 16%|‚ñà‚ñå        | 156/999 [01:17&lt;08:07,  1.73it/s] 16%|‚ñà‚ñå        | 157/999 [01:18&lt;08:14,  1.70it/s] 16%|‚ñà‚ñå        | 158/999 [01:18&lt;07:59,  1.75it/s] 16%|‚ñà‚ñå        | 159/999 [01:19&lt;07:58,  1.75it/s] 16%|‚ñà‚ñå        | 160/999 [01:19&lt;08:07,  1.72it/s] 16%|‚ñà‚ñå        | 161/999 [01:20&lt;08:05,  1.73it/s] 16%|‚ñà‚ñå        | 162/999 [01:20&lt;08:09,  1.71it/s] 16%|‚ñà‚ñã        | 163/999 [01:21&lt;08:13,  1.69it/s] 16%|‚ñà‚ñã        | 164/999 [01:22&lt;08:23,  1.66it/s] 17%|‚ñà‚ñã        | 165/999 [01:22&lt;08:22,  1.66it/s] 17%|‚ñà‚ñã        | 166/999 [01:23&lt;08:20,  1.67it/s] 17%|‚ñà‚ñã        | 167/999 [01:23&lt;08:19,  1.66it/s] 17%|‚ñà‚ñã        | 168/999 [01:24&lt;08:19,  1.66it/s] 17%|‚ñà‚ñã        | 169/999 [01:25&lt;08:08,  1.70it/s] 17%|‚ñà‚ñã        | 170/999 [01:25&lt;08:12,  1.68it/s] 17%|‚ñà‚ñã        | 171/999 [01:26&lt;08:05,  1.70it/s] 17%|‚ñà‚ñã        | 172/999 [01:26&lt;08:20,  1.65it/s] 17%|‚ñà‚ñã        | 173/999 [01:27&lt;08:15,  1.67it/s] 17%|‚ñà‚ñã        | 174/999 [01:28&lt;08:06,  1.69it/s] 18%|‚ñà‚ñä        | 175/999 [01:28&lt;08:11,  1.68it/s] 18%|‚ñà‚ñä        | 176/999 [01:29&lt;08:13,  1.67it/s] 18%|‚ñà‚ñä        | 177/999 [01:29&lt;08:23,  1.63it/s] 18%|‚ñà‚ñä        | 178/999 [01:30&lt;08:15,  1.66it/s] 18%|‚ñà‚ñä        | 179/999 [01:31&lt;08:13,  1.66it/s] 18%|‚ñà‚ñä        | 180/999 [01:31&lt;08:03,  1.69it/s] 18%|‚ñà‚ñä        | 181/999 [01:32&lt;08:05,  1.68it/s] 18%|‚ñà‚ñä        | 182/999 [01:32&lt;08:02,  1.69it/s] 18%|‚ñà‚ñä        | 183/999 [01:33&lt;08:23,  1.62it/s] 18%|‚ñà‚ñä        | 184/999 [01:34&lt;08:12,  1.66it/s] 19%|‚ñà‚ñä        | 185/999 [01:34&lt;08:20,  1.63it/s] 19%|‚ñà‚ñä        | 186/999 [01:35&lt;08:34,  1.58it/s] 19%|‚ñà‚ñä        | 187/999 [01:36&lt;08:31,  1.59it/s] 19%|‚ñà‚ñâ        | 188/999 [01:36&lt;08:38,  1.56it/s] 19%|‚ñà‚ñâ        | 189/999 [01:37&lt;08:30,  1.59it/s] 19%|‚ñà‚ñâ        | 190/999 [01:37&lt;08:26,  1.60it/s] 19%|‚ñà‚ñâ        | 191/999 [01:38&lt;08:10,  1.65it/s] 19%|‚ñà‚ñâ        | 192/999 [01:39&lt;08:18,  1.62it/s] 19%|‚ñà‚ñâ        | 193/999 [01:39&lt;08:03,  1.67it/s] 19%|‚ñà‚ñâ        | 194/999 [01:40&lt;07:45,  1.73it/s] 20%|‚ñà‚ñâ        | 195/999 [01:40&lt;07:36,  1.76it/s] 20%|‚ñà‚ñâ        | 196/999 [01:41&lt;07:40,  1.74it/s] 20%|‚ñà‚ñâ        | 197/999 [01:41&lt;07:50,  1.70it/s] 20%|‚ñà‚ñâ        | 198/999 [01:42&lt;07:42,  1.73it/s] 20%|‚ñà‚ñâ        | 199/999 [01:43&lt;07:36,  1.75it/s] 20%|‚ñà‚ñà        | 200/999 [01:43&lt;07:51,  1.69it/s] 20%|‚ñà‚ñà        | 201/999 [01:44&lt;07:43,  1.72it/s] 20%|‚ñà‚ñà        | 202/999 [01:44&lt;07:41,  1.73it/s] 20%|‚ñà‚ñà        | 203/999 [01:45&lt;07:36,  1.74it/s] 20%|‚ñà‚ñà        | 204/999 [01:45&lt;07:13,  1.83it/s] 21%|‚ñà‚ñà        | 205/999 [01:46&lt;07:06,  1.86it/s] 21%|‚ñà‚ñà        | 206/999 [01:46&lt;07:08,  1.85it/s] 21%|‚ñà‚ñà        | 207/999 [01:47&lt;07:24,  1.78it/s] 21%|‚ñà‚ñà        | 208/999 [01:48&lt;07:26,  1.77it/s] 21%|‚ñà‚ñà        | 209/999 [01:48&lt;07:48,  1.69it/s] 21%|‚ñà‚ñà        | 210/999 [01:49&lt;07:58,  1.65it/s] 21%|‚ñà‚ñà        | 211/999 [01:50&lt;08:06,  1.62it/s] 21%|‚ñà‚ñà        | 212/999 [01:50&lt;08:02,  1.63it/s] 21%|‚ñà‚ñà‚ñè       | 213/999 [01:51&lt;08:08,  1.61it/s] 21%|‚ñà‚ñà‚ñè       | 214/999 [01:51&lt;07:57,  1.64it/s] 22%|‚ñà‚ñà‚ñè       | 215/999 [01:52&lt;07:56,  1.65it/s] 22%|‚ñà‚ñà‚ñè       | 216/999 [01:53&lt;07:50,  1.67it/s] 22%|‚ñà‚ñà‚ñè       | 217/999 [01:53&lt;07:39,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 218/999 [01:54&lt;07:40,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 219/999 [01:54&lt;07:33,  1.72it/s] 22%|‚ñà‚ñà‚ñè       | 220/999 [01:55&lt;07:37,  1.70it/s] 22%|‚ñà‚ñà‚ñè       | 221/999 [01:56&lt;07:41,  1.69it/s] 22%|‚ñà‚ñà‚ñè       | 222/999 [01:56&lt;07:31,  1.72it/s] 22%|‚ñà‚ñà‚ñè       | 223/999 [01:57&lt;07:21,  1.76it/s] 22%|‚ñà‚ñà‚ñè       | 224/999 [01:57&lt;07:26,  1.73it/s] 23%|‚ñà‚ñà‚ñé       | 225/999 [01:58&lt;07:42,  1.67it/s] 23%|‚ñà‚ñà‚ñé       | 226/999 [01:58&lt;07:40,  1.68it/s] 23%|‚ñà‚ñà‚ñé       | 227/999 [01:59&lt;07:26,  1.73it/s] 23%|‚ñà‚ñà‚ñé       | 228/999 [02:00&lt;07:27,  1.72it/s] 23%|‚ñà‚ñà‚ñé       | 229/999 [02:00&lt;07:17,  1.76it/s] 23%|‚ñà‚ñà‚ñé       | 230/999 [02:01&lt;07:30,  1.71it/s] 23%|‚ñà‚ñà‚ñé       | 231/999 [02:01&lt;07:26,  1.72it/s] 23%|‚ñà‚ñà‚ñé       | 232/999 [02:02&lt;07:28,  1.71it/s] 23%|‚ñà‚ñà‚ñé       | 233/999 [02:02&lt;07:33,  1.69it/s] 23%|‚ñà‚ñà‚ñé       | 234/999 [02:03&lt;07:29,  1.70it/s] 24%|‚ñà‚ñà‚ñé       | 235/999 [02:04&lt;07:28,  1.70it/s] 24%|‚ñà‚ñà‚ñé       | 236/999 [02:04&lt;07:32,  1.69it/s] 24%|‚ñà‚ñà‚ñé       | 237/999 [02:05&lt;07:37,  1.67it/s] 24%|‚ñà‚ñà‚ñç       | 238/999 [02:06&lt;07:55,  1.60it/s] 24%|‚ñà‚ñà‚ñç       | 239/999 [02:06&lt;08:02,  1.57it/s] 24%|‚ñà‚ñà‚ñç       | 240/999 [02:07&lt;08:10,  1.55it/s] 24%|‚ñà‚ñà‚ñç       | 241/999 [02:08&lt;08:06,  1.56it/s] 24%|‚ñà‚ñà‚ñç       | 242/999 [02:08&lt;08:12,  1.54it/s] 24%|‚ñà‚ñà‚ñç       | 243/999 [02:09&lt;08:23,  1.50it/s] 24%|‚ñà‚ñà‚ñç       | 244/999 [02:10&lt;08:25,  1.49it/s] 25%|‚ñà‚ñà‚ñç       | 245/999 [02:10&lt;08:22,  1.50it/s] 25%|‚ñà‚ñà‚ñç       | 246/999 [02:11&lt;08:28,  1.48it/s] 25%|‚ñà‚ñà‚ñç       | 247/999 [02:12&lt;08:23,  1.49it/s] 25%|‚ñà‚ñà‚ñç       | 248/999 [02:12&lt;08:31,  1.47it/s] 25%|‚ñà‚ñà‚ñç       | 249/999 [02:13&lt;08:32,  1.46it/s] 25%|‚ñà‚ñà‚ñå       | 250/999 [02:14&lt;08:32,  1.46it/s] 25%|‚ñà‚ñà‚ñå       | 251/999 [02:14&lt;08:28,  1.47it/s] 25%|‚ñà‚ñà‚ñå       | 252/999 [02:15&lt;08:24,  1.48it/s] 25%|‚ñà‚ñà‚ñå       | 253/999 [02:16&lt;08:32,  1.45it/s] 25%|‚ñà‚ñà‚ñå       | 254/999 [02:16&lt;08:45,  1.42it/s] 26%|‚ñà‚ñà‚ñå       | 255/999 [02:17&lt;08:30,  1.46it/s] 26%|‚ñà‚ñà‚ñå       | 256/999 [02:18&lt;08:19,  1.49it/s] 26%|‚ñà‚ñà‚ñå       | 257/999 [02:18&lt;08:08,  1.52it/s] 26%|‚ñà‚ñà‚ñå       | 258/999 [02:19&lt;08:10,  1.51it/s] 26%|‚ñà‚ñà‚ñå       | 259/999 [02:20&lt;07:59,  1.54it/s] 26%|‚ñà‚ñà‚ñå       | 260/999 [02:20&lt;07:56,  1.55it/s] 26%|‚ñà‚ñà‚ñå       | 261/999 [02:21&lt;07:49,  1.57it/s] 26%|‚ñà‚ñà‚ñå       | 262/999 [02:21&lt;07:32,  1.63it/s] 26%|‚ñà‚ñà‚ñã       | 263/999 [02:22&lt;07:30,  1.63it/s] 26%|‚ñà‚ñà‚ñã       | 264/999 [02:23&lt;07:42,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 265/999 [02:23&lt;07:42,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 266/999 [02:24&lt;07:35,  1.61it/s] 27%|‚ñà‚ñà‚ñã       | 267/999 [02:25&lt;07:33,  1.61it/s] 27%|‚ñà‚ñà‚ñã       | 268/999 [02:25&lt;07:45,  1.57it/s] 27%|‚ñà‚ñà‚ñã       | 269/999 [02:26&lt;07:42,  1.58it/s] 27%|‚ñà‚ñà‚ñã       | 270/999 [02:27&lt;07:48,  1.56it/s] 27%|‚ñà‚ñà‚ñã       | 271/999 [02:27&lt;07:38,  1.59it/s] 27%|‚ñà‚ñà‚ñã       | 272/999 [02:28&lt;07:45,  1.56it/s] 27%|‚ñà‚ñà‚ñã       | 273/999 [02:28&lt;07:49,  1.55it/s] 27%|‚ñà‚ñà‚ñã       | 274/999 [02:29&lt;07:44,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 275/999 [02:30&lt;07:48,  1.55it/s] 28%|‚ñà‚ñà‚ñä       | 276/999 [02:30&lt;07:48,  1.54it/s] 28%|‚ñà‚ñà‚ñä       | 277/999 [02:31&lt;07:40,  1.57it/s] 28%|‚ñà‚ñà‚ñä       | 278/999 [02:32&lt;07:41,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 279/999 [02:32&lt;07:41,  1.56it/s] 28%|‚ñà‚ñà‚ñä       | 280/999 [02:33&lt;07:54,  1.52it/s] 28%|‚ñà‚ñà‚ñä       | 281/999 [02:34&lt;07:56,  1.51it/s] 28%|‚ñà‚ñà‚ñä       | 282/999 [02:34&lt;08:04,  1.48it/s] 28%|‚ñà‚ñà‚ñä       | 283/999 [02:35&lt;08:11,  1.46it/s] 28%|‚ñà‚ñà‚ñä       | 284/999 [02:36&lt;08:23,  1.42it/s] 29%|‚ñà‚ñà‚ñä       | 285/999 [02:37&lt;08:33,  1.39it/s] 29%|‚ñà‚ñà‚ñä       | 286/999 [02:37&lt;08:23,  1.42it/s] 29%|‚ñà‚ñà‚ñä       | 287/999 [02:38&lt;08:30,  1.39it/s] 29%|‚ñà‚ñà‚ñâ       | 288/999 [02:39&lt;08:16,  1.43it/s] 29%|‚ñà‚ñà‚ñâ       | 289/999 [02:39&lt;08:29,  1.39it/s] 29%|‚ñà‚ñà‚ñâ       | 290/999 [02:40&lt;08:27,  1.40it/s] 29%|‚ñà‚ñà‚ñâ       | 291/999 [02:41&lt;08:21,  1.41it/s] 29%|‚ñà‚ñà‚ñâ       | 292/999 [02:42&lt;08:17,  1.42it/s] 29%|‚ñà‚ñà‚ñâ       | 293/999 [02:42&lt;08:11,  1.44it/s] 29%|‚ñà‚ñà‚ñâ       | 294/999 [02:43&lt;08:09,  1.44it/s] 30%|‚ñà‚ñà‚ñâ       | 295/999 [02:44&lt;08:12,  1.43it/s] 30%|‚ñà‚ñà‚ñâ       | 296/999 [02:44&lt;08:21,  1.40it/s] 30%|‚ñà‚ñà‚ñâ       | 297/999 [02:45&lt;08:23,  1.39it/s] 30%|‚ñà‚ñà‚ñâ       | 298/999 [02:46&lt;08:34,  1.36it/s] 30%|‚ñà‚ñà‚ñâ       | 299/999 [02:47&lt;08:17,  1.41it/s] 30%|‚ñà‚ñà‚ñà       | 300/999 [02:47&lt;08:17,  1.41it/s] 30%|‚ñà‚ñà‚ñà       | 301/999 [02:48&lt;08:22,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 302/999 [02:49&lt;08:19,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 303/999 [02:49&lt;08:19,  1.39it/s] 30%|‚ñà‚ñà‚ñà       | 304/999 [02:50&lt;08:16,  1.40it/s] 31%|‚ñà‚ñà‚ñà       | 305/999 [02:51&lt;08:14,  1.40it/s] 31%|‚ñà‚ñà‚ñà       | 306/999 [02:52&lt;08:04,  1.43it/s] 31%|‚ñà‚ñà‚ñà       | 307/999 [02:52&lt;07:59,  1.44it/s] 31%|‚ñà‚ñà‚ñà       | 308/999 [02:53&lt;07:49,  1.47it/s] 31%|‚ñà‚ñà‚ñà       | 309/999 [02:54&lt;07:55,  1.45it/s] 31%|‚ñà‚ñà‚ñà       | 310/999 [02:54&lt;08:00,  1.43it/s] 31%|‚ñà‚ñà‚ñà       | 311/999 [02:55&lt;08:03,  1.42it/s] 31%|‚ñà‚ñà‚ñà       | 312/999 [02:56&lt;08:11,  1.40it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 313/999 [02:56&lt;08:11,  1.40it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 314/999 [02:57&lt;08:04,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 315/999 [02:58&lt;08:06,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 316/999 [02:59&lt;08:05,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 317/999 [02:59&lt;08:03,  1.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 318/999 [03:00&lt;07:56,  1.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 319/999 [03:01&lt;08:07,  1.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 320/999 [03:01&lt;08:11,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 321/999 [03:02&lt;08:24,  1.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 322/999 [03:03&lt;08:10,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 323/999 [03:04&lt;08:09,  1.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 324/999 [03:04&lt;08:10,  1.38it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 325/999 [03:05&lt;07:55,  1.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 326/999 [03:06&lt;07:52,  1.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 327/999 [03:06&lt;08:04,  1.39it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 328/999 [03:07&lt;07:56,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 329/999 [03:08&lt;07:56,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 330/999 [03:09&lt;07:52,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 331/999 [03:09&lt;08:01,  1.39it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 332/999 [03:10&lt;07:52,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 333/999 [03:11&lt;07:51,  1.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 334/999 [03:11&lt;07:47,  1.42it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 335/999 [03:12&lt;07:43,  1.43it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 336/999 [03:13&lt;07:50,  1.41it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 337/999 [03:14&lt;07:47,  1.41it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 338/999 [03:14&lt;07:55,  1.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 339/999 [03:15&lt;07:43,  1.42it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 340/999 [03:16&lt;07:35,  1.45it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 341/999 [03:16&lt;07:41,  1.43it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 342/999 [03:17&lt;07:35,  1.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 343/999 [03:18&lt;07:34,  1.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 344/999 [03:18&lt;07:38,  1.43it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 345/999 [03:19&lt;07:31,  1.45it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 346/999 [03:20&lt;07:34,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 347/999 [03:21&lt;07:36,  1.43it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 348/999 [03:21&lt;07:40,  1.41it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 349/999 [03:22&lt;07:31,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 350/999 [03:23&lt;07:31,  1.44it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 351/999 [03:23&lt;07:37,  1.42it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 352/999 [03:24&lt;07:38,  1.41it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 353/999 [03:25&lt;07:42,  1.40it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 354/999 [03:25&lt;07:40,  1.40it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 355/999 [03:26&lt;07:32,  1.42it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 356/999 [03:27&lt;07:26,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 357/999 [03:28&lt;07:23,  1.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 358/999 [03:28&lt;07:19,  1.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 359/999 [03:29&lt;07:27,  1.43it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 360/999 [03:30&lt;07:22,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 361/999 [03:30&lt;07:36,  1.40it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 362/999 [03:31&lt;07:30,  1.42it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 363/999 [03:32&lt;07:20,  1.44it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 364/999 [03:32&lt;07:15,  1.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 365/999 [03:33&lt;07:17,  1.45it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 366/999 [03:34&lt;07:04,  1.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 367/999 [03:34&lt;07:07,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 368/999 [03:35&lt;07:07,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 369/999 [03:36&lt;07:08,  1.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 370/999 [03:36&lt;07:06,  1.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 371/999 [03:37&lt;07:04,  1.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 372/999 [03:38&lt;06:59,  1.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 373/999 [03:38&lt;06:56,  1.50it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 374/999 [03:39&lt;06:55,  1.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 375/999 [03:40&lt;06:49,  1.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 376/999 [03:40&lt;06:48,  1.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 377/999 [03:41&lt;06:48,  1.52it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 378/999 [03:42&lt;06:52,  1.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 379/999 [03:42&lt;06:50,  1.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 380/999 [03:43&lt;06:59,  1.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 381/999 [03:44&lt;07:05,  1.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 382/999 [03:44&lt;06:53,  1.49it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 383/999 [03:45&lt;06:50,  1.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 384/999 [03:46&lt;07:00,  1.46it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 385/999 [03:46&lt;07:01,  1.46it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 386/999 [03:47&lt;07:01,  1.45it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 387/999 [03:48&lt;06:45,  1.51it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 388/999 [03:48&lt;06:52,  1.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 389/999 [03:49&lt;06:51,  1.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 390/999 [03:50&lt;06:50,  1.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 391/999 [03:51&lt;06:52,  1.47it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 392/999 [03:51&lt;06:58,  1.45it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 393/999 [03:52&lt;07:01,  1.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 394/999 [03:53&lt;06:57,  1.45it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 395/999 [03:53&lt;06:57,  1.45it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 396/999 [03:54&lt;07:02,  1.43it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 397/999 [03:55&lt;07:04,  1.42it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 398/999 [03:56&lt;07:12,  1.39it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 399/999 [03:56&lt;07:19,  1.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 400/999 [03:57&lt;07:20,  1.36it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 401/999 [03:58&lt;07:17,  1.37it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 402/999 [03:58&lt;07:11,  1.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 403/999 [03:59&lt;07:43,  1.29it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 404/999 [04:00&lt;07:26,  1.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 405/999 [04:01&lt;07:14,  1.37it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 406/999 [04:01&lt;07:01,  1.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 407/999 [04:02&lt;06:55,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 408/999 [04:03&lt;06:52,  1.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 409/999 [04:03&lt;06:56,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 410/999 [04:04&lt;06:53,  1.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 411/999 [04:05&lt;07:00,  1.40it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 412/999 [04:06&lt;06:48,  1.44it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 413/999 [04:06&lt;06:42,  1.46it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 414/999 [04:07&lt;06:42,  1.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 415/999 [04:08&lt;06:44,  1.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/999 [04:08&lt;06:53,  1.41it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 417/999 [04:09&lt;06:59,  1.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 418/999 [04:10&lt;06:55,  1.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 419/999 [04:11&lt;07:03,  1.37it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/999 [04:11&lt;07:06,  1.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 421/999 [04:12&lt;07:06,  1.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 422/999 [04:13&lt;07:09,  1.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 423/999 [04:14&lt;07:09,  1.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 424/999 [04:14&lt;07:06,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 425/999 [04:15&lt;07:09,  1.34it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 426/999 [04:16&lt;07:03,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 427/999 [04:17&lt;07:12,  1.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 428/999 [04:17&lt;07:04,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 429/999 [04:18&lt;07:16,  1.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 430/999 [04:19&lt;07:02,  1.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 431/999 [04:19&lt;06:45,  1.40it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/999 [04:20&lt;06:35,  1.43it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 433/999 [04:21&lt;06:32,  1.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 434/999 [04:22&lt;06:41,  1.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 435/999 [04:22&lt;06:37,  1.42it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 436/999 [04:23&lt;06:44,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/999 [04:24&lt;06:49,  1.37it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 438/999 [04:24&lt;06:43,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 439/999 [04:25&lt;06:40,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/999 [04:26&lt;06:45,  1.38it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 441/999 [04:27&lt;06:39,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 442/999 [04:27&lt;06:38,  1.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 443/999 [04:28&lt;06:38,  1.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 444/999 [04:29&lt;06:23,  1.45it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 445/999 [04:29&lt;06:29,  1.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 446/999 [04:30&lt;06:32,  1.41it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 447/999 [04:31&lt;06:33,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/999 [04:32&lt;06:23,  1.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 449/999 [04:32&lt;06:25,  1.43it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 450/999 [04:33&lt;06:32,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 451/999 [04:34&lt;06:32,  1.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 452/999 [04:34&lt;06:43,  1.36it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 453/999 [04:35&lt;06:46,  1.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 454/999 [04:36&lt;06:47,  1.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 455/999 [04:37&lt;06:40,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 456/999 [04:37&lt;06:35,  1.37it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 457/999 [04:38&lt;06:31,  1.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 458/999 [04:39&lt;06:36,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 459/999 [04:40&lt;06:41,  1.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/999 [04:40&lt;06:39,  1.35it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 461/999 [04:41&lt;06:49,  1.31it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 462/999 [04:42&lt;06:35,  1.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 463/999 [04:43&lt;06:32,  1.37it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/999 [04:43&lt;06:36,  1.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 465/999 [04:44&lt;06:37,  1.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 466/999 [04:45&lt;06:40,  1.33it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 467/999 [04:46&lt;06:27,  1.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 468/999 [04:46&lt;06:21,  1.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 469/999 [04:47&lt;06:15,  1.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 470/999 [04:48&lt;06:20,  1.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 471/999 [04:48&lt;06:26,  1.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 472/999 [04:49&lt;06:31,  1.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 473/999 [04:50&lt;06:36,  1.33it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 474/999 [04:51&lt;06:21,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 475/999 [04:51&lt;06:16,  1.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 476/999 [04:52&lt;06:09,  1.41it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 477/999 [04:53&lt;06:06,  1.42it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 478/999 [04:53&lt;06:12,  1.40it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 479/999 [04:54&lt;06:15,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/999 [04:55&lt;06:16,  1.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 481/999 [04:56&lt;06:26,  1.34it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 482/999 [04:56&lt;06:23,  1.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 483/999 [04:57&lt;06:24,  1.34it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 484/999 [04:58&lt;06:09,  1.39it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 485/999 [04:59&lt;06:07,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 486/999 [04:59&lt;05:59,  1.43it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 487/999 [05:00&lt;06:05,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 488/999 [05:01&lt;06:04,  1.40it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 489/999 [05:01&lt;06:01,  1.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 490/999 [05:02&lt;05:51,  1.45it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 491/999 [05:03&lt;05:44,  1.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 492/999 [05:03&lt;05:40,  1.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 493/999 [05:04&lt;05:46,  1.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 494/999 [05:05&lt;05:52,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 495/999 [05:05&lt;05:51,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/999 [05:06&lt;05:56,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 497/999 [05:07&lt;06:00,  1.39it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 498/999 [05:08&lt;06:16,  1.33it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/999 [05:08&lt;05:53,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/999 [05:09&lt;05:48,  1.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 501/999 [05:10&lt;05:37,  1.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 502/999 [05:10&lt;05:51,  1.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 503/999 [05:11&lt;05:42,  1.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/999 [05:12&lt;05:31,  1.49it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 505/999 [05:12&lt;05:29,  1.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 506/999 [05:13&lt;05:35,  1.47it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 507/999 [05:14&lt;05:38,  1.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 508/999 [05:14&lt;05:25,  1.51it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 509/999 [05:15&lt;05:22,  1.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 510/999 [05:16&lt;05:30,  1.48it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 511/999 [05:16&lt;05:20,  1.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 512/999 [05:17&lt;05:14,  1.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 513/999 [05:18&lt;05:11,  1.56it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 514/999 [05:18&lt;05:12,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 515/999 [05:19&lt;05:17,  1.53it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 516/999 [05:20&lt;05:13,  1.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 517/999 [05:20&lt;05:13,  1.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 518/999 [05:21&lt;05:13,  1.53it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 519/999 [05:22&lt;05:10,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/999 [05:22&lt;05:05,  1.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 521/999 [05:23&lt;05:07,  1.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 522/999 [05:23&lt;05:06,  1.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 523/999 [05:24&lt;05:00,  1.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 524/999 [05:25&lt;05:06,  1.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 525/999 [05:26&lt;05:21,  1.47it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 526/999 [05:26&lt;05:22,  1.46it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 527/999 [05:27&lt;05:15,  1.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/999 [05:28&lt;05:16,  1.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 529/999 [05:28&lt;05:13,  1.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 530/999 [05:29&lt;05:24,  1.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 531/999 [05:30&lt;05:24,  1.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 532/999 [05:30&lt;05:26,  1.43it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 533/999 [05:31&lt;05:31,  1.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 534/999 [05:32&lt;05:26,  1.43it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 535/999 [05:32&lt;05:19,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 536/999 [05:33&lt;05:18,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 537/999 [05:34&lt;05:17,  1.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 538/999 [05:34&lt;05:11,  1.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 539/999 [05:35&lt;05:06,  1.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/999 [05:36&lt;05:01,  1.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 541/999 [05:36&lt;05:00,  1.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 542/999 [05:37&lt;04:57,  1.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 543/999 [05:38&lt;05:07,  1.49it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/999 [05:38&lt;05:09,  1.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 545/999 [05:39&lt;05:12,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 546/999 [05:40&lt;05:10,  1.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 547/999 [05:41&lt;05:12,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 548/999 [05:41&lt;05:08,  1.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 549/999 [05:42&lt;05:02,  1.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 550/999 [05:43&lt;05:01,  1.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 551/999 [05:43&lt;05:08,  1.45it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 552/999 [05:44&lt;05:02,  1.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 553/999 [05:45&lt;05:02,  1.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 554/999 [05:45&lt;05:05,  1.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 555/999 [05:46&lt;05:07,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 556/999 [05:47&lt;05:06,  1.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 557/999 [05:47&lt;05:08,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 558/999 [05:48&lt;05:07,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 559/999 [05:49&lt;05:09,  1.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/999 [05:49&lt;05:07,  1.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/999 [05:50&lt;05:03,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 562/999 [05:51&lt;05:02,  1.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 563/999 [05:52&lt;05:11,  1.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 564/999 [05:52&lt;05:05,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 565/999 [05:53&lt;05:06,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 566/999 [05:54&lt;05:00,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 567/999 [05:54&lt;04:57,  1.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 568/999 [05:55&lt;04:59,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 569/999 [05:56&lt;05:04,  1.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 570/999 [05:57&lt;05:03,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 571/999 [05:57&lt;05:02,  1.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 572/999 [05:58&lt;04:56,  1.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 573/999 [05:59&lt;04:53,  1.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 574/999 [05:59&lt;04:57,  1.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 575/999 [06:00&lt;04:49,  1.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/999 [06:01&lt;04:43,  1.49it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 577/999 [06:01&lt;04:46,  1.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 578/999 [06:02&lt;04:43,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 579/999 [06:03&lt;04:39,  1.50it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/999 [06:03&lt;04:36,  1.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 581/999 [06:04&lt;04:45,  1.46it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 582/999 [06:05&lt;04:41,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 583/999 [06:05&lt;04:41,  1.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 584/999 [06:06&lt;04:44,  1.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 585/999 [06:07&lt;04:45,  1.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 586/999 [06:07&lt;04:51,  1.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 587/999 [06:08&lt;04:53,  1.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 588/999 [06:09&lt;04:54,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 589/999 [06:10&lt;04:49,  1.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 590/999 [06:10&lt;04:51,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 591/999 [06:11&lt;04:55,  1.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/999 [06:12&lt;04:51,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 593/999 [06:12&lt;04:49,  1.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 594/999 [06:13&lt;05:00,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 595/999 [06:14&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 596/999 [06:15&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 597/999 [06:16&lt;05:04,  1.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 598/999 [06:16&lt;04:57,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/999 [06:17&lt;04:59,  1.34it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/999 [06:18&lt;04:55,  1.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 601/999 [06:18&lt;04:50,  1.37it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 602/999 [06:19&lt;04:46,  1.39it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 603/999 [06:20&lt;04:40,  1.41it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 604/999 [06:21&lt;04:50,  1.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 605/999 [06:21&lt;04:47,  1.37it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 606/999 [06:22&lt;04:52,  1.34it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 607/999 [06:23&lt;04:44,  1.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/999 [06:23&lt;04:36,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 609/999 [06:24&lt;04:38,  1.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 610/999 [06:25&lt;04:36,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 611/999 [06:26&lt;04:35,  1.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 612/999 [06:26&lt;04:37,  1.39it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 613/999 [06:27&lt;04:31,  1.42it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 614/999 [06:28&lt;04:26,  1.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 615/999 [06:28&lt;04:28,  1.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 616/999 [06:29&lt;04:37,  1.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 617/999 [06:30&lt;04:31,  1.40it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 618/999 [06:31&lt;04:27,  1.42it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 619/999 [06:31&lt;04:21,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/999 [06:32&lt;04:20,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 621/999 [06:33&lt;04:18,  1.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 622/999 [06:33&lt;04:19,  1.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 623/999 [06:34&lt;04:23,  1.43it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/999 [06:35&lt;04:14,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 625/999 [06:35&lt;04:14,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 626/999 [06:36&lt;04:12,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 627/999 [06:37&lt;04:05,  1.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 628/999 [06:37&lt;04:09,  1.49it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 629/999 [06:38&lt;04:09,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 630/999 [06:39&lt;04:11,  1.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 631/999 [06:39&lt;04:08,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/999 [06:40&lt;04:08,  1.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 633/999 [06:41&lt;04:04,  1.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 634/999 [06:41&lt;04:06,  1.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 635/999 [06:42&lt;04:04,  1.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 636/999 [06:43&lt;04:03,  1.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 637/999 [06:43&lt;03:59,  1.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 638/999 [06:44&lt;04:04,  1.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 639/999 [06:45&lt;04:04,  1.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/999 [06:45&lt;04:05,  1.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 641/999 [06:46&lt;04:05,  1.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 642/999 [06:47&lt;04:06,  1.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 643/999 [06:48&lt;04:05,  1.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 644/999 [06:48&lt;04:04,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 645/999 [06:49&lt;03:57,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 646/999 [06:50&lt;04:02,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 647/999 [06:50&lt;04:03,  1.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 648/999 [06:51&lt;03:55,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 649/999 [06:52&lt;03:51,  1.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 650/999 [06:52&lt;03:46,  1.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 651/999 [06:53&lt;03:45,  1.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 652/999 [06:53&lt;03:51,  1.50it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 653/999 [06:54&lt;03:52,  1.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 654/999 [06:55&lt;03:43,  1.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 655/999 [06:55&lt;03:38,  1.58it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/999 [06:56&lt;03:47,  1.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 657/999 [06:57&lt;03:49,  1.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 658/999 [06:57&lt;03:42,  1.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 659/999 [06:58&lt;03:42,  1.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/999 [06:59&lt;03:44,  1.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 661/999 [06:59&lt;03:42,  1.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 662/999 [07:00&lt;03:45,  1.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 663/999 [07:01&lt;03:48,  1.47it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 664/999 [07:01&lt;03:49,  1.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 665/999 [07:02&lt;03:45,  1.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 666/999 [07:03&lt;03:41,  1.50it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 667/999 [07:03&lt;03:36,  1.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 668/999 [07:04&lt;03:39,  1.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 669/999 [07:05&lt;03:43,  1.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 670/999 [07:05&lt;03:43,  1.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 671/999 [07:06&lt;03:36,  1.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/999 [07:07&lt;03:31,  1.54it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 673/999 [07:07&lt;03:32,  1.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 674/999 [07:08&lt;03:27,  1.57it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/999 [07:09&lt;03:29,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 676/999 [07:09&lt;03:28,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 677/999 [07:10&lt;03:27,  1.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 678/999 [07:11&lt;03:28,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 679/999 [07:11&lt;03:27,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/999 [07:12&lt;03:27,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 681/999 [07:13&lt;03:24,  1.56it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 682/999 [07:13&lt;03:19,  1.59it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 683/999 [07:14&lt;03:24,  1.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 684/999 [07:14&lt;03:27,  1.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/999 [07:15&lt;03:28,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 686/999 [07:16&lt;03:28,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 687/999 [07:17&lt;03:32,  1.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/999 [07:17&lt;03:32,  1.46it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 689/999 [07:18&lt;03:31,  1.46it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 690/999 [07:19&lt;03:28,  1.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 691/999 [07:19&lt;03:26,  1.49it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 692/999 [07:20&lt;03:23,  1.51it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 693/999 [07:21&lt;03:24,  1.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 694/999 [07:21&lt;03:31,  1.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 695/999 [07:22&lt;03:32,  1.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 696/999 [07:23&lt;03:38,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 697/999 [07:24&lt;03:38,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 698/999 [07:24&lt;03:38,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/999 [07:25&lt;03:37,  1.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/999 [07:26&lt;03:34,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 701/999 [07:26&lt;03:33,  1.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 702/999 [07:27&lt;03:41,  1.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 703/999 [07:28&lt;03:39,  1.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/999 [07:29&lt;03:34,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 705/999 [07:29&lt;03:42,  1.32it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 706/999 [07:30&lt;03:38,  1.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 707/999 [07:31&lt;03:32,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 708/999 [07:32&lt;03:32,  1.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 709/999 [07:32&lt;03:30,  1.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 710/999 [07:33&lt;03:34,  1.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 711/999 [07:34&lt;03:32,  1.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 712/999 [07:35&lt;03:31,  1.36it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 713/999 [07:35&lt;03:25,  1.39it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 714/999 [07:36&lt;03:29,  1.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 715/999 [07:37&lt;03:28,  1.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 716/999 [07:37&lt;03:27,  1.37it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 717/999 [07:38&lt;03:33,  1.32it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 718/999 [07:39&lt;03:31,  1.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/999 [07:40&lt;03:33,  1.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/999 [07:41&lt;03:38,  1.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 721/999 [07:41&lt;03:41,  1.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 722/999 [07:42&lt;03:38,  1.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 723/999 [07:43&lt;03:34,  1.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 724/999 [07:44&lt;03:31,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 725/999 [07:45&lt;03:32,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 726/999 [07:45&lt;03:30,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 727/999 [07:46&lt;03:27,  1.31it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/999 [07:47&lt;03:28,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 729/999 [07:48&lt;03:26,  1.31it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 730/999 [07:48&lt;03:32,  1.26it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 731/999 [07:49&lt;03:32,  1.26it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 732/999 [07:50&lt;03:27,  1.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 733/999 [07:51&lt;03:24,  1.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 734/999 [07:51&lt;03:18,  1.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 735/999 [07:52&lt;03:21,  1.31it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/999 [07:53&lt;03:28,  1.26it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 737/999 [07:54&lt;03:25,  1.27it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 738/999 [07:55&lt;03:38,  1.19it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 739/999 [07:56&lt;03:44,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 740/999 [07:57&lt;03:45,  1.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 741/999 [07:57&lt;03:42,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 742/999 [07:58&lt;03:41,  1.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 743/999 [07:59&lt;03:37,  1.18it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 744/999 [08:00&lt;03:40,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 745/999 [08:01&lt;03:37,  1.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 746/999 [08:02&lt;03:36,  1.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 747/999 [08:03&lt;03:33,  1.18it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 748/999 [08:03&lt;03:37,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 749/999 [08:04&lt;03:36,  1.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 750/999 [08:05&lt;03:38,  1.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 751/999 [08:06&lt;03:38,  1.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/999 [08:07&lt;03:33,  1.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 753/999 [08:08&lt;03:31,  1.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 754/999 [08:09&lt;03:31,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 755/999 [08:10&lt;03:34,  1.14it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 756/999 [08:10&lt;03:31,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 757/999 [08:11&lt;03:31,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 758/999 [08:12&lt;03:28,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 759/999 [08:13&lt;03:27,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 760/999 [08:14&lt;03:26,  1.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 761/999 [08:15&lt;03:28,  1.14it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 762/999 [08:16&lt;03:25,  1.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 763/999 [08:17&lt;03:29,  1.12it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 764/999 [08:17&lt;03:27,  1.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 765/999 [08:18&lt;03:21,  1.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 766/999 [08:19&lt;03:20,  1.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 767/999 [08:20&lt;03:16,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/999 [08:21&lt;03:15,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 769/999 [08:22&lt;03:20,  1.15it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 770/999 [08:23&lt;03:14,  1.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 771/999 [08:23&lt;03:10,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 772/999 [08:24&lt;03:09,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 773/999 [08:25&lt;03:07,  1.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 774/999 [08:26&lt;03:05,  1.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 775/999 [08:27&lt;03:05,  1.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 776/999 [08:27&lt;03:06,  1.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 777/999 [08:28&lt;03:08,  1.17it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 778/999 [08:29&lt;03:11,  1.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 779/999 [08:30&lt;03:07,  1.17it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 780/999 [08:31&lt;03:05,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/999 [08:32&lt;03:04,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 782/999 [08:33&lt;03:03,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 783/999 [08:33&lt;03:03,  1.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/999 [08:34&lt;03:00,  1.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 785/999 [08:35&lt;03:08,  1.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 786/999 [08:36&lt;03:09,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 787/999 [08:37&lt;03:08,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 788/999 [08:38&lt;03:07,  1.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 789/999 [08:39&lt;03:09,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 790/999 [08:40&lt;03:07,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 791/999 [08:41&lt;03:04,  1.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 792/999 [08:42&lt;03:04,  1.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 793/999 [08:42&lt;03:05,  1.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 794/999 [08:43&lt;03:05,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 795/999 [08:44&lt;03:02,  1.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 796/999 [08:45&lt;02:59,  1.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 797/999 [08:46&lt;02:59,  1.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 798/999 [08:47&lt;03:00,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/999 [08:48&lt;03:02,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/999 [08:49&lt;03:01,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 801/999 [08:50&lt;02:58,  1.11it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 802/999 [08:51&lt;02:59,  1.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 803/999 [08:52&lt;02:59,  1.09it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 804/999 [08:52&lt;02:59,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 805/999 [08:53&lt;02:57,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 806/999 [08:54&lt;02:57,  1.08it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 807/999 [08:55&lt;02:56,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 808/999 [08:56&lt;02:53,  1.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 809/999 [08:57&lt;02:53,  1.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 810/999 [08:58&lt;02:50,  1.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 811/999 [08:59&lt;02:47,  1.12it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 812/999 [09:00&lt;02:45,  1.13it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 813/999 [09:01&lt;02:47,  1.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 814/999 [09:02&lt;02:52,  1.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 815/999 [09:03&lt;02:52,  1.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/999 [09:03&lt;02:49,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 817/999 [09:04&lt;02:48,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 818/999 [09:05&lt;02:47,  1.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 819/999 [09:06&lt;02:55,  1.03it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 820/999 [09:07&lt;02:55,  1.02it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 821/999 [09:08&lt;02:55,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 822/999 [09:09&lt;02:54,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 823/999 [09:10&lt;02:54,  1.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 824/999 [09:11&lt;02:54,  1.00it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 825/999 [09:12&lt;02:56,  1.01s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 826/999 [09:13&lt;02:50,  1.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 827/999 [09:14&lt;02:43,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 828/999 [09:15&lt;02:42,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 829/999 [09:16&lt;02:42,  1.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 830/999 [09:17&lt;02:38,  1.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 831/999 [09:18&lt;02:33,  1.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/999 [09:19&lt;02:33,  1.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 833/999 [09:20&lt;02:25,  1.14it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 834/999 [09:20&lt;02:26,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 835/999 [09:21&lt;02:27,  1.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 836/999 [09:22&lt;02:25,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 837/999 [09:23&lt;02:24,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 838/999 [09:24&lt;02:25,  1.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 839/999 [09:25&lt;02:23,  1.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 840/999 [09:26&lt;02:20,  1.13it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 841/999 [09:27&lt;02:16,  1.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 842/999 [09:28&lt;02:17,  1.14it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 843/999 [09:28&lt;02:15,  1.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 844/999 [09:29&lt;02:13,  1.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 845/999 [09:30&lt;02:11,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/999 [09:31&lt;02:10,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 847/999 [09:32&lt;02:09,  1.18it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 848/999 [09:33&lt;02:08,  1.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 849/999 [09:34&lt;02:10,  1.15it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 850/999 [09:34&lt;02:08,  1.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 851/999 [09:35&lt;02:09,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 852/999 [09:36&lt;02:08,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 853/999 [09:37&lt;02:08,  1.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 854/999 [09:38&lt;02:04,  1.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 855/999 [09:39&lt;02:05,  1.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 856/999 [09:40&lt;02:02,  1.17it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 857/999 [09:40&lt;01:58,  1.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 858/999 [09:41&lt;01:58,  1.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 859/999 [09:42&lt;01:59,  1.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/999 [09:43&lt;01:58,  1.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 861/999 [09:44&lt;02:01,  1.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 862/999 [09:45&lt;01:57,  1.17it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 863/999 [09:46&lt;01:59,  1.13it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 864/999 [09:47&lt;01:58,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 865/999 [09:47&lt;01:58,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 866/999 [09:48&lt;01:56,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 867/999 [09:49&lt;01:57,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 868/999 [09:50&lt;01:54,  1.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 869/999 [09:51&lt;01:51,  1.17it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 870/999 [09:52&lt;01:49,  1.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 871/999 [09:53&lt;01:50,  1.16it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 872/999 [09:54&lt;01:52,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 873/999 [09:54&lt;01:51,  1.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/999 [09:55&lt;01:51,  1.12it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 875/999 [09:56&lt;01:52,  1.11it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 876/999 [09:57&lt;01:51,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 877/999 [09:58&lt;01:52,  1.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 878/999 [09:59&lt;01:49,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 879/999 [10:00&lt;01:49,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 880/999 [10:01&lt;01:49,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 881/999 [10:02&lt;01:47,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 882/999 [10:03&lt;01:46,  1.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 883/999 [10:04&lt;01:46,  1.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 884/999 [10:04&lt;01:43,  1.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 885/999 [10:05&lt;01:39,  1.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 886/999 [10:06&lt;01:39,  1.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 887/999 [10:07&lt;01:38,  1.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/999 [10:08&lt;01:41,  1.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 889/999 [10:09&lt;01:40,  1.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 890/999 [10:10&lt;01:37,  1.12it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 891/999 [10:11&lt;01:34,  1.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 892/999 [10:11&lt;01:31,  1.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 893/999 [10:12&lt;01:30,  1.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 894/999 [10:13&lt;01:30,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 895/999 [10:14&lt;01:29,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 896/999 [10:15&lt;01:28,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 897/999 [10:16&lt;01:27,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 898/999 [10:17&lt;01:26,  1.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/999 [10:17&lt;01:25,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 900/999 [10:18&lt;01:24,  1.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 901/999 [10:19&lt;01:22,  1.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/999 [10:20&lt;01:22,  1.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 903/999 [10:21&lt;01:19,  1.21it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 904/999 [10:22&lt;01:19,  1.19it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 905/999 [10:23&lt;01:20,  1.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 906/999 [10:23&lt;01:19,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 907/999 [10:24&lt;01:18,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 908/999 [10:25&lt;01:16,  1.18it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 909/999 [10:26&lt;01:16,  1.18it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 910/999 [10:27&lt;01:16,  1.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 911/999 [10:28&lt;01:15,  1.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 912/999 [10:29&lt;01:15,  1.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 913/999 [10:29&lt;01:14,  1.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 914/999 [10:30&lt;01:13,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 915/999 [10:31&lt;01:13,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/999 [10:32&lt;01:12,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 917/999 [10:33&lt;01:12,  1.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 918/999 [10:34&lt;01:10,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 919/999 [10:35&lt;01:09,  1.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 920/999 [10:35&lt;01:06,  1.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 921/999 [10:36&lt;01:05,  1.19it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 922/999 [10:37&lt;01:03,  1.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 923/999 [10:38&lt;01:03,  1.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 924/999 [10:39&lt;01:02,  1.20it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 925/999 [10:40&lt;01:02,  1.18it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 926/999 [10:40&lt;01:01,  1.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 927/999 [10:41&lt;01:01,  1.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 928/999 [10:42&lt;01:00,  1.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 929/999 [10:43&lt;01:00,  1.16it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 930/999 [10:44&lt;00:59,  1.15it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 931/999 [10:45&lt;00:59,  1.15it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 932/999 [10:46&lt;00:58,  1.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 933/999 [10:47&lt;00:58,  1.13it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 934/999 [10:47&lt;00:56,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 935/999 [10:48&lt;00:54,  1.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 936/999 [10:49&lt;00:54,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 937/999 [10:50&lt;00:55,  1.11it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 938/999 [10:51&lt;00:53,  1.14it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 939/999 [10:52&lt;00:53,  1.13it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 940/999 [10:53&lt;00:50,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 941/999 [10:54&lt;00:50,  1.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 942/999 [10:54&lt;00:49,  1.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 943/999 [10:55&lt;00:48,  1.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/999 [10:56&lt;00:47,  1.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 945/999 [10:57&lt;00:45,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 946/999 [10:58&lt;00:45,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 947/999 [10:59&lt;00:43,  1.20it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 948/999 [10:59&lt;00:41,  1.22it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 949/999 [11:00&lt;00:41,  1.21it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 950/999 [11:01&lt;00:41,  1.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 951/999 [11:02&lt;00:41,  1.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 952/999 [11:03&lt;00:40,  1.15it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 953/999 [11:04&lt;00:40,  1.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 954/999 [11:05&lt;00:40,  1.12it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 955/999 [11:06&lt;00:39,  1.11it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 956/999 [11:07&lt;00:39,  1.10it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 957/999 [11:08&lt;00:38,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 958/999 [11:08&lt;00:37,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 959/999 [11:09&lt;00:37,  1.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 960/999 [11:10&lt;00:35,  1.09it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 961/999 [11:11&lt;00:35,  1.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 962/999 [11:12&lt;00:33,  1.09it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 963/999 [11:13&lt;00:33,  1.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 964/999 [11:14&lt;00:33,  1.04it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/999 [11:15&lt;00:32,  1.05it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 966/999 [11:16&lt;00:31,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 967/999 [11:17&lt;00:30,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 968/999 [11:18&lt;00:28,  1.07it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 969/999 [11:19&lt;00:28,  1.05it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 970/999 [11:20&lt;00:27,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 971/999 [11:21&lt;00:26,  1.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 972/999 [11:22&lt;00:24,  1.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 973/999 [11:23&lt;00:24,  1.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 974/999 [11:24&lt;00:23,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 975/999 [11:24&lt;00:22,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 976/999 [11:25&lt;00:21,  1.09it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 977/999 [11:26&lt;00:20,  1.06it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 978/999 [11:27&lt;00:19,  1.06it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/999 [11:28&lt;00:19,  1.04it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 980/999 [11:29&lt;00:18,  1.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 981/999 [11:30&lt;00:17,  1.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 982/999 [11:31&lt;00:16,  1.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 983/999 [11:32&lt;00:15,  1.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 984/999 [11:33&lt;00:14,  1.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 985/999 [11:34&lt;00:13,  1.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 986/999 [11:35&lt;00:12,  1.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 987/999 [11:36&lt;00:11,  1.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 988/999 [11:37&lt;00:10,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 989/999 [11:38&lt;00:09,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 990/999 [11:39&lt;00:08,  1.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 991/999 [11:40&lt;00:07,  1.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 992/999 [11:41&lt;00:06,  1.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/999 [11:42&lt;00:05,  1.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 994/999 [11:43&lt;00:04,  1.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 995/999 [11:44&lt;00:03,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 996/999 [11:45&lt;00:02,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 997/999 [11:46&lt;00:01,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 998/999 [11:47&lt;00:00,  1.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [11:47&lt;00:00,  1.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [11:47&lt;00:00,  1.41it/s]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(quantiles_theoriques, quantiles_empiriques)\nplt.plot(quantiles_theoriques, quantiles_theoriques, color='red', label='Premi√®re bissectrice')\nplt.title('QQ Plot - Quantiles empiriques vs Quantiles th√©oriques')\nplt.xlabel('Quantiles th√©oriques (distribution Skew Student)')\nplt.ylabel('Quantiles empiriques')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# # kstest y revenir\n# ks_stat, ks_p_value = kstest(data_train, skew_student_pdf, args=(**params_sstd,))\n\n# print(\"=\"*80)\n# print(\"H0 : Les donn√©es suivent une loi de Skew Student\")\n# print(f\"Statistique de test : {ks_stat:.4f}\")\n# print(f\"P-value : {ks_p_value:.4f}\")\n# print(\"=\"*80)\n# A revoir\n\n\n\nII.4.2. Calcul de la VaR Skew Student\n\n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var_fct(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\nsstd_var = sstd_var_fct(alpha, params_sstd)\nprint(f\"La VaR skew student pour h=1j et alpha={alpha} est : {sstd_var:.4%}\")\n\nLa VaR skew student pour h=1j et alpha=0.99 est : 4.2576%"
  },
  {
    "objectID": "3A/risque_def.html",
    "href": "3A/risque_def.html",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "",
    "text": "En finance, le risque peut √™tre d√©fini comme la survenance d‚Äôun √©v√©nement incertain qui peut avoir des cons√©quences n√©gatives sur le bilan, ou le compte de r√©sultat d‚Äôune banque. Par exemple, une fraude aura un impact n√©gative sur la r√©putation d‚Äôune banque ce qui peut entrainer des pertes importants ayant un impact n√©gatif sur le r√©sultat net de celle-ci. En √©conomie, le risque est un √©v√©nement probabilisable tandis que l‚Äôincertitude est non probabilisable.\nNous pouvons caract√©riser 3 grands types de risques √©tablis par le comit√© de B√¢le qui veille au renforcement et √† la stabilit√© du syst√®me financier. (rang√©s par ordre d‚Äôimportance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la cons√©quence. Toutefois, certains risque sont difficiles √† distinguer. Ils se trouvent √† la fronti√®re entre le risque de march√©, de cr√©dit et le risque op√©rationnel.\nIl est important de noter que le but d‚Äôune banque n‚Äôest pas de prendre le moins de risque, mais d‚Äôatteindre une rentabilit√© maximale pour un risque donn√©. La th√©orie financi√®re nous apprend que seul le risque est r√©mun√©r√©. La banque proc√®de donc √† une arbitrage entre risque et rentabilit√©. C‚Äôest pourquoi la gestion des risques est un √©l√©ment cl√© de la strat√©gie de d√©cision de la banque. La mesure du risque intervient pour calculer les fonds propres n√©cessaires pour assurer chaque op√©ration financi√®re. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "3A/risque_def.html#les-mesures-de-risque",
    "href": "3A/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al.¬†ont d√©fini les propri√©t√©s que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  coh√©rente. Une mesure de risque est une fonction qui permet de quantifier le risque d‚Äôun portefeuille. Elle est coh√©rente si elle satisfait les propri√©t√©s suivantes :\n\nsous-additivit√© : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomog√©n√©it√© positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivit√© signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal √† la somme des risques des actifs qui le composent. Ce ph√©nom√®ne est appel√© effet de diversification. En effet, la diversification permet de r√©duire le risque d‚Äôun portefeuille en investissant dans des actifs non corr√©l√©s. Ainsi, en agr√©geant deux porte-feuilles, il n‚Äôy a pas de cr√©ation de risque suppl√©mentaire.\nL‚Äôhomog√©n√©it√© positive signifie que le risque d‚Äôun portefeuille est proportionnel √† la taille du portefeuille. Cette propri√©t√© ignore les probl√®mes de liquidit√©.\nL‚Äôinvariance par translation signifie que l‚Äôaddition au portefeuille initiale un montant s√ªr r√©mun√©r√© au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d‚Äôimmobiliser des fonds propres √©gaux √† la mesure du risque.\nLa monotonie signifie que le risque d‚Äôun portefeuille est inf√©rieur ou √©gal au risque d‚Äôun autre portefeuille si la distribution de probabilit√© de la perte potentielle du premier portefeuille est inf√©rieure ou √©gale √† celle du deuxi√®me portefeuille. Cel√† traduit l‚Äôordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d‚Äôun portefeuille √† un instant t donn√©, le risque est la variation n√©gative de ce portefeuille dans le futur. Le risque se caract√©risait donc par une perte relativfe (par rapport √† la valeur initiale du portefeuille √† un instant t). Pendant tr√®s longtemps, les banques utilisaient la volatilit√© (√©cart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup √©volu√©e et celle qui est la plus r√©pandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d‚Äôun portefeuille √† un horizon \\(h\\) donn√© et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\no√π F est la distribution de probabilit√© de la perte potentielle du portefeuille.\nPar exemple, une VaR √† \\(\\alpha=1\\%\\) de 1 million d‚Äôeuros signifie que la probabilit√© que la banque perde plus de 1 million d‚Äôeuros est √©gale √† 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d‚Äôeuros sur une p√©riode donn√©e (C‚Äôest la perte maximale encourue par la banque avec un intervalle de confiance √† 99%). Nous allons pr√©f√©rer la deuxi√®me formulation de l‚Äôinterpr√©tation.\nDeux √©l√©ments sont n√©cessaires pour d√©terminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance √† un indicateur de tol√©rance pour le risque. Une couverture √† 99% est beaucoup plus exigente et donc plus co√ªteuse qu‚Äôune couverture √† 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait d√©finir l‚Äôhorizon h. Par exemple, une couverture √† 1 jour est moins co√ªteuse qu‚Äôune couverture √† 1 mois. C‚Äôest la combinaison de ces deux √©l√©ments qui d√©termine le degr√© de la couverture qui peut √™tre exprim√© en temps de retour 1 \\(t¬∞\\)qui est la dur√©e moyenne entre deux d√©passements de la VaR. Il permet de caract√©riser la raret√© d‚Äôun √©v√®nement (dont la probabilit√© d‚Äôoccurence est petite)\n\\[t¬∞= \\frac{h}{1-\\alpha}\\]\nLorsqu‚Äôon entend parler de gestion de risque d√©cennal, cel√† revient √† consid√©rer une valeur en risque (VaR) journali√®re (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non coh√©rente car elle ne respecte pas la propri√©t√© de sous-additivit√©. De nombreux professionnels recommanderaient alors l‚Äôutilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque coh√©rente. La CVAR est l‚Äôesp√©rance de la perte au del√† de la VaR. Toutefois, la VaR reste une mesure de risque tr√®s utilis√©e en pratique, qui ne respecte pas la propri√©t√© de sous-additivit√© que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilit√©s sont principalement localis√©es dans les quantiles extr√™mes.\n\n\n\nD‚Äôautres mesures de risque\nD‚Äôautres mesures peuvent √™tre d√©finis comme celle de la perte exceptionnelle (Unexpected Loss - UL) d√©finie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] o√π L est la distribution de la perte potentielle.\nIl s‚Äôagit l√† de la diff√©rence entre la VaR et la perte moyenne (expected loss - EL). Il y a √©galement le regret esp√©r√© d√©fini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donn√© repr√©sentant le montant de la perte tol√©rable par l‚Äôinstitut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l‚Äôinstitut financier. Lorsque H est endog√®ne, c‚Äôest-√†-dire d√©pendant de la distribution de la perte potentielle, et √©gale √† la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l‚Äôesp√©rance de la perte au del√† de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au del√† d‚Äôun certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au del√† de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c‚Äôest la semi variance."
  },
  {
    "objectID": "3A/risque_def.html#footnotes",
    "href": "3A/risque_def.html#footnotes",
    "title": "Le risque, qu‚Äôest ce que c‚Äôest ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\np√©riode de retour doit √™tre interpr√©t√©e comme la probabilit√© statistique qu‚Äôun √©v√®nement se produise‚Ü©Ô∏é"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "Le mod√®le classique de volatilit√© stochastique est d√©fini par les √©quations suivantes :\n\nProcessus des rendements :\n\\[ r_t = \\exp(x_t / 2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0,1) \\]\nProcessus de la volatilit√© logarithmique :\n\\[ x_t = \\mu + \\phi x_{t-1} + \\sigma_t \\eta_t, \\quad \\eta_t \\sim N(0,1) \\]\n\n\n( x_t ) suit un processus autor√©gressif de premier ordre (AR(1)) et suit une distribution normale conditionnelle : \\[ p(x_t) \\sim N(\\frac{\\mu}{1-\\phi} , \\frac{\\sigma_t^2}{1-\\phi^2}) \\]\n\\[ x_t | x_{t-1} \\sim N(\\mu + \\phi x_{t-1}, \\sigma_t^2) \\]\n( r_t ) suit une distribution normale conditionnelle :\n\\[ r_t | x_t \\sim N(0, \\exp(x_t)) \\]\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\nn &lt;- 252\nmu &lt;- -0.8\nphi &lt;- 0.9\nsigma_squared &lt;- 0.09\n\nx &lt;- numeric(n)  # Log-volatilit√©\nr &lt;- numeric(n)  # Rendements simul√©s\n\nfor (t in 1:n) {\n  if (t == 1) {\n    # Densit√© de transition stationnaire de x_t\n    x[t] &lt;- rnorm(1, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n  } else {\n    # √âvolution de l'√©tat\n    x[t] &lt;- mu + phi *x[t-1] + sqrt(sigma_squared) * rnorm(1, mean = 0, sd = 1)\n  }\n  # Simulation des rendements\n  r[t] &lt;- exp(x[t] / 2) * rnorm(1, mean = 0, sd = 1)\n}\n\n# extraction dans fichier csv\nwrite.csv(data.frame(r, x), \"true_sv_taylor.csv\", row.names = FALSE)\n\n\npar(mfrow=c(1,2))\nplot(x, lwd = 2, type = \"l\", col = \"blue\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© simul√©\")\nplot(r, lwd = 2, type = \"l\", col = \"red\", ylab = \"Rendements\", xlab = \"Temps\", main = \"Rendements simul√©s\")\n\n\n\n\n\n\n\n\n\n\n\nparams &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "params &lt;- c(mu,phi,sigma_squared)\nparams\n\n[1] -0.80  0.90  0.09\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Define parameters (ensure they are properly initialized)\nmu &lt;- params[1]\nphi &lt;- params[2]\nsigma_squared &lt;- params[3]\n\n# Definition des variables\n# D√©finition des param√®tres\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "",
    "text": "set.seed(123)  # Pour rendre les simulations reproductibles\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu/(1-phi), sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * x_particle[t - 1, ], sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-avec-les-rendements-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap avec les rendements",
    "text": "Filtre bootstrap avec les rendements\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\n# Definition des variables\n# D√©finition des param√®tres\n\nparams &lt;- c(mu,phi,sigma_squared)\n\nn &lt;- length(r)  # Nombre d'observations\nM &lt;- 10000      # Nombre de particules\n\n# Initialisation des matrices et vecteurs\nx_hat &lt;- numeric(n)                   # Estimation de x\nx_particle &lt;- matrix(nrow = n, ncol = M)  # Particules\nw &lt;- matrix(nrow = n, ncol = M)        # Poids des particules\nw_normalized &lt;- matrix(nrow = n, ncol = M) # Poids normalis√©s\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t,] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ] - mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- dnorm(r[t], mean = 0, sd = sqrt(exp(x_particle[t, ])))\n    \n    #  Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.2403607 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le Taylor\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",col=\"#1B9E77\",main = \"Mod√®le Taylor\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", bty=\"n\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nUtilisation de library(pmhtutorial)\n\nlibrary(pmhtutorial)\n\n# particleFilterSVmodel takes sigma as parameters\nparams[3] &lt;- sqrt(params[3])\nx_hat_2&lt;- particleFilterSVmodel(r,params,M)\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"log-volatilit√©\")\nlines(x_hat_2$xHatFiltered, type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Log-volatilit√© estim√©\")\n# legend\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat_2$xHatFiltered / 2) * rnorm(length(x_hat_2$xHatFiltered), mean = 0, sd = 1)\n\n# Superposition des trajectoires\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", bty=\"n\",\n  col=\"#1B9E77\", main=\"True returns\")\n\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\", \n     main = \"Estimated returns\",bty=\"n\")\n\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "href": "3A/proc_stochastique/modele_log_sv-part2.html#filtre-bootstrap-sur-le-mod√®le-log-sv-de-taylor-1",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre particulaire",
    "section": "Filtre bootstrap sur le mod√®le log-sv de taylor",
    "text": "Filtre bootstrap sur le mod√®le log-sv de taylor\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nparams &lt;- c(mu,phi,sigma_squared)\n\ny_hat &lt;- numeric(n)\nw_hat &lt;- numeric(n)\nx_particle &lt;- matrix(nrow = n, ncol = M)\nw &lt;- matrix(nrow = n, ncol = M)\nw_normalized &lt;- matrix(nrow = n, ncol = M)\ny &lt;- log(r**2)\n\n# Densit√© d'une log chi-deux de df 1\nlog_chi2 &lt;- function(x) {\n  density &lt;- exp( (x/2) - exp(x/2))/sqrt(2*pi)\n  return( density)\n}\n\n# Filtre particulaire bootstrap\nfor (t in 1:n) {\n  if (t == 1) {\n    # Initialisation des particules √† t = 0\n    x_particle[t, ] &lt;- rnorm(M, mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Poids initiaux (bas√©s sur la distribution de l'√©tat initial)\n    w[t, ] &lt;- dnorm(x_particle[t, ], mean = mu, sd = sqrt(sigma_squared / (1 - phi^2)))\n    \n    # Normalisation des poids\n    w_normalized[t, ] &lt;- w[t, ] / sum(w[t, ])\n    \n    # Estimation initiale\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  } else {\n    #  √âtape de pr√©diction (√©chantillonnage de nouvelles particules)\n    x_particle[t, ] &lt;- rnorm(M, mean = mu + phi * (x_particle[t - 1, ]-mu), sd = sqrt(sigma_squared))\n    \n    #  Mise √† jour des poids avec la vraisemblance de l'observation\n    w[t, ] &lt;- log_chi2(y[t] - x_particle[t,])\n    \n    #  Normalisation des poids\n    w_normalized[t,] &lt;- w[t,] / sum(w[t,])\n    \n    #  R√©√©chantillonnage des particules selon leurs poids\n    index &lt;- sample(1:M, size = M, prob = w_normalized[t, ], replace = TRUE)\n    x_particle[t, ] &lt;- x_particle[t, index]\n    \n    # R√©initialisation des poids apr√®s r√©√©chantillonnage (uniforme)\n    w_normalized[t, ] &lt;- 1 / M\n    \n    #  Estimation de x √† l'instant t (pond√©r√©e)\n    x_hat[t] &lt;- sum(w_normalized[t, ] * x_particle[t, ])\n  }\n}\n\n\n# Compute MSE\nmse &lt;- mean((x - x_hat)^2)\ncat(\"MSE:\", mse, \"\\n\")\n\nMSE: 0.3450866 \n\n\n\nplot(x, type = \"l\", col = \"black\", ylab = \"Log-volatilit√©\", xlab = \"Temps\", main = \"Mod√®le log-sv\")\nlines(x_hat,  type = \"l\", col = \"red\", ylab = \"Log-volatilit√©\", xlab = \"Temps\")\nlegend(\"topright\", legend=c(\"Log-vol obs.\", \"Log-vol est.\"), col=c(\"black\", \"red\"), lty=1:1, cex=0.8)\n\n\n\n\n\n\n\n\n\nset.seed(123)  # Pour rendre les simulations reproductibles\n\nr_est &lt;- exp(x_hat / 2) * rnorm(length(x_hat), mean = 0, sd = 1)\n\n# Plot the estimate and the true state\nplot(r, type=\"l\", xlab=\"time\", ylab=\"log-returns\", col=\"#1B9E77\", main = \"Mod√®le log-sv\")\n\n# Superposition des trajectoires\nlines(r_est, type = \"l\", col = \"red\",  xlab = \"Temps\", ylab = \"Rendements\")\n\n# legend\nlegend(\"topright\", legend=c(\"Rendements obs.\", \"Rendements est.\"), col=c(\"#1B9E77\", \"red\"), lty=1:1, cex=0.8)"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html",
    "href": "3A/proc_stochastique/modele_heston.html",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "",
    "text": "Le but de ce TP est de calculer des prix d‚Äôoptions sous le mod√®le d‚ÄôHeston puis de calibrer ce mod√®le par filtrage. On consid√®re le mod√®le suivant :\n\\[\n\\begin{cases}\ndS_s = S_s \\left( rds + \\sqrt{v_s} dW_s^1 \\right) \\\\\ndv_s = \\kappa (\\beta - v_s) ds + \\sigma \\sqrt{v_s} dW_s^2 \\\\\ndW_s^1 dW_s^2 = \\rho ds\n\\end{cases}\n\\quad (1)\n\\]\no√π \\(W_s^1\\) et \\(W_s^2\\) sont deux mouvements browniens et \\(r\\) est le taux sans risque. Pour ce mod√®le, les rendements sont mod√©lis√©s par un mouvement brownien g√©om√©trique avec une variance stochastique.\nLa volatilit√© non observ√©e \\(v_t\\) est d√©termin√©e par un processus stochastique de retour √† la moyenne (1) introduit en 1985 par Cox, Ingersoll et Ross pour la mod√©lisation des taux d‚Äôint√©r√™t √† court terme.\nLe param√®tre \\(\\kappa\\) est le param√®tre de retour √† la moyenne positive, \\(\\beta\\) est le param√®tre positif √† long terme et \\(\\eta\\) la volatilit√© positive du param√®tre de variance. De plus, Heston a introduit une corr√©lation entre les deux mouvements browniens \\(W_s^1\\) et \\(W_s^2\\), repr√©sent√©e par le param√®tre \\(\\rho\\) appartenant √† \\([-1,1]\\)."
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-forme-close",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la forme close",
    "text": "Avec la forme close\nSoit un Call de strike K et √† √©ch√©ance \\(\\tau\\) sous le mod√®le (1) avec les param√®tres suivants : \\(\\kappa\\) = 4,\\(\\beta\\) = 0.03,\\(\\sigma\\) = 0.4,r =0.05,\\(\\rho\\)=‚àí0.5,\\(\\tau\\) = 1, \\(S_0\\) = K=100,\\(v_0\\) = \\(\\beta\\).\nPour calculer le prix d‚Äôun Call, on peut utiliser la formule close de Heston (Heston 1993) :\n\\[\nC(S_0, K, \\tau) = S_0 P_1 - K e^{-r \\tau} P_2\n\\]\navec :\n\\[\nP_j(x, \\nu, T, \\ln(K)) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\Re \\left( \\frac{e^{-i \\ln(K) u} f_j(x,\\nu,t,u)}{i u} \\right) du\n\\]\no√π :\n\\[\nx = \\ln(S_t), \\quad f(x,\\nu,t,u) = \\exp(C(t,u) + D(t,u) \\nu + i \\phi x)\n\\]\net :\n\\[\nC(T-t = \\tau, \\phi) = r i \\phi t + \\frac{a}{\\sigma^2} \\left( (bj - \\rho \\sigma \\phi i + d)\\tau - 2 \\ln \\left( \\frac{1 - g e^{d \\tau}}{1 - g} \\right) \\right)\n\\]\n\\[\nD(T-t = \\tau, \\phi) = \\left( \\frac{bj - \\rho \\sigma \\phi i + d}{\\sigma^2} \\right) \\left( \\frac{1 - e^{d \\tau}}{1 - g e^{d \\tau}} \\right)\n\\]\n\\[\ng = \\frac{bj - \\rho \\sigma \\phi i + d}{bj - \\rho \\sigma \\phi i - d}\n\\]\n\\[\nd = \\sqrt{(\\rho \\sigma \\phi i - bj)^2 - \\sigma^2 (2 u_j \\phi i - \\phi^2)}\n\\]\n\\[\nu_1 = 1/2, \\quad u_2 = -1/2, a = \\lambda, b = \\kappa \\beta, \\quad t_1 = \\kappa - \\rho \\sigma, \\quad t_2 = \\kappa\n\\]\nPour ce faire, nous allons utiliser la fonction Heston_Call_Function.R qui permet de calculer le prix d‚Äôun Call sous le mod√®le d‚ÄôHeston avec la formule close.\n\n# Param√®tres\nkappa &lt;- 4\nbeta &lt;- 0.03\nsigma &lt;- 0.4\nr &lt;- 0.05\nrho &lt;- -0.5\ntau &lt;- 1\nS0&lt;- 100\nK &lt;- 100\nv0 &lt;- beta\n\n# Import Heston_Call_Function.R\nsource(\"data/Heston_Call_Function.R\")\n\n# Calcul du prix du Call\nCall_Heston &lt;- HestonCallClosedForm(lambda = kappa, vbar = beta, eta = sigma, rho = rho, v0 = v0, r = r, tau = tau, S0 = S0, K = K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.410405"
  },
  {
    "objectID": "3A/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "href": "3A/proc_stochastique/modele_heston.html#avec-la-m√©thode-de-monte-carlo-sch√©ma-deuler",
    "title": "Calibration avec le mod√®le d‚ÄôHeston",
    "section": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)",
    "text": "Avec la m√©thode de Monte Carlo (Sch√©ma d‚ÄôEuler)\nLorsqu‚Äôon a pas acc√®s √† la formule close, on peut utiliser la m√©thode de Monte Carlo pour calculer le prix d‚Äôun Call. Il s‚Äôagit de simuler le mod√®le (1) et de calculer le prix du Call √† partir des simulations. Pour simuler le mod√®le (1), on peut utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston (Euler and Milstein Discretization, Fabrice Douglas Rouah) ou utiliser la formule de Ito pour le mod√®le de Heston.\nDans notre cas, nous allons utiliser la discr√©tisation d‚ÄôEuler du mod√®le de Heston pour simuler le mod√®le (1) comme suit : \\[\n\\begin{cases}\nS_t = S_{t-1} \\left(1 + r \\Delta + \\sqrt{\\Delta v_t} W_t^1 \\right) \\\\[10pt]\nv_t = \\left| v_{t-1} + \\kappa \\Delta (\\beta - v_{t-1}) + \\sigma \\sqrt{v_{t-1}} \\Delta W_t^2 \\right| \\\\[10pt]\n\\text{Cov}(W_t^1, W_t^2) = \\rho\n\\end{cases}\n\\]\navec \\(W_t^1\\) et \\(W_t^2\\) des variables al√©atoires gaussiennes centr√©es r√©duites et corr√©l√©es entre elles telles que \\(\\text{Cov}(W_t^1, W_t^2) = \\rho\\). De plus, \\(\\Delta = \\frac{\\tau}{n}\\) est le pas de discr√©tisation, avec \\(n\\) le nombre de pas de discr√©tisation.\nDans notre cas, on d√©finit \\(n = 100\\) et on simule \\(M = 1000\\) mod√®le (1) pour calculer le prix d‚Äôun Call.\n\nHestonCallMC &lt;- function(M, N, lambda, vbar, eta, rho, v0, r, tau, S0, K){\n  # M: Number of Monte Carlo simulations\n  # N: Number of time steps\n  \n  set.seed(123)\n  dt &lt;- tau / N  # Time step\n\n  # Store final stock prices\n  ST &lt;- numeric(M)\n  \n  for (i in 1:M){\n    S &lt;- numeric(N+1)\n    v &lt;- numeric(N+1)\n    \n    S[1] &lt;- S0\n    v[1] &lt;- v0\n    \n    for (t in 1:N){\n      # Generate correlated Brownian motions\n      W1 &lt;- rnorm(1)\n      W2 &lt;- rho * W1 + sqrt(1 - rho^2) * rnorm(1)\n      \n      # Euler discretization of variance process (ensure non-negativity)\n      v[t+1] &lt;- abs(v[t] + lambda * (vbar - v[t]) * dt + eta * sqrt(v[t] * dt) * W1)\n      \n      # Euler discretization of the stock price process (log-normal form)\n      S[t+1] &lt;- S[t] * exp((r - 0.5 * v[t]) * dt + sqrt(v[t] * dt) * W2)\n    }\n    \n    # Store final stock price\n    ST[i] &lt;- S[N+1]\n  }\n\n  # Compute Call option price using Monte Carlo method\n  Call &lt;- exp(-r * tau) * mean(pmax(ST - K, 0), na.rm=TRUE)\n  \n  return(Call)\n}\n\nM &lt;- 1000\nN &lt;- 100\nCall_Heston &lt;-HestonCallMC(M,N, kappa, beta, sigma, rho, v0, r, tau, S0, K)\ncat(\"Le prix du Call est de \", Call_Heston)\n\nLe prix du Call est de  9.797915"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html",
    "href": "3A/gestion_actifs/profil_liquid.html",
    "title": "Profil d‚Äô√©coulement/ de liquidation de portefeuille",
    "section": "",
    "text": "Nous souhaitons calculer le profil d‚Äô√©coulement/liquidation dans les sc√©narios suivants :\nDans l‚Äôordre des √©tapes, il s‚Äôagira dans ce TP de faire :\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\ndef get_data(start_date, end_date, index_ticker, tickers, column=\"Close\"):\n    \"\"\"\n    Extraction de donn√©es de cours d'actions\n    Args:\n        start_date (str): Date de d√©but au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des volumes historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    # Extraction des volumes historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=3*31)\n\nselected_assets = {\n    \"AC.PA\": \"Accor\",\n    \"AI.PA\": \"Air Liquide\",\n    \"AIR.PA\": \"Airbus\",\n    \"MT.AS\": \"ArcelorMittal\",\n    \"CS.PA\": \"AXA\",\n    \"BNP.PA\": \"BNP Paribas\",\n    \"EN.PA\": \"Bouygues\",\n    \"BVI.PA\": \"Bureau Veritas\",\n    \"CAP.PA\": \"Capgemini\",\n    \"CA.PA\": \"Carrefour\",\n    \"ACA.PA\": \"Cr√©dit Agricole\",\n    \"BN.PA\": \"Danone\",\n    \"DSY.PA\": \"Dassault Syst√®mes\",\n    \"EDEN.PA\": \"Edenred\",\n    \"ENGI.PA\": \"Engie\",\n    \"EL.PA\": \"EssilorLuxottica\",\n    \"ERF.PA\": \"Eurofins Scientific\",\n    \"RMS.PA\": \"Herm√®s\",\n    \"KER.PA\": \"Kering\",\n    \"LR.PA\": \"Legrand\",\n    \"OR.PA\": \"L'Or√©al\",\n    \"MC.PA\": \"LVMH\",\n    \"ML.PA\": \"Michelin\",\n    \"ORA.PA\": \"Orange\",\n    \"RI.PA\": \"Pernod Ricard\",\n    \"PUB.PA\": \"Publicis\",\n    \"RNO.PA\": \"Renault\",\n    \"SAF.PA\": \"Safran\",\n    \"SGO.PA\": \"Saint-Gobain\",\n    \"SAN.PA\": \"Sanofi\",\n    \"SU.PA\": \"Schneider Electric\",\n    \"GLE.PA\": \"Soci√©t√© G√©n√©rale\",\n    \"STLA\": \"Stellantis\",\n    \"STMPA.PA\": \"STMicroelectronics\",\n    \"TEP.PA\": \"Teleperformance\",\n    \"HO.PA\": \"Thales\",\n    \"TTE.PA\": \"TotalEnergies\",\n    \"UNBLF\": \"Unibail-Rodamco-Westfield\",\n    \"VIE.PA\": \"Veolia\",\n    \"DG.PA\": \"Vinci\",\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker, column=\"Volume\")\n\n[                       0%                       ][                       0%                       ][****                   8%                       ]  3 of 40 completed[*****                 10%                       ]  4 of 40 completed[*****                 10%                       ]  4 of 40 completed[*******               15%                       ]  6 of 40 completed[*********             18%                       ]  7 of 40 completed[**********            20%                       ]  8 of 40 completed[***********           22%                       ]  9 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[***************       32%                       ]  13 of 40 completed[*****************     35%                       ]  14 of 40 completed[******************    38%                       ]  15 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************52%                       ]  21 of 40 completed[**********************55%*                      ]  22 of 40 completed[**********************57%**                     ]  23 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************90%******************     ]  36 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nAI.PA\nAIR.PA\nBN.PA\nBNP.PA\nBVI.PA\nCA.PA\nCAP.PA\nCS.PA\n...\nSAF.PA\nSAN.PA\nSGO.PA\nSTLA\nSTMPA.PA\nSU.PA\nTEP.PA\nTTE.PA\nUNBLF\nVIE.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-11-07\n698430.0\n13751044.0\n813490.0\n1091627.0\n1059857.0\n5322315.0\n646260.0\n2794758.0\n326259.0\n4163091.0\n...\n576180.0\n1527095.0\n1430888.0\n6819700.0\n2082219.0\n892806.0\n404002.0\n3786884.0\n0.0\n2809097.0\n\n\n2024-11-08\n640605.0\n4781311.0\n712553.0\n1541520.0\n1045775.0\n4731196.0\n367776.0\n4129105.0\n340411.0\n2828675.0\n...\n672105.0\n1414195.0\n1037715.0\n8197900.0\n1881978.0\n737207.0\n288437.0\n3327420.0\n100.0\n2139007.0\n\n\n2024-11-11\n544390.0\n3965216.0\n615456.0\n1013673.0\n1139407.0\n3062744.0\n621229.0\n2577272.0\n332633.0\n2860191.0\n...\n678397.0\n1208511.0\n877175.0\n7181500.0\n2067857.0\n808043.0\n268883.0\n3669304.0\n100.0\n1474874.0\n\n\n2024-11-12\n476433.0\n6774868.0\n957769.0\n1451643.0\n1319645.0\n3871602.0\n503435.0\n2216855.0\n419941.0\n4554097.0\n...\n928357.0\n1941126.0\n1059643.0\n5832100.0\n3211099.0\n939216.0\n376479.0\n5104044.0\n0.0\n2051518.0\n\n\n2024-11-13\n503706.0\n5911519.0\n752386.0\n1381466.0\n1085131.0\n2782805.0\n674357.0\n1985037.0\n520292.0\n3538137.0\n...\n665527.0\n1394445.0\n1431965.0\n6821700.0\n2456917.0\n990623.0\n206989.0\n4171178.0\n100.0\n2188795.0\n\n\n\n\n5 rows √ó 40 columns\nportfolio_data.index\n\nDatetimeIndex(['2024-11-07', '2024-11-08', '2024-11-11', '2024-11-12',\n               '2024-11-13', '2024-11-14', '2024-11-15', '2024-11-18',\n               '2024-11-19', '2024-11-20', '2024-11-21', '2024-11-22',\n               '2024-11-25', '2024-11-26', '2024-11-27', '2024-11-28',\n               '2024-11-29', '2024-12-02', '2024-12-03', '2024-12-04',\n               '2024-12-05', '2024-12-06', '2024-12-09', '2024-12-10',\n               '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-16',\n               '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20',\n               '2024-12-23', '2024-12-24', '2024-12-26', '2024-12-27',\n               '2024-12-30', '2024-12-31', '2025-01-02', '2025-01-03',\n               '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09',\n               '2025-01-10', '2025-01-13', '2025-01-14', '2025-01-15',\n               '2025-01-16', '2025-01-17', '2025-01-20', '2025-01-21',\n               '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-27',\n               '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31',\n               '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06',\n               '2025-02-07'],\n              dtype='datetime64[ns]', name='Date', freq=None)\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\nadv_3m\n\nADV = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\n\n\n\n\nAC.PA\n2.166276e+06\n\n\nAI.PA\n6.087734e+05\n\n\nAIR.PA\n3.842644e+05\n\n\nMT.AS\n2.288744e+05\n\n\nCS.PA\n5.688459e+05\n# G√©n√©ration des quantit√©s\nnp.random.seed(123)\nADV[\"Quantity\"] =  round(1.5 * np.random.rand(len(ADV[\"ADV\"])) * ADV[\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n\n\nCS.PA\n5.688459e+05\n613900.0\nOn fait l‚Äôhypoth√®se que la profondeur de march√© est de 20%. Cel√† signifie que l‚Äôon peut vendre 20% de la quantit√© sans impacter le prix de fa√ßon consid√©rable. Au del√†, le prix est impact√©. Cette profondeur est ce qui est observ√© en pratique dans les carnets d‚Äôordre √† tel point que l‚ÄôAMF le recommande.\nmarket_depth = 20/100\nADV[\"Quantity in 1day\"] = round(ADV[\"Quantity\"] * market_depth)\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n# Calcul du nombre de jours de liquidation\nADV[\"Days of liquidation\"] = ADV[\"Quantity\"]/ADV[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV[\"Days of liquidation\"] = ADV[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\nDays of liquidation\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n5\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n5\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n5\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n5\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n5\nprint(f\"Temps de liquidation du portefeuille : {ADV['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 5 jours"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#pr√©sence-de-d√©formation",
    "href": "3A/gestion_actifs/profil_liquid.html#pr√©sence-de-d√©formation",
    "title": "Profil d‚Äô√©coulement/ de liquidation de portefeuille",
    "section": "Pr√©sence de d√©formation",
    "text": "Pr√©sence de d√©formation\n\nSous conditions normales avec d√©formation (waterfall liquidation)\nOn peut √™tre √©galement interess√© par la quantit√© de liquidation sur plusieurs jours. Pour cel√†, on fait l‚Äôhypoth√®se qu‚Äôon liquide les prochains jours aux prix observ√©s aujourd‚Äôhui. Ce que je peux v√©ritablement liquider en 1 jour est donc la quantit√© que je peux vendre sans impacter le prix, i.e.¬†min(quantit√© liquidable en 1 jour, quantit√© restant dans le portefeuille).\nOn peut calculer la valeur du portefeuille initiale et sur les jours de liquidation d√©sir√©e. On l‚Äôexprime g√©n√©raleent en pourcentage des encours totaux. On peut √©galement calculer le cumul du pourcentage liquid√© sur les jours de liquidation d√©sir√©e. Cela nous permet d‚Äôobtenir le profil d‚Äô√©coulement.\n\n# Initialisation d'une colonne pour suivre les quantit√©s liquid√©es\nADV[\"Quantity liquidated\"] = 0  # Initialement, rien n'est liquid√©\n\n# Cr√©ation d'une liste pour suivre la liquidation jour par jour\n# Au jour 0, on a liquid√© 0. La colonne 0 sert de quantit√© initiale\nquantity_liquidated_per_day = [ADV[\"Quantity\"]]\n\nfor nb_day in range(1, 8):  # Pour chaque jour\n    # Calculer la quantit√© liquide au jour i\n    liquidated_today = np.minimum(ADV[\"Quantity in 1day\"], ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"])\n    \n    # Mettre √† jour les quantit√©s liquid√©es dans le DataFrame\n    ADV[\"Quantity liquidated\"] += liquidated_today\n    \n    # Stocker les quantit√©s liquid√©es ce jour dans une liste\n    quantity_liquidated_per_day.append(liquidated_today)\n\n# Conversion des r√©sultats jour par jour en DataFrame pour plus de clart√©\nliquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\nliquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n\nliquidation_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n2263116.0\n452623.0\n452623.0\n452623.0\n452623.0\n452623.0\n1.0\n0.0\n\n\nAI.PA\n261291.0\n52258.0\n52258.0\n52258.0\n52258.0\n52258.0\n1.0\n0.0\n\n\nAIR.PA\n130756.0\n26151.0\n26151.0\n26151.0\n26151.0\n26151.0\n1.0\n0.0\n\n\nMT.AS\n189273.0\n37855.0\n37855.0\n37855.0\n37855.0\n37853.0\n0.0\n0.0\n\n\nCS.PA\n613900.0\n122780.0\n122780.0\n122780.0\n122780.0\n122780.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=1)\nprice_data = get_data(start_date, end_date, index, assets_ticker, column=\"Close\")\n\nprice_data[\"portfolio_data\"].head()\nprice_dict = price_data[\"portfolio_data\"].iloc[-1].to_dict()\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********            20%                       ]  8 of 40 completed[**********            20%                       ]  8 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************48%                       ]  19 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************75%***********            ]  30 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Valeur liquide des actions par jour de liquidation\nmarket_value =[\n    price_dict[ticker] * liquidation_df.loc[ticker]\n    for ticker in selected_assets\n]\n\nmarket_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\nmarket_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n1.129295e+08\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n49.900002\n0.0\n\n\nAI.PA\n4.418953e+07\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n169.119995\n0.0\n\n\nAIR.PA\n2.182579e+07\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n166.919998\n0.0\n\n\nMT.AS\n5.178509e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035658e+06\n0.000000\n0.0\n\n\nCS.PA\n2.305808e+07\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n0.000000\n0.0\n\n\n\n\n\n\n\n\n# Calcul de la valeur de march√© initiale et totale\nmarket_value_0 = market_value.iloc[:, 0]\ntotal_market_value_0 = market_value_0.sum()\n\n# Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\ncumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\ncumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\ncumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\ncumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\nweights = {}\nfor ticker in assets_ticker :\n    weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\nweights = pd.DataFrame(weights).T\nweights.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n0.010344\n0.010344\n0.010344\n0.010344\n0.010344\n0.012815\nNaN\nNaN\n\n\nAI.PA\n0.004048\n0.004048\n0.004048\n0.004048\n0.004048\n0.043432\nNaN\nNaN\n\n\nAIR.PA\n0.001999\n0.001999\n0.001999\n0.001999\n0.001999\n0.042867\nNaN\nNaN\n\n\nMT.AS\n0.000474\n0.000474\n0.000474\n0.000474\n0.000474\n0.000000\nNaN\nNaN\n\n\nCS.PA\n0.002112\n0.002112\n0.002112\n0.002112\n0.002112\n0.000000\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Initialiser le graphique\nplt.figure(figsize=(12, 6))\n\n# Barplot empil√©\nbottom = None\nfor asset in weights.index:\n    plt.bar(\n        pd.to_numeric(weights.columns),  # Les jours\n        weights.loc[asset],  # Poids de l'actif pour chaque jour\n        bottom=bottom,  # Position de d√©part pour empiler les barres\n        label=selected_assets[asset]  # L√©gende pour chaque actif\n    )\n    bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\nplt.xlabel(\"Days of Liquidation\")\nplt.ylabel(\"Portfolio Weights\")\nplt.title(\"D√©formation du portefeuille\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nPour un fond de droit fran√ßais reglement√©, on a pas le droit d‚Äôinvestir plus de 5% du portefeuille dans une soci√©t√©. Exceptionnellement, pour certains titre, on a le droit d‚Äôinvestir jusqu‚Äô√† 10% du portefeuille, √† condition que les titres qui sont expos√©es √† plus de 5% du portefeuille ne d√©passent pas 40% du portefeuille. C‚Äôest la r√®gle des 5/10/40. C‚Äôest un ratio r√©glementaire pour les OPC. Toutes les pertes r√©alis√©es en raison du d√©faut de ce ratio doivent √™tre support√©es par la soci√©t√© de gestion. Ces depassements doivent √™tre d√©clar√©s √† l‚ÄôAMF. Dans notre cas, ce ratio n‚Äôest pas respect√©, l‚Äô√©quilibre du portefeuille est chamboul√©.\n\n# Valeur liquide du portefeuille\nmarket_value_df = pd.DataFrame()\n\nmarket_value_df[\"market_value\"] = market_value.sum(axis=0)\n\n# Calculer la valeur liquide relative par rapport au jour 0\nmarket_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n\n# Calculer la valeur cumul√©e liquide relative du portefeuille\nmarket_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n\n# Afficher le DataFrame r√©sultant\nprint(market_value_df)\n\n   market_value  relative value  cumulative value\n0  1.091699e+10    1.000000e+00               0.0\n1  2.183399e+09    2.000001e-01               0.2\n2  2.183399e+09    2.000001e-01               0.4\n3  2.183399e+09    2.000001e-01               0.6\n4  2.183399e+09    2.000001e-01               0.8\n5  2.183392e+09    1.999994e-01               1.0\n6  3.893920e+03    3.566844e-07               1.0\n7  0.000000e+00    0.000000e+00               1.0\n\n\n\nimport matplotlib.pyplot as plt\nmarket_value_df = market_value_df.iloc[1:]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,  # Center text\n        height,  # Position slightly above the bar\n        f'{height:.2f}',  # Format with 2 decimal places\n        ha='center',  # Center horizontally\n        va='bottom',  # Position text at the bottom\n        fontsize=10, color=\"black\"\n    )\n\n# Set labels and title\nplt.xlabel(\"Days\")\nplt.ylabel(\"Cumulative Value (%)\")\nplt.title(\"Profil de liquidation du portefeuille\")\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nPour voir ce qui arrive au profil d‚Äô√©coulement lorsque les quantit√©s varient, on va utiliser un facteur de modulation de la quantit√©. Cela permet de d√©terminer quelle est la taille cible du portefeuille qui permet d‚Äôavoir la liquidit√© pour un certain niveau en nombre de jours qu‚Äôon se fixe. Cet exercice est fait une seule fois √† l‚Äôinitialisation du portefeuille.\nLa liquidit√© d‚Äôun portefeuille d√©pend de la liquidit√© intrins√®que des titres et la quantit√© de titres.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumul√©es.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (cl√© = actif, valeur = prix).\n        selected_assets (list): Liste des actifs s√©lectionn√©s.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantit√©s.\n        plot_graphs (bool): Indique si les graphiques doivent √™tre affich√©s.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumul√©es et relatives.\n    \"\"\"\n    # Initialisation des quantit√©s liquid√©es\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantit√©s liquid√©es par jour\n    for _ in range(1, nb_liquidation+1):\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        ADV[\"Quantity liquidated\"] += liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de march√© initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empil√©)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empil√©\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de d√©part pour empiler les barres\n                label=selected_assets[asset]  # L√©gende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Cr√©ation du DataFrame des valeurs de march√©\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=0.5\nnb_liquidation=6\n\nnew_market_value_df, new_market_value, new_weights = liquidation_profile(ADV, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew_market_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\nAC.PA\n5.646475e+07\n2.258589e+07\n2.258589e+07\n1.129297e+07\n0.0\n0.0\n0.0\n\n\nAI.PA\n2.209477e+07\n8.837873e+06\n8.837873e+06\n4.419021e+06\n0.0\n0.0\n0.0\n\n\nAIR.PA\n1.091290e+07\n4.365125e+06\n4.365125e+06\n2.182646e+06\n0.0\n0.0\n0.0\n\n\nMT.AS\n2.589255e+06\n1.035713e+06\n1.035713e+06\n5.178291e+05\n0.0\n0.0\n0.0\n\n\nCS.PA\n1.152904e+07\n4.611617e+06\n4.611617e+06\n2.305808e+06\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nSous conditions stress√©es avec d√©formation\nPour avoir des conditions stress√©es, on joue sur la quantit√© liquidable en un jour et de fait sur la profondeur de march√©. Pour des conditions stress√©es √† la baisse, on divise la profondeur de march√© par 2. Pour des conditions stress√©es √† la hausse, on multiplie la profondeur de march√© par 2.\n\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\n\nADV_stressed = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\n\n# G√©n√©ration des quantit√©s\nnp.random.seed(42)\nADV_stressed[\"Quantity\"] =  round(1.5 * np.random.uniform(0, 1, size=len(ADV)) * ADV[\"ADV\"])\n\n# Quantit√© journali√®re\nmarket_depth = (20/100)/2  # On stresse la liquidit√©\nADV_stressed[\"Quantity in 1day\"] = round(ADV_stressed[\"Quantity\"] * market_depth)\n\n# Calcul du nombre de jours de liquidation\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Quantity\"]/ADV_stressed[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\n\nprint(f\"Temps de liquidation du portefeuille : {ADV_stressed['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 10 jours\n\n\n\nfact_modulation=1\nnb_liquidation=12\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "href": "3A/gestion_actifs/profil_liquid.html#absence-de-d√©formation-du-portefeuille-pro-forma",
    "title": "Profil d‚Äô√©coulement/ de liquidation de portefeuille",
    "section": "Absence de d√©formation du portefeuille (pro forma)",
    "text": "Absence de d√©formation du portefeuille (pro forma)\nL‚Äôobjectif est de conserver la distribution du portefeuille √† mesure qu‚Äôil se liquide. Tout d‚Äôabord, on estime la quantit√© liquidable √† un jour de chacun des titres comme fait pr√©c√©demment. Cel√† permet d‚Äôavoir le pourcentage liquidable en un jour.\nSi on veut que le portefeuille se liquide √† la m√™me vitesse, il faut aller √† la vitesse du titre le plus lent. On peut calculer le pourcentage liquidable en un jour pour chaque titre. On prendra le minimum de ces pourcentages pour d√©terminer le pourcentage liquidable en un jour du portefeuille.\nLe portefeuille prend ainsi plus de temps √† se liquider et fatalement, le portefeuille finit par se d√©former.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile_pro_forma(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumul√©es.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (cl√© = actif, valeur = prix).\n        selected_assets (list): Liste des actifs s√©lectionn√©s.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantit√©s.\n        plot_graphs (bool): Indique si les graphiques doivent √™tre affich√©s.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumul√©es et relatives.\n    \"\"\"\n    # Initialisation des quantit√©s liquid√©es\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantit√©s liquid√©es par jour\n    for _ in range(1, nb_liquidation+1):        \n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        min_liquidated_today = (liquidated_today/ADV[\"Quantity in 1day\"]).min() # On liquide √† la vitesse de l'actif le moins liquide\n        ADV[\"Quantity liquidated\"] += min_liquidated_today*liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des r√©sultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de march√© initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de march√© cumul√©e (√† partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empil√©)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empil√©\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de d√©part pour empiler les barres\n                label=selected_assets[asset]  # L√©gende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"D√©formation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Cr√©ation du DataFrame des valeurs de march√©\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumul√©es\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumul√©e\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=1\nnb_liquidation=10\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile_pro_forma(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour g√©rer la liquidit√© d‚Äôun portefeuille et donc pr√©server la qualit√© du portefeuille, on peut suspendre les souscriptions et les rachats par des m√©canismes √©mis par la loi. Les r√©gulateurs des SGP annoncent que les investisseurs annoncent que les indivdus ne peuvent plus souscrire ou faire un rachat.\nM√©canismes de gestion de la liquidit√©:\n\nLes Gates consistent √† plafonner les rachats. Si les rachats totaux sont sup√©rieures √† 5% de l‚Äôactif net, la SGP a le droit et non l‚Äôobligatoire ne pas honorer les rachats de plus de 5%. Elle limite donc les rachats en un jour √† 5% et ventiler le reste sur les jours suivants en fonction des conditions du march√©. Cela permet de ne pas impacter le prix de fa√ßon consid√©rable. C‚Äôest une mesure de protection des investisseurs restants. Les gates restent quand m√™me un signal n√©gatif pour les investisseurs restants. Ils permettent toutefois de mettre de l‚Äôordre dans le portefeuille. L‚ÄôAMF le fait figurer dans le prospectus, sauf si la SGP arrive √† justifier qu‚Äôelle n‚Äôa pas besoin de le faire. Il n‚Äôen demeure pas moins que l‚Äôactivation des gates est optionnelle"
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "",
    "text": "L‚Äôanalyse financi√®re constitue l‚Äôensemble des outils permettant de donner un avis objectif d‚Äôune organisation (entreprises, fondations, etc.) sur la sant√© finani√®re et les risques financiers auxquels elle sera confront√©e. Il s‚Äôagit de determiner quels sont les crit√®res d‚Äôune sant√© financi√®re, qu‚Äôest le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le g√®re-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financi√®re d‚Äôune entreprise. Il s‚Äôagit du bilan et du compte de r√©sultat. Ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise. Comprendre comment ils fonctionnent permet de mieux appr√©hender la situation financi√®re d‚Äôune banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un √©tat des lieux de la situation patrimoniale de l‚Äôentreprise √† un moment donn√©. Il est compos√© de deux parties : l‚Äôactif et le passif. L‚Äôactif ou l‚Äôemploi regroupe l‚Äôensemble des biens et des droits de l‚Äôentreprise tandis que le passif regroupe l‚Äôensemble des ressources de l‚Äôentreprise (d‚Äôo√π vient l‚Äôargent et o√π peut-on s‚Äôen procurer). Le bilan est √©quilibr√© en valeur nette, c‚Äôest-√†-dire que l‚Äôactif est √©gal au passif.\nLe compte de r√©sultats, quant √† lui, est un document qui permet de faire un √©tat des lieux des performances de l‚Äôentreprise sur une p√©riode donn√©e (il r√©sume les b√©n√©fices ou pertes g√©n√©r√©es). Il est compos√© du d√©tail des produits et des charges de l‚Äôentreprise. Les produits sont les √©l√©ments qui g√©n√®rent des revenus pour l‚Äôentreprise tandis que les charges sont les √©l√©ments qui g√©n√®rent des d√©penses pour l‚Äôentreprise. Le compte de r√©sultat alimente par ailleurs la partie ‚Äúr√©sultat de l‚Äôexercice‚Äù du bilan comptable.\nLe coeur de l‚Äôentreprise √† analyser comme ressources suppl√©mentaires dans le compte de r√©sultat est l‚Äôensembles des charges financi√®res & exceptionnelles ainsi que l‚Äôensemble des produits d‚Äôexploitation et financiers. Ces √©l√©ments cl√©s permettent de d√©terminer la rentabilit√© de l‚Äôentreprise. En effet, si les charges sont sup√©rieures aux produits, l‚Äôentreprise est en perte. Si les produits sont sup√©rieurs aux charges, l‚Äôentreprise est en b√©n√©fice.\nIl est important de noter que ces deux documents sont compl√©mentaires et permettent de donner une vision globale de la situation financi√®re de l‚Äôentreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d‚Äôanalyser la situation financi√®re d‚Äôune entreprise, il faut donc le remodeler en un bilan ‚Äúfonctionnel‚Äù pour pouvoir l‚Äôanalyser. Le bilan fonctionnel est un document qui permet de faire un √©tat des lieux de la situation financi√®re de l‚Äôentreprise en fonction de son activit√©, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d‚Äôinvestissement √† long terme\nEmplois stables\n\nactifs immobilis√©s en valeur brute\n\nCycle de financement √† long terme\nRessources stables\n\nCapitaux propres,\nEmprunts √† long terme,\nAmortissements et d√©pr√©ciation,\nProvisions pour risques\n\n\n\nCycle d‚Äôexploitation\nEmplois d‚Äôexploitation\n\nStocks et encours\nCr√©ances\n\nCycle d‚Äôexploitation\nRessources d‚Äôexploitation\n\nDettes circulantes\n\n\n\nTr√©sorerie active\n\nDisponibilit√©s\n\nTr√©sorerie passive\n\nD√©couverts bancaires\n\n\n\n\nLes ressources stables font r√©f√©rence aux ressources saines du bilan etfont face aux emplois stables. La tr√©sorerie passive fait r√©f√©rence aux d√©couverts bancaires. Il est important de souligner qu‚Äôune tr√©sorerie passive est per√ßue n√©gativement dans le bilan fonctionnel. En effet, une tr√©sorerie passive signifie que l‚Äôentreprise a des dettes √† court terme qui ne sont pas couvertes par des actifs √† court terme d‚Äôo√π la n√©cessit√© d‚Äôavoir des d√©couverts bancaires.\nNb : La provision pour le risque peuve √™tre consid√©r√©e comme une ressource stable ou une ressource d‚Äôexploitation en fonction de l‚Äôentreprise. Tout d√©pend de la longevit√© des provisions.\n\nEquilibre financier\nNous dirons qu‚Äôil y a √©quilibre financier lorsque :\n\nLes emplois stables soient enti√®rement financ√©s par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a nec√©ssit√© d‚Äôun fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) d√©pend du cycle d‚Äôexploitation (entre autre, la rapidit√© de rotation des stocks et des cr√©ances). Il doit couvrir les besoins de financement du cycle d‚Äôexploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, cr√©ances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la tr√©sorerie (\\(\\text{Tr√©sorerie}=FDR-BFR\\)). Si la tr√©sorerie est positive, il y a √©quilibre financier. Cel√† signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d‚Äôexploitation. Lorsqu‚Äôil est n√©gatif, il faut trouver des ressources pour financer le cycle d‚Äôexploitation. Si la tr√©sorerie est nulle, il y a √©quilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-m√™me le cycle d‚Äôexploitation de l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le cr√©dit fournisseur. Il est important de noter que le cr√©dit fournisseur est une source de financement gratuite pour l‚Äôentreprise. C‚Äôest le cas des E-commerce o√π les acteurs encaissent leurs clients avant m√™me d‚Äôacheter les stocks aupr√®s des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transform√© en ressources en fonds de roulement, cel√† est une situation tr√®s favorable pour l‚Äôentreprise et est appel√©e ‚Äúcr√©dit inter-entreprises‚Äù."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-r√©sultat",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "Analyse du compte de r√©sultat",
    "text": "Analyse du compte de r√©sultat\nNous pouvons faire les m√™mes critiques faites au bilan comptable sur le compte de r√©sultat. En effet, le compte de r√©sultat est con√ßu de sorte √† fournir des informations au seul d√©tenteur du capital, √† savoir les actionnaires. Il fait apparaitre uniquement le b√©n√©fice ou la perte. C‚Äôest un document d‚Äôint√©r√™t pour l‚ÄôEtat pour d√©terminer si un pays est en croissance ou en r√©cession. Pour en faire un vrai diagnostic financier, il faut le d√©couper en sous-soldes appel√©s ‚Äúsoldes interm√©diaires de gestion‚Äù (SIG). Les SIG permettent de d√©terminer la rentabilit√© de l‚Äôentreprise, sa capacit√© d‚Äôautofinancement, sa capacit√© de remboursement, sa capacit√© de financement, etc.\nIl existe 9 soldes interm√©diaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajout√©e\nL‚Äôexc√©dent brut d‚Äôexploitation (EBE)\nLe r√©sultat d‚Äôexploitation\nLe r√©sultat courant avant imp√¥t\nLe r√©sultat exceptionnel\nLe r√©sultat net\nLa plus ou moins value de cession\n\nSelon la th√©orie de prise de d√©cisions, il y a deux grands types de d√©cisions : des d√©cisions qui permettent de cr√©er de la riches (Marge co., production et valeur ajout√©e) et des d√©cisions qui permettent de distribuer/d√©penser de la richesse (EBE, r√©sultat d‚Äôexploitation, r√©sultat courant avant imp√¥t, r√©sultat exceptionnel, r√©sultat net et plus ou moins value de cession). Lorsqu‚Äôon d√©pense la riches, il faudrait qu‚Äôelle soit bien d√©pens√©e.\n\nSoldes de cr√©ation de richesse\nLes soldes qui contribuent √† la cr√©ation de richesse sont la marge commerciale, la production et la valeur ajout√©e :\n\nLa marge commerciale est la diff√©rence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C‚Äôest un solde des entreprises commerciales (par exemple, les supermarch√©s). Pour une entreprise qui n‚Äôont pas de marchandises, le marge commerciale est nulle.\nLa production de l‚Äôexercice est la somme des produits vendus(\\(\\pm\\) les produits stock√©es) et des produits immobilis√©es par l‚Äôentreprise (certaines entreprises peuvent se vendre des produits √† elles-m√™mes). C‚Äôest un solde des entreprises industrielles.\nLa valeur ajout√©e est la richesse cr√©√©e par l‚Äôentreprise. C‚Äôest la somme des marges commerciales, de la production de l‚Äôexercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services ext√©rieurs).\n\nLa valeur ajout√© est un indicateur tr√®s suivi par l‚ÄôEtat pour d√©terminer le produit int√©rieur brut (PIB) afin de d√©terminer si un pays est en croissance ou en r√©cession. Par ailleurs, la valeur ajout√©e divis√©e par le nombre de salari√©s permet de d√©terminer le niveau de technicit√© de l‚Äôentreprise. Plus la valeur ajout√©e par salari√© est √©lev√©e, plus l‚Äôentreprise est techniquement avanc√©e.\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© √©conomique\nIl existe 5 tiers √† qui l‚Äôentreprise redistribue la VA (rang√©e par ordre de priorit√©) :\n\nLe personnel (√† travers les salaires),\nL‚ÄôEtat (√† travers les imp√¥ts),\nLe capital technique (via les amortissements),\nLes banques (via les int√©r√™ts),\nLes actionnaires ou les associ√©s (via le b√©n√©fice comptable)\n\nLes soldes qui permettent de financer l‚Äôactivit√© √©conomique (Etat, personnel, capital technique) sont l‚Äôexc√©dent brut d‚Äôexploitation et le r√©sultat d‚Äôexploitation :\n\nLe solde EBE r√©mun√®re le personnel et l‚ÄôEtat. Il repr√©sente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d‚Äôentreprise pour l‚ÄôEtat. Un EBE positif signifie que l‚Äôentreprise est capable de r√©mun√©rer le personnel et l‚ÄôEtat, et donc de financer l‚Äôemploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajout√©e} - \\text{Imp√¥ts, t√¢xes et versements assimil√©s} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de r√©sultat d‚Äôexploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de r√©mun√©rer le capital technique (machines etc.) et appartient √† tout ceux qui d√©pendent du capital financier et mesure les performances industrielles et commerciales de l‚Äôentreprise.\n\n\\[\\begin{align*}\n\\text{R√©sultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse d√©di√©e √† l‚Äôactivit√© financi√®re\nLes soldes qui permettent de financer l‚Äôactivit√© financi√®re (banques, actionnaires) sont le r√©sultat courant avant imp√¥t, le r√©sultat exceptionnel, le r√©sultat net et la plus ou moins value de cession :\n\nLe r√©sultat courant avant imp√¥t est le solde qui permet de r√©mun√©rer les banques. Il est un indicateur de la capacit√© de l‚Äôentreprise √† rembourser ses dettes et est un t√©moin de l‚Äôincidence de la politique financi√®re de l‚Äôentreprise sur son r√©sultat. Il faut distinguer les int√©r√™ts √† long terme et ceux de court terme. Plus ceux ci sont li√©s √† des dettes de court terme (ex. : d√©couverts), on peut dire que l‚Äôentreprise est en difficult√© financi√®re tandis que l‚Äôendettement √† long terme est un signe de bonne sant√© financi√®re, car il est voulu plut√¥t que subi. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat courant avant imp√¥t} &= \\text{R√©sultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financi√®res}\n\\end{align*}\\]\n\nLe r√©sultat exceptionnel est le solde qui est le moins analys√© car il est souvent li√© √† des √©v√®nements exceptionnels (ex. : vente d‚Äôun bien immobilier). Il est calcul√© comme √©tant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe r√©sultat net est le solde qui permet de r√©mun√©rer les actionnaires. C‚Äôest le solde en bas du compte de r√©sultat. Il est calcul√© comme suit :\n\n\\[\\begin{align*}\n\\text{R√©sultat net} &= \\text{R√©sultat courant avant imp√¥t} + \\text{R√©sultat exceptionnel} \\\\\n&- \\text{participations des salari√©s} - \\text{Imp√¥ts sur les b√©n√©fices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu‚Äôune entreprise vend une immobilisation. Ce ratio permet de d√©terminer si l‚Äôentreprise a vendu une immobilisation √† un prix sup√©rieur ou inf√©rieur √† sa valeur comptable. Cel√† constitue un temoin d‚Äôalerte sur la sant√© de l‚Äôentreprise et permet de d√©terminer si l‚Äôentreprise est en difficult√© financi√®re (car rien ne l‚Äôoblige √† vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacit√©-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de r√©sultat d‚Äôune entreprise",
    "section": "La capacit√© d‚Äôautofinancement",
    "text": "La capacit√© d‚Äôautofinancement\nLa capacit√© d‚Äôautofinancement (CAF) est un indicateur qui permet de d√©terminer si l‚Äôentreprise est capable de financer ses investissements sans recourir √† des financements ext√©rieurs. Elle regroupe la capacit√© √† d√©gager de la liquidit√©. Il n‚Äôy a pas de correspondance entre la tr√©sorerie et le b√©n√©fice. En effet, une entreprise peut √™tre en b√©n√©fice mais en difficult√© financi√®re. Pour la calculer, il faut √©liminer les sommes non encaissanles et non d√©caissables (ex. : Dotations, provision, reprise sur amortissements, les √©critures exceptionnelles).\nPour passer du b√©n√©fice √† la CAF, on ne conserve que les √©l√©ments qui sont encaissables et d√©caissables et est calcul√©e comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges d√©caissables (int√©r√™t bancaire, imp√¥t sur b√©n√©fice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu vers√© par l‚Äôentreprise √† ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas √† elle toute seule de d√©terminer l‚Äôautofinancement de l‚Äôentreprise. Dans le cadre l√©gal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu‚Äô√† 95% du b√©n√©fice comptable impos√© par l‚ÄôEtat et garder 5% √† l‚Äôentreprise. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende l√©gal‚Äù. Au del√† de 10%, les actionnaires peuvent retirer jusqu‚Äô√† 100% du b√©n√©fice comptable. C‚Äôest ce qu‚Äôon appelle le ‚Äúdividende statutaire‚Äù.\nAinsi, l‚Äôautofinancement est la somme qui reste de la CAF apr√®s le dividende l√©gal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaiss√©s par les actionnaires d√©termine la politique d‚Äôautofinancement de l‚Äôentreprise.\nL‚Äôautofinancement est essentiel pour l‚Äôentreprise car il permet de:\n\nrembourser les emprunts,\nam√©liorer la tr√©sorerie,\ncouvrir les risque de l‚Äôentreprise (provisions pour risque),\nfinancer l‚Äôexploitation (stocks & cr√©ances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html",
    "href": "3A/Apprentisage-stat/Tp3.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "AdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\nHence, for M weak learners, the final prediction is given by \\(g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)\\) where \\(g_m(x)\\) is the m-th weak learner and \\(\\alpha_m\\) is the weight associated with the m-th weak learner. The optimisation problem of AdaBoost is given by:\n\\[ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) \\]\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function \\(l(y,f(x))=\\exp(-yf(x))\\), \\(y \\in \\{-1,+1\\}\\). It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n\nInitialize the observation weights \\(w_i^{(1)} = 1/n\\) for \\(i=1,\\dots,n\\)\n\n\nFor m=1 to M:\n\n\n\nFit a weak learner \\(g_m(x)\\) to the training data using weights \\(w_i^{(m)}\\)\n\n\nCompute the error rate \\(err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))\\) where \\(I\\) is the indicator function\n\n\nCompute the weight \\(\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)\\)\n\n\nUpdate the weights \\(w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)\\)\n\n\n\nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems. It is a generalization of the AdaBoost algorithm to multi-class classification problems.\nTo inspect how the errors and the weights vary with the number of iterations, we will the function make_gaussian_quantiles from sklearn. This function generates a multi-dimensional standard normale distribution with a given number of samples \\(n\\) per class \\(K\\). We will generate a dataset of size \\(n=2000\\) with \\(K=3\\) classes and \\(d=10\\) features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "href": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "title": "Gradient boosting",
    "section": "",
    "text": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.46"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "href": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "title": "Gradient boosting",
    "section": "",
    "text": "# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:\n\nThe parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n\n\n\nAccuracy:  0.7275\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html",
    "href": "3A/Apprentisage-stat/Tp1.html",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]\n\n\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It‚Äôs the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let‚Äôs start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10¬≤ and 10‚Å¥ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : ‚àíŒª(X^T X + ŒªId) (‚àí1) Œ∏\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "",
    "text": "If we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don‚Äôt have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Œª')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('Œª')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "1. Ridge regression",
    "text": "1. Ridge regression\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter \\(\\lambda\\) by cross-validation. We will use the function RidgeCV from the sklearn library to perform the cross-validation. Since we didn‚Äôt center the covariables, we will set the parameter fit_intercept to True in order to include an intercept in the model.\nBy default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter Œª. This is called regularization path. We can do this by fitting the model for different values of Œª and store the coefficients.\n\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'Œª = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter \\(\\lambda\\) wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients.\n In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. \n\na. Check on the intercept value of the model using lambda found by cross validation\n\nprint(f'Intercept value : {ridge_cv.intercept_}')\n\nIntercept value : 152.28937186306018\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "2. Lasso regression",
    "text": "2. Lasso regression\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation. It is important to perform a cross-validation in order to choose the best value of the regularization parameter \\(\\lambda\\) as we have demonstrated in the first activity. For the lasso regression, we will use the function LassoCV from the sklearn library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n\n\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('Œª')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'Œª = {lasso_cv.alpha_:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter \\(\\lambda\\) given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of \\(\\lambda\\), we can select more or less variables.\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n\na. Check on the intercept value of the model using lambda found by cross validation\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n\nIntercept value : 151.95282341561403"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "title": "Ridge regression vs.¬†Lasso regression",
    "section": "3. Quality of the models (ridge regression vs lasso regression)",
    "text": "3. Quality of the models (ridge regression vs lasso regression)\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\\[\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2\\]\nwhere \\(\\hat g(x)\\) is the predicted value of the output variable y given the input variable x, \\(\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j\\).\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html",
    "href": "2A/App_sup/reg_lin.html",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "La r√©gression lin√©aire est une m√©thode d‚Äôapprentissage supervis√© qui vise √† √©valuer, lorsqu‚Äôil existe, la relation lin√©aire entre une variable d‚Äôint√©r√™t et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de donn√©es constitu√© de n √©chantillons iid (ind√©pendant et identiquement distribu√©), le mod√®le de regression lin√©aire s‚Äô√©crit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\no√π \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l‚Äôerreur, l‚Äôinformation que les autres variables explicatives ne donnent pas.\nL‚Äôhypoth√®se fondamentale de la r√©gression lin√©aire est l‚Äôexistence d‚Äôune relation lin√©aire entre la variable cible et les variables explicatives. Pour s‚Äôassurer de la pertinence de cette hypoth√®se avant de proc√©der √† la r√©gression lin√©aire (√† l‚Äôaide de visualisation ou de tests- spearman, pearson, etc.)\nL‚Äôhypoth√®se de rang plein est la seconde plus grande hypoth√®se, elle stipule que les variables explicatives ne soient pas corr√©l√©es entre elles. Cette condition est n√©cessaire pour garantir l‚Äôunicit√© des estimations des param√®tres du mod√®le et ainsi l‚Äôidentifiabilit√© du mod√®le √©tudi√©\nPar ailleurs pour que les estimations des param√®tres du mod√®le lin√©aire soient fiables, les erreurs du mod√®le, repr√©sent√©es par \\(\\xi_i\\), doivent r√©pondre √† plusieurs crit√®res :\n\nErreurs centr√©es : La moyenne attendue des erreurs doit √™tre nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le mod√®le ne pr√©sente pas de biais syst√©matique dans les pr√©dictions.\nHomosc√©dasticit√© : La variance des erreurs doit √™tre constante pour toutes les observations, exprim√©e par \\(V[\\xi_i] = \\sigma^2\\). Cette propri√©t√© garantit que la pr√©cision des estimations est uniforme √† travers la gamme des valeurs pr√©dites.\nD√©corr√©lation des erreurs : Les erreurs doivent √™tre mutuellement ind√©pendantes, c‚Äôest-√†-dire que la covariance entre toute paire d‚Äôerreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour √©viter les biais dans les estimations des param√®tres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d‚Äôobserver une hypoth√®se suppl√©mentaire sur la loi des erreurs. En effet, les erreurs sont souvent suppos√©es suivre une loi normale, c‚Äôest √† dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Cel√† nous permet de faire des inf√©rences sur les param√®tres du mod√®le et de construire des intervalles de confiance.\n\n\nToutes les hypoth√®ses √©tant respect√©es, et sous reserve qu‚Äôil n‚Äôy a pas de multicolin√©arit√© entre les variables explicatives du mod√®les i.e.¬†\\(X^T X\\) est inversible(l‚Äôhypoth√®se de rang plein est respect√©e), l‚Äôestimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carr√© ordinaire est donn√© par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD‚Äôapr√®s le th√©or√®me de Gauss-Markov, l‚Äôestimateur \\(\\hat \\beta\\) est le meilleur estimateur lin√©aire non biais√© des param√®tres du mod√®le. En effet, il est l‚Äôestimateur avec la plus petite variance, parmi les estimateurs lin√©aires sans biais qui existent. Cet estimateur est ainsi appel√© BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont suppos√©es suivre une loi normale, l‚Äôestimateur \\(\\hat \\beta\\) est √©galement l‚Äôestimateur du maximum de vraisemblance des param√®tres du mod√®le et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL‚Äôestimateur de la variance des erreurs \\(\\sigma^2\\) est donn√© par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carr√©s des r√©sidus.\n\n\n\nDans l‚Äôoptique de mesurer la qualit√© du mod√®le, plusieurs m√©triques sont utilis√©es : le R¬≤, le R¬≤ ajust√©, l‚Äôerreur quadratique moyenne (MSE), des crit√®res d‚Äôinformations (AIC, BIC) etc.\n\n\nLe coefficient de d√©termination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliqu√©e par le mod√®le. Il est d√©fini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carr√©s totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carr√©s des r√©sidus.\nN√©anmoins, le \\(R^2\\) n‚Äôest pas une mesure parfaite de la qualit√© du mod√®le. En effet, il augmente avec le nombre de variables explicatives, m√™me si ces variables n‚Äôont pas de lien avec la variable cible. Pour pallier √† ce probl√®me, le \\(R^2\\) ajust√© est utilis√©. Il est d√©fini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes crit√®res AIC et BIC sont des crit√®res d‚Äôinformation qui servent √† mesurer l‚Äôattache du mod√®le aux donn√©es que nous avons ajust√©s avec une p√©nalit√© li√© soit aux nombres de variables inclus dans le mod√®les et/ou la taille de l‚Äô√©chantillon √©tudi√©. De fait, plus l‚ÄôAIC ou le BIC est faible, meilleur est le mod√®le, car cela signifie qu‚Äôil a le mod√®le choisie a une probabilit√© plus √©lev√©e d‚Äô√™tre correct et une complexit√© plus faible.\nMath√©matiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces √©quations sont visibles, nous constatons que le BIC est un crit√®re plus parcimonieux que le crit√®re AIC en raison de la p√©nalisation qui est plus √©lev√© lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l‚Äô√©chantillon s√©lectionn√©."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#estimation-des-param√®tres",
    "href": "2A/App_sup/reg_lin.html#estimation-des-param√®tres",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "Toutes les hypoth√®ses √©tant respect√©es, et sous reserve qu‚Äôil n‚Äôy a pas de multicolin√©arit√© entre les variables explicatives du mod√®les i.e.¬†\\(X^T X\\) est inversible(l‚Äôhypoth√®se de rang plein est respect√©e), l‚Äôestimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carr√© ordinaire est donn√© par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD‚Äôapr√®s le th√©or√®me de Gauss-Markov, l‚Äôestimateur \\(\\hat \\beta\\) est le meilleur estimateur lin√©aire non biais√© des param√®tres du mod√®le. En effet, il est l‚Äôestimateur avec la plus petite variance, parmi les estimateurs lin√©aires sans biais qui existent. Cet estimateur est ainsi appel√© BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont suppos√©es suivre une loi normale, l‚Äôestimateur \\(\\hat \\beta\\) est √©galement l‚Äôestimateur du maximum de vraisemblance des param√®tres du mod√®le et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL‚Äôestimateur de la variance des erreurs \\(\\sigma^2\\) est donn√© par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carr√©s des r√©sidus."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le",
    "title": "La r√©gression lin√©aire",
    "section": "",
    "text": "Dans l‚Äôoptique de mesurer la qualit√© du mod√®le, plusieurs m√©triques sont utilis√©es : le R¬≤, le R¬≤ ajust√©, l‚Äôerreur quadratique moyenne (MSE), des crit√®res d‚Äôinformations (AIC, BIC) etc.\n\n\nLe coefficient de d√©termination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliqu√©e par le mod√®le. Il est d√©fini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carr√©s totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carr√©s des r√©sidus.\nN√©anmoins, le \\(R^2\\) n‚Äôest pas une mesure parfaite de la qualit√© du mod√®le. En effet, il augmente avec le nombre de variables explicatives, m√™me si ces variables n‚Äôont pas de lien avec la variable cible. Pour pallier √† ce probl√®me, le \\(R^2\\) ajust√© est utilis√©. Il est d√©fini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes crit√®res AIC et BIC sont des crit√®res d‚Äôinformation qui servent √† mesurer l‚Äôattache du mod√®le aux donn√©es que nous avons ajust√©s avec une p√©nalit√© li√© soit aux nombres de variables inclus dans le mod√®les et/ou la taille de l‚Äô√©chantillon √©tudi√©. De fait, plus l‚ÄôAIC ou le BIC est faible, meilleur est le mod√®le, car cela signifie qu‚Äôil a le mod√®le choisie a une probabilit√© plus √©lev√©e d‚Äô√™tre correct et une complexit√© plus faible.\nMath√©matiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces √©quations sont visibles, nous constatons que le BIC est un crit√®re plus parcimonieux que le crit√®re AIC en raison de la p√©nalisation qui est plus √©lev√© lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l‚Äô√©chantillon s√©lectionn√©."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#simulation-des-donn√©es",
    "href": "2A/App_sup/reg_lin.html#simulation-des-donn√©es",
    "title": "La r√©gression lin√©aire",
    "section": "1. Simulation des donn√©es",
    "text": "1. Simulation des donn√©es\nPour √©valuer l‚Äôint√©r√™t de la regr√©ssion lin√©aire, nous allons simuler un √©chantillon de taille n=200, o√π la variable cible Y est une fonction lin√©aire de la variable explicative X. La vraie relation est donn√©e par \\(Y = 2 + 3X + \\epsilon\\), o√π \\(\\epsilon \\sim N(0, 1.6)\\). De fait le mod√®le lin√©aire est ad√©quat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un mod√®le lin√©aire simple √† nos donn√©es, nous obtenons une estimation des param√®tres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du mod√®le suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le-1",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-mod√®le-1",
    "title": "La r√©gression lin√©aire",
    "section": "2. Evaluation du mod√®le",
    "text": "2. Evaluation du mod√®le\n\n2.1. Hypoth√®ses sur les erreurs et l‚Äôexistence d‚Äôune relation lin√©aire\nPour √©valuer la qualit√© du mod√®le, nous allons tracer les r√©sidus studentis√©s en fonction des valeurs ajust√©es. Les r√©sidus studentis√©s sont les r√©sidus divis√©s par l‚Äô√©cart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajust√©es\", ylab=\"R√©sidus studentis√©s\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les r√©ponses pr√©dites par le mod√®le (fitted values) augmentent, les r√©sidus restent globalement uniform√©ment distribu√©s de part et d‚Äôautre de 0. Cela montre, qu‚Äôen moyenne, la droite de r√©gression, est bien adapt√©e aux donn√©es, et donc que l‚Äôhypoth√®se de lin√©arit√© est acceptable.\nSi l‚Äôon observait une forme de trompette, cel√† reviendrait √† soulever une question sur l‚Äôh√©t√©rosc√©dascit√© des r√©sidus, tandis qu‚Äôune forme de banane rev√®le plut√¥t une relation de non-lin√©arit√©.\nLorsque le nuage de point n‚Äôa pas de structure particuli√®re, a priori l‚Äôhypoth√®se d‚Äôhomosc√©dascticit√© n‚Äôest pas remise en question, comme cela semble √™tre le cas ici. Attention : ces principes peuvent parfois √™tre mis en d√©faut et il vaut toujours mieux r√©aliser plusieurs contr√¥les diff√©rents.\nPour v√©rifier l‚Äôhypoth√®se d‚Äôhomosc√©dasticit√©, nouspouvons √©galement utiliser le test de Breusch-Pagan. Ce test est bas√© sur la r√©gression des carr√©s des r√©sidus sur les variables explicatives. Si le test est significatif, l‚Äôhypoth√®se d‚Äôhomosc√©dasticit√© est rejet√©e.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a n√©cessit√© le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l‚Äôhypoth√®se de non corr√©lation des r√©sidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est bas√© sur l‚Äôautocorr√©lation des r√©sidus. Si le test est significatif, l‚Äôhypoth√®se de non corr√©lation des r√©sidus est rejet√©e.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l‚Äôhypoth√®se de normalit√© des r√©sidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est bas√© sur la comparaison des r√©sidus avec une loi normale. Si le test est significatif, l‚Äôhypoth√®se de normalit√© des r√©sidus est rejet√©e.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l‚Äôhypoth√®se de normalit√© des r√©sidus n‚Äôest pas rejet√©e.\n\n\n2.2. Qualit√© du mod√®le\nPour √©valuer la qualit√© du mod√®le, nous allons calculer le coefficient de d√©termination \\(R^2\\) et le \\(R^2\\) ajust√©.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajust√© de 0.97. Cela signifie que 97% de la variance de la variable cible est expliqu√©e par le mod√®le. Notre mod√®le de r√©gression lin√©aire est bien ajust√© √† nos donn√©es."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html",
    "href": "3A/Apprentisage-stat/Tp2.html",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear.\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable. In particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. The optimization problem of KRR is given by: \\[\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n\\] where \\(x_i\\) is the \\(i\\)-th row of the matrix \\(X\\) and \\(y_i\\) is the \\(i\\)-th element of the vector \\(y\\). The parameter \\(\\lambda\\) is the regularization parameter. The solution of the optimization problem is given by:\n\\[\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n\\]\nwhere \\(I_d\\) and \\(I_n\\) are the identity matrix.\nIn prediction, the target variable is given by: \\[\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n\\] where \\(\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}\\). We easily see that the prediction is a linear combination of the scalar product between the test point \\(x^*\\) and the training points \\(x_i\\), we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\\[\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n\\]\nwhere \\(K(x_i, x^*)\\) is the kernel function.\n\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Now we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinuso√Ødal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let‚Äôs try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "href": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Linear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "title": "Kernel Trick and SVM",
    "section": "I. Toy dataset",
    "text": "I. Toy dataset\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n\nText(0.5, 1.0, 'Two moons dataset')\n\n\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n\n\n1. K-nearest neighbors\n\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nBest parameters by CV : {'n_neighbors': 1}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Logistic regression\n\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n\n\n\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. SVM\n\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 0.030888435964774818, 'gamma': 3.727593720314938}\n\n\n\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Conclusion\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\nWhenever we have a classification problem, it is hence always useful to try the SVM and the KNN."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "title": "Kernel Trick and SVM",
    "section": "II. Image dataset",
    "text": "II. Image dataset\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255). We are going to use the SVM with the gaussian kernel to classify the images.\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n\nDescription of train dataset : \n\n\n\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n\n\n\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n\n\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n\n\n\n\n\n\n\n\n\n\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n\n\n\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html",
    "href": "3A/Apprentisage-stat/Tp4.html",
    "title": "Features selection",
    "section": "",
    "text": "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features.\nThe usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\nIn this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n\\[I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)\\]\nwhere \\(p(x,y)\\) is the joint probability distribution function of X and Y, and \\(p(x)\\) and \\(p(y)\\) are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\nIn order to have confidence in this measure, we will consider a bivariate gaussian variable \\(Z=(X,Y)\\) with mean \\(\\mu = (0,0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}\\). We will compute the mutual information between X and Y for a grid a \\(\\rho\\) between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to \\(-\\frac{1}{2} \\log(1-\\rho^2)\\).\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset.\n\n\n\nWe will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n\n\n\n\nWe can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "href": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "title": "Features selection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "href": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "title": "Features selection",
    "section": "",
    "text": "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "href": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "title": "Features selection",
    "section": "",
    "text": "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "href": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "title": "Features selection",
    "section": "",
    "text": "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [ 0  3  5  6  9 18]"
  },
  {
    "objectID": "3A/gestion_actifs/mesures_rsq.html",
    "href": "3A/gestion_actifs/mesures_rsq.html",
    "title": "Gestion de risques de portefeuille",
    "section": "",
    "text": "Pour g√©rer les risques, on proc√®de en trois √©tapes : 1. Identification : Nous avons un portefeuille d‚Äôaction, donc le risque auquel on fait face est le risque de march√© action.\n\nMetrique de risque : Volatilit√© ex-ante, Value at Risk ex-ante, Tracking error ex-ante (i.e.¬†par anticipation, on se base sur l‚Äô√©tat du portefeuille √† l‚Äôinstant t et non aux instants pass√©s - ex-post)\nEncadrement\n\nDans notre cas, on va constituer le portefeuille avec 10 actifs du CAC 40 de notre choix et leur allouer des poids al√©atoires :\n\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_data(start_date, end_date, index_ticker, tickers):\n    \"\"\"\n    Extraction de donn√©es de cours d'actions\n    Args:\n        start_date (str): Date de d√©but au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des prix historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    # Extraction des prix historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=2*365)\n\nselected_assets = {\n    \"SAN.PA\" : \"Sanofi\",\n    \"GLE.PA\" : \"Soci√©t√© g√©n√©rale\",\n    \"HO.PA\" : \"Thales\",\n    \"ENGI.PA\" : \"Engie\",\n    \"CAP.PA\" : \"Capgemini\",\n    \"CA.PA\" : \"Carrefour\",\n    \"ORA.PA\" : \"Orange\",\n    \"AC.PA\" : \"Accor\",\n    \"OR.PA\" : \"L'Oreal\",\n    \"ACA.PA\" : \"Cr√©dit agricole\"\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker)\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nCA.PA\nCAP.PA\nENGI.PA\nGLE.PA\nHO.PA\nOR.PA\nORA.PA\nSAN.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-02-08\n29.441587\n9.554356\n15.175895\n177.961441\n10.856310\n23.891726\n113.740341\n359.421448\n8.415674\n81.924522\n\n\n2023-02-09\n29.232716\n9.961541\n15.267509\n179.503082\n10.846358\n23.828888\n113.931816\n362.677155\n8.366665\n82.563179\n\n\n2023-02-10\n27.998465\n9.850800\n14.928535\n176.419815\n10.950842\n23.564072\n116.804039\n359.660858\n8.478687\n82.100380\n\n\n2023-02-13\n28.425705\n9.833763\n14.745306\n177.672379\n10.887820\n23.743610\n119.676277\n373.114624\n8.490064\n81.128510\n\n\n2023-02-14\n28.606094\n9.850800\n15.043054\n178.202316\n10.979035\n23.797468\n122.021935\n371.295227\n8.686104\n81.554283\n\n\n\n\n\n\n\n\nbenchmark_data = data[\"benchmark_data\"]\nbenchmark_data.head()\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2023-02-08\n7119.830078\n\n\n2023-02-09\n7188.359863\n\n\n2023-02-10\n7129.729980\n\n\n2023-02-13\n7208.589844\n\n\n2023-02-14\n7213.810059\n\n\n\n\n\n\n\n\n# On attribue des poids √©quitables pour chaque action\nweights_by_asset = {ticker: 1 / len(assets_ticker) for ticker in assets_ticker}\n\nOn souhaite connaitre la valeur totale du actifs du portefeuille, i.e.¬†l‚Äôasset under management(AUM) :\n\\[\nAUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times P_i(T_n)\n\\]\n\naum_series = portfolio_data.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\naum_series\n\nAUM = pd.DataFrame(aum_series, columns=[\"AUM\"])\n\n\nAUM.head()\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-02-08\n83.038330\n\n\n2023-02-09\n83.617891\n\n\n2023-02-10\n83.075649\n\n\n2023-02-13\n84.771806\n\n\n2023-02-14\n85.003632\n\n\n\n\n\n\n\n\n# Evolution de la valeur totale du portefeuille\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.plot(AUM, label=\"AUM\")\nplt.title(\"Evolution de l'actif sous gestion\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# On s'interesse aux variations/rendements de l'AUM\n\nAUM[\"Variation\"] = AUM[\"AUM\"].pct_change()\nAUM[\"Variation\"].head()\n\nDate\n2023-02-08         NaN\n2023-02-09    0.006979\n2023-02-10   -0.006485\n2023-02-13    0.020417\n2023-02-14    0.002735\nName: Variation, dtype: float64\n\n\n\nEstimation de la volatilit√©\nPour estimer la volatilit√© du portefeuille, on peut calculer l‚Äô√©cart-type des variations de l‚ÄôAUM. On fait le choix de calculer une volatilit√© ex-ante en se basant sur les variation historiques des prix des actifs avec une profondeur historique de 2 ans. Vu qu‚Äôon a une volatilit√© quotidienne, on va l‚Äôannualiser en multipliant par \\(\\sqrt{252}\\).\nEn g√©n√©ral, sur le march√© action, la volatilit√© quotidienne est environ de 1% et la volatilit√© annuelle est entre 10% et 20%.\n\n# Calcul de la volatilit√© du portefeuille\nvolatility_portfolio = np.std(AUM[\"Variation\"])\nannualized_volatility_portfolio = volatility_portfolio * np.sqrt(252)\nprint(f\"Volatilit√© de la performance quotidienne : {volatility_portfolio : .2%}\")\nprint(f\"Volatilit√© de la performance annuelle : {annualized_volatility_portfolio : .2%}\")\n\nVolatilit√© de la performance quotidienne :  0.87%\nVolatilit√© de la performance annuelle :  13.80%\n\n\n\n# Calcul de la volatilit√© de l'indice CAC 40\n\nbenchmark_data[\"Variation\"] = benchmark_data[\"^FCHI\"].pct_change()\nvolatility_benchmark = np.std(benchmark_data[\"Variation\"])\nannualized_volatility_benchmark = volatility_benchmark * np.sqrt(252)\n\nprint(f\"Volatilit√© de l'indice CAC 40 : {volatility_benchmark : .2%}\")\nprint(f\"Volatilit√© de l'indice CAC 40 annuelle : {annualized_volatility_benchmark : .2%}\")\n\nVolatilit√© de l'indice CAC 40 :  0.84%\nVolatilit√© de l'indice CAC 40 annuelle :  13.34%\n\n\nOn retrouve sur √† peu pr√®s la m√™me volatilit√© du portefeuille et celle du CAC 40. Il y a donc une certaine homog√©n√©it√©.\n\n\nEstimation de la tracking error/erreur de suivi\nLa tracking error est une mesure de l‚Äô√©cart entre la performance d‚Äôun portefeuille et celle de son indice de r√©f√©rence. Elle est calcul√©e comme la volatilit√© de la diff√©rence entre les rendements du portefeuille et de l‚Äôindice de r√©f√©rence :\n\\[\nTE = \\sqrt{Var(R_p - R_b)}\n\\]\nLa tracking error mesure l‚Äôincercitude du portefeuille par rapport √† l‚Äôindice de r√©f√©rence, c‚Äôest une mesure relative. Plus la tracking error est √©lev√©e, plus le portefeuille est risqu√©. On ne souhaite sous ou sur-performer l‚Äôindice de r√©f√©rence. On souhaite suivre v√©ritablement l‚Äôindice de r√©f√©rence.\nPour l‚Äôannualiser, on multiplie par \\(\\sqrt{252}\\) en supposant que les performances quotidiennes sont ind√©pendantes et donc un utilise l‚Äôadditivit√© des variances.\n\nperformance_relative = AUM[\"Variation\"] - benchmark_data[\"Variation\"]\n\nplt.figure(figsize=(12, 4))\nplt.plot(performance_relative, label=\"Performance\")\nplt.title(\"Performance du portefeuille par rapport √† l'indice CAC 40\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Performance\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcul de la tracking error\n\nTE = np.std(AUM[\"Variation\"] - benchmark_data[\"Variation\"]) \nprint(f\"Tracking error : {TE : .2%}\")\n\nTE_annualized = TE * np.sqrt(252)\nprint(f\"Tracking error annualis√© : {TE_annualized : .2%}\")\n\nTracking error :  0.52%\nTracking error annualis√© :  8.20%\n\n\n\n\nEstimation de la Value-at-Risk (VaR)\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l‚Äôon peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donn√©. Par exemple, une VaR √† 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut √©galement raisonner en terme de gain, i.e.¬†Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements pass√©s selon l‚Äôhorizon fix√© pour estimer la VaR, √† l‚Äôaide d‚Äôun quantile empirique d‚Äôordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la m√©thode de rescaling, i.e.¬†\\(VaR = \\sigma \\times \\Phi^{-1}(\\alpha)\\). 2. Approche param√©trique : On suppose que les rendements suivent une loi normale. 3. Approche Monte Carlo : On simule les rendements futurs.\n\n# VaR historique\nseuil = 99/100\n\nVaR_hist_portfolio = np.percentile(AUM[\"Variation\"].dropna(), 100*(1- seuil))\nprint(f\"VaR historique sur le portefeuille : {- VaR_hist_portfolio : .2%}\")\nprint(f\"VaR historique sur 20 jours sur le portefeuille : {-VaR_hist_portfolio*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\nVaR_hist_benchmark = np.percentile(benchmark_data[\"Variation\"].dropna(), 100*(1 - seuil))\nprint(f\"VaR historique sur l'indice CAC 40 : {-VaR_hist_benchmark : .2%}\")\nprint(f\"VaR historique sur 20 jours sur l'indice CAC 40 : {-VaR_hist_benchmark*np.sqrt(20) : .2%}\")\n\nVaR historique sur le portefeuille :  2.29%\nVaR historique sur 20 jours sur le portefeuille :  10.23%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nVaR historique sur l'indice CAC 40 :  2.13%\nVaR historique sur 20 jours sur l'indice CAC 40 :  9.51%\n\n\n\n# VaR param√©trique\n# PnL ~ N(mu, sigma) ==&gt; PnL = mu + sigma * Z, o√π Z ~ N(0,1)\n# P(PnL &gt; -VaR) = alpha &lt;=&gt; P(mu + sigma * Z &gt; -VaR) = alpha &lt;=&gt; P(Z &lt; (-VaR - mu) / sigma) = 1 - alpha\n# Donc, -VaR = mu + sigma * quantile(1 - alpha), o√π quantile(1 - alpha) est le quantile de la loi normale standard\n\nfrom scipy.stats import norm\n\nmu = np.mean(AUM[\"Variation\"].dropna())\nprint(f\"mu sur le portefeuille : {mu : .2}\")\nsigma = np.std(AUM[\"Variation\"].dropna())\nprint(f\"sigma sur le portefeuille : {sigma : .2}\")\n\nVaR_param_portfolio  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_portfolio : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_portfolio * np.sqrt(20): .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(benchmark_data[\"Variation\"].dropna())\nprint(f\"mu sur le benchmark: {mu : .2}\")\nsigma = np.std(benchmark_data[\"Variation\"].dropna())\nprint(f\"sigma sur le benchmark : {sigma : .2}\")\n\nVaR_param_benchmark  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique sur le portefeuille : {VaR_param_benchmark : .2%}\")\nprint(f\"VaR param√©trique sur 20 jours sur le portefeuille : {VaR_param_benchmark * np.sqrt(20): .2%}\")\n\nmu sur le portefeuille :  0.00024\nsigma sur le portefeuille :  0.0087\nVaR param√©trique sur le portefeuille :  2.00%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.94%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu sur le benchmark:  0.00027\nsigma sur le benchmark :  0.0084\nVaR param√©trique sur le portefeuille :  1.93%\nVaR param√©trique sur 20 jours sur le portefeuille :  8.63%\n\n\nLa VaR relative suit une philosophie proche du tracking error. Elle se calcule sur les √©carts entre le portefeuille et le benchmark. Elle sert √† mesurer de combien mon portefeuille sous-performe par rapport √† l‚Äôindice de r√©f√©rence.\n\nperformance_relative\n\n\nVaR_hist_relative = np.percentile(performance_relative.dropna(), 100*(1- seuil))\nprint(f\"VaR historique relative : {- VaR_hist_relative : .2%}\")\nprint(f\"VaR historique relative sur 20 jours : {-VaR_hist_relative*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(performance_relative.dropna())\nprint(f\"mu des performances relatives: {mu : .2}\")\nsigma = np.std(performance_relative.dropna())\nprint(f\"sigma des performances relatives : {sigma : .2}\")\n\nVaR_param_relative  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR param√©trique relative : {VaR_param_relative : .2%}\")\nprint(f\"VaR param√©trique relative sur 20 jours : {VaR_param_relative * np.sqrt(20): .2%}\")\n\nVaR historique relative :  1.07%\nVaR historique relative sur 20 jours :  4.80%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu des performances relatives: -2.5e-05\nsigma des performances relatives :  0.0052\nVaR param√©trique relative :  1.20%\nVaR param√©trique relative sur 20 jours :  5.39%\n\n\n\n\nStress test\nLes stress test permettent de tester les performances du portefeuille dans des conditions extr√™mes. Ils sont de deux natures : 1. Stress test historique : On soumet le portefeuille √† une p√©riode historique ou on estime avoir eu une condition extr√™me (Covid, Subprime crisis). On rejoue un sc√©nario qui s‚Äôest d√©j√† pass√©.\n\nStress test hypoth√©tique : On joue un sc√©nario qui ne s‚Äôest jamais r√©alis√©. Exemple, si les actions chutent de 40%, notre portefeuille d‚Äôaction chute de 40%.\n\nNote : bp = 0,01%\n\n# Recuperons les prix des actifs le 19/02/2020 et le 18/03/2020\n# On va valoriser notre portefeuille √† ces dates et calculer les performances\n# A ces dates, le CAC 40 a connu de fortes pertes pendant la COVID-19\n# data_1902 = get_data\n\nstart_date = pd.to_datetime(\"19-02-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1902 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1902=data_1902[\"portfolio_data\"]\nbenchmark_data_1902=data_1902['benchmark_data']\n\nstart_date = pd.to_datetime(\"18-03-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1803 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1803=data_1803[\"portfolio_data\"]\nbenchmark_data_1803=data_1803['benchmark_data']\n\n# Concat√©ner les donn√©es des deux dates pour le portefeuille et le benchmark\nportfolio_data_stress = pd.concat([portfolio_data_1902, portfolio_data_1803], ignore_index=False)\nbenchmark_data_stress = pd.concat([benchmark_data_1902, benchmark_data_1803], ignore_index=False)\n\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n[                       0%                       ][                       0%                       ][                       0%                       ][*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Stress test covid sur le portefeuille\naum_series_stress = portfolio_data_stress.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\n\nAUM_stress = pd.DataFrame(aum_series_stress, columns=[\"AUM\"])\nAUM_stress[\"Variation\"] = AUM_stress[\"AUM\"].pct_change()\n\nAUM_stress\n\n\n\n\n\n\n\n\nAUM\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n63.108800\nNaN\n\n\n2020-03-18\n43.740069\n-0.30691\n\n\n\n\n\n\n\n\nbenchmark_data_stress[\"Variation\"]=benchmark_data_stress[\"^FCHI\"].pct_change()\nbenchmark_data_stress\n\n\n\n\n\n\n\nTicker\n^FCHI\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n6111.240234\nNaN\n\n\n2020-03-18\n3754.840088\n-0.385585\n\n\n\n\n\n\n\nNotre portefeuille permet de mieux resister au stress test covid que le CAC 40.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html",
    "href": "3A/proc_stochastique/modele_bs.html",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "Le mod√®le de Black-Scholes est un mod√®le math√©matique qui permet de d√©terminer le prix d‚Äôune option √† partir de plusieurs param√®tres. Il est bas√© sur l‚Äôhypoth√®se que le prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique :\n\\[\ndS_t = \\mu S_t dt + \\sigma S_t dW_t\n\\]\nAvec \\(S_t\\) le prix de l‚Äôactif, \\(\\mu\\) le taux de rendement moyen, \\(\\sigma\\) la volatilit√© et \\(W_t\\) un mouvement brownien.\nDe ce fait, le prix d‚Äôune option europ√©enne peut √™tre calcul√© par la formule de Black-Scholes :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\nAvec \\(C\\) le prix de l‚Äôoption, \\(S_t\\) le prix de l‚Äôactif sous-jacent, \\(K\\) le prix d‚Äôexercice de l‚Äôoption, \\(T\\) la maturit√© de l‚Äôoption, \\(r\\) le taux d‚Äôint√©r√™t sans risque, \\(\\sigma\\) la volatilit√© de l‚Äôactif, \\(N\\) la fonction de r√©partition de la loi normale centr√©e r√©duite, et :\n\\[\nd_1 = \\frac{1}{\\sigma \\sqrt{T}} \\left( \\ln \\left( \\frac{S_t}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) T \\right)\n\\]\n\\[\nd_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\n\n# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "href": "3A/proc_stochastique/modele_bs.html#calcul-dun-call",
    "title": "Calibration du mod√®le Black-Scholes",
    "section": "",
    "text": "# Calcul d'un call\nfrom scipy.stats import norm\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef FormulaBS(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    call = S*norm.cdf(d1) - K* np.exp( - tau * r) * norm.cdf(d2)\n    return call\n\nLorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix d‚Äôune option, on peut utiliser la m√©thode de Monte-Carlo pour estimer ce prix. Pour cela, on simule un grand nombre de trajectoires du prix de l‚Äôactif sous-jacent, et on calcule la valeur de l‚Äôoption √† chaque date de maturit√©. On fait ensuite la moyenne de ces valeurs pour obtenir une estimation du prix de l‚Äôoption.\nLe prix de l‚Äôactif sous-jacent suit un mouvement brownien g√©om√©trique, et on peut simuler ce mouvement en utilisant la formule d‚ÄôIto :\n\\[\nS_t = S_0 e^{(\\mu - \\frac{\\sigma^2}{2})t + \\sigma W_t}\n\\]\n\ndef FormulaBSMC(K,r,tau,sigma,M, S0) :\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n    payoff = np.maximum(S_T - K, 0)\n\n    call = np.exp(-r*tau) * np.mean(payoff)\n    return call\n\n\n# Calcul du prix d'un call lors t=O, T=6mois, S0= 42, K=40, r=10%, sigma=20%\nS0 = 42\nK = 40\nr = 0.1\ntau = 6\nsigma = 0.2\n\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nLe prix d'un call est de maturit√© 6mois et de strike 40 est de 20.67722481517296\n\n\n\nM_values = [500, 5000, 50000]\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call avec M=500, de maturit√© 6mois et de strike 40 est de 20.21549073157966\nLe prix d'un call avec M=5000, de maturit√© 6mois et de strike 40 est de 20.341643527763484\nLe prix d'un call avec M=50000, de maturit√© 6mois et de strike 40 est de 20.71029139679338\n\n\n\n# Calcul du prix d'un call lors t=O, T=3mois, S0= 42, K=40, r=10%, sigma=20%\ntau = 3\ncall_BS = FormulaBS(S0,K,r,tau,sigma)\n\nprint(f\"Le prix d'un call est de maturit√© {tau}mois et de strike {K} est de {call_BS}\")\n\nfor M in M_values:\n    call_BSMC = FormulaBSMC(K, r, tau, sigma, M, S0)\n    print(f\"Le prix d'un call avec M={M}, de maturit√© {tau}mois et de strike {K} est de {call_BSMC}\")\n\nLe prix d'un call est de maturit√© 3mois et de strike 40 est de 13.362666146646749\nLe prix d'un call avec M=500, de maturit√© 3mois et de strike 40 est de 13.949252228829925\nLe prix d'un call avec M=5000, de maturit√© 3mois et de strike 40 est de 13.440144185789123\nLe prix d'un call avec M=50000, de maturit√© 3mois et de strike 40 est de 13.383668100223682\n\n\nComme nous pouvons le constater, les deux m√©thodes permettent d‚Äôavoir des r√©sultats similaires. De plus, plus le nombre de simulations est grand, plus la pr√©cision de l‚Äôestimation est grande.\n\n\nLes greeks sont des indicateurs qui permettent de mesurer la sensibilit√© du prix d‚Äôune option √† diff√©rents param√®tres. Les principaux greeks sont :\n\nDelta : mesure la sensibilit√© du prix de l‚Äôoption par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Delta = \\frac{\\partial C}{\\partial S}\n\\]\nGamma : mesure la sensibilit√© du delta par rapport au prix de l‚Äôactif sous-jacent \\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\n\\]\nVega : mesure la sensibilit√© du prix de l‚Äôoption par rapport √† la volatilit√© de l‚Äôactif \\[\nVega = \\frac{\\partial C}{\\partial \\sigma}\n\\]\n\nIl en existe d‚Äôautres, mais ces trois-l√† sont les plus couramment utilis√©s.\nAvec le mod√®le de Black-Scholes, on peut calculer ces greeks de mani√®re analytique :\n\\[\n\\Delta = N(d_1)\n\\]\n\\[\n\\Gamma = \\frac{N(d_1)}{S_t \\sigma \\sqrt{T}}\n\\]\n\\[\nVega = S_t \\sqrt{T} N(d_1)\n\\]\nCependant, lorsqu‚Äôon a pas acc√®s √† une formule analytique pour le prix de l‚Äôoption, on peut utiliser la m√©thode de Monte-Carlo pour estimer ces greeks. Pour cela, on calcule le prix de l‚Äôoption pour une petite variation de chaque param√®tre, et on fait la diff√©rence entre ces deux prix pour obtenir une estimation du greek. On peut √©galement utiliser la m√©thode des diff√©rences finies pour calculer ces greeks.\n\ndef FormulaBSGreeks(S,K,r,tau,sigma):\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau ) / (sigma * np.sqrt(tau))\n    # d2 = d1 - sigma * np.sqrt(tau)\n\n    delta = norm.cdf(d1)\n    gamma = (1/(S*sigma*np.sqrt(tau))) * norm.pdf(d1)\n    vega = S * np.sqrt(tau) * norm.pdf(d1)\n\n    return delta, gamma, vega\n\n\nS = 100\nK = 110\nr = 0.1\ntau = 0.5\nsigma = 0.2\n\ndelta, gamma, vega = FormulaBSGreeks(S=S,K=K,r=r,tau=tau,sigma=sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.40141715171302983, gamma=0.027343746144537384, vega=27.343746144537384\n\n\n\n# Supposons qu'on a pas la formule des greeks\n# comment calculer les greeks\n# Approche montecarlo \n\ndef FormulaBSGreeksMC(K,r,tau,sigma,M, S0) :\n    d1 = (np.log(S/K) + (r + 0.5 *sigma**2)*tau) / (sigma * np.sqrt(tau))\n    # Simulation du M mouvement brownien de loi N(0, sigma*sqrt(T))\n    var_brown = sigma * np.sqrt(tau)\n    W_T= np.random.normal(0,var_brown,M)\n\n    S_T = S0 * np.exp((r - 0.5 * sigma**2) * tau + W_T)\n\n    delta = np.exp(-r*tau) * np.mean((S_T &gt; K) * S_T / S0)\n    gamma = norm.pdf(d1)/(S0 * sigma * np.sqrt(tau))\n    vega = np.exp(-r*tau) * np.mean((S_T &gt; K) * (S_T/sigma) *(np.log(S_T/S0) - (r + 0.5 * sigma**2)*tau))\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeksMC(K,r,tau,sigma,500, S)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.42713711028344337, gamma=0.027343746144537384, vega=27.94419858966199\n\n\n\n# M√©rhode de diff√©rencee finie bas√© sur la m√©thode de Taylor\ndef FormulaBSGreeks_FD_num(S, K, r, tau, sigma, delta_S=1e-5):\n    \"\"\"\n    Calcule les Grecs (Delta, Gamma, Vega) pour une option call\n    par diff√©rences finies.\n\n    Param√®tres:\n    S : float - Prix actuel de l'actif sous-jacent\n    K : float - Prix d'exercice de l'option (strike)\n    r : float - Taux d'int√©r√™t sans risque (annualis√©)\n    tau : float - Temps jusqu'√† la maturit√© (en ann√©es)\n    sigma : float - Volatilit√© de l'actif sous-jacent (annualis√©e)\n    epsilon : float - Petit incr√©ment pour les diff√©rences finies\n\n    Retour:\n    tuple - (Delta, Gamma, Vega)\n    \"\"\"\n    # Delta\n    delta = (FormulaBS(S + delta_S, K, r, tau, sigma) - FormulaBS(S - delta_S, K, r, tau, sigma)) / (2 * delta_S)\n\n    # Gamma\n    gamma = (FormulaBS(S + delta_S, K, r, tau, sigma) - 2 * FormulaBS(S, K, r, tau, sigma) + FormulaBS(S - delta_S, K, r, tau, sigma)) / (delta_S**2)\n\n    # Vega\n    vega = (FormulaBS(S, K, r, tau, sigma + delta_S) - FormulaBS(S, K, r, tau, sigma - delta_S)) / (2 * delta_S)\n\n    return delta, gamma, vega\n\ndelta, gamma, vega = FormulaBSGreeks_FD_num(S,K,r,tau,sigma)\nprint(f\"Les greeks d'un call sont de maturit√© {tau}mois et de strike {K} sont delta={delta}, gamma={gamma}, vega={vega}\")\n\nLes greeks d'un call sont de maturit√© 0.5mois et de strike 110 sont delta=0.4014171516075748, gamma=0.027142732506035824, vega=27.34374614092871\n\n\n\n\n\nL‚Äôint√©r√™t du mod√®le de Black-Scholes est qu‚Äôil permet de calculer la volatilit√© implicite d‚Äôun actif √† partir du prix de l‚Äôoption. En effet, si on connait le prix de l‚Äôoption, le prix de l‚Äôactif sous-jacent, le prix d‚Äôexercice de l‚Äôoption, la maturit√© de l‚Äôoption et le taux d‚Äôint√©r√™t sans risque, on peut calculer la volatilit√© implicite en r√©solvant l‚Äô√©quation de Black-Scholes pour \\(\\sigma\\) :\n\\[\nC(S_t, K, T, r, \\sigma) = S_t N(d_1) - K e^{-rT} N(d_2)\n\\]\n\ndef ImpliedVolBS(S,K,r,tau, Call):\n    # optim function\n    def obj_func(sigma):\n        return (Call - FormulaBS(S,K,r,tau,sigma))**2\n    \n    res = minimize(obj_func, 0.2)\n    sigma = res.x[0]\n    return sigma\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 3 mois et de strike K=S=4.58 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 3/12\nCall = 4.58\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.25mois et de strike 100 est de 0.19821832844648876\n\n\n\n# Calcul de la volatilit√© d'une option d'achat √† la monnaie de maturit√© 6 mois et de strike K=S=5.53 sachant que S0=100; r=5%\n\nS = 100\nK = 100\nr = 0.05\ntau = 0.5\nCall = 5.53\nsigma = ImpliedVolBS(S,K,r,tau, Call)\n\nprint(f\"La volatilit√© implicite d'un call est de maturit√© {tau}mois et de strike {K} est de {sigma}\")\n\nLa volatilit√© implicite d'un call est de maturit√© 0.5mois et de strike 100 est de 0.15010660990708588"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le mod√®le √† volatilit√© stochastique de Taylor est un mod√®le de volatilit√© stochastique qui est utilis√© pour mod√©liser la volatilit√© des actifs financiers. Le mod√®le est d√©fini par l‚Äô√©quation suivante :\n\\[\n\\begin{aligned}\nr_t &= \\exp(x_t/2) \\cdot \\varepsilon_t, \\quad \\varepsilon_t \\sim \\text{N}(0,1) \\\\\nx_t &= \\mu + \\phi \\cdot x_{t-1} + \\sigma_t \\cdot \\eta_t\n\\end{aligned}\n\\]\no√π \\(r_t\\) est le rendement de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), \\(\\mu\\) est la moyenne de la volatilit√©, \\(\\phi\\) est le coefficient d‚Äôautor√©gression, \\(\\sigma_t\\) est l‚Äô√©cart-type de la volatilit√© √† l‚Äôinstant \\(t\\), \\(\\eta_t\\) est un bruit blanc gaussien, et \\(\\varepsilon_t\\) est un bruit blanc gaussien.\nPour extraire la volatilit√©, nous utilisons le filtre de Kalman sur le logarithme des rendements au carr√© \\(y_t = \\log(r_t^2)\\), afin de lin√©ariser le mod√®le. Le mod√®le lin√©aris√© est d√©fini par l‚Äô√©quation suivante :\n\\[\ny_t = x_t + \\varepsilon_t\n\\]\no√π \\(y_t\\) est le logarithme des rendements au carr√© √† l‚Äôinstant \\(t\\), \\(x_t\\) est la volatilit√© de l‚Äôactif financier √† l‚Äôinstant \\(t\\), et \\(\\varepsilon_t\\) est un bruit blanc de loi log-\\(\\chi^2\\).\nDe ce fait, nous pouvons utiliser le filtre de Kalman pour estimer la volatilit√© de l‚Äôactif financier en utilisant les rendements observ√©s. En effet, le filtre de Kalman est un algorithme r√©cursif qui permet d‚Äôestimer l‚Äô√©tat cach√© d‚Äôun syst√®me dynamique √† partir d‚Äôobservations bruit√©es. Il s‚Äôapplique √† des mod√®les lin√©aires dont le bruit est gaussien. Dans notre cas, nous avons lin√©aris√© le mod√®le pour qu‚Äôil soit compatible avec le filtre de Kalman. Cependant, le bruit n‚Äôest pas gaussien, mais log-\\(\\chi^2\\). L‚Äôobjectif de ce notebook est d‚Äôobserver le comportement du filtre de Kalman o√π le bruit n‚Äôest pas gaussien.\nNous poss√©dons d√©j√† d‚Äôun fichier avec les rendements de l‚Äôactif financier et la vraie volatilit√© simul√©s avec les param√®tres suivants \\(\\mu = -0.8\\), \\(\\phi = 0.9\\), \\(\\sigma = 0.09\\). Nous allons donc utiliser ces donn√©es pour estimer la volatilit√© de l‚Äôactif financier en utilisant le filtre de Kalman. N√©anmoins, le code est √©galement fourni pour simuler les donn√©es si vous souhaitez tester le filtre de Kalman sur d‚Äôautres param√®tres.\n\n# Simulation d'un mod√®le √† vol stochastique de Taylor\n\n# r_t = exp(x_t/2)*eps_t (eps_t iid N(0,1))\n# x_t = mu + phi * x_{t-1} + sigma_t * eta_t  (eta_t iid N(0,1))\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Param√®tres\nn = 252\nmu = -0.8\nphi = 0.9\nsigma_squared = 0.09\n\n# Simulation\n# np.random.seed(0)\n\n# x = np.zeros(n)\n# r = np.zeros(n)\n\n# for t in range(0, n):\n#     if t == 0:\n#         x[t] = np.random.normal(loc= mu/(1-phi), scale=np.sqrt(sigma_squared/(1-phi**2))) # Densit√© de transition stationnaire de x_t\n#     else:\n#         x[t] = mu + phi * x[t-1] + np.sqrt(sigma_squared) * np.random.normal(loc=0, scale=1)\n#     r[t] = np.exp(x[t]/2) * np.random.normal(loc=0, scale=1)\n\ndata  = pd.read_csv('true_sv_taylor.csv')\nr = data['r']\nx = data['x']\n\n\n# Affichage des trajectoires\nimport matplotlib.pyplot as plt\n\nplt.plot(r, color=\"black\")\nplt.title(\"Trajectoire des rendements\")\nplt.show()\n\nplt.plot(x, color='red')\nplt.title(\"Trajectoire de log-volatilit√©\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Construction du mod√®le log-SV (mod√®le de Taylor)\n\n# Pour appliquer le filtre de Kalman, il faut que les bruits soient centr√©s.\nmu_r_squared = -1.27 # car log(eps**2) suit une log chi-deux\ny = np.log(r**2) - mu_r_squared\n\n\n\nLe filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-connus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Le filtre de Kalman fonctionne en deux √©tapes : pr√©diction et mise √† jour. La pr√©diction consiste √† pr√©dire l‚Äô√©tat cach√© du syst√®me √† l‚Äôinstant \\(t\\) en utilisant les observations jusqu‚Äô√† l‚Äôinstant \\(t-1\\). La mise √† jour consiste √† mettre √† jour l‚Äôestimation de l‚Äô√©tat cach√© en utilisant l‚Äôobservation √† l‚Äôinstant \\(t\\). Posons :\n\n\\(x_{\\text{hat}[t]}\\) l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P[t]\\) la matrice de covariance de l‚Äôestimation de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(x_{\\text{hat\\_m}}\\) la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(P_m\\) la matrice de covariance de la pr√©diction de la volatilit√© √† l‚Äôinstant \\(t\\),\n\\(y[t]\\) l‚Äôobservation √† l‚Äôinstant \\(t\\),\n\\(K\\) le gain de Kalman.\n\n$$\n\\[\\begin{aligned}\n\\text{Pr√©diction} & : \\\\\nx_{\\text{hat\\_m}} &= \\mu + \\phi \\cdot x_{\\text{hat}[t-1]} \\\\\nP_m &= \\phi^2 \\cdot P[t-1] + \\sigma^2 \\\\\ny_m &= x_{\\text{hat\\_m}} \\\\\n\n\\text{Mise √† jour} & : \\\\\nK &= \\frac{P_m}{P_m + \\pi^2/2} \\\\\nP[t] &= (1 - K) \\cdot P_m \\\\\nx_{\\text{hat}[t]} &= x_{\\text{hat\\_m}} + K \\cdot (y[t] - y_m)\n\\end{aligned}\\]\n$$\nPour initialiser le filtre de Kalman, il est conseill√© de connaitre la loi stationnaire de la volatilit√©. En effet, la volatilit√© suit une loi normale stationnaire de param√®tre \\(\\mu/(1 - \\phi)\\) et \\(\\sigma^2/(1 - \\phi^2)\\). Par cons√©quent, nous pouvons initialiser le filtre de Kalman avec ces param√®tres. En ce qui concerne la matrice de covariance de l‚Äô√©tat initial, nous pouvons la fixer √† une valeur √©lev√©e, par exemple \\(10^2\\).\nDans notre cas, nous allons tester deux initialisations diff√©rentes : une initialisation avec les param√®tres stationnaires et une initialisation avec des param√®tres al√©atoires.\n\n# Script de filtre de kalman pour estimer la volatilit√© √† chaque instant t en supposant les param√®tres connus\n\ndef kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P):\n    x_hat = np.zeros(n)\n    P = np.zeros(n)\n\n    # Initialisation\n    x_hat[0] = init_x\n    P[0] = init_P # Plus P est grand moins on fait confiance √† l'apriori sur la valeur de la volatilit√©\n\n    for t in range(1, n):\n        # Pr√©diction\n        x_hat_m = mu + phi * x_hat[t-1] \n        P_m = phi**2 * P[t-1] + sigma_squared\n        y_m = x_hat_m\n\n        # Mise √† jour\n        K = P_m / (P_m + (np.pi**2)/2)\n        P[t] = (1 - K) * P_m\n        x_hat[t] = x_hat_m + K * (y[t] - y_m)\n    return x_hat, P, K\n\n\n# Initialisation √† 0 et 0.01\ninit_x = 0\ninit_P = 0.01\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='grey', label='Volatilit√© estim√©e')\n\n\n\n\n\n\n\n\n\n# Initialisation √† x_0 et sigma_squared/(1-phi**2)\ninit_x = x[0]\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n\n# Compute MSE, MAE, and RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.2794557366826766\nMAE =  0.4120099778966342\nRMSE =  0.5286357315606622\n\n\n\n\n\n\n\n\n\n\n# Initialisation √† mu/(1-phi) et sigma_squared/(1-phi**2)\ninit_x = mu/(1-phi)\ninit_P = sigma_squared/(1-phi**2)\nx_hat, P, K = kalman_filter(y, mu, phi, sigma_squared, n, init_x, init_P)\nplt.plot(x_hat, color='red', label='Volatilit√© estim√©e')\nplt.plot(x, color='grey', label = 'Volatilit√© r√©elle')\nplt.legend()\n\n# Compute MSE, MAE, RMSE\nmse = np.mean((x - x_hat)**2)\nmae = np.mean(np.abs(x - x_hat))\nrmse = np.sqrt(mse)\nprint(\"MSE = \", mse)\nprint(\"MAE = \", mae)\nprint(\"RMSE = \", rmse)\n\nMSE =  0.27543214871007066\nMAE =  0.4078063371140131\nRMSE =  0.5248162999660649"
  },
  {
    "objectID": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "href": "3A/proc_stochastique/modele_log_sv-part1.html#filtre-de-kalman-avec-param√®tres-inconnus",
    "title": "Calibration du mod√®le √† volatilit√© stochastique de Taylor : Filtre de Kalman",
    "section": "",
    "text": "Dans le cas o√π les param√®tres du mod√®le sont inconnus, nous pouvons les estimer en utilisant le filtre de Kalman. En effet, nous pouvons utiliser l‚Äôalgorithme EM pour estimer les param√®tres du mod√®le. L‚Äôalgorithme EM est un algorithme it√©ratif qui permet d‚Äôestimer les param√®tres d‚Äôun mod√®le en maximisant la vraisemblance des donn√©es observ√©es. Dans notre cas, nous allons utiliser l‚Äôalgorithme EM pour estimer les param√®tres \\(\\mu\\), \\(\\phi\\) et \\(\\sigma\\) du mod√®le.\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define the log-likelihood function for the log-SV model with Kalman filter\ndef log_sv_kalman(params, y):\n    # Extract parameters\n    mu, phi, sigma_eta = params\n\n    # Number of observations\n    n = len(y)\n\n    # Initialize state and variance\n    x_t = mu  # Initial state (log-volatility)\n    P_t = sigma_eta**2 / (1 - phi**2)  # Initial variance (stationarity assumption)\n\n    # Log-likelihood accumulator\n    log_likelihood = 0\n\n    for t in range(n):\n        # Observation equation: y_t ~ x_t + nu_t\n\n        # Prediction step\n        y_t_pred = x_t\n        F_t = P_t + np.pi**2 / 2  # Variance of observation noise\n\n        # Update step\n        v_t = y[t] - y_t_pred  # Prediction error\n        K_t = P_t / F_t  # Kalman gain\n        x_t = x_t + K_t * v_t\n        P_t = (1 - K_t) * P_t + sigma_eta**2  # Update variance\n\n        # Update log-likelihood\n        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(F_t) + (v_t**2 / F_t))\n\n        # State evolution\n        x_t = mu + phi * (x_t - mu)  # State equation\n\n    return -log_likelihood  # Negative log-likelihood for minimization\n\n# Initial parameter guesses\ninitial_params = [-0.7, 0.8, np.sqrt(0.05)]\n\n# Constrain phi between -1 and 1 and sigma_eta &gt; 0\nbounds = [(-np.inf, np.inf), (-1, 1), (1e-6, np.inf)]\n\n# Optimize parameters using the log-likelihood function\nresult = minimize(log_sv_kalman, initial_params, args=(y,), bounds=bounds, method='L-BFGS-B')\n\n# Extract estimated parameters\nmu_est, phi_est, sigma_eta_est = result.x\n\n# Print results\nprint(\"Estimated parameters:\")\nprint(f\"mu: {mu_est:.4f}\")\nprint(f\"phi: {phi_est:.4f}\")\nprint(f\"sigma_eta: {sigma_eta_est:.4f}\")\n\nEstimated parameters:\nmu: -0.8021\nphi: 0.8001\nsigma_eta: 1.2184"
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La r√©glementation prudentielle",
    "section": "",
    "text": "La r√©glementation prudentielle a √©t√© initi√©e par le d√©veloppement des march√©s financiers et des chocs aliment√©s par diverses crises financi√®res. Face √† ce constat, les autorit√©s de contr√¥le bancaire ainsi que les autorit√©s de march√© ont pris des d√©cisions pour r√©guler les march√©s. C‚Äôest notamment le r√¥le qu‚Äôoccupe le Comit√© de B√¢le ou la Commission bancaire, qui ont pour objectif de renforcer la stabilit√© des march√©s financiers. En France, l‚ÄôACPR (Autorit√© de Contr√¥le Prudentiel et de R√©solution) et la Banque de France sont membres du Comit√© de B√¢le et participent √† ses travaux et d√©cisions.\nIl existe par ailleurs plusieurs textes r√©glementaires ou documents relatifs au risque de march√©. Parmi ces textes, on peut citer le document de r√©f√©rence pour calculer le ratio de solvabilit√© de la Commission bancaire, intitul√© ‚ÄúModalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL‚Äù, actualis√© tous les ans par l‚ÄôACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-march√©",
    "title": "La r√©glementation prudentielle",
    "section": "Approche standard de mesure du risque de march√©",
    "text": "Approche standard de mesure du risque de march√©\nL‚Äôapproche standard de mesure du risque de march√© consiste √† calculer les exigences en fonds propres pour chaque cat√©gorie de risque, √† savoir :\n\nle risque de taux (g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque li√© aux titres de propri√©t√©(g√©n√©ral et sp√©cifique) calcul√© sur le p√©rim√®tre du portefeuille de n√©gociation ;\nle risque de change calcul√© sur l‚Äôensemble des op√©rations appartenant aussi bien au portefeuille de n√©gociation ou non;\nle risque sur mati√®res premi√®res calcul√© sur l‚Äôensemble des op√©rations du portefeuille de n√©gociation ou non;\nles risques op√©rationnels calcul√©s sur les options associ√©es √† chachune des cat√©gories de risque cit√©es ci-dessus.\n\nPar la suite, il s‚Äôagit de les additionner de mani√®re arithm√©tique. Par exemple, pour les titres de propri√©t√©, l‚Äôexigence de fonds propres est la somme de l‚Äôexigence de fonds propres pour le risque g√©n√©ral et l‚Äôexigence de fonds propres pour le risque sp√©cifique.\nPour le calcul des exigences de fonds propres au titre des risques de march√©, il faut tout d‚Äôabord d√©terminer les positions nettes. Les positions de titrisation log√©es dans le portefeuille de n√©gociation sont trait√©es comme tout instrument de dette au titre du risque de taux.\nPour le risque sp√©cifique, l‚Äôexigence en fonds propres sera la somme des positions nettes multipli√©es par un coefficient de pond√©ration (2%, 4%, 8% ou 12%) choisi en fonction de la liquidit√© et la diversification de la position. Pour le risque g√©n√©ral, l‚Äôexigence en fonds propres est la somme des positions nettes globales (pour chaque march√© national) multipli√©es par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-mod√®le-interne",
    "href": "3A/reglementation_prudentielle.html#approche-mod√®le-interne",
    "title": "La r√©glementation prudentielle",
    "section": "Approche mod√®le interne",
    "text": "Approche mod√®le interne\nL‚Äôapproche mod√®le interne est une m√©thode de calcul des exigences en fonds propres pour le risque de march√© qui permet aux √©tablissements de calculer leurs propres exigences. L‚Äôexigence en fonds propres est g√©n√©ralement un calcul de la VaR. Cette approche est soumise √† des conditions strictes et √† une validation par l‚ÄôACPR.\nConcernant l‚Äôutilisation conjointe des mod√®les internes et de l‚Äôapproche standard, la position de la commission pr√™te une attention particuli√®re √† la permanence des m√©thodes ainsi qu‚Äô√† leur √©volution. L‚Äôobjectif est de s‚Äôorienter vers un mod√®le global qui tient compte de l‚Äôensemble des risques de march√©.\n\nAinsi, un √©tablissement commen√ßant √† utiliser des mod√®les pour une ou plusieurs cat√©gories de facteurs de risque doit en principe √©tendre progressivement ce syst√®me √† tous ses risques √† la m√©thodologie standardis√©e (√† moins que la Commission Bancaire ne lui ait retir√© son agr√©ment pour ses mod√®les).\n\nPour une banque, la construction d‚Äôun mod√®le interne doit permettre de fournir une mesure plus √©conomique du risque de march√©. Au titre de l‚Äôarticle 363 du CRR (R√®glement sur les exigences de fonds propres), l‚Äôautorit√© comp√©tente autorise les √©tablissements assujettis √† utiliser leurs mod√®les internes pour calculer les exigences de fonds propres pour risques de march√©, apr√®s avoir v√©rifi√© qu‚Äôils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3√®me partie du CRR [@journal]. L‚Äôautorisation d‚Äôutiliser des mod√®les internes accord√©e par les autorit√©s comp√©tentes est requise pour chaque cat√©gorie de risques (risque g√©n√©ral et sp√©cifique li√©s aux actions et titres de cr√©ance, risque de change et risque sur mati√®res premi√®res), et elle n‚Äôest accord√©e que si le mod√®le interne couvre une part importante des positions d‚Äôune certaine cat√©gorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives √† la mesure du risque (articles 367) mais aussi d‚Äôordre g√©n√©ral (article 365).\n\nExigences g√©n√©rales\nLe calcul de la valeur en risque vis√©e √† l‚Äôarticle 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprim√© en centiles et unilat√©ral, de 99 %;\nune p√©riode de d√©tention de dix jours;\nune p√©riode effective d‚Äôobservation historique d‚Äôau moins un an, √† moins qu‚Äôune p√©riode d‚Äôobservation plus courte ne soit justifi√©e par une augmentation significative de la volatilit√© des prix;\ndes mises √† jour au moins mensuelles des s√©ries de donn√©es.\n\nL‚Äô√©tablissement peut utiliser des mesures de la valeur en risque calcul√©es sur la base de p√©riodes de d√©tention inf√©rieures √† dix jours, qu‚Äôil porte √† dix jours selon une m√©thode appropri√©e qu‚Äôil revoit r√©guli√®rement.\nChaque √©tablissement doit √©galement calculer, au moins hebdomadairement, une ‚Äúvaleur en risque en situation de tensions‚Äù (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit √™tre calcul√©e conform√©ment aux m√™mes exigences que la VaR standard √©nonc√©es plus haut (intervalle de confiance de 99% etc.). Cependant, les donn√©es d‚Äôentr√©e du mod√®le de Stressed VaR doivent √™tre calibr√©es par rapport √† une p√©riode historique de tensions financi√®res significatives d‚Äôau moins 12 mois, pertinente pour le portefeuille de l‚Äô√©tablissement. Le choix de cette p√©riode de tensions historiques fait l‚Äôobjet d‚Äôun examen au moins annuel par l‚Äô√©tablissement, qui en communique les r√©sultats aux autorit√©s comp√©tentes. L‚Äôobjectif est de s‚Äôassurer que la Stressed VaR refl√®te de mani√®re ad√©quate les risques auxquels l‚Äô√©tablissement serait expos√© en p√©riode de crise financi√®re.\nPour r√©sumer, les √©tablissements doivent calculer la perte potentielle quotidiennement pour une p√©riode de d√©tention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent √©galement calculer une Stressed VaR au moins une fois par semaine, en utilisant des donn√©es historiques de p√©riodes de tensions financi√®res significatives.\nNotons \\(VaR(t)\\) la valeur en risque √† la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions √† la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) √† la date t pour le risque de march√© sont calcul√©es comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\no√π \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu‚Äôon vera plus tard.\nDans des p√©riodes normales, l‚Äôexigence en fonds propres sera donc la somme d‚Äôun multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n‚Äôest que dans les p√©riodes de crises financi√®res que l‚Äôexigence en fonds propres correspond √† la VaR ou √† la sVaR du jour pr√©c√©dent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est √©gal √† la somme du chiffre 3, au minimum, et d‚Äôun cumulateur compris entre 0 et 1 conform√©ment au tableau 1. Ce cumulateur d√©pend du nombre de d√©passements, sur les 250 derniers jours ouvr√©s, mis en √©vidence par les contr√¥les a posteriori de la mesure de la valeur en risque, au sens de l‚Äôarticle 365, paragraphe 1, effectu√©s par l‚Äô√©tablissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqu√©s depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqu√©s depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.d√©passements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque sp√©cifique, tout mod√®le interne utilis√© pour calculer les exigences de fonds propres et tout mod√®le interne utilis√© pour la n√©gociation en corr√©lation satisfont aux exigences suppl√©mentaires suivantes:\n\nle mod√®le interne explique la variation historique des prix √† l‚Äôint√© rieur du portefeuille;\nil refl√®te la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement d√©favorable;\nil est valid√© par des contr√¥les a posteriori(backtesting) visant √† √©tablir si le risque sp√©cifique a √©t√© correctement pris en compte. Si l‚Äô√©tablissement effectue ces contr√¥les a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de mani√®re coh√©rente;\nil tient compte du risque de base li√© √† la signature et, en particulier, il est sensible aux diff√©rences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d‚Äô√©v√©nement.\n\nLe risque sp√©cifique vise √† tenir compte du risque de contrepartie li√© √† l‚Äôemetteur de l‚Äôinstrument.\nPour en savoir plus, reportez au r√®glement (UE) No 575/2013 du parlement europ√©en du journal officiel de l‚ÄôUnion Europ√©enne, appel√© aussi r√®glement CRR. (voir aussi la notice 2020 relative aux ¬´ Modalit√©s de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV¬ª)."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html",
    "href": "3A/value-at-risk/var_application.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous int√©resser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d‚Äôestimer les pertes maximales potentielles d‚Äôun portefeuille d‚Äôactifs financiers sur un horizon de temps donn√©, √† un certain niveau de confiance. Elle est largement utilis√©e par les institutions financi√®res pour √©valuer et g√©rer les risques de march√©, de cr√©dit et de liquidit√© (cf.¬†Value at-Risk).\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-analytique",
    "href": "3A/value-at-risk/var_application.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est bas√©e sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR √† horizon 1 jour. La VaR √† horizon 1 jour est d√©finie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] o√π \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) √† \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l‚Äôhypoth√®se nulle est que la population √©tudi√©e suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\nnp.float64(5.073604966554165e-41)\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l‚Äôhypoth√®se de la distribution normale de nos rendements. Cel√† est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(8, 6))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Premi√®re bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR √† horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR √† horizon 1 jour est de 0.0326\n\n\nLa VaR √† horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l‚Äô√©chantillon test les rendements avec la VaR √† horizon 1 jour. Si le rendement est inf√©rieur √† l‚Äôoppos√© de la VaR gaussienne, alors la VaR est viol√©e et cel√† correspond √† une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d‚Äôexceptions que nous comptabilisons sur nos donn√©es test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 30 et en d√©duisons que le taux d‚Äôexception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.13\n\n\nPour tester la pertinence de la VaR calcul√©e, il faudrait id√©alement que le taux d‚Äôexception soit inf√©rieur √† 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\nnp.float64(0.27668172410611824)\n\n\nLa pvaleur de ce test est 3.70%, cel√† est inf√©rieur √† 5% donc nous rejetons l‚Äôhypoth√®se nulle selon laquelle le taux d‚Äôexception est √©gale √† 0.01 au risque 5% de se tromper. Cel√† nous indique que la VaR gaussienne n‚Äôest pas performante. Ceci n‚Äôest pas surprenant √©tant donn√© que nous faisons une hypoth√®se sur la distribution des rendements qui n‚Äôest pas v√©rifi√©e."
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-historique",
    "href": "3A/value-at-risk/var_application.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est bas√©e sur les rendements historiques. Elle est d√©finie comme l‚Äôoppos√© du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsid√©rons les mouvements de prix quotidiens pour l‚Äôindice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 sc√©narios ou cas qui serviront de guide pour les performances futures de l‚Äôindice, c‚Äôest-√†-dire que les 6513 derniers jours seront repr√©sentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour √† 99% correspond au 1er percentile de la distribution de probabilit√© des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique √† horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique √† horizon 1 jour est de 0.0396\n\n\nNous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que la VaR gaussienne. Le taux d‚Äôexception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d‚Äôexceptions pour la VaR √† horizon 1 jour qui est √©gale √† 14 et en d√©duisons que le taux d‚Äôexception est 0.64%. Ce taux d‚Äôexception est statistiquement sup√©rieur √† 1% (car la pvaleur est d‚Äôenviron 0.95). Ainsi, la VaR historique est performante pour la p√©riode consid√©r√©e.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\nnp.float64(0.9853349189614367)"
  },
  {
    "objectID": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "href": "3A/value-at-risk/var_application.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est bas√©e sur la simulation de trajectoires de rendements. Nous allons simuler jusqu‚Äô√† 10000 sc√©narios de rendements et calculer la VaR √† horizon 1 jour en posant une hypoth√®se de normalit√© sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise √† partir de 3000 sc√©narios. Nous utiliserons donc 3000 simulations de rendements. Nous en d√©duisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\nnp.float64(0.046018349211036626)\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins viol√©e dans l‚Äô√©chantillon test que les deux autres VaRs. En effet, le taux d‚Äôexception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inf√©rieur √† 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\nnp.float64(0.9994129059878106)"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html",
    "href": "3A/value-at-risk/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque r√©glementaire correspond √† la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s‚Äôagit dans cette section de d√©velopper la notion de VaR pour des portefeuilles lin√©aires et non lin√©aires."
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#le-backtesting",
    "href": "3A/value-at-risk/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contr√¥le de la qualit√© de la VaR pour un horizon de 1 jour. Il permet de v√©rifier si la VaR est bien calibr√©e. Pour cela, on compare la VaR calcul√©e avec la perte r√©elle. Si la VaR est bien calibr√©e, la perte r√©elle ne doit pas d√©passer la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d‚Äôexception pour valider le mod√®le. Notons PnL le profit and loss du portefeuille et VaR la valeur √† risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsid√©rons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inf√©rieur √† l‚Äôoppos√© de la VaR avec probabilit√© \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une p√©riode ouvr√© comptant n jours, la probabilit√© d‚Äôavoir \\(i\\) exceptions est donn√©e par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir plus de \\(k\\) exceptions est donn√©e par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilit√© d‚Äôavoir au plus de \\(i\\) exceptions est donn√©e par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilit√© d‚Äôexception n‚Äôexc√®de pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d‚Äôexceptions empirique est sup√©rieur √† celui attendu, le mod√®le est rejet√© :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donn√©e par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d‚Äôexceptions observ√©e lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donn√©e par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) o√π \\(t\\) est la valeur observ√©e de la statistique de test et \\(\\phi\\) est la fonction de r√©partition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#cas-g√©n√©ral",
    "href": "3A/value-at-risk/var_def.html#cas-g√©n√©ral",
    "title": "La VaR",
    "section": "2.2 Cas g√©n√©ral",
    "text": "2.2 Cas g√©n√©ral\nDans cette approche, nous consid√©rons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en d√©duisons donc que la VaR est calcul√© comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction d√©croissante de l‚Äôesp√©rance de PnL et une fonction croissante de la volatilit√© du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de pr√©voir l‚Äôesp√©rance du PnL futur.\n\n2.2.1 Exemple\nNous consid√©rons une position courte de 1 million de dollars sur le contrat √† terme S&P 500. Nous estimons que la volatilit√© annualis√©e \\(\\sigma_{\\text{SPX}}\\) est √©gale √† 35%.\nLa perte du portefeuille est √©gale √† \\(L(w) = N \\times R_{\\text{SPX}}\\) o√π \\(N\\) est le montant de l‚Äôexposition (‚àí1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l‚Äôindice S&P 500. Nous d√©duisons que la volatilit√© de la perte annualis√©e est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur √† risque pour une p√©riode de d√©tention d‚Äôun an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l‚Äôinvestisseur sur un 1an s‚Äô√©l√®ve √† 815 500‚Ç¨ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre p√©riode de d√©tention, nous utilisons la r√®gle de la racine carr√© pour convertir la volatilit√© pour une fr√©quence donn√© \\(f_1\\) en une autre volatilit√© pour une autre fr√©quence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les r√©sultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calcul√© sur 1 jour, pour l‚Äôavoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "href": "3A/value-at-risk/var_def.html#mod√®les-lin√©aires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Mod√®les lin√©aires de facteurs",
    "text": "2.3 Mod√®les lin√©aires de facteurs\nNous consid√©rons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est lin√©aire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La premi√®re id√©e est de choisir les facteurs comme √©tant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du march√© et chaque actif poss√®de son propre facteur de risque.\nLe probl√®me est que les prix sont loin d‚Äô√™tre stationnaires, ce qui nous am√®ne √† devoir affronter certains probl√®mes pour mod√©liser la distribution \\(F_t\\). Une autre id√©e est de r√©crire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] o√π \\(R_i(t;h)\\) est le retour de l‚Äôactif entre \\(t\\) et \\(t+h\\).\nNous d√©duisons que le PnL al√©atoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\no√π \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l‚Äôexposition nominale)dans l‚Äôactif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s‚Äôensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous d√©duisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l‚Äô√âquation (2.6), l‚Äôexpression de la valeur √† risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d‚Äôestimer la matrice de covariance des retours des actifs pour calculer la valeur √† risque. Cela explique la popularit√© de ce mod√®le, surtout lorsque le P&L du portefeuille est une fonction lin√©aire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsid√©rons l‚Äôexemple des entreprises d‚ÄôApple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l‚Äô√©cart type estim√© des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corr√©lation crois√©e est √©gale √† 12,0787 %. Il s‚Äôensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous d√©duisons que la valeur √† risque quotidienne √† 99% est de 41,21 $. Nous obtenons une figure inf√©rieure √† celle de la valeur √† risque historique, qui √©tait de 47,39 $. Nous expliquons ce r√©sultat par le fait que la distribution gaussienne sous-estime la probabilit√© des √©v√©nements extr√™mes et n‚Äôest donc pas adapt√©e √† des calculs pr√©cis de risque dans des situations de march√© volatiles.\n\n\n2.3.2 Exemple de portefeuille lin√©aire d‚Äôactifs\nConsid√©rons un portefeuille lin√©aire compos√© de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne √©gaux √† : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilit√©s journali√®res sont √©gales √† 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244‚Ç¨, 135‚Ç¨,315‚Ç¨. La matrice de corr√©lation est donn√©e par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journ√©e est donc de 18.42‚Ç¨ avec un risque 1% de se tromper.\n\n2.3.2.1 Impl√©mentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les √©carts-types et les corr√©lations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "3A/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "href": "3A/value-at-risk/var_def.html#mod√®les-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Mod√®les factoriels de risque",
    "text": "2.4 Mod√®les factoriels de risque\nNous supposons que la valeur du portefeuille d√©pend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille √† \\(t+h\\) d√©pend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\no√π g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit lin√©aire par rapport aux facteurs de risque, ainsi donc le retour des actifs √† l‚Äôhorizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] o√π \\(B\\) est la matrice des sensibilit√©s du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque √† l‚Äôhorizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables al√©atoires gaussiennes ind√©pendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable al√©atoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calcul√© comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette m√©thode repose sur 3 hypoth√®ses : l‚Äôind√©pendance temporelle des variations de la valeur du portefeuille, la normalit√© des facteurs et la relation lin√©aire entre les facteurs et la valeur du portefeuille. En g√©n√©ral, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons √† partir des donn√©es historiques des facteurs et \\(B\\) est le vecteur des sensibilit√©s du portefeuille aux facteurs de risque. La seuil difficult√© de cette m√©thode est l‚Äôestimation de la matrice de variance covariance.\n\n2.4.1 Exemple d‚Äôun portefeuille obligataire sans risque de cr√©dit\nNous consid√©rons une exposition sur une obligation am√©ricaine √† $t=$31 d√©cembre 2014. Le nominal de l‚Äôobligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont √©gaux √† 5, \\(t_m&gt;t\\). La maturit√© r√©siduelle est de cinq ans et les dates de fixation sont √† la fin de d√©cembre (\\(n_C=5\\). Le nombre d‚Äôobligations d√©tenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d‚Äôune obligation z√©ro coupon (montant qu‚Äôun investisseur serait pr√™t √† payer aujourd‚Äôhui pour recevoir un paiement fixe √† une date future : combien me rapport un euro √† maturit√© \\(T\\) aujourd‚Äôhui?) au temps \\(t\\) pour l‚Äô√©ch√©ance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) o√π \\(R_t(T)\\) est le taux de rendement z√©ro coupon.\nLa valeur de l‚Äôobligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en d√©duit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\no√π \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l‚Äôobligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l‚Äôobligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en d√©duisons que le prix de l‚Äôobligation est de \\(P(t)=115,47 \\$\\) et l‚Äôexposition totale est de 1 154 706 $. En utilisant la p√©riode historique de l‚Äôann√©e 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d‚Äôint√©r√™t √† coupon z√©ro sachant que l‚Äô√©cart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), o√π \\(h\\) correspond √† un jour de bourse. Pour la matrice de corr√©lation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en d√©duisons que la valeur √† risque √† 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur √† risque de 4970$ pour une p√©riode de d√©tention d‚Äôun jour.\n\n2.4.1.1 Impl√©mentation en R\n\n# D√©finition des √©carts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corr√©lation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html",
    "href": "3A/value-at-risk/var_garch.html",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "",
    "text": "Ce TP est une continuit√© du TP-1 et du TP-2 dans lequel on souhaitait impl√©menter la VaR (Value at Risk) et l‚ÄôES (Expected Shortfall) en utilisant les m√©thodes classiques propos√©es dans la r√©glementation b√¢loise, i.e.¬†la m√©thode historique, param√©trique et bootstrap (TP1). Cependant, une limite de ces m√©thodes est qu‚Äôelles ne prennent pas en compte la queue de distribution de la perte. Pour rem√©dier √† cela, nous avons utilis√© des m√©thodes avec la th√©orie des valeurs extr√™mes, i.e.¬†l‚Äôapproche Block Maxima et l‚Äôapproche Peaks Over Threshold (TP2). Jusqu‚Äô√† maintenant, on consid√©rait que la s√©rie est iid. Cependant, dans la r√©alit√©, les s√©ries financi√®res sont souvent caract√©ris√©es par une d√©pendance temporelle et une volatilit√© conditionnelle.\nDans le cadre du TP3, il s‚Äôagira de prendre en compte la d√©pendance temporelle et la volatilit√© conditionnelle dans les s√©ries temporelles financi√®res. Pour ce faire, nous utiliserons un mod√®le de VAR dynamique avec le mod√®le GARCH.\nLe mod√®le GARCH (Generalized Autoregressive Conditional Heteroskedasticity) est un mod√®le de volatilit√© conditionnelle qui permet de mod√©liser la volatilit√© des rendements financiers. Il a √©t√© introduit par Bollerslev en 1986. Le mod√®le GARCH est une extension du mod√®le ARCH (Autoregressive Conditional Heteroskedasticity) introduit par Engle en 1982. Le mod√®le GARCH est d√©fini par les √©quations suivantes:\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\sum \\alpha_i \\epsilon_{t-i}^2 + \\sum \\beta_i \\sigma_{t-i}^2\n\\]\nDans ce mod√®le \\(\\mu_t\\) est un param√®tre de tendance moyenne √† identifier, \\(\\epsilon_t\\) est le r√©sidu, \\(\\sigma_t^2\\) est la variance conditionnelle, \\(z_t\\) est un bruit blanc, \\(\\omega\\) est un param√®tre de constante, \\(\\alpha_i\\) et \\(\\beta_i\\) sont les param√®tres du mod√®le GARCH √† identifier.\n# D√©finition des librairies\nimport yfinance as yf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n# Import des donn√©es du CAC 40\ndata = yf.download(\"^FCHI\")\n\n# Calcul des rendements logarithmiques\ndata['log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n\n# Retirer la premi√®re ligne\ndata = data.dropna()\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\ntrain = data[['log_return',\"Close\"]]['15-10-2008':'26-07-2022']\ndata_train = train['log_return']\n\ntest = data[['log_return',\"Close\"]]['27-07-2022':'11-06-2024']\ndata_test = test['log_return']"
  },
  {
    "objectID": "3A/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "href": "3A/value-at-risk/var_garch.html#i.-impl√©mentation-de-la-var-dynamique",
    "title": "TP3:M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)",
    "section": "I. Impl√©mentation de la VaR dynamique",
    "text": "I. Impl√©mentation de la VaR dynamique\n\nI.1. Pertinence du mod√®le AR(1)-GARCH(1,1)\nLe mod√®le AR(1)-GARCH(1,1) est le mod√®le qui, en pratique, est utilis√© pour r√©aliser la VaR dynamique. Cependant, il n‚Äôest pas tout le temps adapt√© aux donn√©es financi√®res. Dans ce TP, nous allons commencer par tester l‚Äô√©ligibilit√© de ce mod√®le dans le cadre des donn√©es que nous poss√©dons.\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, label='Train')\nplt.title('CAC 40 Log Returns')\nplt.show()\n\n\n\n\n\n\n\n\n\n## ACF et PACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(data_train, ax=plt.gca(), lags=40)\nplt.subplot(222)\nplot_pacf(data_train, ax=plt.gca(), lags=40)\nplt.show()\n\n\n\n\n\n\n\n\nDans la s√©rie temporelle que nous poss√©dons, nous constatons que la s√©rie peut √™tre mod√©liser par un AR(1). Pour un test plus rigoureux de cette hypoth√®se, nous allons utiliser la m√©thode de Lljung Box afin de d√©terminer le meilleur mod√®le qui puisse mod√©liser la s√©rie. Ainsi, pour un ordre pmax = 2 et qmax=2, nous allons : 1. Estimer les param√®tres du mod√®le ARMA(p,q) pour chaque combinaison de p et q 2. Calculer la statistique de Ljung Box pour chaque combinaison de p et q afin d‚Äôexaminer si les r√©sidus d‚Äôun mod√®le sont du bruit blanc 3. Filtrer les mod√®les pour lesquels les r√©sidus sont du bruit blanc 4. Choisir le meilleur mod√®le en utilisant le crit√®re d‚ÄôAkaike\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom scipy.stats import boxcox\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Param√®tres du mod√®le\np_max = 2\nq_max = 2\nbest_aic = np.inf\nbest_order = (0, 0, 0)\n\n# Chargement de la s√©rie temporelle (remplacer par la vraie s√©rie data_unindex)\n# Exemple fictif avec des donn√©es al√©atoires\nnp.random.seed(42)\ndata_unindex = data_train.copy()\ndata_unindex.reset_index(drop=True, inplace=True)\n\n# Cr√©ation de la matrice pour stocker les AIC\naic_matrix = pd.DataFrame(np.nan, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\nbb_test = pd.DataFrame(0, index=[f\"p={p}\" for p in range(p_max+1)], \n                          columns=[f\"q={q}\" for q in range(q_max+1)])\n\n# Boucle pour estimer les mod√®les et stocker les AIC\nfor p in range(p_max + 1):\n    for q in range(q_max + 1):\n        try:\n            model = ARIMA(data_unindex, order=(p, 0, q))\n            out = model.fit()\n            aic_matrix.loc[f\"p={p}\", f\"q={q}\"] = out.aic  # Stockage de l'AIC\n            \n            # Test de la blancheur des r√©sidus\n            ljung_box_result = acorr_ljungbox(out.resid, lags=[1], return_df=True)\n            p_value = ljung_box_result['lb_pvalue'].iloc[0]\n\n            if p_value &gt; 0.05:\n                bb_test.loc[f\"p={p}\", f\"q={q}\"] = 1\n            \n            # Mise √† jour du meilleur mod√®le\n            if out.aic &lt; best_aic :\n                best_aic = out.aic\n                best_order = (p, 0, q)\n                \n        except Exception as e:\n            print(f\"Erreur avec (p={p}, q={q}): {e}\")\n\nprint(f\"Meilleur mod√®le ARIMA: {best_order} avec AIC={best_aic}\")\n\nprint(\"=\"*30)\nprint(\"Matrice des AIC:\")\nprint(aic_matrix)\nprint(\"=\"*30)\nprint(\"Matrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\")\nprint(bb_test)\n\nMeilleur mod√®le ARIMA: (0, 0, 0) avec AIC=-20100.176479566246\n==============================\nMatrice des AIC:\n              q=0           q=1           q=2\np=0 -20100.176480 -20098.205891 -20097.679059\np=1 -20098.227385 -20099.862840 -20097.046957\np=2 -20097.887027 -20098.545030 -20094.033191\n==============================\nMatrice des test de Lljung box (1 lorsque r√©sidus non autocorr√©l√©s):\n     q=0  q=1  q=2\np=0    1    1    1\np=1    1    1    1\np=2    1    1    1\n\n\n\np = 1\nq = 0\n\nAR1 = ARIMA(data_unindex, order=(p, 0, q))\nprint(AR1.fit().summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nEn utilisant la m√©thode √©nonc√©e plus haut, nous constatons que le mod√®le ARMA(0,0) est le meilleur mod√®le. En effet, c‚Äôest le mod√®le avec le crit√®re d‚ÄôAkaike le plus faible. Cela porte √† croire que la tendance moyenne de la s√©rie est constante. Nous allons tout de m√™me utiliser un mod√®le AR(1) pour la mod√©liser. En effet, c‚Äôest le deuxi√®me mod√®le avec un AIC faible.\nDans la s√©rie des r√©sidus, nous constatons des clusters de volatilit√© ce qui est signe d‚Äôune volatilit√© conditionnelle, et donc de la pr√©sence d‚Äôun GARCH. De plus, dans la s√©rie des r√©sidus du log-rendement, nous constatons une faible autocorr√©lation, ce qui les fait ressembler √† du bruit blanc. Toutefois, lorsque l‚Äôon examine ces r√©sidus au carr√©, la s√©rie temporelle pr√©sente g√©n√©ralement une forte autocorr√©lation, mise en √©vidence par la pr√©sence de grappes de volatilit√©. Cela sugg√®re que les rendements repr√©sentent un processus h√©t√©rosc√©dastique, ce qui rend le mod√®le GARCH particuli√®rement pertinent dans le cadre de notre √©tude.\n\nAR1_resid = AR1.fit().resid\nplt.figure(figsize=(10, 5))\nplt.plot(AR1_resid)\nplt.title(\"R√©sidus du mod√®le AR(1)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(AR1_resid, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1)\")\nplt.subplot(222)\nplot_acf(AR1_resid**2, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus AR(1) au carr√©\")\nplt.show()\n\n\n\n\n\n\n\n\nMotiv√©s par les commentaires de (Franke, H√§rdle et Hafner 2004) sugg√©rant que, dans les applications pratiques, les mod√®les GARCH avec des ordres plus petits d√©crivent souvent suffisamment les donn√©es et que dans la plupart des cas GARCH(1,1) est ad√©quat, nous avons consid√©r√© quatre combinaisons diff√©rentes de p=0, 1 et q=1, 2 pour chaque p√©riode afin d‚Äôentra√Æner le mod√®le GARCH, en supposant que les r√©sidus standardis√©s suivent une distribution normale.\n\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\n\ndef find_garch(p_min, p_max, q_min, q_max, data, dist=\"normal\"):\n    \"\"\"\n    Trouve le meilleur mod√®le GARCH(p, q) en minimisant l'AIC.\n\n    Param√®tres :\n    - p_min, p_max : Bornes pour p (ordre de l'AR dans la variance)\n    - q_min, q_max : Bornes pour q (ordre de MA dans la variance)\n    - data : S√©rie temporelle utilis√©e pour l'estimation\n    - dist : Distribution des erreurs (\"normal\", \"t\", \"ged\", etc.)\n\n    Retour :\n    - DataFrame contenant les valeurs de AIC pour chaque combinaison (p, q)\n    - Meilleur mod√®le GARCH trouv√© en fonction du crit√®re AIC\n    \"\"\"\n    \n    best_aic = np.inf\n    best_order = (0, 0, 0)\n    \n    results = []\n\n    for p in range(p_min, p_max + 1):\n        for q in range(q_min, q_max + 1):\n            try:\n                # Sp√©cification du mod√®le GARCH(p, q)\n                garch_spec = arch_model(data, vol='Garch', p=p, q=q, mean='Zero', dist=dist)\n                out = garch_spec.fit(disp=\"off\")\n                \n                # Calcul de l'AIC\n                current_aic = out.aic * len(data)\n\n                # Mettre √† jour le meilleur mod√®le si un plus petit AIC est trouv√©\n                if current_aic &lt; best_aic:\n                    best_aic = current_aic\n                    best_order = (p, 0, q)\n                \n                # Ajouter les r√©sultats dans la liste\n                results.append({'p': p, 'q': q, 'aic': current_aic, 'relative_gap': np.nan})\n            \n            except Exception as e:\n                print(f\"Erreur pour (p={p}, q={q}): {e}\")\n                continue\n    \n    # Convertir en DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Calculer l'√©cart relatif par rapport au meilleur AIC\n    results_df['relative_gap'] = (results_df['aic'] - best_aic) * 100 / best_aic\n    \n    return results_df, best_order\n\nresults_df, best_garch_order = find_garch(p_min=1, p_max=2, q_min=0, q_max=2, data=data_unindex, dist=\"normal\")\n\nprint(f\"Meilleur mod√®le GARCH: {best_garch_order} avec AIC={best_aic}\")\nprint(\"=\"*30)\nprint(\"R√©sultats pour les mod√®les test√©s:\")\nresults_df.sort_values(by='relative_gap', ascending=False)\n\nMeilleur mod√®le GARCH: (1, 0, 1) avec AIC=-20100.176479566246\n==============================\nR√©sultats pour les mod√®les test√©s:\n\n\n\n\n\n\n\n\n\np\nq\naic\nrelative_gap\n\n\n\n\n1\n1\n1\n-7.493455e+07\n-0.000000\n\n\n4\n2\n1\n-7.491663e+07\n-0.023918\n\n\n2\n1\n2\n-7.486519e+07\n-0.092567\n\n\n5\n2\n2\n-7.483014e+07\n-0.139333\n\n\n3\n2\n0\n-7.189514e+07\n-4.056091\n\n\n0\n1\n0\n-7.073946e+07\n-5.598342\n\n\n\n\n\n\n\nEn utilisant le crit√®re AIC pour s√©lectionner le meilleur mod√®le, nous avons conclu que GARCH(1,1) est effectivement le meilleur mod√®le.\n\ngarch11 = arch_model(data_unindex, vol='Garch', p=1, q=1, mean='Zero', dist='normal')\nprint(\"=\"*78)\nprint(\"R√©sum√© du mod√®le GARCH(1,1)\")\nprint(\"=\"*78)\nprint(garch11.fit(disp=\"off\").summary())\n\n==============================================================================\nR√©sum√© du mod√®le GARCH(1,1)\n==============================================================================\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\n\n\n\ncond_resid =garch11.fit(disp=\"off\").conditional_volatility # Volatilit√© conditionnelle =&gt; sigma_t\nresid = garch11.fit(disp=\"off\").resid # r√©sidus du mod√®le =&gt; eps_t\nresid_std = garch11.fit(disp=\"off\").std_resid  # r√©sidus studentis√©s =&gt; eta_t\n\n# jarque bera test\n\nfrom scipy.stats import jarque_bera\n\njb_test = jarque_bera(resid_std)\nprint(\"H0: Les r√©sidus studentis√©s suivent une loi normale\")\nprint(f\"Test de Jarque-Bera sur les r√©sidus studentis√©s: JB={jb_test[0]}, p-value={jb_test[1]}\")\n# reject the null hypothesis of normality for the distribution of the residuals, \n# as a rule of thumb, which implies that the data to be fitted is not\n# normally distributed\n\nH0: Les r√©sidus studentis√©s suivent une loi normale\nTest de Jarque-Bera sur les r√©sidus studentis√©s: JB=848.8557767883675, p-value=4.71313744144075e-185\n\n\n\n### y revenir\n\n### coeff &lt;1\n\n\n# Test d'homosc√©dasticit√©\n# Ljung-Box test sur r√©sidus\nlb_test_resid = acorr_ljungbox(resid_std, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur r√©sidus:\\n\", lb_test_resid)\n\n# Ljung-Box test sur carr√©s des r√©sidus\nlb_test_resid_sq = acorr_ljungbox(resid_std**2, lags=[i for i in range(1, 13)], return_df=True)\nprint(\"Ljung-Box Test sur carr√©s des r√©sidus:\\n\", lb_test_resid_sq)\n\nLjung-Box Test sur r√©sidus:\n      lb_stat  lb_pvalue\n1   0.028087   0.866904\n2   0.572026   0.751253\n3   0.690100   0.875530\n4   1.235029   0.872298\n5   2.199461   0.820914\n6   2.491801   0.869384\n7   2.828137   0.900433\n8   2.941010   0.938005\n9   3.793237   0.924486\n10  4.644433   0.913631\n11  4.727144   0.943657\n12  6.763448   0.872842\nLjung-Box Test sur carr√©s des r√©sidus:\n       lb_stat  lb_pvalue\n1    0.280711   0.596235\n2    0.339634   0.843819\n3    6.670837   0.083163\n4    7.395445   0.116409\n5    8.091586   0.151260\n6    8.233789   0.221471\n7    8.724987   0.273009\n8    9.386238   0.310768\n9    9.938908   0.355454\n10  11.579309   0.314198\n11  13.394501   0.268325\n12  13.845698   0.310670\n\n\n\n# LM test pour les effets ARCH\nfrom statsmodels.stats.diagnostic import het_arch\n\nlm_test = het_arch(resid_std)\nprint('LM Test Statistique: %.3f, p-value: %.3f' % (lm_test[0], lm_test[1]))\n\nLM Test Statistique: 12.218, p-value: 0.271\n\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(221)\nplot_acf(resid_std, lags=40, ax=plt.gca())\nplt.title(\"ACF des r√©sidus studentis√©s\")\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\nplt.subplot(222)\nplot_pacf(resid_std, lags=40, ax=plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\nLe mod√®le AR(1)-GARCH(1,1) estim√© est le suivant :\n\\[\nr_t = \\mu_t + \\epsilon_t\n\\]\no√π \\(\\mu_t = 0.0001 - 0.0037 r_{t-1}\\)\n\\[\n\\epsilon_t = \\sigma_t \\eta_t\n\\]\n\\[\n\\sigma_t^2 = 3.89 \\times 10^{-6} + 0.10 \\times \\epsilon_{t-i}^2 + 0.88 \\times \\sigma_{t-i}^2\n\\]\navec \\(\\eta_t\\) un bruit blanc suppos√©e gaussien.\nDans ce cas, nous rencontrons des probl√®mes au niveau de la significativit√© du coefficient AR(1). En effet, il aurait √©t√© plus judicieux de ne pas mod√©liser la tendance moyenne du rendement et la supposer constante. De plus, au niveau du GARCH(1,1), les r√©sidus sont bien des bruits blancs homosc√©dastiques (test de lljung box et test LM). Cependant, nous avons suppos√© que \\(\\eta_t\\) est un bruit blanc gaussien. Cela n‚Äôest pas v√©rifi√©. Il aurait √©t√© judicieux de tester d‚Äôautres distributions telles que Students‚Äôs t (‚Äôt‚Äô, ‚Äòstudentst‚Äô), Skewed Student‚Äôs t (‚Äòskewstudent‚Äô, ‚Äòskewt‚Äô) ou encore Generalized Error Distribution (GED).\n**Test de Lagrange Multiplier (LM) pour l'effet ARCH**\n\nLe test de Lagrange Multiplier (LM) pour l'effet ARCH est un outil statistique qui v√©rifie la pr√©sence d'effets ARCH (AutoRegressive Conditional Heteroskedasticity) dans une s√©rie temporelle.\n\nL'effet ARCH se manifeste lorsque la variance d'une erreur est une fonction de ses erreurs pass√©es. Cette propri√©t√© est courante dans les s√©ries temporelles financi√®res, o√π de grandes variations des rendements sont souvent suivies par de grandes variations et vice versa.\n\nLe test de LM v√©rifie l'hypoth√®se nulle que les erreurs sont homosc√©dastiques (variance constante). Si la p-value du test est inf√©rieure √† un seuil pr√©d√©fini (g√©n√©ralement 0,05), l'hypoth√®se nulle est rejet√©e, indiquant la pr√©sence d'effets ARCH.\n\n# Cr√©ation de la figure avec des sous-graphiques align√©s verticalement\nplt.figure(figsize=(10, 12))\n\n# Premier graphique : CAC 40\nplt.subplot(311)\nplt.plot(resid) \nplt.title(\"R√©sidus du mod√®le AR(1)\")\n\n# Deuxi√®me graphique : R√©sidus du mod√®le AR(1)\nplt.subplot(312)\nplt.plot(cond_resid)\nplt.title(\"Volatile conditionnelle du mod√®le GARCH(1,1)\")\n\n# Troisi√®me graphique : R√©sidus studentis√©s du mod√®le GARCH(1,1)\nplt.subplot(313)\nplt.plot(resid_std, label='R√©sidus studentis√©s du mod√®le GARCH(1,1)')\nplt.title(\"R√©sidus studentis√©s du mod√®le GARCH(1,1)\")\n\n# Affichage des graphiques\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nI.2. Dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\)\n\\[\nr_t = \\mu_t + \\sigma_t \\times \\epsilon_t\n\\quad\n\\text{avec} \\quad\n\\begin{cases}\n    \\mu_t = \\mu + \\varphi r_{t-1} \\\\\n    \\sigma_t^2 = \\omega + a (r_{t-1} - \\mu_{t-1})^2 + b \\sigma_{t-1}^2\n\\end{cases}\n\\]\nPour avoir la dynamique historique de \\(\\mu_t\\) et \\(\\sigma_t\\), nous allons utiliser les donn√©es historiques de la s√©rie temporelle ainsi que les estimations des param√®tres \\(\\Theta = (\\mu, \\varphi, \\omega, a, b)\\) du mod√®le AR(1)-GARCH(1,1) que nous avons estim√© pr√©c√©demment par maximum de vraisemblance.\nPour \\(t=1\\), nous allons initialiser \\(\\mu_1\\) par la moyenne \\(\\hat{\\mu}\\) et \\(\\sigma_1\\) par la variance √† long terme \\(\\frac{\\omega}{1 - a - b}\\).\n\nprint(AR1.fit().summary())\n\n# tester arima avec arch_model ou arch\nmu = AR1.fit().params[0]\nprint(f\"Param√®tre mu: {mu}\")\nphi = AR1.fit().params[1]\nprint(f\"Param√®tre phi: {phi}\")\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             log_return   No. Observations:                 3523\nModel:                 ARIMA(1, 0, 0)   Log Likelihood               10052.114\nDate:                Fri, 28 Feb 2025   AIC                         -20098.227\nTime:                        22:15:48   BIC                         -20079.726\nSample:                             0   HQIC                        -20091.627\n                               - 3523                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0001      0.000      0.621      0.535      -0.000       0.001\nar.L1         -0.0037      0.012     -0.321      0.748      -0.027       0.019\nsigma2         0.0002   2.16e-06     89.947      0.000       0.000       0.000\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):              7957.49\nProb(Q):                              1.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.61   Skew:                            -0.30\nProb(H) (two-sided):                  0.00   Kurtosis:                        10.34\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\nParam√®tre mu: 0.00014959052741773347\nParam√®tre phi: -0.003743634042716415\n\n\n\nprint(garch11.fit(disp=\"off\").summary())\nomega = garch11.fit(disp=\"off\").params[0]\nprint(f\"Param√®tre omega: {omega}\")\na = garch11.fit(disp=\"off\").params[1]\nprint(f\"Param√®tre alpha: {a}\")\nb = garch11.fit(disp=\"off\").params[2]\nprint(f\"Param√®tre beta: {b}\")\n\n                       Zero Mean - GARCH Model Results                        \n==============================================================================\nDep. Variable:             log_return   R-squared:                       0.000\nMean Model:                 Zero Mean   Adj. R-squared:                  0.000\nVol Model:                      GARCH   Log-Likelihood:                10638.0\nDistribution:                  Normal   AIC:                          -21270.1\nMethod:            Maximum Likelihood   BIC:                          -21251.6\n                                        No. Observations:                 3523\nDate:                Fri, Feb 28 2025   Df Residuals:                     3523\nTime:                        22:15:48   Df Model:                            0\n                              Volatility Model                              \n============================================================================\n                 coef    std err          t      P&gt;|t|      95.0% Conf. Int.\n----------------------------------------------------------------------------\nomega      3.8930e-06  4.514e-10   8624.704      0.000 [3.892e-06,3.894e-06]\nalpha[1]       0.1000  5.098e-03     19.615  1.161e-85   [9.001e-02,  0.110]\nbeta[1]        0.8800  7.520e-03    117.017      0.000     [  0.865,  0.895]\n============================================================================\n\nCovariance estimator: robust\nParam√®tre omega: 3.892997741815931e-06\nParam√®tre alpha: 0.1\nParam√®tre beta: 0.88\n\n\n\nT_train = len(data_train)\nT_test = len(data_test)\n\nT = T_train + T_test\n\n# Initialisation des s√©ries\nr = pd.concat([data_train, data_test], axis=0)\nmu_t = np.zeros(T)    # Composante moyenne\nsigma2 = np.zeros(T)  # Variance conditionnelle\n\n# Conditions initiales\nmu_t[0] = mu\nsigma2[0] = omega / (1 - a - b)  # Variance de long terme\n\n# Simulation du mod√®le\nfor t in range(1, T):\n    mu_t[t] = mu + phi * r[t-1]  # Partie moyenne\n    sigma2[t] = omega + a * (r[t-1] - mu_t[t-1])**2 + b * sigma2[t-1]  # Variance conditionnelle\n\n# Affichage des r√©sultats\nfig, ax = plt.subplots(3, 1, figsize=(10, 12))\n\nax[0].plot(r, color=\"blue\")\nax[0].set_title(\"Rendements $r_t$\")\nax[0].legend()\n\nax[1].plot(mu_t, color=\"green\")\nax[1].set_title(\"Composante moyenne $\\mu_t$\")\nax[1].legend()\n\nax[2].plot(np.sqrt(sigma2), color=\"red\")\nax[2].set_title(\"Volatilit√© conditionnelle $\\sigma_t$\")\nax[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEn analysant la dynamique de \\(\\mu_t\\), nous constatons que la tendance moyenne est tr√®s semblable √† la s√©rie des log-rendements. Cela est d√ª au fait que le mod√®le AR(1) n‚Äôest pas pertinent pour mod√©liser la s√©rie. En effet, la s√©rie des log-rendements ressemble d√©j√† √† un bruit blanc. Par ailleurs, nous observons de fortes p√©riodes de volatilit√© dans la s√©rie des log-rendements pendant les p√©riodes de crises, i.e.¬†2008-2009 qui correspond √† la crise des subprimes et 2020 qui correspond √† la crise du Covid-19. Le mod√®le GARCH semble bien capturer ces p√©riodes de volatilit√© dans la volatilit√© conditionnelle calibr√©e.\n\n\nI.3. Estimation de la VaR\n\n# VaR historique\n\ndef historical_var(data, alpha=0.99):\n    \"\"\"\n    Calcul de la VaR historique\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    return -np.percentile(data, 100*(1 - alpha))\n\n# VaR gaussienne\n\ndef gaussian_var(data, alpha):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n    from scipy.stats import norm\n\n    mu = np.mean(data)\n    sigma = np.std(data)\n    return -(mu + sigma * norm.ppf(1 - alpha))\n\n# Loi de Skew Student par maximum de vraisemblance.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\n\ndef skew_student_pdf(x, mu, sigma, gamma, nu ):\n    \"\"\"\n    Compute the Skew Student-t probability density function (PDF).\n    \"\"\"\n    \n\n    t_x = ((x - mu) * gamma / sigma) * np.sqrt((nu + 1) / (nu + ((x - mu) / sigma) ** 2))\n    # PDF of the standard Student-t distribution\n    pdf_t = t.pdf(x , df=nu,  loc=mu, scale=sigma)\n    # CDF of the transformed Student-t distribution\n    cdf_t = t.cdf(t_x, df=nu + 1,loc=0, scale=1)\n\n    # Skew Student density function\n    density = 2 * pdf_t * cdf_t\n\n    return density\n\n\ndef skew_student_log_likelihood(params, data):\n    \"\"\"\n    Calcul de la log-vraisemblance de la loi de Skew Student\n    params [mu, sigma, gamma, nu]: les param√®tres de la loi\n    data : les rendements logarithmiques\n    \"\"\"\n    mu, sigma, gamma, nu = params\n    density = skew_student_pdf(data , mu, sigma, gamma, nu)\n    # log-vraisemblance\n    loglik = np.sum(np.log(density))\n    \n    return - loglik\n\n# Optimisation des param√®tres avec contraintes de positivit√© sur sigma et nu\ndef skew_student_fit(data):\n    \"\"\"\n    Estimation des param√®tres de la loi de Skew Student\n    \"\"\"\n    # initial guess\n    x0 = np.array([np.mean(data), np.std(data), 1, 4])\n\n    # contraintes\n    bounds = [(None, None), (0, None), (None, None), (None, None)]\n\n    # optimisation\n    res = minimize(skew_student_log_likelihood, x0, args=(data), bounds=bounds)\n\n    return res.x\n\nparams = skew_student_fit(resid_std)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de Skew Student sont : \")\nprint(\"-\"*15)\nprint(\"Mu : \", params[0])\nprint(\"Sigma : \", params[1])\nprint(\"Gamma : \", params[2])\nprint(\"Nu : \", params[3])\nprint(\"=\"*80)\n\n\nparams_sstd = {\n    \"mu\" : params[0], \n    \"sigma\" : params[1],\n    \"gamma\" : params[2],\n    \"nu\" : params[3]\n}\n\n\n## Int√©gration de la fonction de densit√©\nfrom scipy import integrate\nfrom scipy.optimize import minimize_scalar\n\ndef integrale_SkewStudent(x,params):\n    borne_inf = -np.inf\n    resultat_integration, erreur = integrate.quad(lambda x: skew_student_pdf(x, **params), borne_inf, x)\n    return resultat_integration\n\ndef fonc_minimize(x, alpha,params):\n    value = integrale_SkewStudent(x,params)-alpha\n    return abs(value)\n\ndef skew_student_quantile(alpha,mu, sigma, gamma, nu ):\n    params = {\n    \"mu\" : mu ,\n    \"sigma\" : sigma,\n    \"gamma\" : gamma,\n    \"nu\" : nu\n    }\n\n    if alpha &lt;0 or alpha &gt;1:\n        raise Exception(\"Veuillez entrer un niveau alpha entre 0 et 1\")\n    else:\n        resultat_minimisation = minimize_scalar(lambda x: fonc_minimize(x, alpha,params))\n        return resultat_minimisation.x\n    \n# Objectif : √©crire une fonction qui calcule la VaR skew-student\n\ndef sstd_var(alpha, params):\n    \"\"\"\n    Calcul de la VaR skew student\n    data : les rendements logarithmiques\n    alpha : le niveau de confiance\n    \"\"\"\n\n    return -skew_student_quantile(1-alpha, **params)\n\n\n#### A FAIRE VAR POT et BM\n\nfrom scipy.stats import genextreme as gev\n\nimport numpy as np\nimport pandas as pd\nneg_resid = -resid_std\n\ndef get_extremes(returns, block_size, min_last_block=0.6):\n    \"\"\"\n    Extrait les valeurs extr√™mes d'une s√©rie de rendements par blocs.\n    \n    Arguments :\n    returns : pandas Series (index = dates, valeurs = rendements)\n    block_size : int, taille du bloc en nombre de jours\n    min_last_block : float, proportion minimale pour inclure le dernier bloc incomplet\n    \n    Retourne :\n    maxima_sample : liste des valeurs maximales par bloc\n    maxima_dates : liste des dates associ√©es aux valeurs maximales\n    \"\"\"\n    n = len(returns)\n    num_blocks = n // block_size\n\n    maxima_sample = []\n    maxima_dates = []\n\n    for i in range(num_blocks):\n        block_start = i * block_size\n        block_end = (i + 1) * block_size\n        block_data = returns.iloc[block_start:block_end]  # S√©lectionner le bloc avec les index\n\n        max_value = block_data.max()\n        max_date = block_data.idxmax()  # R√©cup√©rer l'index de la valeur max\n\n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n\n    # Gestion du dernier bloc s'il reste des donn√©es suffisantes\n    block_start = num_blocks * block_size\n    block_data = returns.iloc[block_start:]\n\n    if len(block_data) &gt;= min_last_block * block_size:\n        max_value = block_data.max()\n        max_date = block_data.idxmax()\n        \n        maxima_sample.append(max_value)\n        maxima_dates.append(max_date)\n        \n    return pd.Series(maxima_sample, index=maxima_dates)  # Retourner une Series avec les dates comme index\n\n\n\ndef BM_var(alpha,s,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    alpha_bm = 1-s*(1-alpha)\n\n    return gev.ppf(alpha_bm, shape, loc = loc, scale = scale),alpha_bm\n\nextremes = get_extremes(neg_resid, block_size=21, min_last_block=0.6)\nparams_gev = gev.fit(extremes)\nprint(\"=\"*80)\nprint(\"Les param√®tres estim√©s de la loi de GEV sont : \")\nprint(\"-\"*15)\nprint(f\"Shape (xi) = {params_gev[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gev[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gev[2]:.2f}\")\nprint(\"=\"*80)\n\n\ndef POT_var(data,alpha,u,shape,loc,scale):\n    \"\"\"\n    Calcul de la VaR gaussienne\n    alpha : le niveau de confiance de la VaR\n    s : le nombre de jours dans un bloc\n    \"\"\"\n    n = len(data)\n    excess_values = [value - u for value in data if value &gt;= u]\n    nu = len(excess_values)\n\n    alpha_pot = 1-n*(1-alpha)/nu\n\n    return genpareto.ppf(alpha_pot, shape, loc = loc, scale = scale) + u,alpha_pot\n\n\nu = 0.03\nexcess_values = [value - u for value in neg_resid if value &gt;= u]\n\nfrom scipy.stats import genpareto\n\nparams_gpd = genpareto.fit(excess_values)\n\n# Afficher les param√®tres estim√©s\nprint(\"=\"*80)\nprint(\"Param√®tres estim√©s de la distribution GPD:\")\nprint(f\"Shape (xi) = {params_gpd[0]:.2f}\")\nprint(f\"Localisation (mu) = {params_gpd[1]:.2f}\")\nprint(f\"Echelle (sigma) = {params_gpd[2]:.2f}\")\nprint(\"=\"*80)\n\n================================================================================\nLes param√®tres estim√©s de la loi de Skew Student sont : \n---------------\nMu :  0.42506987856855155\nSigma :  0.8686238872541445\nGamma :  -0.6074089740677895\nNu :  5.607559653340765\n================================================================================\n================================================================================\nLes param√®tres estim√©s de la loi de GEV sont : \n---------------\nShape (xi) = -0.01\nLocalisation (mu) = 1.64\nEchelle (sigma) = 0.72\n================================================================================\n================================================================================\nParam√®tres estim√©s de la distribution GPD:\nShape (xi) = -0.04\nLocalisation (mu) = 0.00\nEchelle (sigma) = 0.80\n================================================================================\n\n\n\nalpha = 0.99\n\nvar_hist_train = historical_var(resid_std, alpha=alpha)\nvar_gauss_train = gaussian_var(resid_std, alpha=alpha)\nvar_sstd_train = sstd_var(alpha, params_sstd)\nvar_BM_train,_ = BM_var(0.99, 21, *params_gev)\nvar_POT_train,_ = POT_var(neg_resid, alpha, u,*params_gpd)\n\n# in a df\nvar = pd.DataFrame({\n    'Historique': [var_hist_train],\n    'Gaussienne': [var_gauss_train],\n    'Skew Student': [var_sstd_train],\n    'Block Maxima': [var_BM_train],\n    'Peak Over Threshold': [var_POT_train]\n})\n\nprint(\"=\"*80)\nprint(\"Value at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\")\nprint(round(100*var,2))\nprint(\"=\"*80)\n\n================================================================================\nValue at Risk sur les r√©sidus studentis√©s (en %) pour h=1j\n   Historique  Gaussienne  Skew Student  Block Maxima  Peak Over Threshold\n0      264.12      229.76        280.29        268.68               284.98\n================================================================================\n\n\n\na. VaR historique dynamique\n\nvar_t = np.zeros(T_test)    # Composante moyenne\nnb_exp = 0\nfor t in range(T_test):\n    var_t[t] = - (mu_t[t+T_train] + np.sqrt(sigma2[t+T_train])*var_hist_train)\n    nb_exp += (r[t+T_train] &lt; var_t[t]).astype(int)\n    \nvar_t = pd.Series(var_t, index=data_test.index)\nprint(f\"Nombre d'exceptions = {nb_exp} sur {T_test} jours\")\n\nNombre d'exceptions = 4 sur 586 jours\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_train, color=\"blue\", label='Train')\nplt.plot(data_test, color=\"orange\", label='Test')\nplt.plot(var_t, color=\"red\",label='VaR dynamique')\nplt.axvline(x=data_test.index[0], color='black', linestyle='--')\nplt.legend()\nplt.title('S√©rie des log-rendements et VaR dynamique')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(data_test, color=\"orange\")\nplt.plot(var_t, color=\"red\")\nplt.title('Zoom sur la VaR dynamique')\n\nText(0.5, 1.0, 'Zoom sur la VaR dynamique')\n\n\n\n\n\n\n\n\n\n\n# backtest √† faire (optionnel)"
  },
  {
    "objectID": "autres/ISR.html",
    "href": "autres/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "autres/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L‚ÄôInvestissement Socialement Responsable (ISR) repr√©sente une approche d‚Äôinvestissement qui privil√©gie l‚Äôint√©gration de crit√®res extra-financiers, notamment les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance), dans les d√©cisions d‚Äôinvestissement, que ce soit pour un individu ou pour une entreprise.\nLes crit√®res ESG englobent des aspects tels que le respect de l‚Äôenvironnement (Environnementaux), le bien-√™tre des salari√©s (Sociaux) et la qualit√© de la gouvernance au sein des entreprises (Gouvernance). Ils servent √† √©valuer la durabilit√© et l‚Äô√©thique des investissements au-del√† des performances financi√®res traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l‚Äôimpact des entreprises sur la soci√©t√© et l‚Äôenvironnement, soulignant l‚Äôimportance croissante des enjeux √©cologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l‚Äôadoption de ces crit√®res est souvent li√©e √† la mise en ≈ìuvre de strat√©gies de Responsabilit√© Soci√©tale des Entreprises (RSE), illustrant un engagement vers une gestion d‚Äôentreprise plus responsable et transparente. Ainsi, l‚ÄôISR incarne non seulement une d√©marche d‚Äôinvestissement √©thique mais √©galement une vision √† long terme visant √† concilier performance √©conomique et impact social positif."
  },
  {
    "objectID": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "autres/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL‚ÄôISR pr√©sente de multiples avantages tant pour les investisseurs que pour les entreprises engag√©es dans cette d√©marche. Ces avantages refl√®tent l‚Äô√©volution des attentes soci√©tales et la reconnaissance croissante de l‚Äôimportance de la durabilit√© et de l‚Äô√©thique dans le monde des affaires. J‚Äôen ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement am√©liorer la r√©putation d‚Äôune entreprise. Elle t√©moigne de son engagement envers des pratiques durables et √©thiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financi√®re : De nombreuses √©tudes d√©montrent que les entreprises avec une notation ESG √©lev√©e tendent de meilleures performances financi√®rement sur le long terme. Cela s‚Äôexplique par une meilleure anticipation des risques, une gestion plus efficace et une capacit√© √† saisir les opportunit√©s de march√© li√©es √† la durabilit√©.\nMeilleure gestion des risques : L‚Äôadoption de pratiques ESG solides permet aux entreprises de mieux identifier et g√©rer les risques, qu‚Äôils soient climatiques, sociaux ou de march√©.\nMeilleure attractivit√© pour les investisseurs : En d√©montrant un engagement clair envers la durabilit√© et l‚Äô√©thique, les entreprises attirent davantage d‚Äôinvestisseurs conscients de l‚Äôimportance des crit√®res ESG. Cette attractivit√© accrue peut se traduire par un acc√®s facilit√© au capital et √† de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution √† la r√©duction de certains risques financiers : En investissant dans des entreprise int√©grant les crit√®res ESG dans leur processus de d√©cision, les investisseurs contribuent indirectement √† une meilleure identification et anticipation les risques li√©s au changement climatique, aux probl√©matiques sociales, et aux d√©fis de gouvernance, ce qui contribue √† une meilleure protection de leur capital sur le long terme.\nImpact positif sur la soci√©t√© : L‚ÄôISR permet aux investisseurs de contribuer activement √† une √©conomie plus durable et √©quitable. En choisissant d‚Äôinvestir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des mod√®les √©conomiques respectueux de l‚Äôenvironnement et du bien-√™tre social.\n\nEn somme, l‚ÄôISR offre une perspective d‚Äôinvestissement qui va au-del√† des retours financiers imm√©diats pour embrasser des b√©n√©fices √† long terme, tant sur le plan √©conomique que social et environnemental."
  },
  {
    "objectID": "autres/ISR.html#comment-investir",
    "href": "autres/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire √† un fonds ISR ?\nSouscrire √† un fonds1 ISR (Investissement Socialement Responsable) est une d√©marche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la soci√©t√© et l‚Äôenvironnement[@comment]. En consultant son conseiller financier ou son √©tablissement bancaire, il est possible de placer son argent dans une vari√©t√© de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d‚Äô√âpargne en Actions (PEA) : (sous r√©server de s‚Äôassurer que le fonds ISR choisi est bien √©ligible au PEA) Offre la possibilit√© de placer son √©pargne en actions de soci√©t√©s europ√©ennes.\nLes compte-titres ordinaires(CTO) : permet d‚Äôinvestir en bourse sur les march√©s financiers fran√ßais et/ou √©trangers et dans tout type de valeurs mobili√®res (OPC2, actions, obligations, mon√©taire, warrants, trackers‚Ä¶).\nL‚Äô√©pargne salariale ou les plans d‚Äô√©pargne d‚Äôentreprise (PEE) : un produit d‚Äô√©pargne collectif qui permet aux salari√©s d‚Äôune entreprise de se constituer un portefeuille de valeurs mobili√®res qui peuvent proposer des fonds ISR.\nEnfin, certains produits d‚Äô√©pargne retraite individuelle, comme le Plan d‚ÄôEpargne Retraite (PER).\n\nCes v√©hicules d‚Äôinvestissement permettent aux particuliers de contribuer √† une √©conomie plus durable tout en recherchant une performance financi√®re. Il est recommand√© de se rapprocher d‚Äôun conseiller pour d√©terminer le produit le mieux adapt√© √† ses objectifs financiers et √† ses valeurs √©thiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) n√©cessite une approche combinant analyses personnelle, financi√®re et extra-financi√®re, cette derni√®re se concentrant sur les crit√®res ESG (Environnementaux, Sociaux et de Gouvernance)[@comment2021].\nPour choisir efficacement une entreprise ISR, il est crucial de r√©aliser une analyse √† triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR¬†qui vous convient le mieux. Elles vous aideront √† identifier le type de placement √† privil√©gier et d√©finir par exemple des fonds th√©matiques, d‚Äôexclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financi√®re : Elle permet d‚Äô√©valuer la performance √©conomique de l‚Äôentreprise, sa sant√© financi√®re, sa capacit√© √† g√©n√©rer des profits et √† maintenir une croissance durable. Cette analyse est indispensable pour s‚Äôassurer que l‚Äôentreprise est non seulement responsable, mais aussi viable et performante √† long terme.\nAnalyse extra-financi√®re (ESG) : Cette analyse compl√®te l‚Äô√©valuation financi√®re en examinant comment l‚Äôentreprise aborde les d√©fis et saisit les opportunit√©s li√©es aux crit√®res environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds int√©grant les crit√®res ESG comme les fonds labellis√©s ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du minist√®re de l‚Äô√©conomie et des finances fran√ßais qui garantit que le fonds d‚Äôinvestissement respecte des crit√®res ESG stricts4 dans ses choix d‚Äôinvestissement. Il assure √©galement que le fonds investit dans des entreprises qui adh√®rent √† ces principes, offrant ainsi une couche suppl√©mentaire de confiance pour les investisseurs soucieux de l‚Äôimpact de leurs placements.\nCe label est attribu√© aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,class√©es en 6 cat√©gories, qui constituent les 6 piliers du r√©f√©rentiel [@crit√®resa]."
  },
  {
    "objectID": "autres/ISR.html#footnotes",
    "href": "autres/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appel√© fonds de placement. Il s‚Äôagit d‚Äôune soci√©t√© d‚Äôordre public ou priv√© qui investit du capital pour soutenir des projets souvent innovants.‚Ü©Ô∏é\norganismes de placement collectif‚Ü©Ô∏é\nLes fonds d‚Äôinvestissement d‚Äôexclusion normatifs font r√©f√©rence aux fonds faisant l‚Äôobjet de plusieurs controverses.‚Ü©Ô∏é\ncf liste des fonds labellis√©s [@listede].‚Ü©Ô∏é"
  },
  {
    "objectID": "autres/spearman-pearson.html",
    "href": "autres/spearman-pearson.html",
    "title": "Corr√©lation de Spearman vs corr√©lation de Pearson",
    "section": "",
    "text": "Corr√©lation de Spearman vs corr√©lation de Pearson\nLa corr√©lation de Spearman est une mesure de corr√©lation non param√©trique qui permet de mesurer la relation monotone entre deux variables. Elle est souvent utilis√©e pour mesurer la d√©pendance entre les variables al√©atoires. La corr√©lation de Spearman est bas√©e sur les rangs des observations et est moins sensible aux valeurs extr√™mes que la corr√©lation de Pearson. Elle est donc plus robuste et plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires.\nLa corr√©lation de Pearson, quant √† elle, ne permet de comparer les d√©pendances lin√©aires des variables. De plus, elle ne permet de comparer les corr√©lation que lorsque les variables al√©atoires sont normales. En effet, soit (\\(X_1,X_2,X_3\\)), si \\(\\rho(X_1,X_2) &gt; \\rho(X_1,X_3)\\), cela ne veut dire que la corr√©lation entre \\(X_1\\) et \\(X_2\\) est plus forte que celle entre \\(X_1\\) et \\(X_3\\) que si ces variables sont gaussiennes.\nNous verrons dans la suite de ce document que la corr√©lation de Spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires, car elle est une mesure de concordance. De ce fait, elle est d√©finit par une copule. Il n‚Äôen est pas ainsi pour la corr√©lation de Pearson.\n\nQu‚Äôest ce qu‚Äôune mesure de concordance ?\nUne mesure de concordance est une mesure qui permet de quantifier la relation entre deux variables al√©atoires. Cinq propri√©t√©s sont g√©n√©ralement attribu√©es √† une mesure de concordance :\n\nSym√©trie : la mesure de concordance entre X et Y est la m√™me que celle entre Y et X.\nNormalisation : la mesure de concordance est comprise entre -1 et 1.\n\\(\\delta(X_1, X_2) = 1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(U) \\right)\\)\n\\(\\delta(X_1, X_2) = -1 \\Leftrightarrow (X_1,X_2) \\overset{\\mathcal{L}}{=}  \\left( F_1^{-1}(U),F_2^{-1}(1-U) \\right)\\)\n\\(\\delta(f(X_1), X_2) = \\delta(X_1, X_2)\\) si \\(f\\) est croissante; \\(\\delta(f(X_1), X_2) = -\\delta(X_1, X_2)\\) si \\(f\\) est d√©croissante.\n\nEn raison des propri√©t√©s 3 et 4, la corr√©lation de pearson n‚Äôest pas une mesure de concordance lorsque les variables al√©atoires ne sont pas gaussiennes. En effet, la corr√©lation de pearson ne v√©rifie pas la propri√©t√© 3. C‚Äôest pourquoi, la corr√©lation de spearman est plus adapt√©e pour mesurer la d√©pendance entre les variables al√©atoires. Une mesure encore plus appropri√©e est la copule. La corr√©lation de spearman, quant √† elle, est une mesure de concordance.\n\n\nExemple :\nSoit un vecteur gaussien X = (\\(X_1, X_2, X_3\\)) suivant une loi N(0,\\(\\Sigma\\)) avec :\n\\[\n\\Sigma = \\begin{pmatrix}\n1 & 0.4 & 0.2 \\\\\n0.4 & 1 & -0.8 \\\\\n0.2 & -0.8 & 1\n\\end{pmatrix}\n\\]\n\nm &lt;- 3\nn &lt;- 2000\nsigma &lt;- matrix(c(1,0.4,0.2,0.4,1,-0.8,0.2,-0.8,1), nrow=3, ncol=3)\nX &lt;- mvrnorm(n,mu=rep(0,m),Sigma=sigma)\npairs.panels(X,method=\"spearman\")\n\n\n\n\n\n\n\n\nOn cr√©e ensuite un vecteur Z constitu√© des fonctions de r√©partition des √©l√©ments de X. On constate que la corr√©lation de Spearman entre les √©l√©ments de Z ne change pas. Cel√† s‚Äôexplique par le fait que nous appliquons une fonction croissante √† chaque √©l√©ment de X. Cependant, la distribution de Z est diff√©rente de celle de X. En effet, les composantes de Z suivent une loi uniforme \\(\\mathcal{U}(0,1)\\).\n\nZ &lt;- pnorm(X, 0,1)\npar(mfrow=c(1,2))\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"pearson\")\n\n\n\n\n\n\n\n\nA partir de Z, nous construisons un vecteur W tel que les composantes de W suivent des lois marginales diff√©rentes, resp \\(\\beta(0,1), \\gamma(2,1), \\beta(2,1)\\). La corr√©lation de Spearman entre les √©l√©ments de W ne change pas bien que la distribution des marginales ait chang√©, puisque nous appliquons une fonction croissante √† chaque √©l√©ment de Z.\n\nw1 &lt;- qbeta(Z[,1], 2, 1)\nw2 &lt;- qgamma(Z[,2], 2, 1)\nw3 &lt;- qbeta(Z[,3], 2,1)\nW &lt;- cbind(w1,w2,w3)\n\npar(mfrow=c(1,2))\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"pearson\")\n\n\n\n\n\n\n\n\nOn en conclut que la structure de d√©pendance d‚Äôun vecteur de variables al√©atoire peut √™tre isol√©e, caract√©ris√©ee, et mod√©lis√©e ind√©pendamment des lois marginales/distributions univari√©es des composantes du vecteur al√©atoire. Le concept de copule permet de mod√©liser cette structure de d√©pendance.\nSi l‚Äôon utilise la correlation de Pearson, on constate que la corr√©lation entre les √©l√©ments de W et Z change. En effet, la corr√©lation de Pearson est une mesure de corr√©lation lin√©aire et ne permet pas de comparer, xdans tous les cas, les d√©pendances entre les variables al√©atoires.\n\npairs.panels(X,method=\"pearson\")\n\n\n\n\n\n\n\npairs.panels(Z,method=\"spearman\")\n\n\n\n\n\n\n\npairs.panels(W,method=\"spearman\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Note\n\n\n\nCompte tenu de ma sp√©cialisation en gestion des risques, les contenus que je pr√©vois de partager dans cette section d√©di√©e √† la 3√®me ann√©e (3A) porteront principalement sur les enseignements sp√©cifiques √† cette sp√©cialisation. Je m‚Äôefforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en esp√©rant qu‚Äôils serviront de guide pr√©cieux pour ceux qui suivront une voie similaire.\n\n\n\n\n\n\nConstruction du bilan d‚Äôentreprise\nReglementation prudentielle TO REWRITE\n\n\n\n\n\n\n\nD√©finition du risque financier\nValue-at-risk (VaR) :\n\nD√©finition de la VaR\nImpl√©mentation de la VaR sur python\nTP1 : M√©thodes traditionnelles de calcul de la VaR et Expected Shortfall (ES)\nTP2 : M√©thodes bas√©es sur la th√©orie des valeurs extr√™mes\nTP3 : M√©thodes d‚Äôune calcul d‚Äôune VaR dynamique (bas√© sur GARCH)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le mod√®le de black-scholes\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre de Kalman\nCalibration du mod√®le √† volatilit√© stochastique de Taylor - Filtre particulaire bootstrap\nMod√®le de Heston :\n\nEstimation de la volatilit√© avec le filtre boostrap\nComparaison du filtre bootstrap au filtre particulaire auxiliaire (APF) TO DO\n\n\n\n\n\n\n\n\nGestion de risques d‚Äôun portefeuille d‚Äôactifs\nProfil d‚Äô√©coulement de portefeuille\nPricing d‚Äôoptions vanilles TO DO\nPricing de taux, swap, d‚Äôobligations TO DO\nTracking error TO DO\nConstruction de portefeuille markowitz TO DO\n\n\n\n\n\n\n\nInvestissement socialement responsable\nFinance durable\nCorr√©lation de spearman vs Corr√©lation de pearson\n\n\n\n\n\n\nProjets\n\nProjet de s√©ries temporelles\nProjet de scoring\nProjet de th√©orie de valeurs extr√™mes 1\nProjet de th√©orie de valeurs extr√™mes 2\n\nApplications d√©ploy√©es\n\nStock price prediction : CAC40\nAsset pricing and management"
  },
  {
    "objectID": "index_gdr.html#projets-et-applications-d√©ploy√©es",
    "href": "index_gdr.html#projets-et-applications-d√©ploy√©es",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Projets\n\nProjet de s√©ries temporelles\nProjet de scoring\nProjet de th√©orie de valeurs extr√™mes 1\nProjet de th√©orie de valeurs extr√™mes 2\n\nApplications d√©ploy√©es\n\nStock price prediction : CAC40\nAsset pricing and management"
  }
]