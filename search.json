[
  {
    "objectID": "index_stat.html",
    "href": "index_stat.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Théorie des valeurs extrêmes\n\n\nRésumé de la théorie des valeurs extrêmes\n\n\n\n\nMicroéconométrie appliquée\n\n\nRésumé des méthodes de microéconométrie appliquée enseignées à la promo 2024-2025 ENSAI\n\n\n\n\nApprentissage supervisé\n\n\nRégression linéaire\nKernel Trick and Support Vector Machines (SVM)\nGradient boosting\nFeature selection\nRégression ridge et lasso\nCART & Random Forest\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Ridge regression vs. Lasso regression\n\n\n12 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Trick and SVM\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\n\n7 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nFeatures selection\n\n\n11 min\n\n\n\n\n\n\nCheryl KOUADIO\n\n\n\n\n\n\n\n\n\n\n\n\nInvestissement Socialement Responsable (ISR): De quoi parle-t-on ?\n\n\n6 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de résultat d’une entreprise\n\n\n27 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nGestion de risques de portefeuille\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nProfil d’écoulement/ de liquidation de portefeuille\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa réglementation prudentielle\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu’est ce que c’est ?\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n10 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n20 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa régression linéaire\n\n\n7 min\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "3A/var_def.html",
    "href": "3A/var_def.html",
    "title": "La VaR",
    "section": "",
    "text": "La mesure de risque réglementaire correspond à la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s’agit dans cette section de développer la notion de VaR pour des portefeuilles linéaires et non linéaires."
  },
  {
    "objectID": "3A/var_def.html#le-backtesting",
    "href": "3A/var_def.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contrôle de la qualité de la VaR pour un horizon de 1 jour. Il permet de vérifier si la VaR est bien calibrée. Pour cela, on compare la VaR calculée avec la perte réelle. Si la VaR est bien calibrée, la perte réelle ne doit pas dépasser la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d’exception pour valider le modèle. Notons PnL le profit and loss du portefeuille et VaR la valeur à risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsidérons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inférieur à l’opposé de la VaR avec probabilité \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une période ouvré comptant n jours, la probabilité d’avoir \\(i\\) exceptions est donnée par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilité d’avoir plus de \\(k\\) exceptions est donnée par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilité d’avoir au plus de \\(i\\) exceptions est donnée par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilité d’exception n’excède pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d’exceptions empirique est supérieur à celui attendu, le modèle est rejeté :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donnée par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d’exceptions observée lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donnée par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) où \\(t\\) est la valeur observée de la statistique de test et \\(\\phi\\) est la fonction de répartition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "3A/var_def.html#cas-général",
    "href": "3A/var_def.html#cas-général",
    "title": "La VaR",
    "section": "2.2 Cas général",
    "text": "2.2 Cas général\nDans cette approche, nous considérons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en déduisons donc que la VaR est calculé comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction décroissante de l’espérance de PnL et une fonction croissante de la volatilité du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de prévoir l’espérance du PnL futur.\n\n2.2.1 Exemple\nNous considérons une position courte de 1 million de dollars sur le contrat à terme S&P 500. Nous estimons que la volatilité annualisée \\(\\sigma_{\\text{SPX}}\\) est égale à 35%.\nLa perte du portefeuille est égale à \\(L(w) = N \\times R_{\\text{SPX}}\\) où \\(N\\) est le montant de l’exposition (−1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l’indice S&P 500. Nous déduisons que la volatilité de la perte annualisée est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur à risque pour une période de détention d’un an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l’investisseur sur un 1an s’élève à 815 500€ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre période de détention, nous utilisons la règle de la racine carré pour convertir la volatilité pour une fréquence donné \\(f_1\\) en une autre volatilité pour une autre fréquence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les résultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calculé sur 1 jour, pour l’avoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "3A/var_def.html#modèles-linéaires-de-facteurs",
    "href": "3A/var_def.html#modèles-linéaires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Modèles linéaires de facteurs",
    "text": "2.3 Modèles linéaires de facteurs\nNous considérons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est linéaire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La première idée est de choisir les facteurs comme étant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du marché et chaque actif possède son propre facteur de risque.\nLe problème est que les prix sont loin d’être stationnaires, ce qui nous amène à devoir affronter certains problèmes pour modéliser la distribution \\(F_t\\). Une autre idée est de récrire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] où \\(R_i(t;h)\\) est le retour de l’actif entre \\(t\\) et \\(t+h\\).\nNous déduisons que le PnL aléatoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\noù \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l’exposition nominale)dans l’actif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s’ensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous déduisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l’Équation (2.6), l’expression de la valeur à risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d’estimer la matrice de covariance des retours des actifs pour calculer la valeur à risque. Cela explique la popularité de ce modèle, surtout lorsque le P&L du portefeuille est une fonction linéaire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsidérons l’exemple des entreprises d’Apple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l’écart type estimé des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corrélation croisée est égale à 12,0787 %. Il s’ensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous déduisons que la valeur à risque quotidienne à 99% est de 41,21 $. Nous obtenons une figure inférieure à celle de la valeur à risque historique, qui était de 47,39 $. Nous expliquons ce résultat par le fait que la distribution gaussienne sous-estime la probabilité des événements extrêmes et n’est donc pas adaptée à des calculs précis de risque dans des situations de marché volatiles.\n\n\n2.3.2 Exemple de portefeuille linéaire d’actifs\nConsidérons un portefeuille linéaire composé de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne égaux à : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilités journalières sont égales à 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244€, 135€,315€. La matrice de corrélation est donnée par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journée est donc de 18.42€ avec un risque 1% de se tromper.\n\n2.3.2.1 Implémentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les écarts-types et les corrélations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "3A/var_def.html#modèles-factoriels-de-risque",
    "href": "3A/var_def.html#modèles-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Modèles factoriels de risque",
    "text": "2.4 Modèles factoriels de risque\nNous supposons que la valeur du portefeuille dépend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille à \\(t+h\\) dépend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\noù g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit linéaire par rapport aux facteurs de risque, ainsi donc le retour des actifs à l’horizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] où \\(B\\) est la matrice des sensibilités du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque à l’horizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables aléatoires gaussiennes indépendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable aléatoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calculé comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette méthode repose sur 3 hypothèses : l’indépendance temporelle des variations de la valeur du portefeuille, la normalité des facteurs et la relation linéaire entre les facteurs et la valeur du portefeuille. En général, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons à partir des données historiques des facteurs et \\(B\\) est le vecteur des sensibilités du portefeuille aux facteurs de risque. La seuil difficulté de cette méthode est l’estimation de la matrice de variance covariance.\n\n2.4.1 Exemple d’un portefeuille obligataire sans risque de crédit\nNous considérons une exposition sur une obligation américaine à $t=$31 décembre 2014. Le nominal de l’obligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont égaux à 5, \\(t_m&gt;t\\). La maturité résiduelle est de cinq ans et les dates de fixation sont à la fin de décembre (\\(n_C=5\\). Le nombre d’obligations détenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d’une obligation zéro coupon (montant qu’un investisseur serait prêt à payer aujourd’hui pour recevoir un paiement fixe à une date future : combien me rapport un euro à maturité \\(T\\) aujourd’hui?) au temps \\(t\\) pour l’échéance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) où \\(R_t(T)\\) est le taux de rendement zéro coupon.\nLa valeur de l’obligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en déduit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\noù \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l’obligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l’obligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en déduisons que le prix de l’obligation est de \\(P(t)=115,47 \\$\\) et l’exposition totale est de 1 154 706 $. En utilisant la période historique de l’année 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d’intérêt à coupon zéro sachant que l’écart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), où \\(h\\) correspond à un jour de bourse. Pour la matrice de corrélation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en déduisons que la valeur à risque à 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur à risque de 4970$ pour une période de détention d’un jour.\n\n2.4.1.1 Implémentation en R\n\n# Définition des écarts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "3A/risque_def.html",
    "href": "3A/risque_def.html",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "",
    "text": "En finance, le risque peut être défini comme la survenance d’un événement incertain qui peut avoir des conséquences négatives sur le bilan, ou le compte de résultat d’une banque. Par exemple, une fraude aura un impact négative sur la réputation d’une banque ce qui peut entrainer des pertes importants ayant un impact négatif sur le résultat net de celle-ci. En économie, le risque est un événement probabilisable tandis que l’incertitude est non probabilisable.\nNous pouvons caractériser 3 grands types de risques établis par le comité de Bâle qui veille au renforcement et à la stabilité du système financier. (rangés par ordre d’importance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la conséquence. Toutefois, certains risque sont difficiles à distinguer. Ils se trouvent à la frontière entre le risque de marché, de crédit et le risque opérationnel.\nIl est important de noter que le but d’une banque n’est pas de prendre le moins de risque, mais d’atteindre une rentabilité maximale pour un risque donné. La théorie financière nous apprend que seul le risque est rémunéré. La banque procède donc à une arbitrage entre risque et rentabilité. C’est pourquoi la gestion des risques est un élément clé de la stratégie de décision de la banque. La mesure du risque intervient pour calculer les fonds propres nécessaires pour assurer chaque opération financière. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "3A/risque_def.html#les-mesures-de-risque",
    "href": "3A/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al. ont défini les propriétés que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  cohérente. Une mesure de risque est une fonction qui permet de quantifier le risque d’un portefeuille. Elle est cohérente si elle satisfait les propriétés suivantes :\n\nsous-additivité : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomogénéité positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivité signifie que le risque d’un portefeuille est inférieur ou égal à la somme des risques des actifs qui le composent. Ce phénomène est appelé effet de diversification. En effet, la diversification permet de réduire le risque d’un portefeuille en investissant dans des actifs non corrélés. Ainsi, en agrégeant deux porte-feuilles, il n’y a pas de création de risque supplémentaire.\nL’homogénéité positive signifie que le risque d’un portefeuille est proportionnel à la taille du portefeuille. Cette propriété ignore les problèmes de liquidité.\nL’invariance par translation signifie que l’addition au portefeuille initiale un montant sûr rémunéré au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d’immobiliser des fonds propres égaux à la mesure du risque.\nLa monotonie signifie que le risque d’un portefeuille est inférieur ou égal au risque d’un autre portefeuille si la distribution de probabilité de la perte potentielle du premier portefeuille est inférieure ou égale à celle du deuxième portefeuille. Celà traduit l’ordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d’un portefeuille à un instant t donné, le risque est la variation négative de ce portefeuille dans le futur. Le risque se caractérisait donc par une perte relativfe (par rapport à la valeur initiale du portefeuille à un instant t). Pendant très longtemps, les banques utilisaient la volatilité (écart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup évoluée et celle qui est la plus répandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d’un portefeuille à un horizon \\(h\\) donné et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\noù F est la distribution de probabilité de la perte potentielle du portefeuille.\nPar exemple, une VaR à \\(\\alpha=1\\%\\) de 1 million d’euros signifie que la probabilité que la banque perde plus de 1 million d’euros est égale à 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d’euros sur une période donnée (C’est la perte maximale encourue par la banque avec un intervalle de confiance à 99%). Nous allons préférer la deuxième formulation de l’interprétation.\nDeux éléments sont nécessaires pour déterminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance à un indicateur de tolérance pour le risque. Une couverture à 99% est beaucoup plus exigente et donc plus coûteuse qu’une couverture à 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait définir l’horizon h. Par exemple, une couverture à 1 jour est moins coûteuse qu’une couverture à 1 mois. C’est la combinaison de ces deux éléments qui détermine le degré de la couverture qui peut être exprimé en temps de retour 1 \\(t°\\)qui est la durée moyenne entre deux dépassements de la VaR. Il permet de caractériser la rareté d’un évènement (dont la probabilité d’occurence est petite)\n\\[t°= \\frac{h}{1-\\alpha}\\]\nLorsqu’on entend parler de gestion de risque décennal, celà revient à considérer une valeur en risque (VaR) journalière (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non cohérente car elle ne respecte pas la propriété de sous-additivité. De nombreux professionnels recommanderaient alors l’utilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque cohérente. La CVAR est l’espérance de la perte au delà de la VaR. Toutefois, la VaR reste une mesure de risque très utilisée en pratique, qui ne respecte pas la propriété de sous-additivité que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilités sont principalement localisées dans les quantiles extrêmes.\n\n\n\nD’autres mesures de risque\nD’autres mesures peuvent être définis comme celle de la perte exceptionnelle (Unexpected Loss - UL) définie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] où L est la distribution de la perte potentielle.\nIl s’agit là de la différence entre la VaR et la perte moyenne (expected loss - EL). Il y a également le regret espéré défini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donné représentant le montant de la perte tolérable par l’institut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l’institut financier. Lorsque H est endogène, c’est-à-dire dépendant de la distribution de la perte potentielle, et égale à la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l’espérance de la perte au delà de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au delà d’un certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au delà de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c’est la semi variance."
  },
  {
    "objectID": "3A/risque_def.html#footnotes",
    "href": "3A/risque_def.html#footnotes",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npériode de retour doit être interprétée comme la probabilité statistique qu’un évènement se produise↩︎"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html",
    "href": "3A/gestion_actifs/profil_liquid.html",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "",
    "text": "Nous souhaitons calculer le profil d’écoulement/liquidation dans les scénarios suivants :\nDans l’ordre des étapes, il s’agira dans ce TP de faire :\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n/Users/cherylkouadio/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\ndef get_data(start_date, end_date, index_ticker, tickers, column=\"Close\"):\n    \"\"\"\n    Extraction de données de cours d'actions\n    Args:\n        start_date (str): Date de début au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des volumes historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    # Extraction des volumes historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)[column]\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=3*31)\n\nselected_assets = {\n    \"AC.PA\": \"Accor\",\n    \"AI.PA\": \"Air Liquide\",\n    \"AIR.PA\": \"Airbus\",\n    \"MT.AS\": \"ArcelorMittal\",\n    \"CS.PA\": \"AXA\",\n    \"BNP.PA\": \"BNP Paribas\",\n    \"EN.PA\": \"Bouygues\",\n    \"BVI.PA\": \"Bureau Veritas\",\n    \"CAP.PA\": \"Capgemini\",\n    \"CA.PA\": \"Carrefour\",\n    \"ACA.PA\": \"Crédit Agricole\",\n    \"BN.PA\": \"Danone\",\n    \"DSY.PA\": \"Dassault Systèmes\",\n    \"EDEN.PA\": \"Edenred\",\n    \"ENGI.PA\": \"Engie\",\n    \"EL.PA\": \"EssilorLuxottica\",\n    \"ERF.PA\": \"Eurofins Scientific\",\n    \"RMS.PA\": \"Hermès\",\n    \"KER.PA\": \"Kering\",\n    \"LR.PA\": \"Legrand\",\n    \"OR.PA\": \"L'Oréal\",\n    \"MC.PA\": \"LVMH\",\n    \"ML.PA\": \"Michelin\",\n    \"ORA.PA\": \"Orange\",\n    \"RI.PA\": \"Pernod Ricard\",\n    \"PUB.PA\": \"Publicis\",\n    \"RNO.PA\": \"Renault\",\n    \"SAF.PA\": \"Safran\",\n    \"SGO.PA\": \"Saint-Gobain\",\n    \"SAN.PA\": \"Sanofi\",\n    \"SU.PA\": \"Schneider Electric\",\n    \"GLE.PA\": \"Société Générale\",\n    \"STLA\": \"Stellantis\",\n    \"STMPA.PA\": \"STMicroelectronics\",\n    \"TEP.PA\": \"Teleperformance\",\n    \"HO.PA\": \"Thales\",\n    \"TTE.PA\": \"TotalEnergies\",\n    \"UNBLF\": \"Unibail-Rodamco-Westfield\",\n    \"VIE.PA\": \"Veolia\",\n    \"DG.PA\": \"Vinci\",\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker, column=\"Volume\")\n\n[                       0%                       ][                       0%                       ][****                   8%                       ]  3 of 40 completed[*****                 10%                       ]  4 of 40 completed[*****                 10%                       ]  4 of 40 completed[*******               15%                       ]  6 of 40 completed[*********             18%                       ]  7 of 40 completed[**********            20%                       ]  8 of 40 completed[***********           22%                       ]  9 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[***************       32%                       ]  13 of 40 completed[*****************     35%                       ]  14 of 40 completed[******************    38%                       ]  15 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************52%                       ]  21 of 40 completed[**********************55%*                      ]  22 of 40 completed[**********************57%**                     ]  23 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************90%******************     ]  36 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nAI.PA\nAIR.PA\nBN.PA\nBNP.PA\nBVI.PA\nCA.PA\nCAP.PA\nCS.PA\n...\nSAF.PA\nSAN.PA\nSGO.PA\nSTLA\nSTMPA.PA\nSU.PA\nTEP.PA\nTTE.PA\nUNBLF\nVIE.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-11-07\n698430.0\n13751044.0\n813490.0\n1091627.0\n1059857.0\n5322315.0\n646260.0\n2794758.0\n326259.0\n4163091.0\n...\n576180.0\n1527095.0\n1430888.0\n6819700.0\n2082219.0\n892806.0\n404002.0\n3786884.0\n0.0\n2809097.0\n\n\n2024-11-08\n640605.0\n4781311.0\n712553.0\n1541520.0\n1045775.0\n4731196.0\n367776.0\n4129105.0\n340411.0\n2828675.0\n...\n672105.0\n1414195.0\n1037715.0\n8197900.0\n1881978.0\n737207.0\n288437.0\n3327420.0\n100.0\n2139007.0\n\n\n2024-11-11\n544390.0\n3965216.0\n615456.0\n1013673.0\n1139407.0\n3062744.0\n621229.0\n2577272.0\n332633.0\n2860191.0\n...\n678397.0\n1208511.0\n877175.0\n7181500.0\n2067857.0\n808043.0\n268883.0\n3669304.0\n100.0\n1474874.0\n\n\n2024-11-12\n476433.0\n6774868.0\n957769.0\n1451643.0\n1319645.0\n3871602.0\n503435.0\n2216855.0\n419941.0\n4554097.0\n...\n928357.0\n1941126.0\n1059643.0\n5832100.0\n3211099.0\n939216.0\n376479.0\n5104044.0\n0.0\n2051518.0\n\n\n2024-11-13\n503706.0\n5911519.0\n752386.0\n1381466.0\n1085131.0\n2782805.0\n674357.0\n1985037.0\n520292.0\n3538137.0\n...\n665527.0\n1394445.0\n1431965.0\n6821700.0\n2456917.0\n990623.0\n206989.0\n4171178.0\n100.0\n2188795.0\n\n\n\n\n5 rows × 40 columns\nportfolio_data.index\n\nDatetimeIndex(['2024-11-07', '2024-11-08', '2024-11-11', '2024-11-12',\n               '2024-11-13', '2024-11-14', '2024-11-15', '2024-11-18',\n               '2024-11-19', '2024-11-20', '2024-11-21', '2024-11-22',\n               '2024-11-25', '2024-11-26', '2024-11-27', '2024-11-28',\n               '2024-11-29', '2024-12-02', '2024-12-03', '2024-12-04',\n               '2024-12-05', '2024-12-06', '2024-12-09', '2024-12-10',\n               '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-16',\n               '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20',\n               '2024-12-23', '2024-12-24', '2024-12-26', '2024-12-27',\n               '2024-12-30', '2024-12-31', '2025-01-02', '2025-01-03',\n               '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09',\n               '2025-01-10', '2025-01-13', '2025-01-14', '2025-01-15',\n               '2025-01-16', '2025-01-17', '2025-01-20', '2025-01-21',\n               '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-27',\n               '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31',\n               '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06',\n               '2025-02-07'],\n              dtype='datetime64[ns]', name='Date', freq=None)\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\nadv_3m\n\nADV = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\n\n\n\n\nAC.PA\n2.166276e+06\n\n\nAI.PA\n6.087734e+05\n\n\nAIR.PA\n3.842644e+05\n\n\nMT.AS\n2.288744e+05\n\n\nCS.PA\n5.688459e+05\n# Génération des quantités\nnp.random.seed(123)\nADV[\"Quantity\"] =  round(1.5 * np.random.rand(len(ADV[\"ADV\"])) * ADV[\"ADV\"])\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n\n\nCS.PA\n5.688459e+05\n613900.0\nOn fait l’hypothèse que la profondeur de marché est de 20%. Celà signifie que l’on peut vendre 20% de la quantité sans impacter le prix de façon considérable. Au delà, le prix est impacté. Cette profondeur est ce qui est observé en pratique dans les carnets d’ordre à tel point que l’AMF le recommande.\nmarket_depth = 20/100\nADV[\"Quantity in 1day\"] = round(ADV[\"Quantity\"] * market_depth)\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n# Calcul du nombre de jours de liquidation\nADV[\"Days of liquidation\"] = ADV[\"Quantity\"]/ADV[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV[\"Days of liquidation\"] = ADV[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\nADV.head()\n\n\n\n\n\n\n\n\nADV\nQuantity\nQuantity in 1day\nDays of liquidation\n\n\n\n\nAC.PA\n2.166276e+06\n2263116.0\n452623.0\n5\n\n\nAI.PA\n6.087734e+05\n261291.0\n52258.0\n5\n\n\nAIR.PA\n3.842644e+05\n130756.0\n26151.0\n5\n\n\nMT.AS\n2.288744e+05\n189273.0\n37855.0\n5\n\n\nCS.PA\n5.688459e+05\n613900.0\n122780.0\n5\nprint(f\"Temps de liquidation du portefeuille : {ADV['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 5 jours"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#présence-de-déformation",
    "href": "3A/gestion_actifs/profil_liquid.html#présence-de-déformation",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "Présence de déformation",
    "text": "Présence de déformation\n\nSous conditions normales avec déformation (waterfall liquidation)\nOn peut être également interessé par la quantité de liquidation sur plusieurs jours. Pour celà, on fait l’hypothèse qu’on liquide les prochains jours aux prix observés aujourd’hui. Ce que je peux véritablement liquider en 1 jour est donc la quantité que je peux vendre sans impacter le prix, i.e. min(quantité liquidable en 1 jour, quantité restant dans le portefeuille).\nOn peut calculer la valeur du portefeuille initiale et sur les jours de liquidation désirée. On l’exprime généraleent en pourcentage des encours totaux. On peut également calculer le cumul du pourcentage liquidé sur les jours de liquidation désirée. Cela nous permet d’obtenir le profil d’écoulement.\n\n# Initialisation d'une colonne pour suivre les quantités liquidées\nADV[\"Quantity liquidated\"] = 0  # Initialement, rien n'est liquidé\n\n# Création d'une liste pour suivre la liquidation jour par jour\n# Au jour 0, on a liquidé 0. La colonne 0 sert de quantité initiale\nquantity_liquidated_per_day = [ADV[\"Quantity\"]]\n\nfor nb_day in range(1, 8):  # Pour chaque jour\n    # Calculer la quantité liquide au jour i\n    liquidated_today = np.minimum(ADV[\"Quantity in 1day\"], ADV[\"Quantity\"] - ADV[\"Quantity liquidated\"])\n    \n    # Mettre à jour les quantités liquidées dans le DataFrame\n    ADV[\"Quantity liquidated\"] += liquidated_today\n    \n    # Stocker les quantités liquidées ce jour dans une liste\n    quantity_liquidated_per_day.append(liquidated_today)\n\n# Conversion des résultats jour par jour en DataFrame pour plus de clarté\nliquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\nliquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n\nliquidation_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n2263116.0\n452623.0\n452623.0\n452623.0\n452623.0\n452623.0\n1.0\n0.0\n\n\nAI.PA\n261291.0\n52258.0\n52258.0\n52258.0\n52258.0\n52258.0\n1.0\n0.0\n\n\nAIR.PA\n130756.0\n26151.0\n26151.0\n26151.0\n26151.0\n26151.0\n1.0\n0.0\n\n\nMT.AS\n189273.0\n37855.0\n37855.0\n37855.0\n37855.0\n37853.0\n0.0\n0.0\n\n\nCS.PA\n613900.0\n122780.0\n122780.0\n122780.0\n122780.0\n122780.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=1)\nprice_data = get_data(start_date, end_date, index, assets_ticker, column=\"Close\")\n\nprice_data[\"portfolio_data\"].head()\nprice_dict = price_data[\"portfolio_data\"].iloc[-1].to_dict()\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********            20%                       ]  8 of 40 completed[**********            20%                       ]  8 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[************          25%                       ]  10 of 40 completed[*******************   40%                       ]  16 of 40 completed[********************  42%                       ]  17 of 40 completed[**********************45%                       ]  18 of 40 completed[**********************48%                       ]  19 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************50%                       ]  20 of 40 completed[**********************60%****                   ]  24 of 40 completed[**********************62%*****                  ]  25 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************65%******                 ]  26 of 40 completed[**********************70%*********              ]  28 of 40 completed[**********************72%**********             ]  29 of 40 completed[**********************75%***********            ]  30 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************78%************           ]  31 of 40 completed[**********************82%**************         ]  33 of 40 completed[**********************85%****************       ]  34 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************88%*****************      ]  35 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************92%*******************    ]  37 of 40 completed[**********************98%********************** ]  39 of 40 completed[*********************100%***********************]  40 of 40 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Valeur liquide des actions par jour de liquidation\nmarket_value =[\n    price_dict[ticker] * liquidation_df.loc[ticker]\n    for ticker in selected_assets\n]\n\nmarket_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\nmarket_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n1.129295e+08\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n2.258589e+07\n49.900002\n0.0\n\n\nAI.PA\n4.418953e+07\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n8.837873e+06\n169.119995\n0.0\n\n\nAIR.PA\n2.182579e+07\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n4.365125e+06\n166.919998\n0.0\n\n\nMT.AS\n5.178509e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035713e+06\n1.035658e+06\n0.000000\n0.0\n\n\nCS.PA\n2.305808e+07\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n4.611617e+06\n0.000000\n0.0\n\n\n\n\n\n\n\n\n# Calcul de la valeur de marché initiale et totale\nmarket_value_0 = market_value.iloc[:, 0]\ntotal_market_value_0 = market_value_0.sum()\n\n# Calcul de la valeur de marché cumulée (à partir de la colonne 1)\ncumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\ncumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\ncumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\ncumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\nweights = {}\nfor ticker in assets_ticker :\n    weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\nweights = pd.DataFrame(weights).T\nweights.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nAC.PA\n0.010344\n0.010344\n0.010344\n0.010344\n0.010344\n0.012815\nNaN\nNaN\n\n\nAI.PA\n0.004048\n0.004048\n0.004048\n0.004048\n0.004048\n0.043432\nNaN\nNaN\n\n\nAIR.PA\n0.001999\n0.001999\n0.001999\n0.001999\n0.001999\n0.042867\nNaN\nNaN\n\n\nMT.AS\n0.000474\n0.000474\n0.000474\n0.000474\n0.000474\n0.000000\nNaN\nNaN\n\n\nCS.PA\n0.002112\n0.002112\n0.002112\n0.002112\n0.002112\n0.000000\nNaN\nNaN\n\n\n\n\n\n\n\n\n# Initialiser le graphique\nplt.figure(figsize=(12, 6))\n\n# Barplot empilé\nbottom = None\nfor asset in weights.index:\n    plt.bar(\n        pd.to_numeric(weights.columns),  # Les jours\n        weights.loc[asset],  # Poids de l'actif pour chaque jour\n        bottom=bottom,  # Position de départ pour empiler les barres\n        label=selected_assets[asset]  # Légende pour chaque actif\n    )\n    bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\nplt.xlabel(\"Days of Liquidation\")\nplt.ylabel(\"Portfolio Weights\")\nplt.title(\"Déformation du portefeuille\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nPour un fond de droit français reglementé, on a pas le droit d’investir plus de 5% du portefeuille dans une société. Exceptionnellement, pour certains titre, on a le droit d’investir jusqu’à 10% du portefeuille, à condition que les titres qui sont exposées à plus de 5% du portefeuille ne dépassent pas 40% du portefeuille. C’est la règle des 5/10/40. C’est un ratio réglementaire pour les OPC. Toutes les pertes réalisées en raison du défaut de ce ratio doivent être supportées par la société de gestion. Ces depassements doivent être déclarés à l’AMF. Dans notre cas, ce ratio n’est pas respecté, l’équilibre du portefeuille est chamboulé.\n\n# Valeur liquide du portefeuille\nmarket_value_df = pd.DataFrame()\n\nmarket_value_df[\"market_value\"] = market_value.sum(axis=0)\n\n# Calculer la valeur liquide relative par rapport au jour 0\nmarket_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n\n# Calculer la valeur cumulée liquide relative du portefeuille\nmarket_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n\n# Afficher le DataFrame résultant\nprint(market_value_df)\n\n   market_value  relative value  cumulative value\n0  1.091699e+10    1.000000e+00               0.0\n1  2.183399e+09    2.000001e-01               0.2\n2  2.183399e+09    2.000001e-01               0.4\n3  2.183399e+09    2.000001e-01               0.6\n4  2.183399e+09    2.000001e-01               0.8\n5  2.183392e+09    1.999994e-01               1.0\n6  3.893920e+03    3.566844e-07               1.0\n7  0.000000e+00    0.000000e+00               1.0\n\n\n\nimport matplotlib.pyplot as plt\nmarket_value_df = market_value_df.iloc[1:]\n\nplt.figure(figsize=(12, 6))\nbars = plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,  # Center text\n        height,  # Position slightly above the bar\n        f'{height:.2f}',  # Format with 2 decimal places\n        ha='center',  # Center horizontally\n        va='bottom',  # Position text at the bottom\n        fontsize=10, color=\"black\"\n    )\n\n# Set labels and title\nplt.xlabel(\"Days\")\nplt.ylabel(\"Cumulative Value (%)\")\nplt.title(\"Profil de liquidation du portefeuille\")\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nPour voir ce qui arrive au profil d’écoulement lorsque les quantités varient, on va utiliser un facteur de modulation de la quantité. Cela permet de déterminer quelle est la taille cible du portefeuille qui permet d’avoir la liquidité pour un certain niveau en nombre de jours qu’on se fixe. Cet exercice est fait une seule fois à l’initialisation du portefeuille.\nLa liquidité d’un portefeuille dépend de la liquidité intrinsèque des titres et la quantité de titres.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumulées.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (clé = actif, valeur = prix).\n        selected_assets (list): Liste des actifs sélectionnés.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantités.\n        plot_graphs (bool): Indique si les graphiques doivent être affichés.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumulées et relatives.\n    \"\"\"\n    # Initialisation des quantités liquidées\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantités liquidées par jour\n    for _ in range(1, nb_liquidation+1):\n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        ADV[\"Quantity liquidated\"] += liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des résultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de marché initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de marché cumulée (à partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empilé)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empilé\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de départ pour empiler les barres\n                label=selected_assets[asset]  # Légende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"Déformation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Création du DataFrame des valeurs de marché\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumulées\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumulée\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=0.5\nnb_liquidation=6\n\nnew_market_value_df, new_market_value, new_weights = liquidation_profile(ADV, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew_market_value.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\nAC.PA\n5.646475e+07\n2.258589e+07\n2.258589e+07\n1.129297e+07\n0.0\n0.0\n0.0\n\n\nAI.PA\n2.209477e+07\n8.837873e+06\n8.837873e+06\n4.419021e+06\n0.0\n0.0\n0.0\n\n\nAIR.PA\n1.091290e+07\n4.365125e+06\n4.365125e+06\n2.182646e+06\n0.0\n0.0\n0.0\n\n\nMT.AS\n2.589255e+06\n1.035713e+06\n1.035713e+06\n5.178291e+05\n0.0\n0.0\n0.0\n\n\nCS.PA\n1.152904e+07\n4.611617e+06\n4.611617e+06\n2.305808e+06\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nSous conditions stressées avec déformation\nPour avoir des conditions stressées, on joue sur la quantité liquidable en un jour et de fait sur la profondeur de marché. Pour des conditions stressées à la baisse, on divise la profondeur de marché par 2. Pour des conditions stressées à la hausse, on multiplie la profondeur de marché par 2.\n\n# Calcul des ADV 3Mois\n\nadv_3m = {portfolio_data[ticker].mean() for ticker in assets_ticker}\n\nADV_stressed = pd.DataFrame(adv_3m, index = assets_ticker, columns = [\"ADV\"])\n\n# Génération des quantités\nnp.random.seed(42)\nADV_stressed[\"Quantity\"] =  round(1.5 * np.random.uniform(0, 1, size=len(ADV)) * ADV[\"ADV\"])\n\n# Quantité journalière\nmarket_depth = (20/100)/2  # On stresse la liquidité\nADV_stressed[\"Quantity in 1day\"] = round(ADV_stressed[\"Quantity\"] * market_depth)\n\n# Calcul du nombre de jours de liquidation\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Quantity\"]/ADV_stressed[\"Quantity in 1day\"]\n\n# floor to 1 and round\nADV_stressed[\"Days of liquidation\"] = ADV_stressed[\"Days of liquidation\"].apply(lambda x: max(1, round(x)))\n\nprint(f\"Temps de liquidation du portefeuille : {ADV_stressed['Days of liquidation'].max()} jours\")\n\nTemps de liquidation du portefeuille : 10 jours\n\n\n\nfact_modulation=1\nnb_liquidation=12\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)"
  },
  {
    "objectID": "3A/gestion_actifs/profil_liquid.html#absence-de-déformation-du-portefeuille-pro-forma",
    "href": "3A/gestion_actifs/profil_liquid.html#absence-de-déformation-du-portefeuille-pro-forma",
    "title": "Profil d’écoulement/ de liquidation de portefeuille",
    "section": "Absence de déformation du portefeuille (pro forma)",
    "text": "Absence de déformation du portefeuille (pro forma)\nL’objectif est de conserver la distribution du portefeuille à mesure qu’il se liquide. Tout d’abord, on estime la quantité liquidable à un jour de chacun des titres comme fait précédemment. Celà permet d’avoir le pourcentage liquidable en un jour.\nSi on veut que le portefeuille se liquide à la même vitesse, il faut aller à la vitesse du titre le plus lent. On peut calculer le pourcentage liquidable en un jour pour chaque titre. On prendra le minimum de ces pourcentages pour déterminer le pourcentage liquidable en un jour du portefeuille.\nLe portefeuille prend ainsi plus de temps à se liquider et fatalement, le portefeuille finit par se déformer.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef liquidation_profile_pro_forma(ADV, price_dict, selected_assets, fact_modulation=0.30,nb_liquidation=8, plot_graphs=True):\n    \"\"\"\n    Calcule le profil de liquidation et visualise les graphiques des poids et des valeurs cumulées.\n    \n    Parameters:\n        ADV (pd.DataFrame): DataFrame contenant les informations sur les actifs (Quantity, Quantity in 1day, etc.).\n        price_dict (dict): Dictionnaire avec les prix des actifs (clé = actif, valeur = prix).\n        selected_assets (list): Liste des actifs sélectionnés.\n        fact_modulation (float): Facteur de modulation pour ajuster les quantités.\n        plot_graphs (bool): Indique si les graphiques doivent être affichés.\n    \n    Returns:\n        pd.DataFrame: DataFrame contenant les valeurs cumulées et relatives.\n    \"\"\"\n    # Initialisation des quantités liquidées\n    ADV = ADV.copy()\n    ADV[\"Quantity liquidated\"] = 0\n    quantity_liquidated_per_day = [ADV[\"Quantity\"] * fact_modulation]\n    \n    # Calcul des quantités liquidées par jour\n    for _ in range(1, nb_liquidation+1):        \n        liquidated_today = np.minimum(\n            ADV[\"Quantity in 1day\"], \n            ADV[\"Quantity\"] * fact_modulation - ADV[\"Quantity liquidated\"]\n        )\n        min_liquidated_today = (liquidated_today/ADV[\"Quantity in 1day\"]).min() # On liquide à la vitesse de l'actif le moins liquide\n        ADV[\"Quantity liquidated\"] += min_liquidated_today*liquidated_today\n        quantity_liquidated_per_day.append(liquidated_today)\n    \n    # Conversion des résultats en DataFrame\n    liquidation_df = pd.DataFrame(quantity_liquidated_per_day).T\n    liquidation_df.columns = [f\"{i}\" for i in range(len(quantity_liquidated_per_day))]\n    \n    # Calcul de la valeur liquide par actif et par jour\n    market_value = [\n        price_dict[ticker] * liquidation_df.loc[ticker]\n        for ticker in selected_assets\n    ]\n    market_value = pd.DataFrame(market_value, index=selected_assets, columns=liquidation_df.columns)\n    \n    # Calcul des poids par jour\n    # Calcul de la valeur de marché initiale et totale\n    market_value_0 = market_value.iloc[:, 0]\n    total_market_value_0 = market_value_0.sum()\n\n    # Calcul de la valeur de marché cumulée (à partir de la colonne 1)\n    cumsum_market_value = market_value.iloc[:, 1:].cumsum(axis=1)\n    cumsum_total_market_value = market_value.iloc[:, 1:].sum(axis=0).cumsum()\n    cumsum_market_value = pd.concat([pd.DataFrame(0, index=market_value.index, columns=[0]), cumsum_market_value], axis=1)\n    cumsum_total_market_value = pd.concat([pd.Series(0, index=[0]), cumsum_total_market_value])\n\n    weights = {}\n    for ticker in selected_assets :\n        weights[ticker] = (market_value_0.loc[ticker] - cumsum_market_value.loc[ticker]) / (total_market_value_0 - cumsum_total_market_value)\n\n    weights = pd.DataFrame(weights).T\n    \n    # Visualisation des poids (barplot empilé)\n    if plot_graphs:\n        # Initialiser le graphique\n        plt.figure(figsize=(12, 6))\n\n        # Barplot empilé\n        bottom = None\n        for asset in weights.index:\n            plt.bar(\n                pd.to_numeric(weights.columns),  # Les jours\n                weights.loc[asset],  # Poids de l'actif pour chaque jour\n                bottom=bottom,  # Position de départ pour empiler les barres\n                label=selected_assets[asset]  # Légende pour chaque actif\n            )\n            bottom = weights.loc[asset] if bottom is None else bottom + weights.loc[asset]\n\n        plt.xlabel(\"Days of Liquidation\")\n        plt.ylabel(\"Portfolio Weights\")\n        plt.title(\"Déformation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.legend(title=\"Assets\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8, ncol=2)\n        plt.tight_layout()\n\n        plt.show()\n\n    \n    # Création du DataFrame des valeurs de marché\n    market_value_df = pd.DataFrame()\n    market_value_df[\"market_value\"] = market_value.sum(axis=0)\n    \n    # Calcul des valeurs relatives et cumulées\n    market_value_df[\"relative value\"] = market_value_df[\"market_value\"] / market_value_df[\"market_value\"].iloc[0]\n    market_value_df[\"cumulative value\"] = market_value_df[\"relative value\"].cumsum() - 1\n    market_value_df = market_value_df.iloc[1:]  # Retirer le jour 0 pour l'analyse cumulée\n    \n    # Visualisation de la valeur cumulative (barplot)\n    if plot_graphs:\n        plt.figure(figsize=(8, 4))\n        plt.bar(market_value_df.index, market_value_df[\"cumulative value\"] * 100, color=\"skyblue\")\n        plt.xlabel(\"Days\")\n        plt.ylabel(\"Cumulative Value (%)\")\n        plt.title(\"Profil de liquidation du portefeuille\")\n        plt.xticks(rotation=45)\n        plt.show()\n    return market_value_df, market_value, weights\n\n\nfact_modulation=1\nnb_liquidation=10\n\nstressed_market_value_df, stressed_market_value, stressed_weights = liquidation_profile_pro_forma(ADV_stressed, price_dict, selected_assets, fact_modulation, nb_liquidation, plot_graphs=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour gérer la liquidité d’un portefeuille et donc préserver la qualité du portefeuille, on peut suspendre les souscriptions et les rachats par des mécanismes émis par la loi. Les régulateurs des SGP annoncent que les investisseurs annoncent que les indivdus ne peuvent plus souscrire ou faire un rachat.\nMécanismes de gestion de la liquidité:\n\nLes Gates consistent à plafonner les rachats. Si les rachats totaux sont supérieures à 5% de l’actif net, la SGP a le droit et non l’obligatoire ne pas honorer les rachats de plus de 5%. Elle limite donc les rachats en un jour à 5% et ventiler le reste sur les jours suivants en fonction des conditions du marché. Cela permet de ne pas impacter le prix de façon considérable. C’est une mesure de protection des investisseurs restants. Les gates restent quand même un signal négatif pour les investisseurs restants. Ils permettent toutefois de mettre de l’ordre dans le portefeuille. L’AMF le fait figurer dans le prospectus, sauf si la SGP arrive à justifier qu’elle n’a pas besoin de le faire. Il n’en demeure pas moins que l’activation des gates est optionnelle"
  },
  {
    "objectID": "3A/bilan_entreprise.html",
    "href": "3A/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "",
    "text": "L’analyse financière constitue l’ensemble des outils permettant de donner un avis objectif d’une organisation (entreprises, fondations, etc.) sur la santé finanière et les risques financiers auxquels elle sera confrontée. Il s’agit de determiner quels sont les critères d’une santé financière, qu’est le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le gère-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financière d’une entreprise. Il s’agit du bilan et du compte de résultat. Ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise. Comprendre comment ils fonctionnent permet de mieux appréhender la situation financière d’une banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un état des lieux de la situation patrimoniale de l’entreprise à un moment donné. Il est composé de deux parties : l’actif et le passif. L’actif ou l’emploi regroupe l’ensemble des biens et des droits de l’entreprise tandis que le passif regroupe l’ensemble des ressources de l’entreprise (d’où vient l’argent et où peut-on s’en procurer). Le bilan est équilibré en valeur nette, c’est-à-dire que l’actif est égal au passif.\nLe compte de résultats, quant à lui, est un document qui permet de faire un état des lieux des performances de l’entreprise sur une période donnée (il résume les bénéfices ou pertes générées). Il est composé du détail des produits et des charges de l’entreprise. Les produits sont les éléments qui génèrent des revenus pour l’entreprise tandis que les charges sont les éléments qui génèrent des dépenses pour l’entreprise. Le compte de résultat alimente par ailleurs la partie “résultat de l’exercice” du bilan comptable.\nLe coeur de l’entreprise à analyser comme ressources supplémentaires dans le compte de résultat est l’ensembles des charges financières & exceptionnelles ainsi que l’ensemble des produits d’exploitation et financiers. Ces éléments clés permettent de déterminer la rentabilité de l’entreprise. En effet, si les charges sont supérieures aux produits, l’entreprise est en perte. Si les produits sont supérieurs aux charges, l’entreprise est en bénéfice.\nIl est important de noter que ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise."
  },
  {
    "objectID": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "3A/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d’analyser la situation financière d’une entreprise, il faut donc le remodeler en un bilan “fonctionnel” pour pouvoir l’analyser. Le bilan fonctionnel est un document qui permet de faire un état des lieux de la situation financière de l’entreprise en fonction de son activité, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d’investissement à long terme\nEmplois stables\n\nactifs immobilisés en valeur brute\n\nCycle de financement à long terme\nRessources stables\n\nCapitaux propres,\nEmprunts à long terme,\nAmortissements et dépréciation,\nProvisions pour risques\n\n\n\nCycle d’exploitation\nEmplois d’exploitation\n\nStocks et encours\nCréances\n\nCycle d’exploitation\nRessources d’exploitation\n\nDettes circulantes\n\n\n\nTrésorerie active\n\nDisponibilités\n\nTrésorerie passive\n\nDécouverts bancaires\n\n\n\n\nLes ressources stables font référence aux ressources saines du bilan etfont face aux emplois stables. La trésorerie passive fait référence aux découverts bancaires. Il est important de souligner qu’une trésorerie passive est perçue négativement dans le bilan fonctionnel. En effet, une trésorerie passive signifie que l’entreprise a des dettes à court terme qui ne sont pas couvertes par des actifs à court terme d’où la nécessité d’avoir des découverts bancaires.\nNb : La provision pour le risque peuve être considérée comme une ressource stable ou une ressource d’exploitation en fonction de l’entreprise. Tout dépend de la longevité des provisions.\n\nEquilibre financier\nNous dirons qu’il y a équilibre financier lorsque :\n\nLes emplois stables soient entièrement financés par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a necéssité d’un fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) dépend du cycle d’exploitation (entre autre, la rapidité de rotation des stocks et des créances). Il doit couvrir les besoins de financement du cycle d’exploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, créances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la trésorerie (\\(\\text{Trésorerie}=FDR-BFR\\)). Si la trésorerie est positive, il y a équilibre financier. Celà signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d’exploitation. Lorsqu’il est négatif, il faut trouver des ressources pour financer le cycle d’exploitation. Si la trésorerie est nulle, il y a équilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-même le cycle d’exploitation de l’entreprise. C’est ce qu’on appelle le crédit fournisseur. Il est important de noter que le crédit fournisseur est une source de financement gratuite pour l’entreprise. C’est le cas des E-commerce où les acteurs encaissent leurs clients avant même d’acheter les stocks auprès des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transformé en ressources en fonds de roulement, celà est une situation très favorable pour l’entreprise et est appelée “crédit inter-entreprises”."
  },
  {
    "objectID": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "href": "3A/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Analyse du compte de résultat",
    "text": "Analyse du compte de résultat\nNous pouvons faire les mêmes critiques faites au bilan comptable sur le compte de résultat. En effet, le compte de résultat est conçu de sorte à fournir des informations au seul détenteur du capital, à savoir les actionnaires. Il fait apparaitre uniquement le bénéfice ou la perte. C’est un document d’intérêt pour l’Etat pour déterminer si un pays est en croissance ou en récession. Pour en faire un vrai diagnostic financier, il faut le découper en sous-soldes appelés “soldes intermédiaires de gestion” (SIG). Les SIG permettent de déterminer la rentabilité de l’entreprise, sa capacité d’autofinancement, sa capacité de remboursement, sa capacité de financement, etc.\nIl existe 9 soldes intermédiaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajoutée\nL’excédent brut d’exploitation (EBE)\nLe résultat d’exploitation\nLe résultat courant avant impôt\nLe résultat exceptionnel\nLe résultat net\nLa plus ou moins value de cession\n\nSelon la théorie de prise de décisions, il y a deux grands types de décisions : des décisions qui permettent de créer de la riches (Marge co., production et valeur ajoutée) et des décisions qui permettent de distribuer/dépenser de la richesse (EBE, résultat d’exploitation, résultat courant avant impôt, résultat exceptionnel, résultat net et plus ou moins value de cession). Lorsqu’on dépense la riches, il faudrait qu’elle soit bien dépensée.\n\nSoldes de création de richesse\nLes soldes qui contribuent à la création de richesse sont la marge commerciale, la production et la valeur ajoutée :\n\nLa marge commerciale est la différence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C’est un solde des entreprises commerciales (par exemple, les supermarchés). Pour une entreprise qui n’ont pas de marchandises, le marge commerciale est nulle.\nLa production de l’exercice est la somme des produits vendus(\\(\\pm\\) les produits stockées) et des produits immobilisées par l’entreprise (certaines entreprises peuvent se vendre des produits à elles-mêmes). C’est un solde des entreprises industrielles.\nLa valeur ajoutée est la richesse créée par l’entreprise. C’est la somme des marges commerciales, de la production de l’exercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services extérieurs).\n\nLa valeur ajouté est un indicateur très suivi par l’Etat pour déterminer le produit intérieur brut (PIB) afin de déterminer si un pays est en croissance ou en récession. Par ailleurs, la valeur ajoutée divisée par le nombre de salariés permet de déterminer le niveau de technicité de l’entreprise. Plus la valeur ajoutée par salarié est élevée, plus l’entreprise est techniquement avancée.\n\n\nLa richesse dédiée à l’activité économique\nIl existe 5 tiers à qui l’entreprise redistribue la VA (rangée par ordre de priorité) :\n\nLe personnel (à travers les salaires),\nL’Etat (à travers les impôts),\nLe capital technique (via les amortissements),\nLes banques (via les intérêts),\nLes actionnaires ou les associés (via le bénéfice comptable)\n\nLes soldes qui permettent de financer l’activité économique (Etat, personnel, capital technique) sont l’excédent brut d’exploitation et le résultat d’exploitation :\n\nLe solde EBE rémunère le personnel et l’Etat. Il représente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d’entreprise pour l’Etat. Un EBE positif signifie que l’entreprise est capable de rémunérer le personnel et l’Etat, et donc de financer l’emploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajoutée} - \\text{Impôts, tâxes et versements assimilés} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de résultat d’exploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de rémunérer le capital technique (machines etc.) et appartient à tout ceux qui dépendent du capital financier et mesure les performances industrielles et commerciales de l’entreprise.\n\n\\[\\begin{align*}\n\\text{Résultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse dédiée à l’activité financière\nLes soldes qui permettent de financer l’activité financière (banques, actionnaires) sont le résultat courant avant impôt, le résultat exceptionnel, le résultat net et la plus ou moins value de cession :\n\nLe résultat courant avant impôt est le solde qui permet de rémunérer les banques. Il est un indicateur de la capacité de l’entreprise à rembourser ses dettes et est un témoin de l’incidence de la politique financière de l’entreprise sur son résultat. Il faut distinguer les intérêts à long terme et ceux de court terme. Plus ceux ci sont liés à des dettes de court terme (ex. : découverts), on peut dire que l’entreprise est en difficulté financière tandis que l’endettement à long terme est un signe de bonne santé financière, car il est voulu plutôt que subi. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat courant avant impôt} &= \\text{Résultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financières}\n\\end{align*}\\]\n\nLe résultat exceptionnel est le solde qui est le moins analysé car il est souvent lié à des évènements exceptionnels (ex. : vente d’un bien immobilier). Il est calculé comme étant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe résultat net est le solde qui permet de rémunérer les actionnaires. C’est le solde en bas du compte de résultat. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat net} &= \\text{Résultat courant avant impôt} + \\text{Résultat exceptionnel} \\\\\n&- \\text{participations des salariés} - \\text{Impôts sur les bénéfices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu’une entreprise vend une immobilisation. Ce ratio permet de déterminer si l’entreprise a vendu une immobilisation à un prix supérieur ou inférieur à sa valeur comptable. Celà constitue un temoin d’alerte sur la santé de l’entreprise et permet de déterminer si l’entreprise est en difficulté financière (car rien ne l’oblige à vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "href": "3A/bilan_entreprise.html#la-capacité-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "La capacité d’autofinancement",
    "text": "La capacité d’autofinancement\nLa capacité d’autofinancement (CAF) est un indicateur qui permet de déterminer si l’entreprise est capable de financer ses investissements sans recourir à des financements extérieurs. Elle regroupe la capacité à dégager de la liquidité. Il n’y a pas de correspondance entre la trésorerie et le bénéfice. En effet, une entreprise peut être en bénéfice mais en difficulté financière. Pour la calculer, il faut éliminer les sommes non encaissanles et non décaissables (ex. : Dotations, provision, reprise sur amortissements, les écritures exceptionnelles).\nPour passer du bénéfice à la CAF, on ne conserve que les éléments qui sont encaissables et décaissables et est calculée comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges décaissables (intérêt bancaire, impôt sur bénéfice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu versé par l’entreprise à ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas à elle toute seule de déterminer l’autofinancement de l’entreprise. Dans le cadre légal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu’à 95% du bénéfice comptable imposé par l’Etat et garder 5% à l’entreprise. C’est ce qu’on appelle le “dividende légal”. Au delà de 10%, les actionnaires peuvent retirer jusqu’à 100% du bénéfice comptable. C’est ce qu’on appelle le “dividende statutaire”.\nAinsi, l’autofinancement est la somme qui reste de la CAF après le dividende légal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaissés par les actionnaires détermine la politique d’autofinancement de l’entreprise.\nL’autofinancement est essentiel pour l’entreprise car il permet de:\n\nrembourser les emprunts,\naméliorer la trésorerie,\ncouvrir les risque de l’entreprise (provisions pour risque),\nfinancer l’exploitation (stocks & créances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html",
    "href": "3A/Apprentisage-stat/Tp4.html",
    "title": "Features selection",
    "section": "",
    "text": "Feature selection is a process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features, therefore, it is important to select only the relevant features.\nThe usual tools used for features selection are based on correlation tests such as Pearson correlation or Spearman correlation when the variables are quantitatives or the Chi-Square test when the variables are qualitatives. However, these do not take in the account the relationship between quantitative and qualitatives variables. Moreover, for the pearson correlation or the spearman one, we are only interested in the linear(pearson) or monotonic(spearman) relationship between the variables.\nIn this notebook, we are interested in measuring any kind of relationship between variables, no matter what type of variables they are. For that, we will introduce the mutual information, based on the Kullback-Leibler divergence, which is a measure of the divergence between the joint distribution of X and Y and the product of their marginal distributions :\n\\[I(X;Y) = \\int_{X} \\int_{Y} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)\\]\nwhere \\(p(x,y)\\) is the joint probability distribution function of X and Y, and \\(p(x)\\) and \\(p(y)\\) are the marginal probability distribution functions of X and Y. If the variables are independent, then the mutual information is equal to 0. If the variables are dependent, then the mutual information is greater than 0.\nIn order to have confidence in this measure, we will consider a bivariate gaussian variable \\(Z=(X,Y)\\) with mean \\(\\mu = (0,0)\\) and covariance matrix \\(\\Sigma = \\begin{pmatrix} \\sigma^2_X & \\rho \\sigma_X \\sigma_X \\\\  \\rho \\sigma_X \\sigma_X & \\sigma^2_Y \\end{pmatrix}\\). We will compute the mutual information between X and Y for a grid a \\(\\rho\\) between 0.01 and 0.99 using 1000 size samples repeated 100 times and see how the mutual information behaves and compare it the theorical value of the mutual information which is equal to \\(-\\frac{1}{2} \\log(1-\\rho^2)\\).\n\n\n\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset.\n\n\n\nWe will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset.\n\n\n\n\nWe can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [0 2 3 5 8]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "href": "3A/Apprentisage-stat/Tp4.html#accuracy-of-mutual-information-estimation",
    "title": "Features selection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.feature_selection import mutual_info_regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Function to generate bivariate Gaussian data\ndef generate_bivariate_gaussian(n, rho):\n    mu_X, mu_Y = np.random.normal(0, 1), np.random.normal(0, 1)\n    sigma_X, sigma_Y = np.random.chisquare(1), np.random.chisquare(1)\n    mean = [mu_X, mu_Y]\n    cov = [[sigma_X**2, rho * sigma_X * sigma_Y], \n           [rho * sigma_X * sigma_Y, sigma_Y**2]]\n    return np.random.multivariate_normal(mean, cov, size=n)\n\n# Function to compute true mutual information\ndef true_mutual_information(rho):\n    return -0.5 * np.log(1 - rho**2)\n\n\n# Initialize parameters\nn_samples = 1000\nn_repeats = 100\nrho_values = np.linspace(0.01, 0.99, 10)\n\n# Store mutual information estimates\nestimated_mi = []\ntrue_mi_values = []\n\nfor rho in rho_values:\n    mi_estimates = []\n    for _ in range(n_repeats):\n        # Generate data\n        data_test = generate_bivariate_gaussian(n_samples, rho)\n        X = data_test[:, 0].reshape(-1, 1)  # Feature\n        Y = data_test[:, 1]  # Target\n        \n        # Estimate mutual information\n        mi = mutual_info_regression(X, Y, discrete_features=False)\n        mi_estimates.append(mi[0])  # mutual_info_regression returns an array\n        \n    # Store results\n    estimated_mi.append(mi_estimates)\n    true_mi_values.append(true_mutual_information(rho))\n\n# Convert estimated MI to array for easy plotting\nestimated_mi = np.array(estimated_mi)\n\n# Plot boxplots of the estimated MI for each value of rho\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=estimated_mi.T)\nplt.plot(np.arange(len(rho_values)), true_mi_values, color='red', marker='o', linestyle='-', label=\"True MI\")\nplt.xticks(ticks=np.arange(len(rho_values)), labels=[f'{rho:.2f}' for rho in rho_values])\nplt.xlabel(r'$\\rho$')\nplt.ylabel('Mutual Information')\nplt.title('Estimated vs True Mutual Information')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see in the graph above, the mutual information is able to capture the relationship between the variables, even if they are not linearly related. In fact, as the parameter \\(\\rho\\) increases, the mutual information increases as well. Morever, between the theoretical value of the mutual information and the empirical one, we can see that they are very close to each other. Hence, the function mutual_info_regression is well implemented and can be used to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "href": "3A/Apprentisage-stat/Tp4.html#comparison-between-mutual-information-and-correlation-tests",
    "title": "Features selection",
    "section": "",
    "text": "We will use a simulate dataset where we know the relationship between the variables and see how the pearson, spearman an mutual information perform. We will consider a dataset where the 15 first variables are normal \\(X_{i = 1,\\dots,15} \\sim \\mathcal{N}(0,1)\\) and the 5 last variables are uniform \\(X_{i = 16,\\dots,20} \\sim \\mathcal{U}(0,1)\\). We will consider the target variable \\(Y\\) as a linear combination of some variables :\n\\[ Y = 2X_1 + X_7^2 + X_4^3 + 3 X_2 X_{11} + 2 \\sin(2\\pi X_{19}) + 3X_6^2\\cos(2\\pi X_{17})\\]\n\n# Function to generate data\ndef generate_data(n):\n    data = pd.DataFrame()\n    for i in range(15):\n        data[f\"X{i+1}\"] = np.random.normal(0, 1, n)\n        \n    for i in range(5):\n        data[f\"X{i+16}\"] = np.random.uniform(0, 1, n)\n    pi = math.pi\n    data[\"Y\"] = 2 * data[\"X1\"] + data[\"X7\"]**2 + data[\"X4\"]**3 + 3 * data[\"X2\"] * data[\"X11\"] + \\\n                2 * np.sin(2 * pi * data[\"X19\"]) + 3 * (data[\"X6\"]**2) * np.cos(2 * pi * data[\"X10\"])\n                \n    return data\n\n\n# compute the correlation between each feature and the target\nfrom scipy.stats import pearsonr, spearmanr\n\nconcat_spearman = []\nconcat_pearson = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    spearman_corr = [spearmanr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    pearson_corr = [pearsonr(data[col], data[\"Y\"])[0] for col in data.columns[:-1]]\n    concat_spearman.append(np.abs(spearman_corr))\n    concat_pearson.append(np.abs(pearson_corr))\n\n\nimport pandas as pd\n\nfeature_names = data.columns[:-1]  # Get feature names from the dataset\n\nspearman_df = pd.DataFrame(concat_spearman, columns=feature_names)\npearson_df = pd.DataFrame(concat_pearson, columns=feature_names)\n\n# Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=spearman_df)\nplt.title('Spearman Correlations for Each Feature')\nplt.ylabel('Spearman Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\n\n# Pearson correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=pearson_df)\nplt.title('Pearson Correlations for Each Feature')\nplt.ylabel('Pearson Correlation')\nplt.xlabel('Feature')\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the spearman and pearson correlation can not identify all the relevant features, in fact, they are only able to identify the relevant variables \\(X_1\\), \\(X_4\\) and \\(X_{19}\\) related to the target variable \\(Y\\). If we are interested in selecting the relevant features using the mutual information, we can identify more relevant features such as \\(X_1\\), \\(X_4\\), \\(X_6\\), \\(X_7\\), and \\(X_{19}\\), however some irrelevant features are also selected such as \\(X_2\\) and \\(X_{11}\\).\n\nmutual_info = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    mi_corr = [mutual_info_regression(data[[col]], data[\"Y\"], discrete_features=False)[0] for col in data.columns[:-1]]\n    mutual_info.append(mi_corr)\n\nmutual_info = pd.DataFrame(mutual_info, columns=feature_names)\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=mutual_info)\nplt.title('Mutual information for each feature')\nplt.ylabel('Mutual info')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()\n\n\n\n\n\n\n\n\nTo conclude, the mutual information seems to be a good tool to select the relevant features in a dataset. In fact, it is able to capture the relationship between the variables, no matter what type of variables they are.\n\n\nWe might be interested in the behavior of the lasso regression in the same dataset. We will use the Lasso class from the sklearn.linear_model module to fit the lasso regression on the dataset and see how it performs in selecting the relevant features. We will use the LassoCV class to select the best value of the regularization parameter \\(\\alpha\\) using cross-validation.\n\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nn_simulations = 50\nn_samples = 500\nn_features = data.shape[1] - 1  \n\nselected_features = np.zeros((n_simulations, n_features))\nscaler = StandardScaler()\nfor sim in range(n_simulations):\n    # Generate data\n    data = generate_data(n_samples)\n    X = data.drop(columns=[\"Y\"]) \n    X_scaled = scaler.fit_transform(X)\n    y = data[\"Y\"] \n    \n    lasso = LassoCV().fit(X_scaled, y)\n    \n    selected_features[sim, :] = (lasso.coef_ != 0).astype(int)\n\n\n# Frequency of selection for each feature\nselection_frequency = np.mean(selected_features, axis=0)\n\n# Create a DataFrame for better visualization\nselection_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selection Frequency': selection_frequency\n})\n\n\n# You can also visualize this with a bar plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Feature', y='Selection Frequency', data=selection_df,color=\"blue\")\nplt.title('Feature Selection Frequency by Lasso')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see, the lasse regression is able to select the relevant features in the dataset. However, it is not able to select all the relevant features, hence it might always be interesting to use the mutual information (combined with lasso regression or other correlations tests) to select the relevant features in a dataset."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "href": "3A/Apprentisage-stat/Tp4.html#hilbert-schmidt-independence-criterion",
    "title": "Features selection",
    "section": "",
    "text": "We can also use the kernel trick applied to the kullback-leibler divergence to measure the mutual information between the variables. This is called the Maximum Mean Discrepancy (MMD) and is defined as :\n\\[MMD^2(X,Y) = \\mathbb{E}_{x,x' \\sim X} [k(x,x')] + \\mathbb{E}_{y,y' \\sim Y} [k(y,y')] - 2 \\mathbb{E}_{x \\sim X, y \\sim Y} [k(x,y)]\\]\nwhere \\(k\\) is a kernel function. The MMD is equal to 0 if and only if the two distributions are equal. We can use the MMD class from the sklearn.metrics.pairwise module to compute the MMD between the variables. For a continuous variables, this writes :\n\\[MMD^2(X,Y) = \\int_X k(x,x') \\left[ p(x) - q(x)\\right] \\left[ p(x') - q(x')\\right] dxdx'\\]\nwhere \\(p\\) and \\(q\\) are the probability distribution functions of X and Y.\nThis is also defined as the Hilbert-Schmidt Independence Criterion (HSIC) when we are interested in the measure of divergence between the joint distribution of X and Y and the product of their marginal distributions. The estimate of the HSIC is given by :\n\\[HSIC(X,Y) = \\frac{1}{n^2} \\text{tr}(KHLH)\\]\nwhere \\(K\\) is the kernel matrix of X and \\(L\\) is the kernel matrix of Y and H is the centering matrix \\(H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). Since the HSIC is not implemented in the mainstream libraries, we will implement it ourselves using the sobolev kernel for X and Y with :\n\\[k(x,x') = 1 + (z_i - 0.5) (z_j - 0.5) + \\frac{1}{2} ((z_i-z_j)^2 - |z_i -z_j| +1/6)\\]\n\ndef sobolev_kernel(x,y):\n    return 1 + (x - 0.5)*(y - 0.5) + (1/2)*( (x-y)**2 - np.abs(x-y) + 1/6)\n\ndef compute_gram_matrix(z):\n    z = np.asarray(z)\n    M = sobolev_kernel(z[:,None],z[None,:])\n    return M\n\ndef hsic(X, Y):\n    n = X.shape[0]\n    H = np.eye(n) - (1 / n) * np.ones((n, n))\n\n    # Compute Gram matrices with Sobolev kernel\n    K = compute_gram_matrix(X)\n    L = compute_gram_matrix(Y)\n\n    # Calculate HSIC\n    hsic_value = (1 / (n - 1) ** 2) * np.trace(K @ H @ L @ H)\n    return hsic_value\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nhsic_values = []\n\nfor sim in range(50):\n    data = generate_data(500)\n    scaler = MinMaxScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    hsic_var = [hsic(data_scaled[col], data_scaled[\"Y\"]) for col in data.columns[:-1]]\n    hsic_values.append(hsic_var)\n\nhsic_df = pd.DataFrame(hsic_values, columns=feature_names)\n\n\n# Plotting boxplots for Spearman correlations\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=hsic_df)\nplt.title('HSIC for Each Feature')\nplt.ylabel('HSIC')\nplt.xlabel('Feature')\nplt.xticks(rotation=90)  # Rotate feature names if there are many features\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "href": "3A/Apprentisage-stat/Tp4.html#threshold-selection-for-mutual-information-and-hsic",
    "title": "Features selection",
    "section": "",
    "text": "The hilbert-schmidt independence criterion, the mutual information are able to capture the relationship between the variables, no matter what type of variables they are. However, it needs a definition of a threshold to select the relevant features in a dataset. As for correlation tests like pearson or speaman, we might use statistical tests to select the relevant features in a dataset. However, for mutual information and HSIC, we use the permutation test where the test statistic distribution under the null hypothesis (X and Y are independent), is estimated with several data permutations. The p-value is then computed as the proportion of test statistics that are more extreme than the observed test statistic.\n\nfrom sklearn.utils import resample\ndata = generate_data(500)\n\nY = data[\"Y\"]\nX = data.drop(columns=[\"Y\"])\n\nscaler = MinMaxScaler()\ndata_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\nX_scaled = data_scaled.drop(columns=[\"Y\"])\nY_scaled = data_scaled[\"Y\"]\n\nmi_train = mutual_info_regression(X, Y)\n\nn_rep = 500\nall_miXYindep = np.zeros((n_rep, n_features))\nall_HSICindep = np.zeros((n_rep, n_features))\n\nfor rep in range(n_rep):\n    # Permutation\n    yb = resample(Y, replace = False)\n    # Compute mutual information between all features and Y\n    mi_temp = mutual_info_regression(X, np.ravel(yb))\n    # Store the MI from this repetition\n    all_miXYindep[rep, :] = mi_temp\n    # Permutation\n    yb_train = resample(Y_scaled, replace = False)\n\n\np_values_mi = np.mean(mi_train &lt; all_miXYindep, axis=0)\n\n\nplt.figure(figsize=(8, 4))\nplt.plot(p_values_mi, 'o')\nplt.title('Mutual Information p-value')\nplt.axhline(y=0.05, color='r', linestyle='-')\nplt.xlabel('Features')\nplt.ylabel('p-value')\nplt.show()\n\n\n\n\n\n\n\n\nDepending on the pvalues, we can select the relevant features in a dataset. Using the mutual information, the variables selected are listed below :\n\nprint(\"Selected variables with MI p-values %s \" % np.where(p_values_mi &lt; 0.05))\n\nSelected variables with MI p-values [0 2 3 5 8]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html",
    "href": "3A/Apprentisage-stat/Tp2.html",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In regression and classification, we often use linear models to predict the target variable. However, in many cases, the relationship between the target variable and the explanatory variables is non-linear. In such cases, we can use the kernel trick whenever there is a scalar product between the explanatory variables. The kernel trick allows us to transform the data into a higher-dimensional space where the relationship is linear.\nIn this first activity, we will explore the kernel trick to transform the data and then use a linear model to predict the target variable. In particular, we will use Kernel ridge regression (KRR) which is a combination of ridge regression and the kernel trick. The optimization problem of KRR is given by: \\[\n\\hat \\theta = \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^T\\theta)  + \\lambda \\sum_{j=1}^d \\theta_j\n\\] where \\(x_i\\) is the \\(i\\)-th row of the matrix \\(X\\) and \\(y_i\\) is the \\(i\\)-th element of the vector \\(y\\). The parameter \\(\\lambda\\) is the regularization parameter. The solution of the optimization problem is given by:\n\\[\n\\hat \\theta = (X^TX + \\lambda I_d)^{-1}X^Ty = X^T (X X^T + \\lambda I_n)^{-1}y\n\\]\nwhere \\(I_d\\) and \\(I_n\\) are the identity matrix.\nIn prediction, the target variable is given by: \\[\n\\hat{y}(x^*) = X^T \\hat{\\theta} = \\langle x^*, \\hat{\\theta} \\rangle = \\left\\langle x^*, \\sum_{i=1}^{n} \\alpha_i x_i \\right\\rangle = \\sum_{i=1}^{n} \\alpha_i \\langle x_i, x^* \\rangle\n\\] where \\(\\alpha_i = \\sum_{j=1}^{n} \\theta_j x_{ij}\\). We easily see that the prediction is a linear combination of the scalar product between the test point \\(x^*\\) and the training points \\(x_i\\), we can use the kernel trick to transform the data into a higher-dimensional space where the relationship is linear. The prediction becomes:\n\\[\n\\hat{y}(x^*) = \\sum_{i=1}^{n} \\alpha_i K(x_i, x^*)\n\\]\nwhere \\(K(x_i, x^*)\\) is the kernel function.\n\n\nIn problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinusoïdal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let’s try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features.\n\n\n\nLinear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-fit-the-kernel-ridge-regression-krr",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "In problem which involves a distance between point, it is a common practice to normalize the data. In this notebook, we are going to normalize the data by dividing the time by 60 to convert it from minutes to hours (and have values between 0 and 1). We are going to use the KernelRidge class from the sklearn library to fit the KRR model. We will start by using the Gaussian/RBF kernel which is defined by: \\[\nK(x, x') = \\exp\\left(-\\gamma||x - x'||^2\\right)\n\\] where \\(\\gamma\\) is an hyperparameter.\n\n# Libraries\nimport pandas as pd \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n#!pip install umap-learn\nimport umap.umap_ as umap\nimport numpy as np\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the data\nmcycle_data = pd.read_csv(\"Data/mcycle.csv\")\nmcycle_data[\"times\"] = mcycle_data[\"times\"]/60 \nmcycle_data.describe()\n\n\n\n\n\n\n\n\ntimes\naccel\n\n\n\n\ncount\n133.000000\n133.000000\n\n\nmean\n0.419649\n-25.545865\n\n\nstd\n0.218868\n48.322050\n\n\nmin\n0.040000\n-134.000000\n\n\n25%\n0.260000\n-54.900000\n\n\n50%\n0.390000\n-13.300000\n\n\n75%\n0.580000\n0.000000\n\n\nmax\n0.960000\n75.000000\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX = mcycle_data[[\"times\"]]\ny = mcycle_data[[\"accel\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nSince we have two hyperparameter (\\(\\gamma\\) and \\(\\lambda\\)) to tune, we can use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\nrbf_krr_model = KernelRidge(kernel=\"rbf\")\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"alpha\": grid_eval, \"gamma\": grid_eval}\nrbf_krr_model_cv = GridSearchCV(rbf_krr_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {rbf_krr_model_cv.best_params_}\")\n\nBest parameters by CV : {'alpha': 0.07196856730011521, 'gamma': 35.564803062231285}\n\n\n\nbest_model = KernelRidge(kernel=\"rbf\", alpha=rbf_krr_model_cv.best_params_[\"alpha\"], gamma=rbf_krr_model_cv.best_params_[\"gamma\"])\nbest_model.fit(X_train, y_train)\ny_pred = best_model.predict(X_test)\n\nprint(f\"Root mean square error: {math.sqrt(mean_squared_error(y_test, y_pred)): .2f}\")\n\nRoot mean square error:  22.98\n\n\n\n# Sort X_test and corresponding y_pred values\nsorted_indices = np.argsort(X_test.values.flatten())\nX_test_sorted = X_test.values.flatten()[sorted_indices]\ny_pred_sorted = y_pred[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\")\nplt.plot(X_test_sorted, y_pred_sorted, color=\"blue\", label=\"Predicted values\")\nplt.title(\"Kernel Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-ridge-regression",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Now we are going to use the basic ridge regression to predict the target variable. There is only one hyperparameter to tune which is the regularization parameter \\(\\lambda\\).\n\nridge_model = RidgeCV(alphas=grid_eval).fit(X_train, y_train)\ny_pred_ridge=ridge_model.predict(X_test)\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge)) : .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test, y_pred_ridge, color=\"red\")\nplt.title(\"Ridge Regression\")\n\nRMSE Ridge regression :  46.17\n\n\nText(0.5, 1.0, 'Ridge Regression')\n\n\n\n\n\n\n\n\n\nAs we can see, the solelly use of the ridge regression is not enough to predict the target variable. We can try to transform the data by using a sinusoïdal transformation. If we try a transformation of the covariable \\(x\\) by $ = (x) $ and apply the ridge regression, we have a slightly different problem. But still, the performance of the model is not good.\n\n# create a function that apply transformation to the features\ndef apply_cos(x,j):\n    pi = np.pi\n    return np.sqrt(2)*np.cos(j*pi*x)/(j*pi)\n\n# perform ridge regression with cosinus modification from j = 1 to 10\nX_train_cos = pd.DataFrame()\nX_test_cos = pd.DataFrame()\nfor j in range(1,11):\n    X_train_cos[f\"X{j}\"] = X_train[\"times\"].apply(lambda x: apply_cos(x,j))\n    X_test_cos[f\"X{j}\"] = X_test[\"times\"].apply(lambda x: apply_cos(x,j))\n\n\n# Train the Ridge regression model\nridge_model1 = RidgeCV(alphas=grid_eval).fit(X_train_cos[[\"X1\"]], y_train)\ny_pred_ridge1 = ridge_model1.predict(X_test_cos[[\"X1\"]])\n\nrmse_ridge = math.sqrt(mean_squared_error(y_test, y_pred_ridge1))\nprint(f'RMSE Ridge regression with x tilde: {rmse_ridge:.2f}')\ny_pred_ridge1_sorted = y_pred_ridge1[sorted_indices]\n\nplt.scatter(X_test, y_test, color=\"black\", label=\"True values\", s=50)\nplt.plot(X_test_sorted, y_pred_ridge1_sorted, color=\"red\", label=\"Predicted values (Ridge)\", linewidth=2)\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt.show()\n\nRMSE Ridge regression with x tilde: 45.87\n\n\n\n\n\n\n\n\n\nEven though, the only transformation applied is the cosinus transformation, the RMSE is lower than the RMSE of the simple Ridge Regression. This is due to the fact that the cosinus transformation is able to capture a little bit of the periodicity of the data. Let’s try to use many sinusoidal transformation to see if we can improve the performance of the model. We will fit y using \\(\\tilde{x} = \\left[ \\frac{\\sqrt(2)}{J\\pi}\\cos(J\\pi x)  \\right]_{J=1,\\dots,10}\\)\n\n# New ridge with many transformated features\nridge_model2 = RidgeCV(alphas=grid_eval).fit(X_train_cos, y_train)\ny_pred_ridge2 = ridge_model2.predict(X_test_cos)\ny_pred_ridge2_sorted = y_pred_ridge2[sorted_indices]\nprint( f'RMSE Ridge regression : {math.sqrt(mean_squared_error(y_test, y_pred_ridge2)): .2f}')\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\",label=\"True values\")\nplt.plot(X_test_sorted, y_pred_ridge2_sorted,color=\"red\",label=\"Predicted values (Ridge)\")\nplt.title(\"Ridge Regression\")\nplt.xlabel(\"Input Feature (X)\")\nplt.ylabel(\"Target (y)\")\nplt.legend(loc=\"best\")\nplt\n\nRMSE Ridge regression :  22.96\n\n\n\n\n\n\n\n\n\nWe observe that the more we increase the number of transformations, the more the performance of the model is improved and close to the performance of the kernel ridge regression using the gaussian kernel. The performance of the model is similar to the performance of the kernel ridge regression. This is due to the fact that the kernel ridge regression is equivalent to use an infinite number of transformations on the features. It is hence useful to use the kernel trick when we have a non-linear relationship between the target variable and the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "href": "3A/Apprentisage-stat/Tp2.html#iii.-insigths",
    "title": "Kernel Trick and SVM",
    "section": "",
    "text": "Linear regression with such sinusoidal transformation is equivalent to the kernel ridge regression with a specific kernel : the sobolev kernel. The sobolev kernel is defined by: \\[\nK(x, x') = 1 + B_1(x)B_2(x') + \\frac{1}{2}B_2(|x-x'|) = 1 + B_2(\\frac{|x-x'|}{2}) + B_2(\\frac{x+x'}{2})\n\\]\nwhere \\(B_1 = x - 1/2\\) and \\(B_2 = x^2 - x - 1/6\\).\nUsing the fourier series expansion for \\(x \\in [0,1]\\), we have : \\[\nB_2(x) = \\sum_{k=1}^{\\infty} \\frac{\\cos(2k\\pi x)}{(k\\pi)^2}\n\\]\n\n\n\ndef sobolev_kernel(x,y):\n    def B2(x):\n        return x**2 - x + 1/6\n    \n    def B1(x):\n        return x - 1/2\n    \n    return 1+ B2(abs(x-y)/2) + B2((x+y)/2)\n\n\nkrr_model_sobolev = KernelRidge(kernel=sobolev_kernel)\nparam_grid = {\"alpha\": grid_eval}\ngrid_sobolev = GridSearchCV(krr_model_sobolev, param_grid).fit(X_train,y_train)\ngrid_sobolev.best_params_\n\n{'alpha': 0.07196856730011521}\n\n\n\n# compute rmse\n# best_sobolev_krr = KernelRidge(kernel=sobolev_kernel, alpha=grid_sobolev.best_params_[\"alpha\"])\n# best_sobolev_krr.fit(X_train, y_train)\ny_pred_sobolev = grid_sobolev.predict(X_test)\ny_pred_sobolev_sorted = y_pred_sobolev[sorted_indices]\nprint( f'Root mean square error : {math.sqrt(mean_squared_error(y_test, y_pred_sobolev_sorted)): .2f}')\n\n\n# Plot the results\nplt.scatter(X_test, y_test, color=\"black\")\nplt.plot(X_test_sorted, y_pred_sobolev_sorted,color=\"red\")\nplt.title(\"Sobolev Kernel Ridge Regression\")\n\nRoot mean square error :  61.98\n\n\nText(0.5, 1.0, 'Sobolev Kernel Ridge Regression')"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#i.-toy-dataset",
    "title": "Kernel Trick and SVM",
    "section": "I. Toy dataset",
    "text": "I. Toy dataset\nHere is the toy dataset that we are going to use to illustrate the SVM. The dataset is composed of two features and the target variable is binary. As we can see, the dataset is not linearly separable. We are going to use the SVM with a gaussian kernel to classify the data, and compare it to a classic classifier such as the k-nearest neighbors and the logistic regression.\n\ntwo_moon_data = pd.read_csv(\"Data/DataTwoMoons.csv\",header=None)\ntwo_moon_data.columns = [\"X1\",\"X2\",\"y\"]\n\nplt.scatter(two_moon_data[\"X1\"], two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"Two moons dataset\")\n\nText(0.5, 1.0, 'Two moons dataset')\n\n\n\n\n\n\n\n\n\n\n# Split the data into train and test sample\nX_train, X_test, y_train, y_test = train_test_split(two_moon_data[[\"X1\",\"X2\"]], two_moon_data[\"y\"], test_size=0.2, random_state=42)\n\n\n1. K-nearest neighbors\n\n# KNN with cross validation\n\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1, 50)}\nknn_cv = GridSearchCV(knn, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {knn_cv.best_params_}\")\n\n# Compute the accuracy\ny_pred = knn_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nBest parameters by CV : {'n_neighbors': 1}\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_knn = DecisionBoundaryDisplay.from_estimator(\n    knn_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_knn.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"])\nplt.title(\"KNN Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Logistic regression\n\n# compute logistic regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.91\nConfusion Matrix:\n[[39  5]\n [ 2 34]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92        44\n           1       0.87      0.94      0.91        36\n\n    accuracy                           0.91        80\n   macro avg       0.91      0.92      0.91        80\nweighted avg       0.92      0.91      0.91        80\n\n\n\n\ndisp_log_reg = DecisionBoundaryDisplay.from_estimator(\n    log_reg,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_log_reg.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. SVM\n\ngrid_eval = np.logspace(-2, 4, 50)\nparam_grid = {\"C\": grid_eval, \"gamma\": grid_eval}\nsvm_model = SVC(kernel=\"rbf\")\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train,y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 0.030888435964774818, 'gamma': 3.727593720314938}\n\n\n\n# Compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 1.00\nConfusion Matrix:\n[[44  0]\n [ 0 36]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        44\n           1       1.00      1.00      1.00        36\n\n    accuracy                           1.00        80\n   macro avg       1.00      1.00      1.00        80\nweighted avg       1.00      1.00      1.00        80\n\n\n\n\ndisp_svm = DecisionBoundaryDisplay.from_estimator(\n    svm_model_cv,\n    X_train,\n    response_method=\"predict\",\n    alpha = 0.3\n)\ndisp_svm.ax_.scatter(two_moon_data[\"X1\"],two_moon_data[\"X2\"], c=two_moon_data[\"y\"], edgecolor=\"k\")\nplt.title(\"SVM Decision Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Conclusion\nAs we can see, the SVM with the gaussian kernel is able to classify the data with a good accuracy. The SVM is able to capture the non-linear relationship between the target variable and the features.\nThe logistic regression, in this case, is not able to classify the data because the relationship between the target variable and the features is non-linear.\nThe k-nearest neighbors is able to classify the data with a performance similar to the SVM. The SVM and the KNN are a good choice when we have a non-linear relationship between the target variable and the features.\nWhenever we have a classification problem, it is hence always useful to try the SVM and the KNN."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "href": "3A/Apprentisage-stat/Tp2.html#ii.-image-dataset",
    "title": "Kernel Trick and SVM",
    "section": "II. Image dataset",
    "text": "II. Image dataset\nThe SVM is also useful for image classification. In this part, we are going to use the famous MNIST dataset to classify the images. The MNIST dataset is composed of 20 000 images (10 000 in the training dataset, and 10 000 also in the test dataset) of handwritten digits from 0 to 9. Each image is a resolution 28x28 pixels that is represented by a matrix of shape (28, 28), with each element being the pixel intensity (values from 0 to 255). We are going to use the SVM with the gaussian kernel to classify the images.\nWe will start by normalizing the data to ensure that all the features contribute equally, and then use the GridSearchCV function from scikit-learn to find the best hyperparameter.\n\ndata_train = pd.read_csv(\"Data/mnist_train_small.csv\")\ndata_test = pd.read_csv(\"Data/mnist_test.csv\")\n\nprint(\"Description of train dataset : \\n\")\ndata_train.iloc[:,1:].describe()\ndata_train[\"label\"].value_counts()\n\nDescription of train dataset : \n\n\n\nlabel\n8    113\n0    111\n1    110\n7    106\n9    100\n2     99\n4     95\n5     93\n6     90\n3     83\nName: count, dtype: int64\n\n\n\n# normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train  = scaler.fit_transform(data_train.iloc[:, 1:])\ny_train = data_train[\"label\"]\nX_test  = scaler.transform(data_test.iloc[:, 1:])\ny_test = data_test[\"label\"]\n\nAs we can see from the umap plot, which is a dimensionality reduction technique, the data is not always linearly separable. We are going to use the SVM with the gaussian kernel to classify the images.\n\n# visualize the data with UMAP\nreducer = umap.UMAP(random_state=42)\nembedding = reducer.fit_transform(X_train)\n\n\nplt.scatter(embedding[:, 0], embedding[:, 1], c=data_train[\"label\"], cmap='Spectral', s=1)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('UMAP projection of the MNIST dataset')\n\nText(0.5, 1.0, 'UMAP projection of the MNIST dataset')\n\n\n\n\n\n\n\n\n\n\nsvm_model = SVC(kernel=\"rbf\")\nfrom itertools import product\n\ngrid_eval_C = [c * factor for c, factor in product([0.1, 1, 10], [1, 5])]\ngrid_eval_gamma = [gamma * factor for gamma, factor in product([10**-3, 10**-2, 10**-1], [1, 5])]\n\n\nparam_grid = {\"C\": grid_eval_C, \"gamma\": grid_eval_gamma}\nsvm_model_cv = GridSearchCV(svm_model, param_grid).fit(X_train, y_train)\nprint(f\"Best parameters by CV : {svm_model_cv.best_params_}\")\n\nBest parameters by CV : {'C': 5, 'gamma': 0.001}\n\n\nAs we can see, the model performs well with an accuracy of 0.88 . As expected from the umap visualization, the model is able to separate the classes well, however there are some errors in the classification. The confusion matrix shows that the model has some difficulty to distinguish between some digits such as 4 and 9, 3.\nThe SVM is a good choice for image classification, however, the model is not able to capture the complexity of the data. In this case, we can use a deep learning model such as the convolutional neural network (CNN) which is able to capture the complexity of the data.\n\n# compute the accuracy\ny_pred = svm_model_cv.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix to see detailed classification performance\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Classification report for detailed metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.88\nConfusion Matrix:\n[[ 931    0   20    1    1   12    9    2    4    0]\n [   0 1121    4    2    0    1    6    0    1    0]\n [  14    6  949   20    7    2    6   10   17    1]\n [   6    2   75  829    2   29    3   30   25    9]\n [   3    5   32    0  881    3    9    4    5   40]\n [   4    3   75   31    5  718   20    9   16   11]\n [  20    5  101    0    8   11  808    0    5    0]\n [   1   12   61    1   10    2    0  913    0   28]\n [   8   14   37   13   11   23    5   18  829   16]\n [   9    6   30   12   37    4    0   57    1  853]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.95      0.94       980\n           1       0.95      0.99      0.97      1135\n           2       0.69      0.92      0.79      1032\n           3       0.91      0.82      0.86      1010\n           4       0.92      0.90      0.91       982\n           5       0.89      0.80      0.85       892\n           6       0.93      0.84      0.89       958\n           7       0.88      0.89      0.88      1028\n           8       0.92      0.85      0.88       974\n           9       0.89      0.85      0.87      1009\n\n    accuracy                           0.88     10000\n   macro avg       0.89      0.88      0.88     10000\nweighted avg       0.89      0.88      0.88     10000\n\n\n\n\n# plot ROC CURVE\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ny_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_score = svm_model_cv.decision_function(X_test)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(10):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\ncolors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\nfor i, color in zip(range(10), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC ={roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html",
    "href": "2A/App_sup/reg_lin.html",
    "title": "La régression linéaire",
    "section": "",
    "text": "La régression linéaire est une méthode d’apprentissage supervisé qui vise à évaluer, lorsqu’il existe, la relation linéaire entre une variable d’intérêt et des variables explicatives.\nPour un ensemble \\((y_i,x_i)\\) de données constitué de n échantillons iid (indépendant et identiquement distribué), le modèle de regression linéaire s’écrit comme suit :\n\\[\\begin{align}\ny_i&= \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\xi_i\\\\\n&= X_i \\beta + \\xi_i\n\\end{align}\\]\noù \\(y_i\\) est la variable cible, \\(x_{i1}, \\dots, x_{ip}\\) sont les variables explicatives et \\(\\xi_i\\) est l’erreur, l’information que les autres variables explicatives ne donnent pas.\nL’hypothèse fondamentale de la régression linéaire est l’existence d’une relation linéaire entre la variable cible et les variables explicatives. Pour s’assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire (à l’aide de visualisation ou de tests- spearman, pearson, etc.)\nL’hypothèse de rang plein est la seconde plus grande hypothèse, elle stipule que les variables explicatives ne soient pas corrélées entre elles. Cette condition est nécessaire pour garantir l’unicité des estimations des paramètres du modèle et ainsi l’identifiabilité du modèle étudié\nPar ailleurs pour que les estimations des paramètres du modèle linéaire soient fiables, les erreurs du modèle, représentées par \\(\\xi_i\\), doivent répondre à plusieurs critères :\n\nErreurs centrées : La moyenne attendue des erreurs doit être nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\nHomoscédasticité : La variance des erreurs doit être constante pour toutes les observations, exprimée par \\(V[\\xi_i] = \\sigma^2\\). Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\nDécorrélation des erreurs : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d’erreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides.\n\nIl est courant d’observer une hypothèse supplémentaire sur la loi des erreurs. En effet, les erreurs sont souvent supposées suivre une loi normale, c’est à dire que \\(\\xi_i \\sim N(0, \\sigma^2)\\). Celà nous permet de faire des inférences sur les paramètres du modèle et de construire des intervalles de confiance.\n\n\nToutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus.\n\n\n\nDans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "href": "2A/App_sup/reg_lin.html#estimation-des-paramètres",
    "title": "La régression linéaire",
    "section": "",
    "text": "Toutes les hypothèses étant respectées, et sous reserve qu’il n’y a pas de multicolinéarité entre les variables explicatives du modèles i.e. \\(X^T X\\) est inversible(l’hypothèse de rang plein est respectée), l’estimateur \\(\\hat \\beta\\) de \\(\\beta\\) obtenus par moindre carré ordinaire est donné par la formule suivante :\n\\[\n\\hat \\beta = (X^T X)^{-1} X^T y\n\\]\nDe ce fait, nous pouvons calculer la variance de cet estimateur : \\[\nVAR(\\hat \\beta) = \\sigma^2 (X^T X)^{-1}\n\\]\nD’après le théorème de Gauss-Markov, l’estimateur \\(\\hat \\beta\\) est le meilleur estimateur linéaire non biaisé des paramètres du modèle. En effet, il est l’estimateur avec la plus petite variance, parmi les estimateurs linéaires sans biais qui existent. Cet estimateur est ainsi appelé BLUE (Best Linear Unbiased Estimator).\nLorsque les erreurs sont supposées suivre une loi normale, l’estimateur \\(\\hat \\beta\\) est également l’estimateur du maximum de vraisemblance des paramètres du modèle et suit une loi normale \\(\\hat \\beta \\sim N(\\beta, \\sigma^2 (X^T X)^{-1})\\).\nL’estimateur de la variance des erreurs \\(\\sigma^2\\) est donné par :\n\\[\n\\hat \\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\hat \\xi_i^2 = \\frac{SCR}{n-p}\n\\] SCR = somme des carrés des résidus."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle",
    "title": "La régression linéaire",
    "section": "",
    "text": "Dans l’optique de mesurer la qualité du modèle, plusieurs métriques sont utilisées : le R², le R² ajusté, l’erreur quadratique moyenne (MSE), des critères d’informations (AIC, BIC) etc.\n\n\nLe coefficient de détermination \\(R^2\\) est une mesure de la proportion de la variance de la variable cible qui est expliquée par le modèle. Il est défini comme suit :\n\\[\nR^2 = 1 - \\frac{SCR}{SCT}\n\\] avec SCT qui est la somme des carrés totaux (\\(SCT = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\)) et SCR qui est la somme des carrés des résidus.\nNéanmoins, le \\(R^2\\) n’est pas une mesure parfaite de la qualité du modèle. En effet, il augmente avec le nombre de variables explicatives, même si ces variables n’ont pas de lien avec la variable cible. Pour pallier à ce problème, le \\(R^2\\) ajusté est utilisé. Il est défini comme suit :\n\\[\nR^2_a = 1 - \\frac{SCR/(n-p)}{SCT/(n-1)}\n\\]\n\n\n\nLes critères AIC et BIC sont des critères d’information qui servent à mesurer l’attache du modèle aux données que nous avons ajustés avec une pénalité lié soit aux nombres de variables inclus dans le modèles et/ou la taille de l’échantillon étudié. De fait, plus l’AIC ou le BIC est faible, meilleur est le modèle, car cela signifie qu’il a le modèle choisie a une probabilité plus élevée d’être correct et une complexité plus faible.\nMathématiquement,\n\\[\n\\text{AIC} = - 2 \\log (l(\\hat{\\theta})) - 2\\times p, \\quad p=|\\hat \\theta|\n\\]\n\\[\n\\text{BIC} = - 2 \\log (l(\\hat{\\theta})) - \\log (n) \\times p\n\\]\nMaintenant que ces équations sont visibles, nous constatons que le BIC est un critère plus parcimonieux que le critère AIC en raison de la pénalisation qui est plus élevé lorsque \\(\\log(n) \\geq 2\\), i.e il y a environ 8 observations dans l’échantillon sélectionné."
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#simulation-des-données",
    "href": "2A/App_sup/reg_lin.html#simulation-des-données",
    "title": "La régression linéaire",
    "section": "1. Simulation des données",
    "text": "1. Simulation des données\nPour évaluer l’intérêt de la regréssion linéaire, nous allons simuler un échantillon de taille n=200, où la variable cible Y est une fonction linéaire de la variable explicative X. La vraie relation est donnée par \\(Y = 2 + 3X + \\epsilon\\), où \\(\\epsilon \\sim N(0, 1.6)\\). De fait le modèle linéaire est adéquat.\n\nset.seed(314)\nn&lt;-200\nX&lt;-runif(n,0,10)\n\nsigma2&lt;-1.6\nepsilon&lt;-rnorm(n,0,sigma2)\nY&lt;- 2 + 3*X + epsilon\n\nsim1&lt;-data.frame(X,Y)\n\nplot(sim1$X,sim1$Y)\n\n\n\n\n\n\n\n\nEn ajustant un modèle linéaire simple à nos données, nous obtenons une estimation des paramètres \\(\\hat \\beta_0 = 1.92\\) et \\(\\hat \\beta_1 = 2.98\\). Les erreurs du modèle suivent une loi normale avec une variance \\(\\hat \\sigma^2 = 1.45\\).\n\nsim1_lm&lt;-lm(Y~X,data=sim1)\nsummary(sim1_lm)\n\n\nCall:\nlm(formula = Y ~ X, data = sim1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6505 -0.9901  0.0830  0.9899  3.9100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.92097    0.20774   9.247   &lt;2e-16 ***\nX            2.97748    0.03582  83.117   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 198 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.972 \nF-statistic:  6908 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "href": "2A/App_sup/reg_lin.html#evaluation-du-modèle-1",
    "title": "La régression linéaire",
    "section": "2. Evaluation du modèle",
    "text": "2. Evaluation du modèle\n\n2.1. Hypothèses sur les erreurs et l’existence d’une relation linéaire\nPour évaluer la qualité du modèle, nous allons tracer les résidus studentisés en fonction des valeurs ajustées. Les résidus studentisés sont les résidus divisés par l’écart-type des erreurs.\n\nplot(sim1_lm$fitted.values,rstudent(sim1_lm),xlab=\"Valeurs ajustées\", ylab=\"Résidus studentisés\")\nabline(h=0,lty=2)\n\n\n\n\n\n\n\n\nLe plot ci dessus nous montre que lorsque les réponses prédites par le modèle (fitted values) augmentent, les résidus restent globalement uniformément distribués de part et d’autre de 0. Cela montre, qu’en moyenne, la droite de régression, est bien adaptée aux données, et donc que l’hypothèse de linéarité est acceptable.\nSi l’on observait une forme de trompette, celà reviendrait à soulever une question sur l’hétéroscédascité des résidus, tandis qu’une forme de banane revèle plutôt une relation de non-linéarité.\nLorsque le nuage de point n’a pas de structure particulière, a priori l’hypothèse d’homoscédascticité n’est pas remise en question, comme cela semble être le cas ici. Attention : ces principes peuvent parfois être mis en défaut et il vaut toujours mieux réaliser plusieurs contrôles différents.\nPour vérifier l’hypothèse d’homoscédasticité, nouspouvons également utiliser le test de Breusch-Pagan. Ce test est basé sur la régression des carrés des résidus sur les variables explicatives. Si le test est significatif, l’hypothèse d’homoscédasticité est rejetée.\n\n#library(leaps)\nlibrary(car)\n\nLe chargement a nécessité le package : carData\n\nncvTest(sim1_lm)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.0003134709, Df = 1, p = 0.98587\n\n\nPour tester l’hypothèse de non corrélation des résidus, nous pouvons utiliser le test de Durbin-Watson. Ce test est basé sur l’autocorrélation des résidus. Si le test est significatif, l’hypothèse de non corrélation des résidus est rejetée.\n\ndurbinWatsonTest(sim1_lm)\n\n lag Autocorrelation D-W Statistic p-value\n   1      0.02199557      1.939509    0.63\n Alternative hypothesis: rho != 0\n\n\nEn ce qui concerne l’hypothèse de normalité des résidus, nous pouvons utiliser le test de Shapiro-Wilk. Ce test est basé sur la comparaison des résidus avec une loi normale. Si le test est significatif, l’hypothèse de normalité des résidus est rejetée.\n\nshapiro.test(sim1_lm$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sim1_lm$residuals\nW = 0.99131, p-value = 0.2749\n\n\nLa p-valeur du test de Shapiro-Wilk est de 0.27, ce qui signifie que l’hypothèse de normalité des résidus n’est pas rejetée.\n\n\n2.2. Qualité du modèle\nPour évaluer la qualité du modèle, nous allons calculer le coefficient de détermination \\(R^2\\) et le \\(R^2\\) ajusté.\n\n(R2&lt;-summary(sim1_lm)$r.squared)\n\n[1] 0.9721382\n\n(R2_adj&lt;-summary(sim1_lm)$adj.r.squared)\n\n[1] 0.9719975\n\n(AIC(sim1_lm))\n\n[1] 719.3729\n\n(BIC(sim1_lm))\n\n[1] 729.2678\n\n\nNous obtenons un \\(R^2\\) et un \\(R^2\\) ajusté de 0.97. Cela signifie que 97% de la variance de la variable cible est expliquée par le modèle. Notre modèle de régression linéaire est bien ajusté à nos données."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html",
    "href": "3A/Apprentisage-stat/Tp1.html",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It’s the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let’s start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10² and 10⁴ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : −λ(X^T X + λId) (−1) θ\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]\n\n\n\nIf we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don’t have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "The ridge regression is a variant of the linear regression that is used to prevent multicolinearity coming from the covariables in the dataset and thus prevent overfitting. In fact, when the features are correlated, the matrix \\(X^TX\\) is not invertible and the least square solution is not unique. Hence, the ridge regression is used to stabilize the solution by adding a L2 penalty term to the loss function. It’s the reason wwhy the ridge regression is also called a L2 regularization.\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_2^2 \\right\\}\n\\]\nThe penalty term is controlled by a hyperparameter \\(\\lambda\\). It has to be choosed wisely (using a cross-validation method) because it balances the trade-off between the bias and the variance of the model. We are going to see in this activity how the bias and the variance of the ridge regression is affected by the hyperparameter \\(\\lambda\\).\nThe bias of an estimator is defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. In the ridge regression, the bias of the ridge estimator is given by the formula:\n\\[\\mathcal{B}(\\theta ^*) = \\lambda (X^{T}X+ \\lambda I_d)^{-1} \\theta^*\\]\nFor the variance, it is given by:\n\\[\\mathcal{V}(\\theta ^*) = \\sigma^2 (X^{T}X+ \\lambda I_d)^{-2} X^{T}X \\]\nwhere \\(\\sigma^2\\) is the variance of the noise in the data.\nHint to prove the bias formula : factorize by \\(\\lambda ( X^TX + \\lambda I_d)^{-1}\\)\nIn this activity, we will numerically check the formula for the bias and the variance by generating random data and calculating the squared bias of the sample mean. Let’s start by create a function to generate the data with a noise \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\n\n#Q1 : Function that generates data from a linear model\nimport numpy as np\n\ndef generate_data(X, theta, sigma_squared) :\n    n = X.shape[0] # Extract the length of data sample\n    noise = np.random.normal(loc=0,scale=np.sqrt(sigma_squared),size=n) # generate the error term /!\\ scale = sd\n    y = X.dot(theta) + noise # Y = X+theta + noise\n    return y\n\nOur goal is now to illustrate the squared bias of each coefficient as a function of \\(\\lambda\\) when it varies between 10² and 10⁴ (in the log-domain). We will then generate a dataset with 100 samples and 10 features and calculate the squared bias of the coefficients and the variance for each value of \\(\\lambda\\). The features will be generated from a normal distribution (not mandatory) and the true coefficients are fixed. We voluntarily put to 0 some coefficients to see the effect of the regularization.\n\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ntheta = [10,1,5,2,8,0,0,0,0,0] # Define the true theta\nlambda_grid = np.logspace(-2, 4, 50) # Define the grid of lambda values\nn, d = 100, 10 # Dimension of dataset\nX = np.random.normal(loc=0, scale=1, size=(n, d)) #features\n\nTo compute numerically the bias and the variance of the ridge regression, we will generation 200 datasets. We will then, for each \\(\\lambda\\), store the mean of each coefficients (in order to compute the numerical bias) and the standard deviation (to compute the numerical variance).\n\nn_sim=1000\nestimated_theta = np.zeros((n_sim,d)) #store the estimated theta for each simulation\nbias_squared = np.zeros((len(lambda_grid),d)) #store the squared bias for each lambda\nvariance = np.zeros((len(lambda_grid),d)) #store the variance for each lambda\n\n# Iterate over lambda values\nfor idx,alpha in enumerate(lambda_grid):\n    for i in range(n_sim): #start simulation\n        y = generate_data(X, theta, 1)\n        fit = Ridge(alpha=alpha,fit_intercept=False).fit(X,y) #we do not want the model to fit an intercept\n        estimated_theta[i,:]= fit.coef_\n    bias_squared[idx,:]=(np.mean(estimated_theta,axis=0)-theta)**2\n    variance[idx,:] = np.std(estimated_theta,axis=0)**2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_squared[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\n# Show the plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#Plot squared theorical bias : −λ(X^T X + λId) (−1) θ\nbias_theoretical = np.zeros((len(lambda_grid), d))\nvariance_theorical = np.zeros((len(lambda_grid), d))\n\nfor idx, alpha in enumerate(lambda_grid):\n    bias_theoretical[idx, :] = (-alpha * np.linalg.inv(X.T @ X + alpha * np.eye(d)) @ theta)**2\n    variance_theorical[idx,:] = np.diag((1 * np.linalg.inv(alpha * np.eye(d) + X.T @ X)**2 @ (X.T @ X)))\n    \nfig,axes = plt.subplots(1,2,figsize=(8,4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, bias_theoretical[:, i], label=f'theta {i+1}')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ (log scale)')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Ridge Regression Empirical Squared Bias')\naxes[0].legend(loc='best')\n\n# Plot variance on the second subplot\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_theorical[:, i], label=f'theta {i+1}')\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ (log scale)')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Ridge Regression Empirical Variance')\n\nText(0.5, 1.0, 'Ridge Regression Empirical Variance')\n\n\n\n\n\n\n\n\n\nAs we can see, both in the numerical and theoretical bias, the bias of the coefficients is increasing with the increase of \\(\\lambda\\). This is due to the fact that the regularization term is increasing and the coefficients are getting closer to 0. The variance is also decreasing with the increase of \\(\\lambda\\) but it is not as significant as the bias. This is due to the fact that the variance is not directly affected by the regularization term.\nOne can have an idea of this behaviour by looking at the formula of the bias and the variance of the ridge regression, using the gram matrix \\(\\frac{X^TX}{n}\\). In face, using the large law of numbers, we can see that the bias is proportional to \\(\\lambda\\) and the variance is inversely proportional to \\(\\lambda\\).\nProof :\n\\(\\frac{X^TX}{n} \\rightarrow E(X^TX) = V(X)\\), when \\(n \\rightarrow \\infty\\), and \\(V(X)\\) is the covariance matrix of the features. When X is centered and normalized, \\(V(X) = I_d\\). Using this in the formula of the bias and the variance, we get:\n\\[\\mathcal{B}(\\theta ^*) = \\frac{-\\lambda}{n+\\lambda} \\theta^*\\]\n\\[\\mathcal{V}(\\theta ^*) = \\frac{\\sigma^2}{(n+\\lambda)^2} I_d\\]"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "",
    "text": "If we are interessed in the variaton of the bias and the variance of the lasso regression which is another variants of the linear regression that is mainly used to perform feature selection, we don’t have a specific formula defined. However, we can numerically check the bias of the lasso regression by generating random data and calculating the squared bias of the sample mean, as we did for the ridge regression. The lasso regression is a L1 regularization and the loss function is defined as:\n\\[\n\\theta ^* = argmin_{\\theta} \\left\\{ || Y - X \\theta||_2 + \\lambda ||\\theta||_1 \\right\\}\n\\]\n\nfrom sklearn.linear_model import Lasso\n\nlasso_bias_squared = np.zeros((len(lambda_grid), d))\nvariance_lasso = np.zeros((len(lambda_grid), d))\nestimated_theta_lasso = np.zeros((n_sim, d))\n\n# Iterate over lambda values\nfor idx, alpha in enumerate(lambda_grid):\n        for i in range(n_sim) :\n            y = generate_data(X, theta, 1)\n            fit = Lasso(alpha=alpha,fit_intercept=False).fit(X,y)\n            estimated_theta_lasso[i,:]= fit.coef_\n        lasso_bias_squared[idx, :] = (np.mean(estimated_theta_lasso,axis=0)- theta) ** 2\n        variance_lasso[idx, :] = np.std(estimated_theta_lasso, axis=0) ** 2\n\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i in range(d):\n    axes[0].plot(lambda_grid, lasso_bias_squared[:, i],label=f'theta {i+1}')\n\naxes[0].set_xscale('log')\naxes[0].set_xlabel('λ')\naxes[0].set_ylabel('Squared Bias')\naxes[0].set_title('Lasso Empirical Squared Bias')\naxes[0].legend(loc='best')\n\nfor i in range(d):\n    axes[1].plot(lambda_grid, variance_lasso[:, i],label=f'theta {i+1}')\n\naxes[1].set_xscale('log')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('Variance')\naxes[1].set_title('Lasso Empirical Variance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#ridge-regression-1",
    "title": "Ridge regression vs. Lasso regression",
    "section": "1. Ridge regression",
    "text": "1. Ridge regression\nOn training samples, we will try to fit a linear model using Ridge regression by choosing the regularization parameter \\(\\lambda\\) by cross-validation. We will use the function RidgeCV from the sklearn library to perform the cross-validation. Since we didn’t center the covariables, we will set the parameter fit_intercept to True in order to include an intercept in the model.\nBy default, the function that performs the cross validation in ridge regression performs \"leave-one-out\" cross-validation. In fact, leave-one-out cross-validation is a special case of k-fold cross-validation where k is equal to the number of samples. It is computationally expensive, but it is useful for small datasets. However, in ridge regression can be useful since the formula of shermann-morrison-woodbury can be used in order to use the estimator of a single ridge regession in other to compute the estimator of the leave-one-out cross-validation.\n\nfrom sklearn.linear_model import RidgeCV\n\nlambda_grid = np.logspace(-2, 4, 50)\nridge_cv = RidgeCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train) #to perform cross validation\n\nWe might interested in visualizing the path of the coefficients as a function of the regularization parameter λ. This is called regularization path. We can do this by fitting the model for different values of λ and store the coefficients.\n\n# plot the coefficients as a function of lambda\ncoefs = []\nfor a in lambda_grid:\n    ridge = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n    coefs.append(ridge.coef_)\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.axvline(x=ridge_cv.alpha_, color='r', linestyle='--', label=f'λ = {ridge_cv.alpha_:.2f}')\nplt.title(\"Ridge path\")\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWe can hereby see that the ridge regression does not really help to select the 10 relevant variables by shrinking the coefficients of the irrelevant variables.\nRidge regression tends to favor a model with a higher number of parameters, as it shrinks less important coefficients but keeps them in the model. This is why it is important to choose the regularization parameter \\(\\lambda\\) wisely. However, it includes all the variables in the model, with reduced but non-zero coefficients.\n In our case, it still gives an indication on the 10 variables that were relevant in the initial dataset before the contamination, but is clearly not the best method to select the relevant variables. \n\na. Check on the intercept value of the model using lambda found by cross validation\n\nprint(f'Intercept value : {ridge_cv.intercept_}')\n\nIntercept value : 152.28937186306015\n\n\nThe intercept value of the model is 152.29, which means that the model predicts a value of 152.29 for the response variables when all the features are zero. It can be interpreted as the base value of the model. Taking in account the context of the dataset, we can say that the patients used in the dataset have a score of 152.29 (which might be quite high or not - depending on the scale) of having diabetes independently of the features."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "href": "3A/Apprentisage-stat/Tp1.html#lasso-regression-1",
    "title": "Ridge regression vs. Lasso regression",
    "section": "2. Lasso regression",
    "text": "2. Lasso regression\nWe will now use the lasso regression to check if it can help use to select the most important variables. We will use the same lambda grid as before and also perform a cross validation. It is important to perform a cross-validation in order to choose the best value of the regularization parameter \\(\\lambda\\) as we have demonstrated in the first activity. For the lasso regression, we will use the function LassoCV from the sklearn library. By default, the function uses the coordinate descent algorithm to fit the model. It is a very efficient algorithm to solve the lasso problem because of the non-smoothness of the L1 norm.\n\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=lambda_grid,fit_intercept=True).fit(X_train, y_train)\n\n\n#LASSO PATH\ncoefs_lasso = []\nfor a in lambda_grid:\n    lasso = Lasso(alpha=a, fit_intercept=True)\n    lasso.fit(X, y)\n    coefs_lasso.append(lasso.coef_)\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_grid, coefs_lasso)\nplt.xscale('log')\nplt.xlabel('λ')\nplt.ylabel('Coefficients')\nplt.title(\"Lasso path\")\nplt.axvline(x=lasso_cv.alpha_, color='r', linestyle='--', label=f'λ = {lasso_cv.alpha_:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lasso regression, we can see that the coefficients of the irrelevant variables are set to zero. This is why the lasso regression is a good method to perform feature selection. Using the default value of the regularization parameter \\(\\lambda\\) given by cross-validation, we can see that the lasso regression is able to select the 6 relevant variables. However, by changing the value of \\(\\lambda\\), we can select more or less variables.\n    # check number of variables selected\n    np.sum(lasso_cv.coef_ != 0)\n\na. Check on the intercept value of the model using lambda found by cross validation\nWe get approximatively the same value for the intercept as the one obtained with Ridge regression.\n\n# check value of intercept\nprint(f'Intercept value : {lasso_cv.intercept_}')\n\nIntercept value : 151.95282341561403"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "href": "3A/Apprentisage-stat/Tp1.html#quality-of-the-models-ridge-regression-vs-lasso-regression",
    "title": "Ridge regression vs. Lasso regression",
    "section": "3. Quality of the models (ridge regression vs lasso regression)",
    "text": "3. Quality of the models (ridge regression vs lasso regression)\nIn linear regression, we evaluate the quality of the model using the quadratic loss function. The quadratic risk is the expected value of the square of the difference between the true value and the predicted value. The mean squared error is then given by the formula:\n\\[\\mathcal{R}(\\hat\\theta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat g(x_i) \\right) ^2\\]\nwhere \\(\\hat g(x)\\) is the predicted value of the output variable y given the input variable x, \\(\\hat g(x) =\\hat  \\theta_0 + \\sum_{j=0}^d \\hat \\theta_j x_j\\).\n\nfrom sklearn.metrics import mean_squared_error\n\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f'Mean Squared Error for Lasso: {mse_lasso:.2f}')\n\ny_pred_ridge = ridge_cv.predict(X_test)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f'Mean Squared Error for Ridge: {mse_ridge:.2f}')\n\nMean Squared Error for Lasso: 2869.43\nMean Squared Error for Ridge: 2923.54\n\n\nAs we can see, the MSE of the lasso regression is less than the error of the ridge regression. This is because the lasso regression is more efficient in selecting the relevant variables. The ridge regression is more efficient for numerical stability and for multicollinearity problem in the dataset, but it does not perform variable selection.\nStill, the MSE of both models are quite high, it might be due many facts such as the response variable is not linearly dependent on the features or that the features are not relevant to predict the response variable. We did not also scale the features nor the response variable, which might affect the performance of the model."
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html",
    "href": "3A/Apprentisage-stat/Tp3.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "AdaBoost is a popular boosting algorithm that is used to boost the performance of decision trees on binary classification problems. It works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. The predictions of the weak learners are then combined through a weighted majority vote to make the final prediction.\nHence, for M weak learners, the final prediction is given by \\(g(x) = \\sum_{m=1}^{M} \\alpha_m g_m(x)\\) where \\(g_m(x)\\) is the m-th weak learner and \\(\\alpha_m\\) is the weight associated with the m-th weak learner. The optimisation problem of AdaBoost is given by:\n\\[ \\underset{\\alpha_m, g_m \\, (m=1,\\dots,M)}{\\arg \\min} \\sum_{i=1}^{N} L\\left(y_i, \\sum_{m=1}^{M} \\alpha_m g_m(x_i)\\right) \\]\nSince, this problem is difficult to solve, AdaBoost uses a forward stagewise additive modeling approach, with the loss function \\(l(y,f(x))=\\exp(-yf(x))\\), \\(y \\in \\{-1,+1\\}\\). It adds one weak learner at a time, and at each iteration, it solves the following optimization problem :\n\n\nInitialize the observation weights \\(w_i^{(1)} = 1/n\\) for \\(i=1,\\dots,n\\)\n\n\nFor m=1 to M:\n\n\n\nFit a weak learner \\(g_m(x)\\) to the training data using weights \\(w_i^{(m)}\\)\n\n\nCompute the error rate \\(err_m = \\sum_{i=1}^{N} w_i^{m} 1(y_i \\neq g_m(x_i))\\) where \\(I\\) is the indicator function\n\n\nCompute the weight \\(\\alpha_m = \\frac{1}{2} \\log \\left(\\frac{1-err_m}{err_m}\\right)\\)\n\n\nUpdate the weights \\(w_i^{(m+1)} = w_i^{(m)} \\exp\\left(\\alpha_m 1(y_i \\neq g_m(x_i))\\right)\\)\n\n\n\nIn this activity, we will implement the SAMME algorithm. SAMME stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. It is a boosting algorithm that is used to boost the performance of decision trees on multi-class classification problems. It is a generalization of the AdaBoost algorithm to multi-class classification problems.\nTo inspect how the errors and the weights vary with the number of iterations, we will the function make_gaussian_quantiles from sklearn. This function generates a multi-dimensional standard normale distribution with a given number of samples \\(n\\) per class \\(K\\). We will generate a dataset of size \\(n=2000\\) with \\(K=3\\) classes and \\(d=10\\) features. We will then train a SAMME classifier on this dataset and plot the errors and the weights as a function of the number of iterations.\n\n# import make_gaussian_quantiles\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_gaussian_quantiles;\nfrom sklearn.model_selection import train_test_split\n\n# Generate the dataset\nX, y = make_gaussian_quantiles(n_samples=2000, n_features=10, n_classes=3)\n\n# Split the dataset into a training and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.49\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\nAccuracy:  0.7425\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "href": "3A/Apprentisage-stat/Tp3.html#i.-decision-tree",
    "title": "Gradient boosting",
    "section": "",
    "text": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Fit normal decision tree\nmodel_tree = DecisionTreeClassifier(max_leaf_nodes=8).fit(X_train, y_train)\ny_pred_tree = model_tree.predict(X_test)\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"Accuracy: \", accuracy_tree)\n\ncm_tree = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm_tree, annot=True)\nplt.title('Decision Tree confusion matrix')\nplt.show()\n\nAccuracy:  0.49"
  },
  {
    "objectID": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "href": "3A/Apprentisage-stat/Tp3.html#ii.-samme-algorithm-multi-class-adaboost-with-decision-treess",
    "title": "Gradient boosting",
    "section": "",
    "text": "# Run adaboost\n\n\n# Create and fit an AdaBoosted decision tree\nmodel_adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=8),\n                         algorithm=\"SAMME\",\n                         n_estimators=300).fit(X_train, y_train)\n\n# Predict the test set\ny_pred = model_adaboost.predict(X_test)\n\n# Compute the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: \", accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n#plot with seaborn\nsns.heatmap(cm, annot=True)\nplt.title('Decision Tree boosting Confusion matrix')\nplt.show()\n\n# run adaboost for decision tree with max leaves = 8 \n\nAccuracy:  0.7425\n\n\n\n\n\n\n\n\n\nAs we can see, the SAMME algorithm performs well on the dataset, than a single decision tree. In fact, we have an accuracy of 76% for the SAMME algorithm, while we have an accuracy of 52% for a single decision tree. If we increase the number of iterations, we can see that the accuracy of the SAMME algorithm increases, while the accuracy of the decision tree remains the same.\n\n# use of staged_predict\naccuracy = []\nfor y_pred in model_adaboost.staged_predict(X_test):\n    accuracy.append(accuracy_score(y_test, y_pred))\n\nplt.plot(range(1, 301), accuracy)\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy')\nplt.title('Accuracy of AdaBoost as a function of the number of trees')\nplt.show()\n\n\n\n\n\n\n\n\nRegarding the weights and errors at each iteration, we observe that the weight decreases with the number of iterations, while the error increases. This is due to the fact that at each iteration, the algorithm focuses on the difficult instances to classify. Hence, the weight of the weak learner increases, while the error increases.\n\n# Visualize how the weights and errors change with the number of trees\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, 301), model_adaboost.estimator_weights_,color='green')\nplt.title('Weights of the trees')\nplt.xlabel('Number of trees')\nplt.ylabel('Weight')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, 301), model_adaboost.estimator_errors_,color='orange')\nplt.xlabel('Number of trees')\nplt.ylabel('Error')\nplt.title('Errors of the trees')\nplt.show()"
  },
  {
    "objectID": "3A/ISR.html",
    "href": "3A/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "3A/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "3A/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise.\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "3A/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "3A/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL’ISR présente de multiples avantages tant pour les investisseurs que pour les entreprises engagées dans cette démarche. Ces avantages reflètent l’évolution des attentes sociétales et la reconnaissance croissante de l’importance de la durabilité et de l’éthique dans le monde des affaires. J’en ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement améliorer la réputation d’une entreprise. Elle témoigne de son engagement envers des pratiques durables et éthiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financière : De nombreuses études démontrent que les entreprises avec une notation ESG élevée tendent de meilleures performances financièrement sur le long terme. Cela s’explique par une meilleure anticipation des risques, une gestion plus efficace et une capacité à saisir les opportunités de marché liées à la durabilité.\nMeilleure gestion des risques : L’adoption de pratiques ESG solides permet aux entreprises de mieux identifier et gérer les risques, qu’ils soient climatiques, sociaux ou de marché.\nMeilleure attractivité pour les investisseurs : En démontrant un engagement clair envers la durabilité et l’éthique, les entreprises attirent davantage d’investisseurs conscients de l’importance des critères ESG. Cette attractivité accrue peut se traduire par un accès facilité au capital et à de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution à la réduction de certains risques financiers : En investissant dans des entreprise intégrant les critères ESG dans leur processus de décision, les investisseurs contribuent indirectement à une meilleure identification et anticipation les risques liés au changement climatique, aux problématiques sociales, et aux défis de gouvernance, ce qui contribue à une meilleure protection de leur capital sur le long terme.\nImpact positif sur la société : L’ISR permet aux investisseurs de contribuer activement à une économie plus durable et équitable. En choisissant d’investir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des modèles économiques respectueux de l’environnement et du bien-être social.\n\nEn somme, l’ISR offre une perspective d’investissement qui va au-delà des retours financiers immédiats pour embrasser des bénéfices à long terme, tant sur le plan économique que social et environnemental."
  },
  {
    "objectID": "3A/ISR.html#comment-investir",
    "href": "3A/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire à un fonds ISR ?\nSouscrire à un fonds1 ISR (Investissement Socialement Responsable) est une démarche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la société et l’environnement[@comment]. En consultant son conseiller financier ou son établissement bancaire, il est possible de placer son argent dans une variété de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d’Épargne en Actions (PEA) : (sous réserver de s’assurer que le fonds ISR choisi est bien éligible au PEA) Offre la possibilité de placer son épargne en actions de sociétés européennes.\nLes compte-titres ordinaires(CTO) : permet d’investir en bourse sur les marchés financiers français et/ou étrangers et dans tout type de valeurs mobilières (OPC2, actions, obligations, monétaire, warrants, trackers…).\nL’épargne salariale ou les plans d’épargne d’entreprise (PEE) : un produit d’épargne collectif qui permet aux salariés d’une entreprise de se constituer un portefeuille de valeurs mobilières qui peuvent proposer des fonds ISR.\nEnfin, certains produits d’épargne retraite individuelle, comme le Plan d’Epargne Retraite (PER).\n\nCes véhicules d’investissement permettent aux particuliers de contribuer à une économie plus durable tout en recherchant une performance financière. Il est recommandé de se rapprocher d’un conseiller pour déterminer le produit le mieux adapté à ses objectifs financiers et à ses valeurs éthiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) nécessite une approche combinant analyses personnelle, financière et extra-financière, cette dernière se concentrant sur les critères ESG (Environnementaux, Sociaux et de Gouvernance)[@comment2021].\nPour choisir efficacement une entreprise ISR, il est crucial de réaliser une analyse à triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR qui vous convient le mieux. Elles vous aideront à identifier le type de placement à privilégier et définir par exemple des fonds thématiques, d’exclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financière : Elle permet d’évaluer la performance économique de l’entreprise, sa santé financière, sa capacité à générer des profits et à maintenir une croissance durable. Cette analyse est indispensable pour s’assurer que l’entreprise est non seulement responsable, mais aussi viable et performante à long terme.\nAnalyse extra-financière (ESG) : Cette analyse complète l’évaluation financière en examinant comment l’entreprise aborde les défis et saisit les opportunités liées aux critères environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds intégrant les critères ESG comme les fonds labellisés ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du ministère de l’économie et des finances français qui garantit que le fonds d’investissement respecte des critères ESG stricts4 dans ses choix d’investissement. Il assure également que le fonds investit dans des entreprises qui adhèrent à ces principes, offrant ainsi une couche supplémentaire de confiance pour les investisseurs soucieux de l’impact de leurs placements.\nCe label est attribué aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,classées en 6 catégories, qui constituent les 6 piliers du référentiel [@critèresa]."
  },
  {
    "objectID": "3A/ISR.html#footnotes",
    "href": "3A/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appelé fonds de placement. Il s’agit d’une société d’ordre public ou privé qui investit du capital pour soutenir des projets souvent innovants.↩︎\norganismes de placement collectif↩︎\nLes fonds d’investissement d’exclusion normatifs font référence aux fonds faisant l’objet de plusieurs controverses.↩︎\ncf liste des fonds labellisés [@listede].↩︎"
  },
  {
    "objectID": "3A/gestion_actifs/mesures_rsq.html",
    "href": "3A/gestion_actifs/mesures_rsq.html",
    "title": "Gestion de portefeuille",
    "section": "",
    "text": "Pour gérer les risques, on procède en trois étapes : 1. Identification : Nous avons un portefeuille d’action, donc le risque auquel on fait face est le risque de marché action.\n\nMetrique de risque : Volatilité ex-ante, Value at Risk ex-ante, Tracking error ex-ante (i.e. par anticipation, on se base sur l’état du portefeuille à l’instant t et non aux instants passés - ex-post)\nEncadrement\n\nDans notre cas, on va constituer le portefeuille avec 10 actifs du CAC 40 de notre choix et leur allouer des poids aléatoires :\n\n# ! pip install yfinance\nfrom datetime import datetime, timedelta\nimport yfinance as yf \nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_data(start_date, end_date, index_ticker, tickers):\n    \"\"\"\n    Extraction de données de cours d'actions\n    Args:\n        start_date (str): Date de début au format 'YYYY-MM-DD'.\n        end_date (str): Date de fin au format 'YYYY-MM-DD'.\n\n    Returns:\n        dict: Contient les prix historiques des indices\n    \"\"\"\n    # Extraction des prix historiques des composants\n    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    # Extraction des prix historiques de l'indice CAC 40\n    index = yf.download(index_ticker, start=start_date, end=end_date, auto_adjust =True)['Close']\n\n    return {\n        \"portfolio_data\": data,\n        \"benchmark_data\": index,\n    }\n\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=2*365)\n\nselected_assets = {\n    \"SAN.PA\" : \"Sanofi\",\n    \"GLE.PA\" : \"Société générale\",\n    \"HO.PA\" : \"Thales\",\n    \"ENGI.PA\" : \"Engie\",\n    \"CAP.PA\" : \"Capgemini\",\n    \"CA.PA\" : \"Carrefour\",\n    \"ORA.PA\" : \"Orange\",\n    \"AC.PA\" : \"Accor\",\n    \"OR.PA\" : \"L'Oreal\",\n    \"ACA.PA\" : \"Crédit agricole\"\n}\n\nindex = \"^FCHI\"\n\nassets_ticker  = list(selected_assets.keys())\n\ndata = get_data(start_date,end_date, index, assets_ticker)\n\n[                       0%                       ][**********            20%                       ]  2 of 10 completed[**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nportfolio_data = data[\"portfolio_data\"]\nportfolio_data.head()\n\n\n\n\n\n\n\nTicker\nAC.PA\nACA.PA\nCA.PA\nCAP.PA\nENGI.PA\nGLE.PA\nHO.PA\nOR.PA\nORA.PA\nSAN.PA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-02-08\n29.441587\n9.554356\n15.175895\n177.961441\n10.856310\n23.891726\n113.740341\n359.421448\n8.415674\n81.924522\n\n\n2023-02-09\n29.232716\n9.961541\n15.267509\n179.503082\n10.846358\n23.828888\n113.931816\n362.677155\n8.366665\n82.563179\n\n\n2023-02-10\n27.998465\n9.850800\n14.928535\n176.419815\n10.950842\n23.564072\n116.804039\n359.660858\n8.478687\n82.100380\n\n\n2023-02-13\n28.425705\n9.833763\n14.745306\n177.672379\n10.887820\n23.743610\n119.676277\n373.114624\n8.490064\n81.128510\n\n\n2023-02-14\n28.606094\n9.850800\n15.043054\n178.202316\n10.979035\n23.797468\n122.021935\n371.295227\n8.686104\n81.554283\n\n\n\n\n\n\n\n\nbenchmark_data = data[\"benchmark_data\"]\nbenchmark_data.head()\n\n\n\n\n\n\n\nTicker\n^FCHI\n\n\nDate\n\n\n\n\n\n2023-02-08\n7119.830078\n\n\n2023-02-09\n7188.359863\n\n\n2023-02-10\n7129.729980\n\n\n2023-02-13\n7208.589844\n\n\n2023-02-14\n7213.810059\n\n\n\n\n\n\n\n\n# On attribue des poids équitables pour chaque action\nweights_by_asset = {ticker: 1 / len(assets_ticker) for ticker in assets_ticker}\n\nOn souhaite connaitre la valeur totale du actifs du portefeuille, i.e. l’asset under management(AUM) :\n\\[\nAUM(T_n) = \\sum_{i=1}^{10} \\omega_i \\times P_i(T_n)\n\\]\n\naum_series = portfolio_data.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\naum_series\n\nAUM = pd.DataFrame(aum_series, columns=[\"AUM\"])\n\n\nAUM.head()\n\n\n\n\n\n\n\n\nAUM\n\n\nDate\n\n\n\n\n\n2023-02-08\n83.038330\n\n\n2023-02-09\n83.617891\n\n\n2023-02-10\n83.075649\n\n\n2023-02-13\n84.771806\n\n\n2023-02-14\n85.003632\n\n\n\n\n\n\n\n\n# Evolution de la valeur totale du portefeuille\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.plot(AUM, label=\"AUM\")\nplt.title(\"Evolution de l'actif sous gestion\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Valeur\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# On s'interesse aux variations/rendements de l'AUM\n\nAUM[\"Variation\"] = AUM[\"AUM\"].pct_change()\nAUM[\"Variation\"].head()\n\nDate\n2023-02-08         NaN\n2023-02-09    0.006979\n2023-02-10   -0.006485\n2023-02-13    0.020417\n2023-02-14    0.002735\nName: Variation, dtype: float64\n\n\n\nEstimation de la volatilité\nPour estimer la volatilité du portefeuille, on peut calculer l’écart-type des variations de l’AUM. On fait le choix de calculer une volatilité ex-ante en se basant sur les variation historiques des prix des actifs avec une profondeur historique de 2 ans. Vu qu’on a une volatilité quotidienne, on va l’annualiser en multipliant par \\(\\sqrt{252}\\).\nEn général, sur le marché action, la volatilité quotidienne est environ de 1% et la volatilité annuelle est entre 10% et 20%.\n\n# Calcul de la volatilité du portefeuille\nvolatility_portfolio = np.std(AUM[\"Variation\"])\nannualized_volatility_portfolio = volatility_portfolio * np.sqrt(252)\nprint(f\"Volatilité de la performance quotidienne : {volatility_portfolio : .2%}\")\nprint(f\"Volatilité de la performance annuelle : {annualized_volatility_portfolio : .2%}\")\n\nVolatilité de la performance quotidienne :  0.87%\nVolatilité de la performance annuelle :  13.80%\n\n\n\n# Calcul de la volatilité de l'indice CAC 40\n\nbenchmark_data[\"Variation\"] = benchmark_data[\"^FCHI\"].pct_change()\nvolatility_benchmark = np.std(benchmark_data[\"Variation\"])\nannualized_volatility_benchmark = volatility_benchmark * np.sqrt(252)\n\nprint(f\"Volatilité de l'indice CAC 40 : {volatility_benchmark : .2%}\")\nprint(f\"Volatilité de l'indice CAC 40 annuelle : {annualized_volatility_benchmark : .2%}\")\n\nVolatilité de l'indice CAC 40 :  0.84%\nVolatilité de l'indice CAC 40 annuelle :  13.34%\n\n\nOn retrouve sur à peu près la même volatilité du portefeuille et celle du CAC 40. Il y a donc une certaine homogénéité.\n\n\nEstimation de la tracking error/erreur de suivi\nLa tracking error est une mesure de l’écart entre la performance d’un portefeuille et celle de son indice de référence. Elle est calculée comme la volatilité de la différence entre les rendements du portefeuille et de l’indice de référence :\n\\[\nTE = \\sqrt{Var(R_p - R_b)}\n\\]\nLa tracking error mesure l’incercitude du portefeuille par rapport à l’indice de référence, c’est une mesure relative. Plus la tracking error est élevée, plus le portefeuille est risqué. On ne souhaite sous ou sur-performer l’indice de référence. On souhaite suivre véritablement l’indice de référence.\nPour l’annualiser, on multiplie par \\(\\sqrt{252}\\) en supposant que les performances quotidiennes sont indépendantes et donc un utilise l’additivité des variances.\n\nperformance_relative = AUM[\"Variation\"] - benchmark_data[\"Variation\"]\n\nplt.figure(figsize=(12, 4))\nplt.plot(performance_relative, label=\"Performance\")\nplt.title(\"Performance du portefeuille par rapport à l'indice CAC 40\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Performance\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calcul de la tracking error\n\nTE = np.std(AUM[\"Variation\"] - benchmark_data[\"Variation\"]) \nprint(f\"Tracking error : {TE : .2%}\")\n\nTE_annualized = TE * np.sqrt(252)\nprint(f\"Tracking error annualisé : {TE_annualized : .2%}\")\n\nTracking error :  0.52%\nTracking error annualisé :  8.20%\n\n\n\n\nEstimation de la Value-at-Risk (VaR)\nLa VaR est une mesure de risque qui donne une estimation de la perte maximale que l’on peut subir avec un certain niveau de confiance \\(\\alpha\\) sur un horizon de temps donné. Par exemple, une VaR à 5% sur 1 jour de 1000 euros signifie que 95% du temps, on ne perdra pas plus de 1000 euros sur un jour.\n\\[P(\\text{Loss} &lt; \\text{VaR}) = \\alpha.\\]\nOn peut également raisonner en terme de gain, i.e. Profit and Loss (PnL).\n\\[P(\\text{PnL} &gt; - \\text{VaR}) = \\alpha.\\]\nLa VaR peut se calculer suivant trois approches : 1. Approche historique : On se base sur les rendements passés selon l’horizon fixé pour estimer la VaR, à l’aide d’un quantile empirique d’ordre \\(\\alpha\\). Autrement, on peut se baser sur les rendements journaliers et utiliser la méthode de rescaling, i.e. \\(VaR = \\sigma \\times \\Phi^{-1}(\\alpha)\\). 2. Approche paramétrique : On suppose que les rendements suivent une loi normale. 3. Approche Monte Carlo : On simule les rendements futurs.\n\n# VaR historique\nseuil = 99/100\n\nVaR_hist_portfolio = np.percentile(AUM[\"Variation\"].dropna(), 100*(1- seuil))\nprint(f\"VaR historique sur le portefeuille : {- VaR_hist_portfolio : .2%}\")\nprint(f\"VaR historique sur 20 jours sur le portefeuille : {-VaR_hist_portfolio*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\nVaR_hist_benchmark = np.percentile(benchmark_data[\"Variation\"].dropna(), 100*(1 - seuil))\nprint(f\"VaR historique sur l'indice CAC 40 : {-VaR_hist_benchmark : .2%}\")\nprint(f\"VaR historique sur 20 jours sur l'indice CAC 40 : {-VaR_hist_benchmark*np.sqrt(20) : .2%}\")\n\nVaR historique sur le portefeuille :  2.29%\nVaR historique sur 20 jours sur le portefeuille :  10.23%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nVaR historique sur l'indice CAC 40 :  2.13%\nVaR historique sur 20 jours sur l'indice CAC 40 :  9.51%\n\n\n\n# VaR paramétrique\n# PnL ~ N(mu, sigma) ==&gt; PnL = mu + sigma * Z, où Z ~ N(0,1)\n# P(PnL &gt; -VaR) = alpha &lt;=&gt; P(mu + sigma * Z &gt; -VaR) = alpha &lt;=&gt; P(Z &lt; (-VaR - mu) / sigma) = 1 - alpha\n# Donc, -VaR = mu + sigma * quantile(1 - alpha), où quantile(1 - alpha) est le quantile de la loi normale standard\n\nfrom scipy.stats import norm\n\nmu = np.mean(AUM[\"Variation\"].dropna())\nprint(f\"mu sur le portefeuille : {mu : .2}\")\nsigma = np.std(AUM[\"Variation\"].dropna())\nprint(f\"sigma sur le portefeuille : {sigma : .2}\")\n\nVaR_param_portfolio  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique sur le portefeuille : {VaR_param_portfolio : .2%}\")\nprint(f\"VaR paramétrique sur 20 jours sur le portefeuille : {VaR_param_portfolio * np.sqrt(20): .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(benchmark_data[\"Variation\"].dropna())\nprint(f\"mu sur le benchmark: {mu : .2}\")\nsigma = np.std(benchmark_data[\"Variation\"].dropna())\nprint(f\"sigma sur le benchmark : {sigma : .2}\")\n\nVaR_param_benchmark  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique sur le portefeuille : {VaR_param_benchmark : .2%}\")\nprint(f\"VaR paramétrique sur 20 jours sur le portefeuille : {VaR_param_benchmark * np.sqrt(20): .2%}\")\n\nmu sur le portefeuille :  0.00024\nsigma sur le portefeuille :  0.0087\nVaR paramétrique sur le portefeuille :  2.00%\nVaR paramétrique sur 20 jours sur le portefeuille :  8.94%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu sur le benchmark:  0.00027\nsigma sur le benchmark :  0.0084\nVaR paramétrique sur le portefeuille :  1.93%\nVaR paramétrique sur 20 jours sur le portefeuille :  8.63%\n\n\nLa VaR relative suit une philosophie proche du tracking error. Elle se calcule sur les écarts entre le portefeuille et le benchmark. Elle sert à mesurer de combien mon portefeuille sous-performe par rapport à l’indice de référence.\n\nperformance_relative\n\n\nVaR_hist_relative = np.percentile(performance_relative.dropna(), 100*(1- seuil))\nprint(f\"VaR historique relative : {- VaR_hist_relative : .2%}\")\nprint(f\"VaR historique relative sur 20 jours : {-VaR_hist_relative*np.sqrt(20) : .2%}\")\n\nprint(\"\\n\",\"=*=\"*10,\"\\n\")\n\nmu = np.mean(performance_relative.dropna())\nprint(f\"mu des performances relatives: {mu : .2}\")\nsigma = np.std(performance_relative.dropna())\nprint(f\"sigma des performances relatives : {sigma : .2}\")\n\nVaR_param_relative  = -(mu + sigma * norm.ppf(1 - seuil))\n\nprint(f\"VaR paramétrique relative : {VaR_param_relative : .2%}\")\nprint(f\"VaR paramétrique relative sur 20 jours : {VaR_param_relative * np.sqrt(20): .2%}\")\n\nVaR historique relative :  1.07%\nVaR historique relative sur 20 jours :  4.80%\n\n =*==*==*==*==*==*==*==*==*==*= \n\nmu des performances relatives: -2.5e-05\nsigma des performances relatives :  0.0052\nVaR paramétrique relative :  1.20%\nVaR paramétrique relative sur 20 jours :  5.39%\n\n\n\n\nStress test\nLes stress test permettent de tester les performances du portefeuille dans des conditions extrêmes. Ils sont de deux natures : 1. Stress test historique : On soumet le portefeuille à une période historique ou on estime avoir eu une condition extrême (Covid, Subprime crisis). On rejoue un scénario qui s’est déjà passé.\n\nStress test hypothétique : On joue un scénario qui ne s’est jamais réalisé. Exemple, si les actions chutent de 40%, notre portefeuille d’action chute de 40%.\n\nNote : bp = 0,01%\n\n# Recuperons les prix des actifs le 19/02/2020 et le 18/03/2020\n# On va valoriser notre portefeuille à ces dates et calculer les performances\n# A ces dates, le CAC 40 a connu de fortes pertes pendant la COVID-19\n# data_1902 = get_data\n\nstart_date = pd.to_datetime(\"19-02-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1902 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1902=data_1902[\"portfolio_data\"]\nbenchmark_data_1902=data_1902['benchmark_data']\n\nstart_date = pd.to_datetime(\"18-03-2020\", dayfirst=True)\nend_date = start_date + timedelta(days=1)\n\n\ndata_1803 = get_data(start_date,end_date, index, assets_ticker)\nportfolio_data_1803=data_1803[\"portfolio_data\"]\nbenchmark_data_1803=data_1803['benchmark_data']\n\n# Concaténer les données des deux dates pour le portefeuille et le benchmark\nportfolio_data_stress = pd.concat([portfolio_data_1902, portfolio_data_1803], ignore_index=False)\nbenchmark_data_stress = pd.concat([benchmark_data_1902, benchmark_data_1803], ignore_index=False)\n\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n[                       0%                       ][                       0%                       ][                       0%                       ][*******************   40%                       ]  4 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Stress test covid sur le portefeuille\naum_series_stress = portfolio_data_stress.apply(lambda row: sum(weights_by_asset[ticker] * row[ticker] for ticker in weights_by_asset), axis=1)\n\nAUM_stress = pd.DataFrame(aum_series_stress, columns=[\"AUM\"])\nAUM_stress[\"Variation\"] = AUM_stress[\"AUM\"].pct_change()\n\nAUM_stress\n\n\n\n\n\n\n\n\nAUM\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n63.108800\nNaN\n\n\n2020-03-18\n43.740069\n-0.30691\n\n\n\n\n\n\n\n\nbenchmark_data_stress[\"Variation\"]=benchmark_data_stress[\"^FCHI\"].pct_change()\nbenchmark_data_stress\n\n\n\n\n\n\n\nTicker\n^FCHI\nVariation\n\n\nDate\n\n\n\n\n\n\n2020-02-19\n6111.240234\nNaN\n\n\n2020-03-18\n3754.840088\n-0.385585\n\n\n\n\n\n\n\nNotre portefeuille permet de mieux resister au stress test covid que le CAC 40.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3A/reglementation_prudentielle.html",
    "href": "3A/reglementation_prudentielle.html",
    "title": "La réglementation prudentielle",
    "section": "",
    "text": "La réglementation prudentielle a été initiée par le développement des marchés financiers et des chocs alimentés par diverses crises financières. Face à ce constat, les autorités de contrôle bancaire ainsi que les autorités de marché ont pris des décisions pour réguler les marchés. C’est notamment le rôle qu’occupe le Comité de Bâle ou la Commission bancaire, qui ont pour objectif de renforcer la stabilité des marchés financiers. En France, l’ACPR (Autorité de Contrôle Prudentiel et de Résolution) et la Banque de France sont membres du Comité de Bâle et participent à ses travaux et décisions.\nIl existe par ailleurs plusieurs textes réglementaires ou documents relatifs au risque de marché. Parmi ces textes, on peut citer le document de référence pour calculer le ratio de solvabilité de la Commission bancaire, intitulé “Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL”, actualisé tous les ans par l’ACPR en France."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "href": "3A/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "title": "La réglementation prudentielle",
    "section": "Approche standard de mesure du risque de marché",
    "text": "Approche standard de mesure du risque de marché\nL’approche standard de mesure du risque de marché consiste à calculer les exigences en fonds propres pour chaque catégorie de risque, à savoir :\n\nle risque de taux (général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque lié aux titres de propriété(général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque de change calculé sur l’ensemble des opérations appartenant aussi bien au portefeuille de négociation ou non;\nle risque sur matières premières calculé sur l’ensemble des opérations du portefeuille de négociation ou non;\nles risques opérationnels calculés sur les options associées à chachune des catégories de risque citées ci-dessus.\n\nPar la suite, il s’agit de les additionner de manière arithmétique. Par exemple, pour les titres de propriété, l’exigence de fonds propres est la somme de l’exigence de fonds propres pour le risque général et l’exigence de fonds propres pour le risque spécifique.\nPour le calcul des exigences de fonds propres au titre des risques de marché, il faut tout d’abord déterminer les positions nettes. Les positions de titrisation logées dans le portefeuille de négociation sont traitées comme tout instrument de dette au titre du risque de taux.\nPour le risque spécifique, l’exigence en fonds propres sera la somme des positions nettes multipliées par un coefficient de pondération (2%, 4%, 8% ou 12%) choisi en fonction de la liquidité et la diversification de la position. Pour le risque général, l’exigence en fonds propres est la somme des positions nettes globales (pour chaque marché national) multipliées par 8%."
  },
  {
    "objectID": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "href": "3A/reglementation_prudentielle.html#approche-modèle-interne",
    "title": "La réglementation prudentielle",
    "section": "Approche modèle interne",
    "text": "Approche modèle interne\nL’approche modèle interne est une méthode de calcul des exigences en fonds propres pour le risque de marché qui permet aux établissements de calculer leurs propres exigences. L’exigence en fonds propres est généralement un calcul de la VaR. Cette approche est soumise à des conditions strictes et à une validation par l’ACPR.\nConcernant l’utilisation conjointe des modèles internes et de l’approche standard, la position de la commission prête une attention particulière à la permanence des méthodes ainsi qu’à leur évolution. L’objectif est de s’orienter vers un modèle global qui tient compte de l’ensemble des risques de marché.\n\nAinsi, un établissement commençant à utiliser des modèles pour une ou plusieurs catégories de facteurs de risque doit en principe étendre progressivement ce système à tous ses risques à la méthodologie standardisée (à moins que la Commission Bancaire ne lui ait retiré son agrément pour ses modèles).\n\nPour une banque, la construction d’un modèle interne doit permettre de fournir une mesure plus économique du risque de marché. Au titre de l’article 363 du CRR (Règlement sur les exigences de fonds propres), l’autorité compétente autorise les établissements assujettis à utiliser leurs modèles internes pour calculer les exigences de fonds propres pour risques de marché, après avoir vérifié qu’ils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3ème partie du CRR [@journal]. L’autorisation d’utiliser des modèles internes accordée par les autorités compétentes est requise pour chaque catégorie de risques (risque général et spécifique liés aux actions et titres de créance, risque de change et risque sur matières premières), et elle n’est accordée que si le modèle interne couvre une part importante des positions d’une certaine catégorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives à la mesure du risque (articles 367) mais aussi d’ordre général (article 365).\n\nExigences générales\nLe calcul de la valeur en risque visée à l’article 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprimé en centiles et unilatéral, de 99 %;\nune période de détention de dix jours;\nune période effective d’observation historique d’au moins un an, à moins qu’une période d’observation plus courte ne soit justifiée par une augmentation significative de la volatilité des prix;\ndes mises à jour au moins mensuelles des séries de données.\n\nL’établissement peut utiliser des mesures de la valeur en risque calculées sur la base de périodes de détention inférieures à dix jours, qu’il porte à dix jours selon une méthode appropriée qu’il revoit régulièrement.\nChaque établissement doit également calculer, au moins hebdomadairement, une “valeur en risque en situation de tensions” (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit être calculée conformément aux mêmes exigences que la VaR standard énoncées plus haut (intervalle de confiance de 99% etc.). Cependant, les données d’entrée du modèle de Stressed VaR doivent être calibrées par rapport à une période historique de tensions financières significatives d’au moins 12 mois, pertinente pour le portefeuille de l’établissement. Le choix de cette période de tensions historiques fait l’objet d’un examen au moins annuel par l’établissement, qui en communique les résultats aux autorités compétentes. L’objectif est de s’assurer que la Stressed VaR reflète de manière adéquate les risques auxquels l’établissement serait exposé en période de crise financière.\nPour résumer, les établissements doivent calculer la perte potentielle quotidiennement pour une période de détention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent également calculer une Stressed VaR au moins une fois par semaine, en utilisant des données historiques de périodes de tensions financières significatives.\nNotons \\(VaR(t)\\) la valeur en risque à la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions à la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) à la date t pour le risque de marché sont calculées comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\noù \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu’on vera plus tard.\nDans des périodes normales, l’exigence en fonds propres sera donc la somme d’un multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n’est que dans les périodes de crises financières que l’exigence en fonds propres correspond à la VaR ou à la sVaR du jour précédent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est égal à la somme du chiffre 3, au minimum, et d’un cumulateur compris entre 0 et 1 conformément au tableau 1. Ce cumulateur dépend du nombre de dépassements, sur les 250 derniers jours ouvrés, mis en évidence par les contrôles a posteriori de la mesure de la valeur en risque, au sens de l’article 365, paragraphe 1, effectués par l’établissement.\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\nNombre.de.dépassements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque spécifique, tout modèle interne utilisé pour calculer les exigences de fonds propres et tout modèle interne utilisé pour la négociation en corrélation satisfont aux exigences supplémentaires suivantes:\n\nle modèle interne explique la variation historique des prix à l’inté rieur du portefeuille;\nil reflète la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement défavorable;\nil est validé par des contrôles a posteriori(backtesting) visant à établir si le risque spécifique a été correctement pris en compte. Si l’établissement effectue ces contrôles a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de manière cohérente;\nil tient compte du risque de base lié à la signature et, en particulier, il est sensible aux différences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d’événement.\n\nLe risque spécifique vise à tenir compte du risque de contrepartie lié à l’emetteur de l’instrument.\nPour en savoir plus, reportez au règlement (UE) No 575/2013 du parlement européen du journal officiel de l’Union Européenne, appelé aussi règlement CRR. (voir aussi la notice 2020 relative aux « Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV»)."
  },
  {
    "objectID": "3A/var_application.html",
    "href": "3A/var_application.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous intéresser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d’estimer les pertes maximales potentielles d’un portefeuille d’actifs financiers sur un horizon de temps donné, à un certain niveau de confiance. Elle est largement utilisée par les institutions financières pour évaluer et gérer les risques de marché, de crédit et de liquidité (cf. Value at-Risk).\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "3A/var_application.html#var-analytique",
    "href": "3A/var_application.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est basée sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR à horizon 1 jour. La VaR à horizon 1 jour est définie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] où \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) à \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l’hypothèse nulle est que la population étudiée suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\nnp.float64(1.0859354571271105e-40)\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l’hypothèse de la distribution normale de nos rendements. Celà est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(8, 6))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR à horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR à horizon 1 jour est de 0.0325\n\n\nLa VaR à horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l’échantillon test les rendements avec la VaR à horizon 1 jour. Si le rendement est inférieur à l’opposé de la VaR gaussienne, alors la VaR est violée et celà correspond à une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d’exceptions que nous comptabilisons sur nos données test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 30 et en déduisons que le taux d’exception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.17\n\n\nPour tester la pertinence de la VaR calculée, il faudrait idéalement que le taux d’exception soit inférieur à 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\nnp.float64(0.20747847649723705)\n\n\nLa pvaleur de ce test est 3.70%, celà est inférieur à 5% donc nous rejetons l’hypothèse nulle selon laquelle le taux d’exception est égale à 0.01 au risque 5% de se tromper. Celà nous indique que la VaR gaussienne n’est pas performante. Ceci n’est pas surprenant étant donné que nous faisons une hypothèse sur la distribution des rendements qui n’est pas vérifiée."
  },
  {
    "objectID": "3A/var_application.html#var-historique",
    "href": "3A/var_application.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est basée sur les rendements historiques. Elle est définie comme l’opposé du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsidérons les mouvements de prix quotidiens pour l’indice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 scénarios ou cas qui serviront de guide pour les performances futures de l’indice, c’est-à-dire que les 6513 derniers jours seront représentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour à 99% correspond au 1er percentile de la distribution de probabilité des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique à horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique à horizon 1 jour est de 0.0396\n\n\nNous en déduisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que la VaR gaussienne. Le taux d’exception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 14 et en déduisons que le taux d’exception est 0.64%. Ce taux d’exception est statistiquement supérieur à 1% (car la pvaleur est d’environ 0.95). Ainsi, la VaR historique est performante pour la période considérée.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\nnp.float64(0.9749462153394883)"
  },
  {
    "objectID": "3A/var_application.html#var-monte-carlo",
    "href": "3A/var_application.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est basée sur la simulation de trajectoires de rendements. Nous allons simuler jusqu’à 10000 scénarios de rendements et calculer la VaR à horizon 1 jour en posant une hypothèse de normalité sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise à partir de 3000 scénarios. Nous utiliserons donc 3000 simulations de rendements. Nous en déduisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\nnp.float64(0.044480943050017555)\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que les deux autres VaRs. En effet, le taux d’exception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inférieur à 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\nnp.float64(0.9987611936960696)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A propos de moi",
    "section": "",
    "text": "Cheryl KOUADIO\n\n\nEtudiante en gestion des risques financiers\n\n\nEmail : cheryl.s.kouadio@gmail.com\n\n\nCV (Français/English) : cv_cheryl_kouadio_fr.pdf / cv_cheryl_kouadio_en.pdf\n\n\nGithub :  github.com/cheryl-kdio\n\n\nLinkedin : /in/cheryl-kouadio-251815206 \n\n\nMedium :  medium.com/@cheryl.s.kouadio\n\n\n\n\n\nA propos de moi\n\nJe suis Cheryl Kouadio, étudiante en gestion des risques bancaires à l’ENSAI. Je me spécialise dans la modélisation des risques financiers, en particulier les risques de marché et de crédit, ainsi que dans l’étude des normes financières, comptables et réglementaires encadrant le système bancaire (Bâle III, IFRS 9, etc.).\nCe site web, conçu et généré avec Quarto, s’adresse principalement aux étudiants de l’ENSAI et a pour vocation d’offrir un soutien à ceux qui, comme moi, ont été confrontés à des défis académiques au cours de leur formation, notamment en deuxième et troisième année. Il ne prétend en aucun cas se substituer à l’enseignement dispensé par nos professeurs, dont la rigueur et l’expertise sont essentielles. Son objectif est plutôt de compléter leur travail en partageant mes expériences personnelles et les projets que j’ai réalisés.\nAu-delà de son utilité pour les étudiants, ce site constitue également une ressource précieuse pour les professionnels souhaitant revisiter certains concepts clés en gestion des risques ou même en statistiques. Il propose des exemples concrets et des pistes de réflexion adaptées aux problématiques actuelles, afin d’aider chacun à mieux appréhender ses projets et à relever ses propres défis académiques et professionnels.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Compte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire.\n\n\n\n\n\nConstruction du bilan d’entreprise\nReglementation prudentielle\n\n\n\n\n\n\n\nDéfinition du risque financier\nValue-at-risk (VaR) :\n\nDéfinition de la VaR\nImplémentation de la VaR sur python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration avec le modèle de black-scholes\nCalibration du modèle à volatilité stochastique de Taylor - partie 1\nCalibration du modèle à volatilité stochastique de Taylor - partie 2\nCalibration avec le modèle de Heston\n\n\n\n\n\n\n\nGestion de risques d’un portefeuille d’actifs\nProfil d’écoulement de portefeuille\nPricing d’options vanilles\nPricing de taux, swap, d’obligations\nTracking error\nConstruction de portefeuille markowitz\n\n\n\n\n\n\n\nInvestissement socialement responsable\nCorrélation de spearman vs Corrélation de pearson\n\n\n\n\n\n\n\nProjet de séries temporelles\nProjet de scoring\nApp de prédiction du cours du CAC40\nProjet de théorie de valeurs extrêmes 1\nProjet de théorie de valeurs extrêmes 2"
  },
  {
    "objectID": "index_gdr.html#projets-et-applications-déployées",
    "href": "index_gdr.html#projets-et-applications-déployées",
    "title": "Cheryl KOUADIO",
    "section": "",
    "text": "Projet de séries temporelles\nProjet de scoring\nApp de prédiction du cours du CAC40\nProjet de théorie de valeurs extrêmes 1\nProjet de théorie de valeurs extrêmes 2"
  }
]